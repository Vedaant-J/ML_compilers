class GraphModule(torch.nn.Module):
    def forward(self, primals_1: "f32[2, 3, 100, 100]", primals_2: "f32[8, 3, 3, 3]", primals_3: "f32[8]"):
         # File: /opt/pytorch/lib/python3.12/site-packages/torch/nn/functional.py:5209 in pad, code: return torch._C._nn.pad(input, pad, mode, value)
        constant_pad_nd: "f32[2, 3, 102, 102]" = torch.ops.aten.constant_pad_nd.default(primals_1, [1, 1, 1, 1], 0.0);  primals_1 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:43 in im2col_manual, code: starting_rows = torch.arange(0, self.H+2*P-KH+1, step=S)
        iota: "i64[100]" = torch.ops.prims.iota.default(100, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:45 in im2col_manual, code: grid_rows, grid_cols = torch.meshgrid(starting_rows, starting_cols, indexing='ij')
        view: "i64[100, 1]" = torch.ops.aten.view.default(iota, [-1, 1])
        expand: "i64[100, 100]" = torch.ops.aten.expand.default(view, [100, 100]);  view = None
        view_1: "i64[1, 100]" = torch.ops.aten.view.default(iota, [1, -1]);  iota = None
        expand_1: "i64[100, 100]" = torch.ops.aten.expand.default(view_1, [100, 100]);  view_1 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:46 in im2col_manual, code: row_patches = grid_rows.flatten()
        clone: "i64[100, 100]" = torch.ops.aten.clone.default(expand, memory_format = torch.contiguous_format);  expand = None
        view_2: "i64[10000]" = torch.ops.aten.view.default(clone, [10000]);  clone = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:47 in im2col_manual, code: row_patches = row_patches.view(-1, 1) + torch.arange(KH).view(1, -1) # (out_h*out_w, KH)
        view_3: "i64[10000, 1]" = torch.ops.aten.view.default(view_2, [-1, 1]);  view_2 = None
        iota_2: "i64[3]" = torch.ops.prims.iota.default(3, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
        view_4: "i64[1, 3]" = torch.ops.aten.view.default(iota_2, [1, -1]);  iota_2 = None
        add: "i64[10000, 3]" = torch.ops.aten.add.Tensor(view_3, view_4);  view_3 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:48 in im2col_manual, code: col_patches = grid_cols.flatten()
        clone_1: "i64[100, 100]" = torch.ops.aten.clone.default(expand_1, memory_format = torch.contiguous_format);  expand_1 = None
        view_5: "i64[10000]" = torch.ops.aten.view.default(clone_1, [10000]);  clone_1 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:49 in im2col_manual, code: col_patches = col_patches.view(-1, 1) + torch.arange(KW).view(1, -1) # (out_h*out_w, KW)
        view_6: "i64[10000, 1]" = torch.ops.aten.view.default(view_5, [-1, 1]);  view_5 = None
        add_1: "i64[10000, 3]" = torch.ops.aten.add.Tensor(view_6, view_4);  view_6 = view_4 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:52 in im2col_manual, code: row_patches = row_patches.unsqueeze(-1)
        unsqueeze: "i64[10000, 3, 1]" = torch.ops.aten.unsqueeze.default(add, -1);  add = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:53 in im2col_manual, code: col_patches = col_patches.unsqueeze(1)
        unsqueeze_1: "i64[10000, 1, 3]" = torch.ops.aten.unsqueeze.default(add_1, 1);  add_1 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:55 in im2col_manual, code: patches = x_pad[:, :, row_patches, col_patches].permute(0, 2, 1, 3, 4)
        index: "f32[2, 3, 10000, 3, 3]" = torch.ops.aten.index.Tensor(constant_pad_nd, [None, None, unsqueeze, unsqueeze_1]);  constant_pad_nd = unsqueeze = unsqueeze_1 = None
        permute: "f32[2, 10000, 3, 3, 3]" = torch.ops.aten.permute.default(index, [0, 2, 1, 3, 4]);  index = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:56 in im2col_manual, code: patches = patches.reshape(N, int(out_h*out_w), C*KH*KW)
        clone_2: "f32[2, 10000, 3, 3, 3]" = torch.ops.aten.clone.default(permute, memory_format = torch.contiguous_format);  permute = None
        view_8: "f32[2, 10000, 27]" = torch.ops.aten.view.default(clone_2, [2, 10000, 27]);  clone_2 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:69 in conv2d_manual, code: weight_flat = self.weight.view(C_out, -1)
        view_9: "f32[8, 27]" = torch.ops.aten.view.default(primals_2, [8, -1]);  primals_2 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:70 in conv2d_manual, code: weight_flat = weight_flat.contiguous().t()  # (C*KH*KW, C_out)
        permute_1: "f32[27, 8]" = torch.ops.aten.permute.default(view_9, [1, 0]);  view_9 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:73 in conv2d_manual, code: out = torch.zeros((N, int(self.out_h*self.out_w), C_out), device=x.device)
        full: "f32[2, 10000, 8]" = torch.ops.aten.full.default([2, 10000, 8], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 0, 16)
        slice_5: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_4, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:78 in conv2d_manual, code: weight_tile = weight_flat[k:k+TILE_SIZE, j:j+TILE_SIZE] # (TILE_SIZE, TILE_SIZE)
        slice_6: "f32[16, 8]" = torch.ops.aten.slice.Tensor(permute_1, 0, 0, 16)
        slice_7: "f32[16, 8]" = torch.ops.aten.slice.Tensor(slice_6, 1, 0, 16);  slice_6 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        slice_9: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(full, 1, 0, 16)
        slice_10: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9, 2, 0, 16)
        clone_3: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_5, memory_format = torch.contiguous_format);  slice_5 = None
        view_10: "f32[32, 16]" = torch.ops.aten.view.default(clone_3, [32, 16]);  clone_3 = None
        mm: "f32[32, 8]" = torch.ops.aten.mm.default(view_10, slice_7)
        view_11: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm, [2, 16, 8]);  mm = None
        add_2: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10, view_11);  slice_10 = view_11 = None
        slice_scatter: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9, add_2, 2, 0, 16);  slice_9 = add_2 = None
        slice_scatter_1: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(full, slice_scatter, 1, 0, 16);  slice_scatter = None
        slice_14: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1, 1, 0, 16)
        slice_15: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14, 2, 0, 16)
        slice_scatter_3: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14, slice_15, 2, 0, 16);  slice_14 = slice_15 = None
        slice_scatter_4: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1, slice_scatter_3, 1, 0, 16);  slice_scatter_1 = slice_scatter_3 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_4, 2, 16, 32);  slice_4 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:78 in conv2d_manual, code: weight_tile = weight_flat[k:k+TILE_SIZE, j:j+TILE_SIZE] # (TILE_SIZE, TILE_SIZE)
        slice_36: "f32[11, 8]" = torch.ops.aten.slice.Tensor(permute_1, 0, 16, 32);  permute_1 = None
        slice_37: "f32[11, 8]" = torch.ops.aten.slice.Tensor(slice_36, 1, 0, 16);  slice_36 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_4: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_35, memory_format = torch.contiguous_format);  slice_35 = None
        view_12: "f32[32, 11]" = torch.ops.aten.view.default(clone_4, [32, 11]);  clone_4 = None
        mm_1: "f32[32, 8]" = torch.ops.aten.mm.default(view_12, slice_37)
        view_13: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1, [2, 16, 8]);  mm_1 = None
        slice_42: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4, 1, 0, 16)
        slice_43: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_42, 2, 0, 16)
        add_3: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_43, view_13);  slice_43 = view_13 = None
        slice_scatter_6: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_42, add_3, 2, 0, 16);  slice_42 = add_3 = None
        slice_scatter_7: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4, slice_scatter_6, 1, 0, 16);  slice_scatter_4 = slice_scatter_6 = None
        slice_47: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7, 1, 0, 16)
        slice_48: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_47, 2, 0, 16)
        slice_scatter_9: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_47, slice_48, 2, 0, 16);  slice_47 = slice_48 = None
        slice_scatter_10: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7, slice_scatter_9, 1, 0, 16);  slice_scatter_7 = slice_scatter_9 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_67: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 16, 32)
        slice_68: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_67, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_5: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_68, memory_format = torch.contiguous_format);  slice_68 = None
        view_14: "f32[32, 16]" = torch.ops.aten.view.default(clone_5, [32, 16]);  clone_5 = None
        mm_2: "f32[32, 8]" = torch.ops.aten.mm.default(view_14, slice_7)
        view_15: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_2, [2, 16, 8]);  mm_2 = None
        slice_75: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_10, 1, 16, 32)
        slice_76: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_75, 2, 0, 16)
        add_4: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_76, view_15);  slice_76 = view_15 = None
        slice_scatter_12: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_75, add_4, 2, 0, 16);  slice_75 = add_4 = None
        slice_scatter_13: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_10, slice_scatter_12, 1, 16, 32);  slice_scatter_10 = slice_scatter_12 = None
        slice_80: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_13, 1, 16, 32)
        slice_81: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_80, 2, 0, 16)
        slice_scatter_15: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_80, slice_81, 2, 0, 16);  slice_80 = slice_81 = None
        slice_scatter_16: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_13, slice_scatter_15, 1, 16, 32);  slice_scatter_13 = slice_scatter_15 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_101: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_67, 2, 16, 32);  slice_67 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_6: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_101, memory_format = torch.contiguous_format);  slice_101 = None
        view_16: "f32[32, 11]" = torch.ops.aten.view.default(clone_6, [32, 11]);  clone_6 = None
        mm_3: "f32[32, 8]" = torch.ops.aten.mm.default(view_16, slice_37)
        view_17: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_3, [2, 16, 8]);  mm_3 = None
        slice_108: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_16, 1, 16, 32)
        slice_109: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_108, 2, 0, 16)
        add_5: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_109, view_17);  slice_109 = view_17 = None
        slice_scatter_18: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_108, add_5, 2, 0, 16);  slice_108 = add_5 = None
        slice_scatter_19: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_16, slice_scatter_18, 1, 16, 32);  slice_scatter_16 = slice_scatter_18 = None
        slice_113: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_19, 1, 16, 32)
        slice_114: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_113, 2, 0, 16)
        slice_scatter_21: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_113, slice_114, 2, 0, 16);  slice_113 = slice_114 = None
        slice_scatter_22: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_19, slice_scatter_21, 1, 16, 32);  slice_scatter_19 = slice_scatter_21 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_133: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 32, 48)
        slice_134: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_133, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_7: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_134, memory_format = torch.contiguous_format);  slice_134 = None
        view_18: "f32[32, 16]" = torch.ops.aten.view.default(clone_7, [32, 16]);  clone_7 = None
        mm_4: "f32[32, 8]" = torch.ops.aten.mm.default(view_18, slice_7)
        view_19: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_4, [2, 16, 8]);  mm_4 = None
        slice_141: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_22, 1, 32, 48)
        slice_142: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_141, 2, 0, 16)
        add_6: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_142, view_19);  slice_142 = view_19 = None
        slice_scatter_24: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_141, add_6, 2, 0, 16);  slice_141 = add_6 = None
        slice_scatter_25: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_22, slice_scatter_24, 1, 32, 48);  slice_scatter_22 = slice_scatter_24 = None
        slice_146: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_25, 1, 32, 48)
        slice_147: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_146, 2, 0, 16)
        slice_scatter_27: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_146, slice_147, 2, 0, 16);  slice_146 = slice_147 = None
        slice_scatter_28: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_25, slice_scatter_27, 1, 32, 48);  slice_scatter_25 = slice_scatter_27 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_167: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_133, 2, 16, 32);  slice_133 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_8: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_167, memory_format = torch.contiguous_format);  slice_167 = None
        view_20: "f32[32, 11]" = torch.ops.aten.view.default(clone_8, [32, 11]);  clone_8 = None
        mm_5: "f32[32, 8]" = torch.ops.aten.mm.default(view_20, slice_37)
        view_21: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_5, [2, 16, 8]);  mm_5 = None
        slice_174: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_28, 1, 32, 48)
        slice_175: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_174, 2, 0, 16)
        add_7: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_175, view_21);  slice_175 = view_21 = None
        slice_scatter_30: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_174, add_7, 2, 0, 16);  slice_174 = add_7 = None
        slice_scatter_31: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_28, slice_scatter_30, 1, 32, 48);  slice_scatter_28 = slice_scatter_30 = None
        slice_179: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_31, 1, 32, 48)
        slice_180: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_179, 2, 0, 16)
        slice_scatter_33: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_179, slice_180, 2, 0, 16);  slice_179 = slice_180 = None
        slice_scatter_34: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_31, slice_scatter_33, 1, 32, 48);  slice_scatter_31 = slice_scatter_33 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_199: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 48, 64)
        slice_200: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_199, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_9: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_200, memory_format = torch.contiguous_format);  slice_200 = None
        view_22: "f32[32, 16]" = torch.ops.aten.view.default(clone_9, [32, 16]);  clone_9 = None
        mm_6: "f32[32, 8]" = torch.ops.aten.mm.default(view_22, slice_7)
        view_23: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_6, [2, 16, 8]);  mm_6 = None
        slice_207: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_34, 1, 48, 64)
        slice_208: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_207, 2, 0, 16)
        add_8: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_208, view_23);  slice_208 = view_23 = None
        slice_scatter_36: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_207, add_8, 2, 0, 16);  slice_207 = add_8 = None
        slice_scatter_37: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_34, slice_scatter_36, 1, 48, 64);  slice_scatter_34 = slice_scatter_36 = None
        slice_212: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_37, 1, 48, 64)
        slice_213: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_212, 2, 0, 16)
        slice_scatter_39: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_212, slice_213, 2, 0, 16);  slice_212 = slice_213 = None
        slice_scatter_40: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_37, slice_scatter_39, 1, 48, 64);  slice_scatter_37 = slice_scatter_39 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_233: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_199, 2, 16, 32);  slice_199 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_10: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_233, memory_format = torch.contiguous_format);  slice_233 = None
        view_24: "f32[32, 11]" = torch.ops.aten.view.default(clone_10, [32, 11]);  clone_10 = None
        mm_7: "f32[32, 8]" = torch.ops.aten.mm.default(view_24, slice_37)
        view_25: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_7, [2, 16, 8]);  mm_7 = None
        slice_240: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_40, 1, 48, 64)
        slice_241: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_240, 2, 0, 16)
        add_9: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_241, view_25);  slice_241 = view_25 = None
        slice_scatter_42: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_240, add_9, 2, 0, 16);  slice_240 = add_9 = None
        slice_scatter_43: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_40, slice_scatter_42, 1, 48, 64);  slice_scatter_40 = slice_scatter_42 = None
        slice_245: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_43, 1, 48, 64)
        slice_246: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_245, 2, 0, 16)
        slice_scatter_45: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_245, slice_246, 2, 0, 16);  slice_245 = slice_246 = None
        slice_scatter_46: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_43, slice_scatter_45, 1, 48, 64);  slice_scatter_43 = slice_scatter_45 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_265: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 64, 80)
        slice_266: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_265, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_11: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_266, memory_format = torch.contiguous_format);  slice_266 = None
        view_26: "f32[32, 16]" = torch.ops.aten.view.default(clone_11, [32, 16]);  clone_11 = None
        mm_8: "f32[32, 8]" = torch.ops.aten.mm.default(view_26, slice_7)
        view_27: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_8, [2, 16, 8]);  mm_8 = None
        slice_273: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_46, 1, 64, 80)
        slice_274: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_273, 2, 0, 16)
        add_10: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_274, view_27);  slice_274 = view_27 = None
        slice_scatter_48: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_273, add_10, 2, 0, 16);  slice_273 = add_10 = None
        slice_scatter_49: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_46, slice_scatter_48, 1, 64, 80);  slice_scatter_46 = slice_scatter_48 = None
        slice_278: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_49, 1, 64, 80)
        slice_279: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_278, 2, 0, 16)
        slice_scatter_51: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_278, slice_279, 2, 0, 16);  slice_278 = slice_279 = None
        slice_scatter_52: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_49, slice_scatter_51, 1, 64, 80);  slice_scatter_49 = slice_scatter_51 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_299: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_265, 2, 16, 32);  slice_265 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_12: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_299, memory_format = torch.contiguous_format);  slice_299 = None
        view_28: "f32[32, 11]" = torch.ops.aten.view.default(clone_12, [32, 11]);  clone_12 = None
        mm_9: "f32[32, 8]" = torch.ops.aten.mm.default(view_28, slice_37)
        view_29: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_9, [2, 16, 8]);  mm_9 = None
        slice_306: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_52, 1, 64, 80)
        slice_307: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_306, 2, 0, 16)
        add_11: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_307, view_29);  slice_307 = view_29 = None
        slice_scatter_54: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_306, add_11, 2, 0, 16);  slice_306 = add_11 = None
        slice_scatter_55: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_52, slice_scatter_54, 1, 64, 80);  slice_scatter_52 = slice_scatter_54 = None
        slice_311: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_55, 1, 64, 80)
        slice_312: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_311, 2, 0, 16)
        slice_scatter_57: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_311, slice_312, 2, 0, 16);  slice_311 = slice_312 = None
        slice_scatter_58: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_55, slice_scatter_57, 1, 64, 80);  slice_scatter_55 = slice_scatter_57 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_331: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 80, 96)
        slice_332: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_331, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_13: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_332, memory_format = torch.contiguous_format);  slice_332 = None
        view_30: "f32[32, 16]" = torch.ops.aten.view.default(clone_13, [32, 16]);  clone_13 = None
        mm_10: "f32[32, 8]" = torch.ops.aten.mm.default(view_30, slice_7)
        view_31: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_10, [2, 16, 8]);  mm_10 = None
        slice_339: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_58, 1, 80, 96)
        slice_340: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_339, 2, 0, 16)
        add_12: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_340, view_31);  slice_340 = view_31 = None
        slice_scatter_60: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_339, add_12, 2, 0, 16);  slice_339 = add_12 = None
        slice_scatter_61: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_58, slice_scatter_60, 1, 80, 96);  slice_scatter_58 = slice_scatter_60 = None
        slice_344: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_61, 1, 80, 96)
        slice_345: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_344, 2, 0, 16)
        slice_scatter_63: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_344, slice_345, 2, 0, 16);  slice_344 = slice_345 = None
        slice_scatter_64: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_61, slice_scatter_63, 1, 80, 96);  slice_scatter_61 = slice_scatter_63 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_365: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_331, 2, 16, 32);  slice_331 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_14: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_365, memory_format = torch.contiguous_format);  slice_365 = None
        view_32: "f32[32, 11]" = torch.ops.aten.view.default(clone_14, [32, 11]);  clone_14 = None
        mm_11: "f32[32, 8]" = torch.ops.aten.mm.default(view_32, slice_37)
        view_33: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_11, [2, 16, 8]);  mm_11 = None
        slice_372: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_64, 1, 80, 96)
        slice_373: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_372, 2, 0, 16)
        add_13: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_373, view_33);  slice_373 = view_33 = None
        slice_scatter_66: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_372, add_13, 2, 0, 16);  slice_372 = add_13 = None
        slice_scatter_67: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_64, slice_scatter_66, 1, 80, 96);  slice_scatter_64 = slice_scatter_66 = None
        slice_377: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_67, 1, 80, 96)
        slice_378: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_377, 2, 0, 16)
        slice_scatter_69: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_377, slice_378, 2, 0, 16);  slice_377 = slice_378 = None
        slice_scatter_70: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_67, slice_scatter_69, 1, 80, 96);  slice_scatter_67 = slice_scatter_69 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_397: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 96, 112)
        slice_398: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_397, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_15: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_398, memory_format = torch.contiguous_format);  slice_398 = None
        view_34: "f32[32, 16]" = torch.ops.aten.view.default(clone_15, [32, 16]);  clone_15 = None
        mm_12: "f32[32, 8]" = torch.ops.aten.mm.default(view_34, slice_7)
        view_35: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_12, [2, 16, 8]);  mm_12 = None
        slice_405: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_70, 1, 96, 112)
        slice_406: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_405, 2, 0, 16)
        add_14: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_406, view_35);  slice_406 = view_35 = None
        slice_scatter_72: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_405, add_14, 2, 0, 16);  slice_405 = add_14 = None
        slice_scatter_73: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_70, slice_scatter_72, 1, 96, 112);  slice_scatter_70 = slice_scatter_72 = None
        slice_410: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_73, 1, 96, 112)
        slice_411: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_410, 2, 0, 16)
        slice_scatter_75: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_410, slice_411, 2, 0, 16);  slice_410 = slice_411 = None
        slice_scatter_76: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_73, slice_scatter_75, 1, 96, 112);  slice_scatter_73 = slice_scatter_75 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_431: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_397, 2, 16, 32);  slice_397 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_16: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_431, memory_format = torch.contiguous_format);  slice_431 = None
        view_36: "f32[32, 11]" = torch.ops.aten.view.default(clone_16, [32, 11]);  clone_16 = None
        mm_13: "f32[32, 8]" = torch.ops.aten.mm.default(view_36, slice_37)
        view_37: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_13, [2, 16, 8]);  mm_13 = None
        slice_438: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_76, 1, 96, 112)
        slice_439: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_438, 2, 0, 16)
        add_15: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_439, view_37);  slice_439 = view_37 = None
        slice_scatter_78: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_438, add_15, 2, 0, 16);  slice_438 = add_15 = None
        slice_scatter_79: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_76, slice_scatter_78, 1, 96, 112);  slice_scatter_76 = slice_scatter_78 = None
        slice_443: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_79, 1, 96, 112)
        slice_444: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_443, 2, 0, 16)
        slice_scatter_81: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_443, slice_444, 2, 0, 16);  slice_443 = slice_444 = None
        slice_scatter_82: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_79, slice_scatter_81, 1, 96, 112);  slice_scatter_79 = slice_scatter_81 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_463: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 112, 128)
        slice_464: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_463, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_17: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_464, memory_format = torch.contiguous_format);  slice_464 = None
        view_38: "f32[32, 16]" = torch.ops.aten.view.default(clone_17, [32, 16]);  clone_17 = None
        mm_14: "f32[32, 8]" = torch.ops.aten.mm.default(view_38, slice_7)
        view_39: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_14, [2, 16, 8]);  mm_14 = None
        slice_471: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_82, 1, 112, 128)
        slice_472: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_471, 2, 0, 16)
        add_16: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_472, view_39);  slice_472 = view_39 = None
        slice_scatter_84: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_471, add_16, 2, 0, 16);  slice_471 = add_16 = None
        slice_scatter_85: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_82, slice_scatter_84, 1, 112, 128);  slice_scatter_82 = slice_scatter_84 = None
        slice_476: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_85, 1, 112, 128)
        slice_477: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_476, 2, 0, 16)
        slice_scatter_87: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_476, slice_477, 2, 0, 16);  slice_476 = slice_477 = None
        slice_scatter_88: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_85, slice_scatter_87, 1, 112, 128);  slice_scatter_85 = slice_scatter_87 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_497: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_463, 2, 16, 32);  slice_463 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_18: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_497, memory_format = torch.contiguous_format);  slice_497 = None
        view_40: "f32[32, 11]" = torch.ops.aten.view.default(clone_18, [32, 11]);  clone_18 = None
        mm_15: "f32[32, 8]" = torch.ops.aten.mm.default(view_40, slice_37)
        view_41: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_15, [2, 16, 8]);  mm_15 = None
        slice_504: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_88, 1, 112, 128)
        slice_505: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_504, 2, 0, 16)
        add_17: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_505, view_41);  slice_505 = view_41 = None
        slice_scatter_90: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_504, add_17, 2, 0, 16);  slice_504 = add_17 = None
        slice_scatter_91: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_88, slice_scatter_90, 1, 112, 128);  slice_scatter_88 = slice_scatter_90 = None
        slice_509: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_91, 1, 112, 128)
        slice_510: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_509, 2, 0, 16)
        slice_scatter_93: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_509, slice_510, 2, 0, 16);  slice_509 = slice_510 = None
        slice_scatter_94: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_91, slice_scatter_93, 1, 112, 128);  slice_scatter_91 = slice_scatter_93 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_529: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 128, 144)
        slice_530: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_529, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_19: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_530, memory_format = torch.contiguous_format);  slice_530 = None
        view_42: "f32[32, 16]" = torch.ops.aten.view.default(clone_19, [32, 16]);  clone_19 = None
        mm_16: "f32[32, 8]" = torch.ops.aten.mm.default(view_42, slice_7)
        view_43: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_16, [2, 16, 8]);  mm_16 = None
        slice_537: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_94, 1, 128, 144)
        slice_538: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_537, 2, 0, 16)
        add_18: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_538, view_43);  slice_538 = view_43 = None
        slice_scatter_96: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_537, add_18, 2, 0, 16);  slice_537 = add_18 = None
        slice_scatter_97: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_94, slice_scatter_96, 1, 128, 144);  slice_scatter_94 = slice_scatter_96 = None
        slice_542: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_97, 1, 128, 144)
        slice_543: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_542, 2, 0, 16)
        slice_scatter_99: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_542, slice_543, 2, 0, 16);  slice_542 = slice_543 = None
        slice_scatter_100: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_97, slice_scatter_99, 1, 128, 144);  slice_scatter_97 = slice_scatter_99 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_563: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_529, 2, 16, 32);  slice_529 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_20: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_563, memory_format = torch.contiguous_format);  slice_563 = None
        view_44: "f32[32, 11]" = torch.ops.aten.view.default(clone_20, [32, 11]);  clone_20 = None
        mm_17: "f32[32, 8]" = torch.ops.aten.mm.default(view_44, slice_37)
        view_45: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_17, [2, 16, 8]);  mm_17 = None
        slice_570: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_100, 1, 128, 144)
        slice_571: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_570, 2, 0, 16)
        add_19: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_571, view_45);  slice_571 = view_45 = None
        slice_scatter_102: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_570, add_19, 2, 0, 16);  slice_570 = add_19 = None
        slice_scatter_103: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_100, slice_scatter_102, 1, 128, 144);  slice_scatter_100 = slice_scatter_102 = None
        slice_575: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_103, 1, 128, 144)
        slice_576: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_575, 2, 0, 16)
        slice_scatter_105: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_575, slice_576, 2, 0, 16);  slice_575 = slice_576 = None
        slice_scatter_106: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_103, slice_scatter_105, 1, 128, 144);  slice_scatter_103 = slice_scatter_105 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_595: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 144, 160)
        slice_596: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_595, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_21: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_596, memory_format = torch.contiguous_format);  slice_596 = None
        view_46: "f32[32, 16]" = torch.ops.aten.view.default(clone_21, [32, 16]);  clone_21 = None
        mm_18: "f32[32, 8]" = torch.ops.aten.mm.default(view_46, slice_7)
        view_47: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_18, [2, 16, 8]);  mm_18 = None
        slice_603: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_106, 1, 144, 160)
        slice_604: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_603, 2, 0, 16)
        add_20: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_604, view_47);  slice_604 = view_47 = None
        slice_scatter_108: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_603, add_20, 2, 0, 16);  slice_603 = add_20 = None
        slice_scatter_109: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_106, slice_scatter_108, 1, 144, 160);  slice_scatter_106 = slice_scatter_108 = None
        slice_608: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_109, 1, 144, 160)
        slice_609: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_608, 2, 0, 16)
        slice_scatter_111: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_608, slice_609, 2, 0, 16);  slice_608 = slice_609 = None
        slice_scatter_112: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_109, slice_scatter_111, 1, 144, 160);  slice_scatter_109 = slice_scatter_111 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_629: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_595, 2, 16, 32);  slice_595 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_22: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_629, memory_format = torch.contiguous_format);  slice_629 = None
        view_48: "f32[32, 11]" = torch.ops.aten.view.default(clone_22, [32, 11]);  clone_22 = None
        mm_19: "f32[32, 8]" = torch.ops.aten.mm.default(view_48, slice_37)
        view_49: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_19, [2, 16, 8]);  mm_19 = None
        slice_636: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_112, 1, 144, 160)
        slice_637: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_636, 2, 0, 16)
        add_21: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_637, view_49);  slice_637 = view_49 = None
        slice_scatter_114: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_636, add_21, 2, 0, 16);  slice_636 = add_21 = None
        slice_scatter_115: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_112, slice_scatter_114, 1, 144, 160);  slice_scatter_112 = slice_scatter_114 = None
        slice_641: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_115, 1, 144, 160)
        slice_642: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_641, 2, 0, 16)
        slice_scatter_117: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_641, slice_642, 2, 0, 16);  slice_641 = slice_642 = None
        slice_scatter_118: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_115, slice_scatter_117, 1, 144, 160);  slice_scatter_115 = slice_scatter_117 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_661: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 160, 176)
        slice_662: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_661, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_23: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_662, memory_format = torch.contiguous_format);  slice_662 = None
        view_50: "f32[32, 16]" = torch.ops.aten.view.default(clone_23, [32, 16]);  clone_23 = None
        mm_20: "f32[32, 8]" = torch.ops.aten.mm.default(view_50, slice_7)
        view_51: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_20, [2, 16, 8]);  mm_20 = None
        slice_669: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_118, 1, 160, 176)
        slice_670: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_669, 2, 0, 16)
        add_22: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_670, view_51);  slice_670 = view_51 = None
        slice_scatter_120: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_669, add_22, 2, 0, 16);  slice_669 = add_22 = None
        slice_scatter_121: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_118, slice_scatter_120, 1, 160, 176);  slice_scatter_118 = slice_scatter_120 = None
        slice_674: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_121, 1, 160, 176)
        slice_675: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_674, 2, 0, 16)
        slice_scatter_123: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_674, slice_675, 2, 0, 16);  slice_674 = slice_675 = None
        slice_scatter_124: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_121, slice_scatter_123, 1, 160, 176);  slice_scatter_121 = slice_scatter_123 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_695: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_661, 2, 16, 32);  slice_661 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_24: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_695, memory_format = torch.contiguous_format);  slice_695 = None
        view_52: "f32[32, 11]" = torch.ops.aten.view.default(clone_24, [32, 11]);  clone_24 = None
        mm_21: "f32[32, 8]" = torch.ops.aten.mm.default(view_52, slice_37)
        view_53: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_21, [2, 16, 8]);  mm_21 = None
        slice_702: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_124, 1, 160, 176)
        slice_703: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_702, 2, 0, 16)
        add_23: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_703, view_53);  slice_703 = view_53 = None
        slice_scatter_126: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_702, add_23, 2, 0, 16);  slice_702 = add_23 = None
        slice_scatter_127: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_124, slice_scatter_126, 1, 160, 176);  slice_scatter_124 = slice_scatter_126 = None
        slice_707: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_127, 1, 160, 176)
        slice_708: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_707, 2, 0, 16)
        slice_scatter_129: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_707, slice_708, 2, 0, 16);  slice_707 = slice_708 = None
        slice_scatter_130: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_127, slice_scatter_129, 1, 160, 176);  slice_scatter_127 = slice_scatter_129 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_727: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 176, 192)
        slice_728: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_727, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_25: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_728, memory_format = torch.contiguous_format);  slice_728 = None
        view_54: "f32[32, 16]" = torch.ops.aten.view.default(clone_25, [32, 16]);  clone_25 = None
        mm_22: "f32[32, 8]" = torch.ops.aten.mm.default(view_54, slice_7)
        view_55: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_22, [2, 16, 8]);  mm_22 = None
        slice_735: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_130, 1, 176, 192)
        slice_736: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_735, 2, 0, 16)
        add_24: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_736, view_55);  slice_736 = view_55 = None
        slice_scatter_132: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_735, add_24, 2, 0, 16);  slice_735 = add_24 = None
        slice_scatter_133: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_130, slice_scatter_132, 1, 176, 192);  slice_scatter_130 = slice_scatter_132 = None
        slice_740: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_133, 1, 176, 192)
        slice_741: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_740, 2, 0, 16)
        slice_scatter_135: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_740, slice_741, 2, 0, 16);  slice_740 = slice_741 = None
        slice_scatter_136: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_133, slice_scatter_135, 1, 176, 192);  slice_scatter_133 = slice_scatter_135 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_761: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_727, 2, 16, 32);  slice_727 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_26: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_761, memory_format = torch.contiguous_format);  slice_761 = None
        view_56: "f32[32, 11]" = torch.ops.aten.view.default(clone_26, [32, 11]);  clone_26 = None
        mm_23: "f32[32, 8]" = torch.ops.aten.mm.default(view_56, slice_37)
        view_57: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_23, [2, 16, 8]);  mm_23 = None
        slice_768: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_136, 1, 176, 192)
        slice_769: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_768, 2, 0, 16)
        add_25: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_769, view_57);  slice_769 = view_57 = None
        slice_scatter_138: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_768, add_25, 2, 0, 16);  slice_768 = add_25 = None
        slice_scatter_139: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_136, slice_scatter_138, 1, 176, 192);  slice_scatter_136 = slice_scatter_138 = None
        slice_773: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_139, 1, 176, 192)
        slice_774: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_773, 2, 0, 16)
        slice_scatter_141: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_773, slice_774, 2, 0, 16);  slice_773 = slice_774 = None
        slice_scatter_142: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_139, slice_scatter_141, 1, 176, 192);  slice_scatter_139 = slice_scatter_141 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_793: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 192, 208)
        slice_794: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_793, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_27: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_794, memory_format = torch.contiguous_format);  slice_794 = None
        view_58: "f32[32, 16]" = torch.ops.aten.view.default(clone_27, [32, 16]);  clone_27 = None
        mm_24: "f32[32, 8]" = torch.ops.aten.mm.default(view_58, slice_7)
        view_59: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_24, [2, 16, 8]);  mm_24 = None
        slice_801: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_142, 1, 192, 208)
        slice_802: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_801, 2, 0, 16)
        add_26: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_802, view_59);  slice_802 = view_59 = None
        slice_scatter_144: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_801, add_26, 2, 0, 16);  slice_801 = add_26 = None
        slice_scatter_145: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_142, slice_scatter_144, 1, 192, 208);  slice_scatter_142 = slice_scatter_144 = None
        slice_806: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_145, 1, 192, 208)
        slice_807: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_806, 2, 0, 16)
        slice_scatter_147: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_806, slice_807, 2, 0, 16);  slice_806 = slice_807 = None
        slice_scatter_148: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_145, slice_scatter_147, 1, 192, 208);  slice_scatter_145 = slice_scatter_147 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_827: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_793, 2, 16, 32);  slice_793 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_28: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_827, memory_format = torch.contiguous_format);  slice_827 = None
        view_60: "f32[32, 11]" = torch.ops.aten.view.default(clone_28, [32, 11]);  clone_28 = None
        mm_25: "f32[32, 8]" = torch.ops.aten.mm.default(view_60, slice_37)
        view_61: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_25, [2, 16, 8]);  mm_25 = None
        slice_834: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_148, 1, 192, 208)
        slice_835: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_834, 2, 0, 16)
        add_27: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_835, view_61);  slice_835 = view_61 = None
        slice_scatter_150: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_834, add_27, 2, 0, 16);  slice_834 = add_27 = None
        slice_scatter_151: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_148, slice_scatter_150, 1, 192, 208);  slice_scatter_148 = slice_scatter_150 = None
        slice_839: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_151, 1, 192, 208)
        slice_840: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_839, 2, 0, 16)
        slice_scatter_153: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_839, slice_840, 2, 0, 16);  slice_839 = slice_840 = None
        slice_scatter_154: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_151, slice_scatter_153, 1, 192, 208);  slice_scatter_151 = slice_scatter_153 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_859: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 208, 224)
        slice_860: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_859, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_29: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_860, memory_format = torch.contiguous_format);  slice_860 = None
        view_62: "f32[32, 16]" = torch.ops.aten.view.default(clone_29, [32, 16]);  clone_29 = None
        mm_26: "f32[32, 8]" = torch.ops.aten.mm.default(view_62, slice_7)
        view_63: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_26, [2, 16, 8]);  mm_26 = None
        slice_867: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_154, 1, 208, 224)
        slice_868: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_867, 2, 0, 16)
        add_28: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_868, view_63);  slice_868 = view_63 = None
        slice_scatter_156: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_867, add_28, 2, 0, 16);  slice_867 = add_28 = None
        slice_scatter_157: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_154, slice_scatter_156, 1, 208, 224);  slice_scatter_154 = slice_scatter_156 = None
        slice_872: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_157, 1, 208, 224)
        slice_873: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_872, 2, 0, 16)
        slice_scatter_159: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_872, slice_873, 2, 0, 16);  slice_872 = slice_873 = None
        slice_scatter_160: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_157, slice_scatter_159, 1, 208, 224);  slice_scatter_157 = slice_scatter_159 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_893: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_859, 2, 16, 32);  slice_859 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_30: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_893, memory_format = torch.contiguous_format);  slice_893 = None
        view_64: "f32[32, 11]" = torch.ops.aten.view.default(clone_30, [32, 11]);  clone_30 = None
        mm_27: "f32[32, 8]" = torch.ops.aten.mm.default(view_64, slice_37)
        view_65: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_27, [2, 16, 8]);  mm_27 = None
        slice_900: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_160, 1, 208, 224)
        slice_901: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_900, 2, 0, 16)
        add_29: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_901, view_65);  slice_901 = view_65 = None
        slice_scatter_162: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_900, add_29, 2, 0, 16);  slice_900 = add_29 = None
        slice_scatter_163: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_160, slice_scatter_162, 1, 208, 224);  slice_scatter_160 = slice_scatter_162 = None
        slice_905: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_163, 1, 208, 224)
        slice_906: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_905, 2, 0, 16)
        slice_scatter_165: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_905, slice_906, 2, 0, 16);  slice_905 = slice_906 = None
        slice_scatter_166: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_163, slice_scatter_165, 1, 208, 224);  slice_scatter_163 = slice_scatter_165 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_925: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 224, 240)
        slice_926: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_925, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_31: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_926, memory_format = torch.contiguous_format);  slice_926 = None
        view_66: "f32[32, 16]" = torch.ops.aten.view.default(clone_31, [32, 16]);  clone_31 = None
        mm_28: "f32[32, 8]" = torch.ops.aten.mm.default(view_66, slice_7)
        view_67: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_28, [2, 16, 8]);  mm_28 = None
        slice_933: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_166, 1, 224, 240)
        slice_934: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_933, 2, 0, 16)
        add_30: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_934, view_67);  slice_934 = view_67 = None
        slice_scatter_168: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_933, add_30, 2, 0, 16);  slice_933 = add_30 = None
        slice_scatter_169: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_166, slice_scatter_168, 1, 224, 240);  slice_scatter_166 = slice_scatter_168 = None
        slice_938: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_169, 1, 224, 240)
        slice_939: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_938, 2, 0, 16)
        slice_scatter_171: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_938, slice_939, 2, 0, 16);  slice_938 = slice_939 = None
        slice_scatter_172: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_169, slice_scatter_171, 1, 224, 240);  slice_scatter_169 = slice_scatter_171 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_959: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_925, 2, 16, 32);  slice_925 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_32: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_959, memory_format = torch.contiguous_format);  slice_959 = None
        view_68: "f32[32, 11]" = torch.ops.aten.view.default(clone_32, [32, 11]);  clone_32 = None
        mm_29: "f32[32, 8]" = torch.ops.aten.mm.default(view_68, slice_37)
        view_69: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_29, [2, 16, 8]);  mm_29 = None
        slice_966: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_172, 1, 224, 240)
        slice_967: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_966, 2, 0, 16)
        add_31: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_967, view_69);  slice_967 = view_69 = None
        slice_scatter_174: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_966, add_31, 2, 0, 16);  slice_966 = add_31 = None
        slice_scatter_175: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_172, slice_scatter_174, 1, 224, 240);  slice_scatter_172 = slice_scatter_174 = None
        slice_971: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_175, 1, 224, 240)
        slice_972: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_971, 2, 0, 16)
        slice_scatter_177: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_971, slice_972, 2, 0, 16);  slice_971 = slice_972 = None
        slice_scatter_178: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_175, slice_scatter_177, 1, 224, 240);  slice_scatter_175 = slice_scatter_177 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_991: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 240, 256)
        slice_992: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_991, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_33: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_992, memory_format = torch.contiguous_format);  slice_992 = None
        view_70: "f32[32, 16]" = torch.ops.aten.view.default(clone_33, [32, 16]);  clone_33 = None
        mm_30: "f32[32, 8]" = torch.ops.aten.mm.default(view_70, slice_7)
        view_71: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_30, [2, 16, 8]);  mm_30 = None
        slice_999: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_178, 1, 240, 256)
        slice_1000: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_999, 2, 0, 16)
        add_32: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1000, view_71);  slice_1000 = view_71 = None
        slice_scatter_180: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_999, add_32, 2, 0, 16);  slice_999 = add_32 = None
        slice_scatter_181: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_178, slice_scatter_180, 1, 240, 256);  slice_scatter_178 = slice_scatter_180 = None
        slice_1004: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_181, 1, 240, 256)
        slice_1005: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1004, 2, 0, 16)
        slice_scatter_183: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1004, slice_1005, 2, 0, 16);  slice_1004 = slice_1005 = None
        slice_scatter_184: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_181, slice_scatter_183, 1, 240, 256);  slice_scatter_181 = slice_scatter_183 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1025: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_991, 2, 16, 32);  slice_991 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_34: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_1025, memory_format = torch.contiguous_format);  slice_1025 = None
        view_72: "f32[32, 11]" = torch.ops.aten.view.default(clone_34, [32, 11]);  clone_34 = None
        mm_31: "f32[32, 8]" = torch.ops.aten.mm.default(view_72, slice_37)
        view_73: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_31, [2, 16, 8]);  mm_31 = None
        slice_1032: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_184, 1, 240, 256)
        slice_1033: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1032, 2, 0, 16)
        add_33: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1033, view_73);  slice_1033 = view_73 = None
        slice_scatter_186: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1032, add_33, 2, 0, 16);  slice_1032 = add_33 = None
        slice_scatter_187: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_184, slice_scatter_186, 1, 240, 256);  slice_scatter_184 = slice_scatter_186 = None
        slice_1037: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_187, 1, 240, 256)
        slice_1038: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1037, 2, 0, 16)
        slice_scatter_189: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1037, slice_1038, 2, 0, 16);  slice_1037 = slice_1038 = None
        slice_scatter_190: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_187, slice_scatter_189, 1, 240, 256);  slice_scatter_187 = slice_scatter_189 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1057: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 256, 272)
        slice_1058: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_1057, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_35: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_1058, memory_format = torch.contiguous_format);  slice_1058 = None
        view_74: "f32[32, 16]" = torch.ops.aten.view.default(clone_35, [32, 16]);  clone_35 = None
        mm_32: "f32[32, 8]" = torch.ops.aten.mm.default(view_74, slice_7)
        view_75: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_32, [2, 16, 8]);  mm_32 = None
        slice_1065: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_190, 1, 256, 272)
        slice_1066: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1065, 2, 0, 16)
        add_34: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1066, view_75);  slice_1066 = view_75 = None
        slice_scatter_192: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1065, add_34, 2, 0, 16);  slice_1065 = add_34 = None
        slice_scatter_193: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_190, slice_scatter_192, 1, 256, 272);  slice_scatter_190 = slice_scatter_192 = None
        slice_1070: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_193, 1, 256, 272)
        slice_1071: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1070, 2, 0, 16)
        slice_scatter_195: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1070, slice_1071, 2, 0, 16);  slice_1070 = slice_1071 = None
        slice_scatter_196: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_193, slice_scatter_195, 1, 256, 272);  slice_scatter_193 = slice_scatter_195 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1091: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_1057, 2, 16, 32);  slice_1057 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_36: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_1091, memory_format = torch.contiguous_format);  slice_1091 = None
        view_76: "f32[32, 11]" = torch.ops.aten.view.default(clone_36, [32, 11]);  clone_36 = None
        mm_33: "f32[32, 8]" = torch.ops.aten.mm.default(view_76, slice_37)
        view_77: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_33, [2, 16, 8]);  mm_33 = None
        slice_1098: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_196, 1, 256, 272)
        slice_1099: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1098, 2, 0, 16)
        add_35: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1099, view_77);  slice_1099 = view_77 = None
        slice_scatter_198: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1098, add_35, 2, 0, 16);  slice_1098 = add_35 = None
        slice_scatter_199: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_196, slice_scatter_198, 1, 256, 272);  slice_scatter_196 = slice_scatter_198 = None
        slice_1103: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_199, 1, 256, 272)
        slice_1104: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1103, 2, 0, 16)
        slice_scatter_201: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1103, slice_1104, 2, 0, 16);  slice_1103 = slice_1104 = None
        slice_scatter_202: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_199, slice_scatter_201, 1, 256, 272);  slice_scatter_199 = slice_scatter_201 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1123: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 272, 288)
        slice_1124: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_1123, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_37: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_1124, memory_format = torch.contiguous_format);  slice_1124 = None
        view_78: "f32[32, 16]" = torch.ops.aten.view.default(clone_37, [32, 16]);  clone_37 = None
        mm_34: "f32[32, 8]" = torch.ops.aten.mm.default(view_78, slice_7)
        view_79: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_34, [2, 16, 8]);  mm_34 = None
        slice_1131: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_202, 1, 272, 288)
        slice_1132: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1131, 2, 0, 16)
        add_36: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1132, view_79);  slice_1132 = view_79 = None
        slice_scatter_204: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1131, add_36, 2, 0, 16);  slice_1131 = add_36 = None
        slice_scatter_205: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_202, slice_scatter_204, 1, 272, 288);  slice_scatter_202 = slice_scatter_204 = None
        slice_1136: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_205, 1, 272, 288)
        slice_1137: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1136, 2, 0, 16)
        slice_scatter_207: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1136, slice_1137, 2, 0, 16);  slice_1136 = slice_1137 = None
        slice_scatter_208: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_205, slice_scatter_207, 1, 272, 288);  slice_scatter_205 = slice_scatter_207 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1157: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_1123, 2, 16, 32);  slice_1123 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_38: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_1157, memory_format = torch.contiguous_format);  slice_1157 = None
        view_80: "f32[32, 11]" = torch.ops.aten.view.default(clone_38, [32, 11]);  clone_38 = None
        mm_35: "f32[32, 8]" = torch.ops.aten.mm.default(view_80, slice_37)
        view_81: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_35, [2, 16, 8]);  mm_35 = None
        slice_1164: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_208, 1, 272, 288)
        slice_1165: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1164, 2, 0, 16)
        add_37: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1165, view_81);  slice_1165 = view_81 = None
        slice_scatter_210: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1164, add_37, 2, 0, 16);  slice_1164 = add_37 = None
        slice_scatter_211: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_208, slice_scatter_210, 1, 272, 288);  slice_scatter_208 = slice_scatter_210 = None
        slice_1169: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_211, 1, 272, 288)
        slice_1170: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1169, 2, 0, 16)
        slice_scatter_213: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1169, slice_1170, 2, 0, 16);  slice_1169 = slice_1170 = None
        slice_scatter_214: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_211, slice_scatter_213, 1, 272, 288);  slice_scatter_211 = slice_scatter_213 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1189: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 288, 304)
        slice_1190: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_1189, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_39: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_1190, memory_format = torch.contiguous_format);  slice_1190 = None
        view_82: "f32[32, 16]" = torch.ops.aten.view.default(clone_39, [32, 16]);  clone_39 = None
        mm_36: "f32[32, 8]" = torch.ops.aten.mm.default(view_82, slice_7)
        view_83: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_36, [2, 16, 8]);  mm_36 = None
        slice_1197: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_214, 1, 288, 304)
        slice_1198: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1197, 2, 0, 16)
        add_38: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1198, view_83);  slice_1198 = view_83 = None
        slice_scatter_216: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1197, add_38, 2, 0, 16);  slice_1197 = add_38 = None
        slice_scatter_217: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_214, slice_scatter_216, 1, 288, 304);  slice_scatter_214 = slice_scatter_216 = None
        slice_1202: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_217, 1, 288, 304)
        slice_1203: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1202, 2, 0, 16)
        slice_scatter_219: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1202, slice_1203, 2, 0, 16);  slice_1202 = slice_1203 = None
        slice_scatter_220: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_217, slice_scatter_219, 1, 288, 304);  slice_scatter_217 = slice_scatter_219 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1223: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_1189, 2, 16, 32);  slice_1189 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_40: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_1223, memory_format = torch.contiguous_format);  slice_1223 = None
        view_84: "f32[32, 11]" = torch.ops.aten.view.default(clone_40, [32, 11]);  clone_40 = None
        mm_37: "f32[32, 8]" = torch.ops.aten.mm.default(view_84, slice_37)
        view_85: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_37, [2, 16, 8]);  mm_37 = None
        slice_1230: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_220, 1, 288, 304)
        slice_1231: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1230, 2, 0, 16)
        add_39: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1231, view_85);  slice_1231 = view_85 = None
        slice_scatter_222: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1230, add_39, 2, 0, 16);  slice_1230 = add_39 = None
        slice_scatter_223: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_220, slice_scatter_222, 1, 288, 304);  slice_scatter_220 = slice_scatter_222 = None
        slice_1235: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_223, 1, 288, 304)
        slice_1236: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1235, 2, 0, 16)
        slice_scatter_225: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1235, slice_1236, 2, 0, 16);  slice_1235 = slice_1236 = None
        slice_scatter_226: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_223, slice_scatter_225, 1, 288, 304);  slice_scatter_223 = slice_scatter_225 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1255: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 304, 320)
        slice_1256: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_1255, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_41: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_1256, memory_format = torch.contiguous_format);  slice_1256 = None
        view_86: "f32[32, 16]" = torch.ops.aten.view.default(clone_41, [32, 16]);  clone_41 = None
        mm_38: "f32[32, 8]" = torch.ops.aten.mm.default(view_86, slice_7)
        view_87: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_38, [2, 16, 8]);  mm_38 = None
        slice_1263: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_226, 1, 304, 320)
        slice_1264: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1263, 2, 0, 16)
        add_40: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1264, view_87);  slice_1264 = view_87 = None
        slice_scatter_228: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1263, add_40, 2, 0, 16);  slice_1263 = add_40 = None
        slice_scatter_229: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_226, slice_scatter_228, 1, 304, 320);  slice_scatter_226 = slice_scatter_228 = None
        slice_1268: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_229, 1, 304, 320)
        slice_1269: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1268, 2, 0, 16)
        slice_scatter_231: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1268, slice_1269, 2, 0, 16);  slice_1268 = slice_1269 = None
        slice_scatter_232: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_229, slice_scatter_231, 1, 304, 320);  slice_scatter_229 = slice_scatter_231 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1289: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_1255, 2, 16, 32);  slice_1255 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_42: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_1289, memory_format = torch.contiguous_format);  slice_1289 = None
        view_88: "f32[32, 11]" = torch.ops.aten.view.default(clone_42, [32, 11]);  clone_42 = None
        mm_39: "f32[32, 8]" = torch.ops.aten.mm.default(view_88, slice_37)
        view_89: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_39, [2, 16, 8]);  mm_39 = None
        slice_1296: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_232, 1, 304, 320)
        slice_1297: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1296, 2, 0, 16)
        add_41: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1297, view_89);  slice_1297 = view_89 = None
        slice_scatter_234: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1296, add_41, 2, 0, 16);  slice_1296 = add_41 = None
        slice_scatter_235: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_232, slice_scatter_234, 1, 304, 320);  slice_scatter_232 = slice_scatter_234 = None
        slice_1301: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_235, 1, 304, 320)
        slice_1302: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1301, 2, 0, 16)
        slice_scatter_237: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1301, slice_1302, 2, 0, 16);  slice_1301 = slice_1302 = None
        slice_scatter_238: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_235, slice_scatter_237, 1, 304, 320);  slice_scatter_235 = slice_scatter_237 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1321: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 320, 336)
        slice_1322: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_1321, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_43: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_1322, memory_format = torch.contiguous_format);  slice_1322 = None
        view_90: "f32[32, 16]" = torch.ops.aten.view.default(clone_43, [32, 16]);  clone_43 = None
        mm_40: "f32[32, 8]" = torch.ops.aten.mm.default(view_90, slice_7)
        view_91: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_40, [2, 16, 8]);  mm_40 = None
        slice_1329: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_238, 1, 320, 336)
        slice_1330: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1329, 2, 0, 16)
        add_42: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1330, view_91);  slice_1330 = view_91 = None
        slice_scatter_240: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1329, add_42, 2, 0, 16);  slice_1329 = add_42 = None
        slice_scatter_241: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_238, slice_scatter_240, 1, 320, 336);  slice_scatter_238 = slice_scatter_240 = None
        slice_1334: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_241, 1, 320, 336)
        slice_1335: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1334, 2, 0, 16)
        slice_scatter_243: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1334, slice_1335, 2, 0, 16);  slice_1334 = slice_1335 = None
        slice_scatter_244: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_241, slice_scatter_243, 1, 320, 336);  slice_scatter_241 = slice_scatter_243 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1355: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_1321, 2, 16, 32);  slice_1321 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_44: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_1355, memory_format = torch.contiguous_format);  slice_1355 = None
        view_92: "f32[32, 11]" = torch.ops.aten.view.default(clone_44, [32, 11]);  clone_44 = None
        mm_41: "f32[32, 8]" = torch.ops.aten.mm.default(view_92, slice_37)
        view_93: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_41, [2, 16, 8]);  mm_41 = None
        slice_1362: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_244, 1, 320, 336)
        slice_1363: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1362, 2, 0, 16)
        add_43: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1363, view_93);  slice_1363 = view_93 = None
        slice_scatter_246: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1362, add_43, 2, 0, 16);  slice_1362 = add_43 = None
        slice_scatter_247: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_244, slice_scatter_246, 1, 320, 336);  slice_scatter_244 = slice_scatter_246 = None
        slice_1367: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_247, 1, 320, 336)
        slice_1368: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1367, 2, 0, 16)
        slice_scatter_249: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1367, slice_1368, 2, 0, 16);  slice_1367 = slice_1368 = None
        slice_scatter_250: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_247, slice_scatter_249, 1, 320, 336);  slice_scatter_247 = slice_scatter_249 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1387: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 336, 352)
        slice_1388: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_1387, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_45: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_1388, memory_format = torch.contiguous_format);  slice_1388 = None
        view_94: "f32[32, 16]" = torch.ops.aten.view.default(clone_45, [32, 16]);  clone_45 = None
        mm_42: "f32[32, 8]" = torch.ops.aten.mm.default(view_94, slice_7)
        view_95: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_42, [2, 16, 8]);  mm_42 = None
        slice_1395: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_250, 1, 336, 352)
        slice_1396: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1395, 2, 0, 16)
        add_44: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1396, view_95);  slice_1396 = view_95 = None
        slice_scatter_252: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1395, add_44, 2, 0, 16);  slice_1395 = add_44 = None
        slice_scatter_253: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_250, slice_scatter_252, 1, 336, 352);  slice_scatter_250 = slice_scatter_252 = None
        slice_1400: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_253, 1, 336, 352)
        slice_1401: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1400, 2, 0, 16)
        slice_scatter_255: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1400, slice_1401, 2, 0, 16);  slice_1400 = slice_1401 = None
        slice_scatter_256: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_253, slice_scatter_255, 1, 336, 352);  slice_scatter_253 = slice_scatter_255 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1421: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_1387, 2, 16, 32);  slice_1387 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_46: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_1421, memory_format = torch.contiguous_format);  slice_1421 = None
        view_96: "f32[32, 11]" = torch.ops.aten.view.default(clone_46, [32, 11]);  clone_46 = None
        mm_43: "f32[32, 8]" = torch.ops.aten.mm.default(view_96, slice_37)
        view_97: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_43, [2, 16, 8]);  mm_43 = None
        slice_1428: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_256, 1, 336, 352)
        slice_1429: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1428, 2, 0, 16)
        add_45: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1429, view_97);  slice_1429 = view_97 = None
        slice_scatter_258: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1428, add_45, 2, 0, 16);  slice_1428 = add_45 = None
        slice_scatter_259: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_256, slice_scatter_258, 1, 336, 352);  slice_scatter_256 = slice_scatter_258 = None
        slice_1433: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_259, 1, 336, 352)
        slice_1434: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1433, 2, 0, 16)
        slice_scatter_261: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1433, slice_1434, 2, 0, 16);  slice_1433 = slice_1434 = None
        slice_scatter_262: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_259, slice_scatter_261, 1, 336, 352);  slice_scatter_259 = slice_scatter_261 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1453: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 352, 368)
        slice_1454: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_1453, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_47: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_1454, memory_format = torch.contiguous_format);  slice_1454 = None
        view_98: "f32[32, 16]" = torch.ops.aten.view.default(clone_47, [32, 16]);  clone_47 = None
        mm_44: "f32[32, 8]" = torch.ops.aten.mm.default(view_98, slice_7)
        view_99: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_44, [2, 16, 8]);  mm_44 = None
        slice_1461: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_262, 1, 352, 368)
        slice_1462: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1461, 2, 0, 16)
        add_46: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1462, view_99);  slice_1462 = view_99 = None
        slice_scatter_264: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1461, add_46, 2, 0, 16);  slice_1461 = add_46 = None
        slice_scatter_265: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_262, slice_scatter_264, 1, 352, 368);  slice_scatter_262 = slice_scatter_264 = None
        slice_1466: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_265, 1, 352, 368)
        slice_1467: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1466, 2, 0, 16)
        slice_scatter_267: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1466, slice_1467, 2, 0, 16);  slice_1466 = slice_1467 = None
        slice_scatter_268: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_265, slice_scatter_267, 1, 352, 368);  slice_scatter_265 = slice_scatter_267 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1487: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_1453, 2, 16, 32);  slice_1453 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_48: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_1487, memory_format = torch.contiguous_format);  slice_1487 = None
        view_100: "f32[32, 11]" = torch.ops.aten.view.default(clone_48, [32, 11]);  clone_48 = None
        mm_45: "f32[32, 8]" = torch.ops.aten.mm.default(view_100, slice_37)
        view_101: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_45, [2, 16, 8]);  mm_45 = None
        slice_1494: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_268, 1, 352, 368)
        slice_1495: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1494, 2, 0, 16)
        add_47: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1495, view_101);  slice_1495 = view_101 = None
        slice_scatter_270: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1494, add_47, 2, 0, 16);  slice_1494 = add_47 = None
        slice_scatter_271: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_268, slice_scatter_270, 1, 352, 368);  slice_scatter_268 = slice_scatter_270 = None
        slice_1499: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_271, 1, 352, 368)
        slice_1500: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1499, 2, 0, 16)
        slice_scatter_273: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1499, slice_1500, 2, 0, 16);  slice_1499 = slice_1500 = None
        slice_scatter_274: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_271, slice_scatter_273, 1, 352, 368);  slice_scatter_271 = slice_scatter_273 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1519: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 368, 384)
        slice_1520: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_1519, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_49: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_1520, memory_format = torch.contiguous_format);  slice_1520 = None
        view_102: "f32[32, 16]" = torch.ops.aten.view.default(clone_49, [32, 16]);  clone_49 = None
        mm_46: "f32[32, 8]" = torch.ops.aten.mm.default(view_102, slice_7)
        view_103: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_46, [2, 16, 8]);  mm_46 = None
        slice_1527: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_274, 1, 368, 384)
        slice_1528: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1527, 2, 0, 16)
        add_48: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1528, view_103);  slice_1528 = view_103 = None
        slice_scatter_276: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1527, add_48, 2, 0, 16);  slice_1527 = add_48 = None
        slice_scatter_277: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_274, slice_scatter_276, 1, 368, 384);  slice_scatter_274 = slice_scatter_276 = None
        slice_1532: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_277, 1, 368, 384)
        slice_1533: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1532, 2, 0, 16)
        slice_scatter_279: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1532, slice_1533, 2, 0, 16);  slice_1532 = slice_1533 = None
        slice_scatter_280: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_277, slice_scatter_279, 1, 368, 384);  slice_scatter_277 = slice_scatter_279 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1553: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_1519, 2, 16, 32);  slice_1519 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_50: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_1553, memory_format = torch.contiguous_format);  slice_1553 = None
        view_104: "f32[32, 11]" = torch.ops.aten.view.default(clone_50, [32, 11]);  clone_50 = None
        mm_47: "f32[32, 8]" = torch.ops.aten.mm.default(view_104, slice_37)
        view_105: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_47, [2, 16, 8]);  mm_47 = None
        slice_1560: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_280, 1, 368, 384)
        slice_1561: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1560, 2, 0, 16)
        add_49: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1561, view_105);  slice_1561 = view_105 = None
        slice_scatter_282: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1560, add_49, 2, 0, 16);  slice_1560 = add_49 = None
        slice_scatter_283: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_280, slice_scatter_282, 1, 368, 384);  slice_scatter_280 = slice_scatter_282 = None
        slice_1565: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_283, 1, 368, 384)
        slice_1566: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1565, 2, 0, 16)
        slice_scatter_285: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1565, slice_1566, 2, 0, 16);  slice_1565 = slice_1566 = None
        slice_scatter_286: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_283, slice_scatter_285, 1, 368, 384);  slice_scatter_283 = slice_scatter_285 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1585: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 384, 400)
        slice_1586: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_1585, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_51: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_1586, memory_format = torch.contiguous_format);  slice_1586 = None
        view_106: "f32[32, 16]" = torch.ops.aten.view.default(clone_51, [32, 16]);  clone_51 = None
        mm_48: "f32[32, 8]" = torch.ops.aten.mm.default(view_106, slice_7)
        view_107: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_48, [2, 16, 8]);  mm_48 = None
        slice_1593: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_286, 1, 384, 400)
        slice_1594: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1593, 2, 0, 16)
        add_50: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1594, view_107);  slice_1594 = view_107 = None
        slice_scatter_288: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1593, add_50, 2, 0, 16);  slice_1593 = add_50 = None
        slice_scatter_289: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_286, slice_scatter_288, 1, 384, 400);  slice_scatter_286 = slice_scatter_288 = None
        slice_1598: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_289, 1, 384, 400)
        slice_1599: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1598, 2, 0, 16)
        slice_scatter_291: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1598, slice_1599, 2, 0, 16);  slice_1598 = slice_1599 = None
        slice_scatter_292: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_289, slice_scatter_291, 1, 384, 400);  slice_scatter_289 = slice_scatter_291 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1619: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_1585, 2, 16, 32);  slice_1585 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_52: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_1619, memory_format = torch.contiguous_format);  slice_1619 = None
        view_108: "f32[32, 11]" = torch.ops.aten.view.default(clone_52, [32, 11]);  clone_52 = None
        mm_49: "f32[32, 8]" = torch.ops.aten.mm.default(view_108, slice_37)
        view_109: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_49, [2, 16, 8]);  mm_49 = None
        slice_1626: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_292, 1, 384, 400)
        slice_1627: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1626, 2, 0, 16)
        add_51: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1627, view_109);  slice_1627 = view_109 = None
        slice_scatter_294: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1626, add_51, 2, 0, 16);  slice_1626 = add_51 = None
        slice_scatter_295: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_292, slice_scatter_294, 1, 384, 400);  slice_scatter_292 = slice_scatter_294 = None
        slice_1631: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_295, 1, 384, 400)
        slice_1632: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1631, 2, 0, 16)
        slice_scatter_297: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1631, slice_1632, 2, 0, 16);  slice_1631 = slice_1632 = None
        slice_scatter_298: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_295, slice_scatter_297, 1, 384, 400);  slice_scatter_295 = slice_scatter_297 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1651: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 400, 416)
        slice_1652: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_1651, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_53: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_1652, memory_format = torch.contiguous_format);  slice_1652 = None
        view_110: "f32[32, 16]" = torch.ops.aten.view.default(clone_53, [32, 16]);  clone_53 = None
        mm_50: "f32[32, 8]" = torch.ops.aten.mm.default(view_110, slice_7)
        view_111: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_50, [2, 16, 8]);  mm_50 = None
        slice_1659: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_298, 1, 400, 416)
        slice_1660: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1659, 2, 0, 16)
        add_52: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1660, view_111);  slice_1660 = view_111 = None
        slice_scatter_300: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1659, add_52, 2, 0, 16);  slice_1659 = add_52 = None
        slice_scatter_301: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_298, slice_scatter_300, 1, 400, 416);  slice_scatter_298 = slice_scatter_300 = None
        slice_1664: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_301, 1, 400, 416)
        slice_1665: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1664, 2, 0, 16)
        slice_scatter_303: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1664, slice_1665, 2, 0, 16);  slice_1664 = slice_1665 = None
        slice_scatter_304: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_301, slice_scatter_303, 1, 400, 416);  slice_scatter_301 = slice_scatter_303 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1685: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_1651, 2, 16, 32);  slice_1651 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_54: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_1685, memory_format = torch.contiguous_format);  slice_1685 = None
        view_112: "f32[32, 11]" = torch.ops.aten.view.default(clone_54, [32, 11]);  clone_54 = None
        mm_51: "f32[32, 8]" = torch.ops.aten.mm.default(view_112, slice_37)
        view_113: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_51, [2, 16, 8]);  mm_51 = None
        slice_1692: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_304, 1, 400, 416)
        slice_1693: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1692, 2, 0, 16)
        add_53: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1693, view_113);  slice_1693 = view_113 = None
        slice_scatter_306: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1692, add_53, 2, 0, 16);  slice_1692 = add_53 = None
        slice_scatter_307: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_304, slice_scatter_306, 1, 400, 416);  slice_scatter_304 = slice_scatter_306 = None
        slice_1697: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_307, 1, 400, 416)
        slice_1698: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1697, 2, 0, 16)
        slice_scatter_309: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1697, slice_1698, 2, 0, 16);  slice_1697 = slice_1698 = None
        slice_scatter_310: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_307, slice_scatter_309, 1, 400, 416);  slice_scatter_307 = slice_scatter_309 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1717: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 416, 432)
        slice_1718: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_1717, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_55: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_1718, memory_format = torch.contiguous_format);  slice_1718 = None
        view_114: "f32[32, 16]" = torch.ops.aten.view.default(clone_55, [32, 16]);  clone_55 = None
        mm_52: "f32[32, 8]" = torch.ops.aten.mm.default(view_114, slice_7)
        view_115: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_52, [2, 16, 8]);  mm_52 = None
        slice_1725: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_310, 1, 416, 432)
        slice_1726: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1725, 2, 0, 16)
        add_54: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1726, view_115);  slice_1726 = view_115 = None
        slice_scatter_312: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1725, add_54, 2, 0, 16);  slice_1725 = add_54 = None
        slice_scatter_313: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_310, slice_scatter_312, 1, 416, 432);  slice_scatter_310 = slice_scatter_312 = None
        slice_1730: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_313, 1, 416, 432)
        slice_1731: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1730, 2, 0, 16)
        slice_scatter_315: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1730, slice_1731, 2, 0, 16);  slice_1730 = slice_1731 = None
        slice_scatter_316: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_313, slice_scatter_315, 1, 416, 432);  slice_scatter_313 = slice_scatter_315 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1751: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_1717, 2, 16, 32);  slice_1717 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_56: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_1751, memory_format = torch.contiguous_format);  slice_1751 = None
        view_116: "f32[32, 11]" = torch.ops.aten.view.default(clone_56, [32, 11]);  clone_56 = None
        mm_53: "f32[32, 8]" = torch.ops.aten.mm.default(view_116, slice_37)
        view_117: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_53, [2, 16, 8]);  mm_53 = None
        slice_1758: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_316, 1, 416, 432)
        slice_1759: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1758, 2, 0, 16)
        add_55: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1759, view_117);  slice_1759 = view_117 = None
        slice_scatter_318: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1758, add_55, 2, 0, 16);  slice_1758 = add_55 = None
        slice_scatter_319: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_316, slice_scatter_318, 1, 416, 432);  slice_scatter_316 = slice_scatter_318 = None
        slice_1763: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_319, 1, 416, 432)
        slice_1764: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1763, 2, 0, 16)
        slice_scatter_321: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1763, slice_1764, 2, 0, 16);  slice_1763 = slice_1764 = None
        slice_scatter_322: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_319, slice_scatter_321, 1, 416, 432);  slice_scatter_319 = slice_scatter_321 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1783: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 432, 448)
        slice_1784: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_1783, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_57: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_1784, memory_format = torch.contiguous_format);  slice_1784 = None
        view_118: "f32[32, 16]" = torch.ops.aten.view.default(clone_57, [32, 16]);  clone_57 = None
        mm_54: "f32[32, 8]" = torch.ops.aten.mm.default(view_118, slice_7)
        view_119: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_54, [2, 16, 8]);  mm_54 = None
        slice_1791: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_322, 1, 432, 448)
        slice_1792: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1791, 2, 0, 16)
        add_56: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1792, view_119);  slice_1792 = view_119 = None
        slice_scatter_324: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1791, add_56, 2, 0, 16);  slice_1791 = add_56 = None
        slice_scatter_325: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_322, slice_scatter_324, 1, 432, 448);  slice_scatter_322 = slice_scatter_324 = None
        slice_1796: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_325, 1, 432, 448)
        slice_1797: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1796, 2, 0, 16)
        slice_scatter_327: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1796, slice_1797, 2, 0, 16);  slice_1796 = slice_1797 = None
        slice_scatter_328: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_325, slice_scatter_327, 1, 432, 448);  slice_scatter_325 = slice_scatter_327 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1817: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_1783, 2, 16, 32);  slice_1783 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_58: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_1817, memory_format = torch.contiguous_format);  slice_1817 = None
        view_120: "f32[32, 11]" = torch.ops.aten.view.default(clone_58, [32, 11]);  clone_58 = None
        mm_55: "f32[32, 8]" = torch.ops.aten.mm.default(view_120, slice_37)
        view_121: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_55, [2, 16, 8]);  mm_55 = None
        slice_1824: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_328, 1, 432, 448)
        slice_1825: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1824, 2, 0, 16)
        add_57: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1825, view_121);  slice_1825 = view_121 = None
        slice_scatter_330: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1824, add_57, 2, 0, 16);  slice_1824 = add_57 = None
        slice_scatter_331: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_328, slice_scatter_330, 1, 432, 448);  slice_scatter_328 = slice_scatter_330 = None
        slice_1829: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_331, 1, 432, 448)
        slice_1830: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1829, 2, 0, 16)
        slice_scatter_333: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1829, slice_1830, 2, 0, 16);  slice_1829 = slice_1830 = None
        slice_scatter_334: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_331, slice_scatter_333, 1, 432, 448);  slice_scatter_331 = slice_scatter_333 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1849: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 448, 464)
        slice_1850: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_1849, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_59: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_1850, memory_format = torch.contiguous_format);  slice_1850 = None
        view_122: "f32[32, 16]" = torch.ops.aten.view.default(clone_59, [32, 16]);  clone_59 = None
        mm_56: "f32[32, 8]" = torch.ops.aten.mm.default(view_122, slice_7)
        view_123: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_56, [2, 16, 8]);  mm_56 = None
        slice_1857: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_334, 1, 448, 464)
        slice_1858: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1857, 2, 0, 16)
        add_58: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1858, view_123);  slice_1858 = view_123 = None
        slice_scatter_336: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1857, add_58, 2, 0, 16);  slice_1857 = add_58 = None
        slice_scatter_337: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_334, slice_scatter_336, 1, 448, 464);  slice_scatter_334 = slice_scatter_336 = None
        slice_1862: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_337, 1, 448, 464)
        slice_1863: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1862, 2, 0, 16)
        slice_scatter_339: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1862, slice_1863, 2, 0, 16);  slice_1862 = slice_1863 = None
        slice_scatter_340: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_337, slice_scatter_339, 1, 448, 464);  slice_scatter_337 = slice_scatter_339 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1883: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_1849, 2, 16, 32);  slice_1849 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_60: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_1883, memory_format = torch.contiguous_format);  slice_1883 = None
        view_124: "f32[32, 11]" = torch.ops.aten.view.default(clone_60, [32, 11]);  clone_60 = None
        mm_57: "f32[32, 8]" = torch.ops.aten.mm.default(view_124, slice_37)
        view_125: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_57, [2, 16, 8]);  mm_57 = None
        slice_1890: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_340, 1, 448, 464)
        slice_1891: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1890, 2, 0, 16)
        add_59: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1891, view_125);  slice_1891 = view_125 = None
        slice_scatter_342: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1890, add_59, 2, 0, 16);  slice_1890 = add_59 = None
        slice_scatter_343: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_340, slice_scatter_342, 1, 448, 464);  slice_scatter_340 = slice_scatter_342 = None
        slice_1895: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_343, 1, 448, 464)
        slice_1896: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1895, 2, 0, 16)
        slice_scatter_345: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1895, slice_1896, 2, 0, 16);  slice_1895 = slice_1896 = None
        slice_scatter_346: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_343, slice_scatter_345, 1, 448, 464);  slice_scatter_343 = slice_scatter_345 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1915: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 464, 480)
        slice_1916: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_1915, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_61: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_1916, memory_format = torch.contiguous_format);  slice_1916 = None
        view_126: "f32[32, 16]" = torch.ops.aten.view.default(clone_61, [32, 16]);  clone_61 = None
        mm_58: "f32[32, 8]" = torch.ops.aten.mm.default(view_126, slice_7)
        view_127: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_58, [2, 16, 8]);  mm_58 = None
        slice_1923: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_346, 1, 464, 480)
        slice_1924: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1923, 2, 0, 16)
        add_60: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1924, view_127);  slice_1924 = view_127 = None
        slice_scatter_348: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1923, add_60, 2, 0, 16);  slice_1923 = add_60 = None
        slice_scatter_349: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_346, slice_scatter_348, 1, 464, 480);  slice_scatter_346 = slice_scatter_348 = None
        slice_1928: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_349, 1, 464, 480)
        slice_1929: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1928, 2, 0, 16)
        slice_scatter_351: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1928, slice_1929, 2, 0, 16);  slice_1928 = slice_1929 = None
        slice_scatter_352: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_349, slice_scatter_351, 1, 464, 480);  slice_scatter_349 = slice_scatter_351 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1949: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_1915, 2, 16, 32);  slice_1915 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_62: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_1949, memory_format = torch.contiguous_format);  slice_1949 = None
        view_128: "f32[32, 11]" = torch.ops.aten.view.default(clone_62, [32, 11]);  clone_62 = None
        mm_59: "f32[32, 8]" = torch.ops.aten.mm.default(view_128, slice_37)
        view_129: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_59, [2, 16, 8]);  mm_59 = None
        slice_1956: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_352, 1, 464, 480)
        slice_1957: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1956, 2, 0, 16)
        add_61: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1957, view_129);  slice_1957 = view_129 = None
        slice_scatter_354: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1956, add_61, 2, 0, 16);  slice_1956 = add_61 = None
        slice_scatter_355: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_352, slice_scatter_354, 1, 464, 480);  slice_scatter_352 = slice_scatter_354 = None
        slice_1961: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_355, 1, 464, 480)
        slice_1962: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1961, 2, 0, 16)
        slice_scatter_357: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1961, slice_1962, 2, 0, 16);  slice_1961 = slice_1962 = None
        slice_scatter_358: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_355, slice_scatter_357, 1, 464, 480);  slice_scatter_355 = slice_scatter_357 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_1981: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 480, 496)
        slice_1982: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_1981, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_63: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_1982, memory_format = torch.contiguous_format);  slice_1982 = None
        view_130: "f32[32, 16]" = torch.ops.aten.view.default(clone_63, [32, 16]);  clone_63 = None
        mm_60: "f32[32, 8]" = torch.ops.aten.mm.default(view_130, slice_7)
        view_131: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_60, [2, 16, 8]);  mm_60 = None
        slice_1989: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_358, 1, 480, 496)
        slice_1990: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1989, 2, 0, 16)
        add_62: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_1990, view_131);  slice_1990 = view_131 = None
        slice_scatter_360: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1989, add_62, 2, 0, 16);  slice_1989 = add_62 = None
        slice_scatter_361: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_358, slice_scatter_360, 1, 480, 496);  slice_scatter_358 = slice_scatter_360 = None
        slice_1994: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_361, 1, 480, 496)
        slice_1995: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_1994, 2, 0, 16)
        slice_scatter_363: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_1994, slice_1995, 2, 0, 16);  slice_1994 = slice_1995 = None
        slice_scatter_364: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_361, slice_scatter_363, 1, 480, 496);  slice_scatter_361 = slice_scatter_363 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2015: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_1981, 2, 16, 32);  slice_1981 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_64: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_2015, memory_format = torch.contiguous_format);  slice_2015 = None
        view_132: "f32[32, 11]" = torch.ops.aten.view.default(clone_64, [32, 11]);  clone_64 = None
        mm_61: "f32[32, 8]" = torch.ops.aten.mm.default(view_132, slice_37)
        view_133: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_61, [2, 16, 8]);  mm_61 = None
        slice_2022: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_364, 1, 480, 496)
        slice_2023: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2022, 2, 0, 16)
        add_63: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2023, view_133);  slice_2023 = view_133 = None
        slice_scatter_366: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2022, add_63, 2, 0, 16);  slice_2022 = add_63 = None
        slice_scatter_367: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_364, slice_scatter_366, 1, 480, 496);  slice_scatter_364 = slice_scatter_366 = None
        slice_2027: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_367, 1, 480, 496)
        slice_2028: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2027, 2, 0, 16)
        slice_scatter_369: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2027, slice_2028, 2, 0, 16);  slice_2027 = slice_2028 = None
        slice_scatter_370: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_367, slice_scatter_369, 1, 480, 496);  slice_scatter_367 = slice_scatter_369 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2047: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 496, 512)
        slice_2048: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_2047, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_65: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_2048, memory_format = torch.contiguous_format);  slice_2048 = None
        view_134: "f32[32, 16]" = torch.ops.aten.view.default(clone_65, [32, 16]);  clone_65 = None
        mm_62: "f32[32, 8]" = torch.ops.aten.mm.default(view_134, slice_7)
        view_135: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_62, [2, 16, 8]);  mm_62 = None
        slice_2055: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_370, 1, 496, 512)
        slice_2056: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2055, 2, 0, 16)
        add_64: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2056, view_135);  slice_2056 = view_135 = None
        slice_scatter_372: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2055, add_64, 2, 0, 16);  slice_2055 = add_64 = None
        slice_scatter_373: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_370, slice_scatter_372, 1, 496, 512);  slice_scatter_370 = slice_scatter_372 = None
        slice_2060: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_373, 1, 496, 512)
        slice_2061: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2060, 2, 0, 16)
        slice_scatter_375: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2060, slice_2061, 2, 0, 16);  slice_2060 = slice_2061 = None
        slice_scatter_376: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_373, slice_scatter_375, 1, 496, 512);  slice_scatter_373 = slice_scatter_375 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2081: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_2047, 2, 16, 32);  slice_2047 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_66: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_2081, memory_format = torch.contiguous_format);  slice_2081 = None
        view_136: "f32[32, 11]" = torch.ops.aten.view.default(clone_66, [32, 11]);  clone_66 = None
        mm_63: "f32[32, 8]" = torch.ops.aten.mm.default(view_136, slice_37)
        view_137: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_63, [2, 16, 8]);  mm_63 = None
        slice_2088: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_376, 1, 496, 512)
        slice_2089: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2088, 2, 0, 16)
        add_65: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2089, view_137);  slice_2089 = view_137 = None
        slice_scatter_378: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2088, add_65, 2, 0, 16);  slice_2088 = add_65 = None
        slice_scatter_379: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_376, slice_scatter_378, 1, 496, 512);  slice_scatter_376 = slice_scatter_378 = None
        slice_2093: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_379, 1, 496, 512)
        slice_2094: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2093, 2, 0, 16)
        slice_scatter_381: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2093, slice_2094, 2, 0, 16);  slice_2093 = slice_2094 = None
        slice_scatter_382: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_379, slice_scatter_381, 1, 496, 512);  slice_scatter_379 = slice_scatter_381 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2113: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 512, 528)
        slice_2114: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_2113, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_67: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_2114, memory_format = torch.contiguous_format);  slice_2114 = None
        view_138: "f32[32, 16]" = torch.ops.aten.view.default(clone_67, [32, 16]);  clone_67 = None
        mm_64: "f32[32, 8]" = torch.ops.aten.mm.default(view_138, slice_7)
        view_139: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_64, [2, 16, 8]);  mm_64 = None
        slice_2121: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_382, 1, 512, 528)
        slice_2122: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2121, 2, 0, 16)
        add_66: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2122, view_139);  slice_2122 = view_139 = None
        slice_scatter_384: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2121, add_66, 2, 0, 16);  slice_2121 = add_66 = None
        slice_scatter_385: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_382, slice_scatter_384, 1, 512, 528);  slice_scatter_382 = slice_scatter_384 = None
        slice_2126: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_385, 1, 512, 528)
        slice_2127: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2126, 2, 0, 16)
        slice_scatter_387: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2126, slice_2127, 2, 0, 16);  slice_2126 = slice_2127 = None
        slice_scatter_388: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_385, slice_scatter_387, 1, 512, 528);  slice_scatter_385 = slice_scatter_387 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2147: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_2113, 2, 16, 32);  slice_2113 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_68: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_2147, memory_format = torch.contiguous_format);  slice_2147 = None
        view_140: "f32[32, 11]" = torch.ops.aten.view.default(clone_68, [32, 11]);  clone_68 = None
        mm_65: "f32[32, 8]" = torch.ops.aten.mm.default(view_140, slice_37)
        view_141: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_65, [2, 16, 8]);  mm_65 = None
        slice_2154: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_388, 1, 512, 528)
        slice_2155: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2154, 2, 0, 16)
        add_67: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2155, view_141);  slice_2155 = view_141 = None
        slice_scatter_390: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2154, add_67, 2, 0, 16);  slice_2154 = add_67 = None
        slice_scatter_391: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_388, slice_scatter_390, 1, 512, 528);  slice_scatter_388 = slice_scatter_390 = None
        slice_2159: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_391, 1, 512, 528)
        slice_2160: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2159, 2, 0, 16)
        slice_scatter_393: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2159, slice_2160, 2, 0, 16);  slice_2159 = slice_2160 = None
        slice_scatter_394: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_391, slice_scatter_393, 1, 512, 528);  slice_scatter_391 = slice_scatter_393 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2179: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 528, 544)
        slice_2180: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_2179, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_69: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_2180, memory_format = torch.contiguous_format);  slice_2180 = None
        view_142: "f32[32, 16]" = torch.ops.aten.view.default(clone_69, [32, 16]);  clone_69 = None
        mm_66: "f32[32, 8]" = torch.ops.aten.mm.default(view_142, slice_7)
        view_143: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_66, [2, 16, 8]);  mm_66 = None
        slice_2187: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_394, 1, 528, 544)
        slice_2188: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2187, 2, 0, 16)
        add_68: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2188, view_143);  slice_2188 = view_143 = None
        slice_scatter_396: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2187, add_68, 2, 0, 16);  slice_2187 = add_68 = None
        slice_scatter_397: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_394, slice_scatter_396, 1, 528, 544);  slice_scatter_394 = slice_scatter_396 = None
        slice_2192: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_397, 1, 528, 544)
        slice_2193: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2192, 2, 0, 16)
        slice_scatter_399: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2192, slice_2193, 2, 0, 16);  slice_2192 = slice_2193 = None
        slice_scatter_400: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_397, slice_scatter_399, 1, 528, 544);  slice_scatter_397 = slice_scatter_399 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2213: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_2179, 2, 16, 32);  slice_2179 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_70: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_2213, memory_format = torch.contiguous_format);  slice_2213 = None
        view_144: "f32[32, 11]" = torch.ops.aten.view.default(clone_70, [32, 11]);  clone_70 = None
        mm_67: "f32[32, 8]" = torch.ops.aten.mm.default(view_144, slice_37)
        view_145: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_67, [2, 16, 8]);  mm_67 = None
        slice_2220: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_400, 1, 528, 544)
        slice_2221: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2220, 2, 0, 16)
        add_69: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2221, view_145);  slice_2221 = view_145 = None
        slice_scatter_402: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2220, add_69, 2, 0, 16);  slice_2220 = add_69 = None
        slice_scatter_403: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_400, slice_scatter_402, 1, 528, 544);  slice_scatter_400 = slice_scatter_402 = None
        slice_2225: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_403, 1, 528, 544)
        slice_2226: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2225, 2, 0, 16)
        slice_scatter_405: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2225, slice_2226, 2, 0, 16);  slice_2225 = slice_2226 = None
        slice_scatter_406: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_403, slice_scatter_405, 1, 528, 544);  slice_scatter_403 = slice_scatter_405 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2245: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 544, 560)
        slice_2246: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_2245, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_71: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_2246, memory_format = torch.contiguous_format);  slice_2246 = None
        view_146: "f32[32, 16]" = torch.ops.aten.view.default(clone_71, [32, 16]);  clone_71 = None
        mm_68: "f32[32, 8]" = torch.ops.aten.mm.default(view_146, slice_7)
        view_147: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_68, [2, 16, 8]);  mm_68 = None
        slice_2253: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_406, 1, 544, 560)
        slice_2254: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2253, 2, 0, 16)
        add_70: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2254, view_147);  slice_2254 = view_147 = None
        slice_scatter_408: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2253, add_70, 2, 0, 16);  slice_2253 = add_70 = None
        slice_scatter_409: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_406, slice_scatter_408, 1, 544, 560);  slice_scatter_406 = slice_scatter_408 = None
        slice_2258: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_409, 1, 544, 560)
        slice_2259: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2258, 2, 0, 16)
        slice_scatter_411: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2258, slice_2259, 2, 0, 16);  slice_2258 = slice_2259 = None
        slice_scatter_412: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_409, slice_scatter_411, 1, 544, 560);  slice_scatter_409 = slice_scatter_411 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2279: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_2245, 2, 16, 32);  slice_2245 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_72: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_2279, memory_format = torch.contiguous_format);  slice_2279 = None
        view_148: "f32[32, 11]" = torch.ops.aten.view.default(clone_72, [32, 11]);  clone_72 = None
        mm_69: "f32[32, 8]" = torch.ops.aten.mm.default(view_148, slice_37)
        view_149: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_69, [2, 16, 8]);  mm_69 = None
        slice_2286: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_412, 1, 544, 560)
        slice_2287: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2286, 2, 0, 16)
        add_71: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2287, view_149);  slice_2287 = view_149 = None
        slice_scatter_414: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2286, add_71, 2, 0, 16);  slice_2286 = add_71 = None
        slice_scatter_415: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_412, slice_scatter_414, 1, 544, 560);  slice_scatter_412 = slice_scatter_414 = None
        slice_2291: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_415, 1, 544, 560)
        slice_2292: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2291, 2, 0, 16)
        slice_scatter_417: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2291, slice_2292, 2, 0, 16);  slice_2291 = slice_2292 = None
        slice_scatter_418: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_415, slice_scatter_417, 1, 544, 560);  slice_scatter_415 = slice_scatter_417 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2311: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 560, 576)
        slice_2312: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_2311, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_73: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_2312, memory_format = torch.contiguous_format);  slice_2312 = None
        view_150: "f32[32, 16]" = torch.ops.aten.view.default(clone_73, [32, 16]);  clone_73 = None
        mm_70: "f32[32, 8]" = torch.ops.aten.mm.default(view_150, slice_7)
        view_151: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_70, [2, 16, 8]);  mm_70 = None
        slice_2319: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_418, 1, 560, 576)
        slice_2320: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2319, 2, 0, 16)
        add_72: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2320, view_151);  slice_2320 = view_151 = None
        slice_scatter_420: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2319, add_72, 2, 0, 16);  slice_2319 = add_72 = None
        slice_scatter_421: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_418, slice_scatter_420, 1, 560, 576);  slice_scatter_418 = slice_scatter_420 = None
        slice_2324: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_421, 1, 560, 576)
        slice_2325: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2324, 2, 0, 16)
        slice_scatter_423: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2324, slice_2325, 2, 0, 16);  slice_2324 = slice_2325 = None
        slice_scatter_424: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_421, slice_scatter_423, 1, 560, 576);  slice_scatter_421 = slice_scatter_423 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2345: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_2311, 2, 16, 32);  slice_2311 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_74: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_2345, memory_format = torch.contiguous_format);  slice_2345 = None
        view_152: "f32[32, 11]" = torch.ops.aten.view.default(clone_74, [32, 11]);  clone_74 = None
        mm_71: "f32[32, 8]" = torch.ops.aten.mm.default(view_152, slice_37)
        view_153: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_71, [2, 16, 8]);  mm_71 = None
        slice_2352: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_424, 1, 560, 576)
        slice_2353: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2352, 2, 0, 16)
        add_73: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2353, view_153);  slice_2353 = view_153 = None
        slice_scatter_426: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2352, add_73, 2, 0, 16);  slice_2352 = add_73 = None
        slice_scatter_427: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_424, slice_scatter_426, 1, 560, 576);  slice_scatter_424 = slice_scatter_426 = None
        slice_2357: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_427, 1, 560, 576)
        slice_2358: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2357, 2, 0, 16)
        slice_scatter_429: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2357, slice_2358, 2, 0, 16);  slice_2357 = slice_2358 = None
        slice_scatter_430: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_427, slice_scatter_429, 1, 560, 576);  slice_scatter_427 = slice_scatter_429 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2377: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 576, 592)
        slice_2378: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_2377, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_75: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_2378, memory_format = torch.contiguous_format);  slice_2378 = None
        view_154: "f32[32, 16]" = torch.ops.aten.view.default(clone_75, [32, 16]);  clone_75 = None
        mm_72: "f32[32, 8]" = torch.ops.aten.mm.default(view_154, slice_7)
        view_155: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_72, [2, 16, 8]);  mm_72 = None
        slice_2385: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_430, 1, 576, 592)
        slice_2386: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2385, 2, 0, 16)
        add_74: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2386, view_155);  slice_2386 = view_155 = None
        slice_scatter_432: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2385, add_74, 2, 0, 16);  slice_2385 = add_74 = None
        slice_scatter_433: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_430, slice_scatter_432, 1, 576, 592);  slice_scatter_430 = slice_scatter_432 = None
        slice_2390: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_433, 1, 576, 592)
        slice_2391: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2390, 2, 0, 16)
        slice_scatter_435: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2390, slice_2391, 2, 0, 16);  slice_2390 = slice_2391 = None
        slice_scatter_436: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_433, slice_scatter_435, 1, 576, 592);  slice_scatter_433 = slice_scatter_435 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2411: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_2377, 2, 16, 32);  slice_2377 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_76: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_2411, memory_format = torch.contiguous_format);  slice_2411 = None
        view_156: "f32[32, 11]" = torch.ops.aten.view.default(clone_76, [32, 11]);  clone_76 = None
        mm_73: "f32[32, 8]" = torch.ops.aten.mm.default(view_156, slice_37)
        view_157: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_73, [2, 16, 8]);  mm_73 = None
        slice_2418: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_436, 1, 576, 592)
        slice_2419: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2418, 2, 0, 16)
        add_75: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2419, view_157);  slice_2419 = view_157 = None
        slice_scatter_438: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2418, add_75, 2, 0, 16);  slice_2418 = add_75 = None
        slice_scatter_439: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_436, slice_scatter_438, 1, 576, 592);  slice_scatter_436 = slice_scatter_438 = None
        slice_2423: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_439, 1, 576, 592)
        slice_2424: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2423, 2, 0, 16)
        slice_scatter_441: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2423, slice_2424, 2, 0, 16);  slice_2423 = slice_2424 = None
        slice_scatter_442: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_439, slice_scatter_441, 1, 576, 592);  slice_scatter_439 = slice_scatter_441 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2443: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 592, 608)
        slice_2444: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_2443, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_77: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_2444, memory_format = torch.contiguous_format);  slice_2444 = None
        view_158: "f32[32, 16]" = torch.ops.aten.view.default(clone_77, [32, 16]);  clone_77 = None
        mm_74: "f32[32, 8]" = torch.ops.aten.mm.default(view_158, slice_7)
        view_159: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_74, [2, 16, 8]);  mm_74 = None
        slice_2451: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_442, 1, 592, 608)
        slice_2452: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2451, 2, 0, 16)
        add_76: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2452, view_159);  slice_2452 = view_159 = None
        slice_scatter_444: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2451, add_76, 2, 0, 16);  slice_2451 = add_76 = None
        slice_scatter_445: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_442, slice_scatter_444, 1, 592, 608);  slice_scatter_442 = slice_scatter_444 = None
        slice_2456: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_445, 1, 592, 608)
        slice_2457: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2456, 2, 0, 16)
        slice_scatter_447: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2456, slice_2457, 2, 0, 16);  slice_2456 = slice_2457 = None
        slice_scatter_448: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_445, slice_scatter_447, 1, 592, 608);  slice_scatter_445 = slice_scatter_447 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2477: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_2443, 2, 16, 32);  slice_2443 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_78: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_2477, memory_format = torch.contiguous_format);  slice_2477 = None
        view_160: "f32[32, 11]" = torch.ops.aten.view.default(clone_78, [32, 11]);  clone_78 = None
        mm_75: "f32[32, 8]" = torch.ops.aten.mm.default(view_160, slice_37)
        view_161: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_75, [2, 16, 8]);  mm_75 = None
        slice_2484: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_448, 1, 592, 608)
        slice_2485: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2484, 2, 0, 16)
        add_77: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2485, view_161);  slice_2485 = view_161 = None
        slice_scatter_450: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2484, add_77, 2, 0, 16);  slice_2484 = add_77 = None
        slice_scatter_451: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_448, slice_scatter_450, 1, 592, 608);  slice_scatter_448 = slice_scatter_450 = None
        slice_2489: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_451, 1, 592, 608)
        slice_2490: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2489, 2, 0, 16)
        slice_scatter_453: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2489, slice_2490, 2, 0, 16);  slice_2489 = slice_2490 = None
        slice_scatter_454: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_451, slice_scatter_453, 1, 592, 608);  slice_scatter_451 = slice_scatter_453 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2509: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 608, 624)
        slice_2510: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_2509, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_79: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_2510, memory_format = torch.contiguous_format);  slice_2510 = None
        view_162: "f32[32, 16]" = torch.ops.aten.view.default(clone_79, [32, 16]);  clone_79 = None
        mm_76: "f32[32, 8]" = torch.ops.aten.mm.default(view_162, slice_7)
        view_163: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_76, [2, 16, 8]);  mm_76 = None
        slice_2517: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_454, 1, 608, 624)
        slice_2518: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2517, 2, 0, 16)
        add_78: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2518, view_163);  slice_2518 = view_163 = None
        slice_scatter_456: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2517, add_78, 2, 0, 16);  slice_2517 = add_78 = None
        slice_scatter_457: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_454, slice_scatter_456, 1, 608, 624);  slice_scatter_454 = slice_scatter_456 = None
        slice_2522: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_457, 1, 608, 624)
        slice_2523: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2522, 2, 0, 16)
        slice_scatter_459: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2522, slice_2523, 2, 0, 16);  slice_2522 = slice_2523 = None
        slice_scatter_460: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_457, slice_scatter_459, 1, 608, 624);  slice_scatter_457 = slice_scatter_459 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2543: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_2509, 2, 16, 32);  slice_2509 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_80: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_2543, memory_format = torch.contiguous_format);  slice_2543 = None
        view_164: "f32[32, 11]" = torch.ops.aten.view.default(clone_80, [32, 11]);  clone_80 = None
        mm_77: "f32[32, 8]" = torch.ops.aten.mm.default(view_164, slice_37)
        view_165: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_77, [2, 16, 8]);  mm_77 = None
        slice_2550: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_460, 1, 608, 624)
        slice_2551: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2550, 2, 0, 16)
        add_79: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2551, view_165);  slice_2551 = view_165 = None
        slice_scatter_462: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2550, add_79, 2, 0, 16);  slice_2550 = add_79 = None
        slice_scatter_463: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_460, slice_scatter_462, 1, 608, 624);  slice_scatter_460 = slice_scatter_462 = None
        slice_2555: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_463, 1, 608, 624)
        slice_2556: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2555, 2, 0, 16)
        slice_scatter_465: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2555, slice_2556, 2, 0, 16);  slice_2555 = slice_2556 = None
        slice_scatter_466: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_463, slice_scatter_465, 1, 608, 624);  slice_scatter_463 = slice_scatter_465 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2575: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 624, 640)
        slice_2576: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_2575, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_81: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_2576, memory_format = torch.contiguous_format);  slice_2576 = None
        view_166: "f32[32, 16]" = torch.ops.aten.view.default(clone_81, [32, 16]);  clone_81 = None
        mm_78: "f32[32, 8]" = torch.ops.aten.mm.default(view_166, slice_7)
        view_167: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_78, [2, 16, 8]);  mm_78 = None
        slice_2583: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_466, 1, 624, 640)
        slice_2584: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2583, 2, 0, 16)
        add_80: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2584, view_167);  slice_2584 = view_167 = None
        slice_scatter_468: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2583, add_80, 2, 0, 16);  slice_2583 = add_80 = None
        slice_scatter_469: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_466, slice_scatter_468, 1, 624, 640);  slice_scatter_466 = slice_scatter_468 = None
        slice_2588: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_469, 1, 624, 640)
        slice_2589: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2588, 2, 0, 16)
        slice_scatter_471: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2588, slice_2589, 2, 0, 16);  slice_2588 = slice_2589 = None
        slice_scatter_472: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_469, slice_scatter_471, 1, 624, 640);  slice_scatter_469 = slice_scatter_471 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2609: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_2575, 2, 16, 32);  slice_2575 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_82: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_2609, memory_format = torch.contiguous_format);  slice_2609 = None
        view_168: "f32[32, 11]" = torch.ops.aten.view.default(clone_82, [32, 11]);  clone_82 = None
        mm_79: "f32[32, 8]" = torch.ops.aten.mm.default(view_168, slice_37)
        view_169: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_79, [2, 16, 8]);  mm_79 = None
        slice_2616: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_472, 1, 624, 640)
        slice_2617: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2616, 2, 0, 16)
        add_81: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2617, view_169);  slice_2617 = view_169 = None
        slice_scatter_474: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2616, add_81, 2, 0, 16);  slice_2616 = add_81 = None
        slice_scatter_475: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_472, slice_scatter_474, 1, 624, 640);  slice_scatter_472 = slice_scatter_474 = None
        slice_2621: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_475, 1, 624, 640)
        slice_2622: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2621, 2, 0, 16)
        slice_scatter_477: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2621, slice_2622, 2, 0, 16);  slice_2621 = slice_2622 = None
        slice_scatter_478: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_475, slice_scatter_477, 1, 624, 640);  slice_scatter_475 = slice_scatter_477 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2641: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 640, 656)
        slice_2642: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_2641, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_83: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_2642, memory_format = torch.contiguous_format);  slice_2642 = None
        view_170: "f32[32, 16]" = torch.ops.aten.view.default(clone_83, [32, 16]);  clone_83 = None
        mm_80: "f32[32, 8]" = torch.ops.aten.mm.default(view_170, slice_7)
        view_171: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_80, [2, 16, 8]);  mm_80 = None
        slice_2649: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_478, 1, 640, 656)
        slice_2650: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2649, 2, 0, 16)
        add_82: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2650, view_171);  slice_2650 = view_171 = None
        slice_scatter_480: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2649, add_82, 2, 0, 16);  slice_2649 = add_82 = None
        slice_scatter_481: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_478, slice_scatter_480, 1, 640, 656);  slice_scatter_478 = slice_scatter_480 = None
        slice_2654: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_481, 1, 640, 656)
        slice_2655: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2654, 2, 0, 16)
        slice_scatter_483: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2654, slice_2655, 2, 0, 16);  slice_2654 = slice_2655 = None
        slice_scatter_484: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_481, slice_scatter_483, 1, 640, 656);  slice_scatter_481 = slice_scatter_483 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2675: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_2641, 2, 16, 32);  slice_2641 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_84: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_2675, memory_format = torch.contiguous_format);  slice_2675 = None
        view_172: "f32[32, 11]" = torch.ops.aten.view.default(clone_84, [32, 11]);  clone_84 = None
        mm_81: "f32[32, 8]" = torch.ops.aten.mm.default(view_172, slice_37)
        view_173: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_81, [2, 16, 8]);  mm_81 = None
        slice_2682: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_484, 1, 640, 656)
        slice_2683: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2682, 2, 0, 16)
        add_83: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2683, view_173);  slice_2683 = view_173 = None
        slice_scatter_486: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2682, add_83, 2, 0, 16);  slice_2682 = add_83 = None
        slice_scatter_487: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_484, slice_scatter_486, 1, 640, 656);  slice_scatter_484 = slice_scatter_486 = None
        slice_2687: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_487, 1, 640, 656)
        slice_2688: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2687, 2, 0, 16)
        slice_scatter_489: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2687, slice_2688, 2, 0, 16);  slice_2687 = slice_2688 = None
        slice_scatter_490: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_487, slice_scatter_489, 1, 640, 656);  slice_scatter_487 = slice_scatter_489 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2707: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 656, 672)
        slice_2708: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_2707, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_85: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_2708, memory_format = torch.contiguous_format);  slice_2708 = None
        view_174: "f32[32, 16]" = torch.ops.aten.view.default(clone_85, [32, 16]);  clone_85 = None
        mm_82: "f32[32, 8]" = torch.ops.aten.mm.default(view_174, slice_7)
        view_175: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_82, [2, 16, 8]);  mm_82 = None
        slice_2715: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_490, 1, 656, 672)
        slice_2716: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2715, 2, 0, 16)
        add_84: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2716, view_175);  slice_2716 = view_175 = None
        slice_scatter_492: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2715, add_84, 2, 0, 16);  slice_2715 = add_84 = None
        slice_scatter_493: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_490, slice_scatter_492, 1, 656, 672);  slice_scatter_490 = slice_scatter_492 = None
        slice_2720: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_493, 1, 656, 672)
        slice_2721: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2720, 2, 0, 16)
        slice_scatter_495: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2720, slice_2721, 2, 0, 16);  slice_2720 = slice_2721 = None
        slice_scatter_496: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_493, slice_scatter_495, 1, 656, 672);  slice_scatter_493 = slice_scatter_495 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2741: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_2707, 2, 16, 32);  slice_2707 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_86: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_2741, memory_format = torch.contiguous_format);  slice_2741 = None
        view_176: "f32[32, 11]" = torch.ops.aten.view.default(clone_86, [32, 11]);  clone_86 = None
        mm_83: "f32[32, 8]" = torch.ops.aten.mm.default(view_176, slice_37)
        view_177: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_83, [2, 16, 8]);  mm_83 = None
        slice_2748: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_496, 1, 656, 672)
        slice_2749: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2748, 2, 0, 16)
        add_85: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2749, view_177);  slice_2749 = view_177 = None
        slice_scatter_498: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2748, add_85, 2, 0, 16);  slice_2748 = add_85 = None
        slice_scatter_499: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_496, slice_scatter_498, 1, 656, 672);  slice_scatter_496 = slice_scatter_498 = None
        slice_2753: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_499, 1, 656, 672)
        slice_2754: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2753, 2, 0, 16)
        slice_scatter_501: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2753, slice_2754, 2, 0, 16);  slice_2753 = slice_2754 = None
        slice_scatter_502: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_499, slice_scatter_501, 1, 656, 672);  slice_scatter_499 = slice_scatter_501 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2773: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 672, 688)
        slice_2774: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_2773, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_87: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_2774, memory_format = torch.contiguous_format);  slice_2774 = None
        view_178: "f32[32, 16]" = torch.ops.aten.view.default(clone_87, [32, 16]);  clone_87 = None
        mm_84: "f32[32, 8]" = torch.ops.aten.mm.default(view_178, slice_7)
        view_179: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_84, [2, 16, 8]);  mm_84 = None
        slice_2781: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_502, 1, 672, 688)
        slice_2782: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2781, 2, 0, 16)
        add_86: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2782, view_179);  slice_2782 = view_179 = None
        slice_scatter_504: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2781, add_86, 2, 0, 16);  slice_2781 = add_86 = None
        slice_scatter_505: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_502, slice_scatter_504, 1, 672, 688);  slice_scatter_502 = slice_scatter_504 = None
        slice_2786: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_505, 1, 672, 688)
        slice_2787: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2786, 2, 0, 16)
        slice_scatter_507: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2786, slice_2787, 2, 0, 16);  slice_2786 = slice_2787 = None
        slice_scatter_508: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_505, slice_scatter_507, 1, 672, 688);  slice_scatter_505 = slice_scatter_507 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2807: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_2773, 2, 16, 32);  slice_2773 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_88: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_2807, memory_format = torch.contiguous_format);  slice_2807 = None
        view_180: "f32[32, 11]" = torch.ops.aten.view.default(clone_88, [32, 11]);  clone_88 = None
        mm_85: "f32[32, 8]" = torch.ops.aten.mm.default(view_180, slice_37)
        view_181: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_85, [2, 16, 8]);  mm_85 = None
        slice_2814: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_508, 1, 672, 688)
        slice_2815: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2814, 2, 0, 16)
        add_87: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2815, view_181);  slice_2815 = view_181 = None
        slice_scatter_510: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2814, add_87, 2, 0, 16);  slice_2814 = add_87 = None
        slice_scatter_511: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_508, slice_scatter_510, 1, 672, 688);  slice_scatter_508 = slice_scatter_510 = None
        slice_2819: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_511, 1, 672, 688)
        slice_2820: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2819, 2, 0, 16)
        slice_scatter_513: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2819, slice_2820, 2, 0, 16);  slice_2819 = slice_2820 = None
        slice_scatter_514: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_511, slice_scatter_513, 1, 672, 688);  slice_scatter_511 = slice_scatter_513 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2839: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 688, 704)
        slice_2840: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_2839, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_89: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_2840, memory_format = torch.contiguous_format);  slice_2840 = None
        view_182: "f32[32, 16]" = torch.ops.aten.view.default(clone_89, [32, 16]);  clone_89 = None
        mm_86: "f32[32, 8]" = torch.ops.aten.mm.default(view_182, slice_7)
        view_183: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_86, [2, 16, 8]);  mm_86 = None
        slice_2847: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_514, 1, 688, 704)
        slice_2848: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2847, 2, 0, 16)
        add_88: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2848, view_183);  slice_2848 = view_183 = None
        slice_scatter_516: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2847, add_88, 2, 0, 16);  slice_2847 = add_88 = None
        slice_scatter_517: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_514, slice_scatter_516, 1, 688, 704);  slice_scatter_514 = slice_scatter_516 = None
        slice_2852: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_517, 1, 688, 704)
        slice_2853: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2852, 2, 0, 16)
        slice_scatter_519: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2852, slice_2853, 2, 0, 16);  slice_2852 = slice_2853 = None
        slice_scatter_520: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_517, slice_scatter_519, 1, 688, 704);  slice_scatter_517 = slice_scatter_519 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2873: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_2839, 2, 16, 32);  slice_2839 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_90: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_2873, memory_format = torch.contiguous_format);  slice_2873 = None
        view_184: "f32[32, 11]" = torch.ops.aten.view.default(clone_90, [32, 11]);  clone_90 = None
        mm_87: "f32[32, 8]" = torch.ops.aten.mm.default(view_184, slice_37)
        view_185: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_87, [2, 16, 8]);  mm_87 = None
        slice_2880: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_520, 1, 688, 704)
        slice_2881: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2880, 2, 0, 16)
        add_89: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2881, view_185);  slice_2881 = view_185 = None
        slice_scatter_522: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2880, add_89, 2, 0, 16);  slice_2880 = add_89 = None
        slice_scatter_523: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_520, slice_scatter_522, 1, 688, 704);  slice_scatter_520 = slice_scatter_522 = None
        slice_2885: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_523, 1, 688, 704)
        slice_2886: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2885, 2, 0, 16)
        slice_scatter_525: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2885, slice_2886, 2, 0, 16);  slice_2885 = slice_2886 = None
        slice_scatter_526: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_523, slice_scatter_525, 1, 688, 704);  slice_scatter_523 = slice_scatter_525 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2905: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 704, 720)
        slice_2906: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_2905, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_91: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_2906, memory_format = torch.contiguous_format);  slice_2906 = None
        view_186: "f32[32, 16]" = torch.ops.aten.view.default(clone_91, [32, 16]);  clone_91 = None
        mm_88: "f32[32, 8]" = torch.ops.aten.mm.default(view_186, slice_7)
        view_187: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_88, [2, 16, 8]);  mm_88 = None
        slice_2913: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_526, 1, 704, 720)
        slice_2914: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2913, 2, 0, 16)
        add_90: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2914, view_187);  slice_2914 = view_187 = None
        slice_scatter_528: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2913, add_90, 2, 0, 16);  slice_2913 = add_90 = None
        slice_scatter_529: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_526, slice_scatter_528, 1, 704, 720);  slice_scatter_526 = slice_scatter_528 = None
        slice_2918: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_529, 1, 704, 720)
        slice_2919: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2918, 2, 0, 16)
        slice_scatter_531: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2918, slice_2919, 2, 0, 16);  slice_2918 = slice_2919 = None
        slice_scatter_532: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_529, slice_scatter_531, 1, 704, 720);  slice_scatter_529 = slice_scatter_531 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2939: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_2905, 2, 16, 32);  slice_2905 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_92: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_2939, memory_format = torch.contiguous_format);  slice_2939 = None
        view_188: "f32[32, 11]" = torch.ops.aten.view.default(clone_92, [32, 11]);  clone_92 = None
        mm_89: "f32[32, 8]" = torch.ops.aten.mm.default(view_188, slice_37)
        view_189: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_89, [2, 16, 8]);  mm_89 = None
        slice_2946: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_532, 1, 704, 720)
        slice_2947: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2946, 2, 0, 16)
        add_91: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2947, view_189);  slice_2947 = view_189 = None
        slice_scatter_534: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2946, add_91, 2, 0, 16);  slice_2946 = add_91 = None
        slice_scatter_535: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_532, slice_scatter_534, 1, 704, 720);  slice_scatter_532 = slice_scatter_534 = None
        slice_2951: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_535, 1, 704, 720)
        slice_2952: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2951, 2, 0, 16)
        slice_scatter_537: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2951, slice_2952, 2, 0, 16);  slice_2951 = slice_2952 = None
        slice_scatter_538: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_535, slice_scatter_537, 1, 704, 720);  slice_scatter_535 = slice_scatter_537 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_2971: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 720, 736)
        slice_2972: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_2971, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_93: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_2972, memory_format = torch.contiguous_format);  slice_2972 = None
        view_190: "f32[32, 16]" = torch.ops.aten.view.default(clone_93, [32, 16]);  clone_93 = None
        mm_90: "f32[32, 8]" = torch.ops.aten.mm.default(view_190, slice_7)
        view_191: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_90, [2, 16, 8]);  mm_90 = None
        slice_2979: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_538, 1, 720, 736)
        slice_2980: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2979, 2, 0, 16)
        add_92: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_2980, view_191);  slice_2980 = view_191 = None
        slice_scatter_540: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2979, add_92, 2, 0, 16);  slice_2979 = add_92 = None
        slice_scatter_541: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_538, slice_scatter_540, 1, 720, 736);  slice_scatter_538 = slice_scatter_540 = None
        slice_2984: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_541, 1, 720, 736)
        slice_2985: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_2984, 2, 0, 16)
        slice_scatter_543: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_2984, slice_2985, 2, 0, 16);  slice_2984 = slice_2985 = None
        slice_scatter_544: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_541, slice_scatter_543, 1, 720, 736);  slice_scatter_541 = slice_scatter_543 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3005: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_2971, 2, 16, 32);  slice_2971 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_94: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_3005, memory_format = torch.contiguous_format);  slice_3005 = None
        view_192: "f32[32, 11]" = torch.ops.aten.view.default(clone_94, [32, 11]);  clone_94 = None
        mm_91: "f32[32, 8]" = torch.ops.aten.mm.default(view_192, slice_37)
        view_193: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_91, [2, 16, 8]);  mm_91 = None
        slice_3012: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_544, 1, 720, 736)
        slice_3013: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3012, 2, 0, 16)
        add_93: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3013, view_193);  slice_3013 = view_193 = None
        slice_scatter_546: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3012, add_93, 2, 0, 16);  slice_3012 = add_93 = None
        slice_scatter_547: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_544, slice_scatter_546, 1, 720, 736);  slice_scatter_544 = slice_scatter_546 = None
        slice_3017: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_547, 1, 720, 736)
        slice_3018: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3017, 2, 0, 16)
        slice_scatter_549: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3017, slice_3018, 2, 0, 16);  slice_3017 = slice_3018 = None
        slice_scatter_550: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_547, slice_scatter_549, 1, 720, 736);  slice_scatter_547 = slice_scatter_549 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3037: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 736, 752)
        slice_3038: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_3037, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_95: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_3038, memory_format = torch.contiguous_format);  slice_3038 = None
        view_194: "f32[32, 16]" = torch.ops.aten.view.default(clone_95, [32, 16]);  clone_95 = None
        mm_92: "f32[32, 8]" = torch.ops.aten.mm.default(view_194, slice_7)
        view_195: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_92, [2, 16, 8]);  mm_92 = None
        slice_3045: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_550, 1, 736, 752)
        slice_3046: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3045, 2, 0, 16)
        add_94: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3046, view_195);  slice_3046 = view_195 = None
        slice_scatter_552: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3045, add_94, 2, 0, 16);  slice_3045 = add_94 = None
        slice_scatter_553: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_550, slice_scatter_552, 1, 736, 752);  slice_scatter_550 = slice_scatter_552 = None
        slice_3050: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_553, 1, 736, 752)
        slice_3051: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3050, 2, 0, 16)
        slice_scatter_555: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3050, slice_3051, 2, 0, 16);  slice_3050 = slice_3051 = None
        slice_scatter_556: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_553, slice_scatter_555, 1, 736, 752);  slice_scatter_553 = slice_scatter_555 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3071: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_3037, 2, 16, 32);  slice_3037 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_96: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_3071, memory_format = torch.contiguous_format);  slice_3071 = None
        view_196: "f32[32, 11]" = torch.ops.aten.view.default(clone_96, [32, 11]);  clone_96 = None
        mm_93: "f32[32, 8]" = torch.ops.aten.mm.default(view_196, slice_37)
        view_197: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_93, [2, 16, 8]);  mm_93 = None
        slice_3078: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_556, 1, 736, 752)
        slice_3079: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3078, 2, 0, 16)
        add_95: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3079, view_197);  slice_3079 = view_197 = None
        slice_scatter_558: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3078, add_95, 2, 0, 16);  slice_3078 = add_95 = None
        slice_scatter_559: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_556, slice_scatter_558, 1, 736, 752);  slice_scatter_556 = slice_scatter_558 = None
        slice_3083: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_559, 1, 736, 752)
        slice_3084: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3083, 2, 0, 16)
        slice_scatter_561: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3083, slice_3084, 2, 0, 16);  slice_3083 = slice_3084 = None
        slice_scatter_562: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_559, slice_scatter_561, 1, 736, 752);  slice_scatter_559 = slice_scatter_561 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3103: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 752, 768)
        slice_3104: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_3103, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_97: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_3104, memory_format = torch.contiguous_format);  slice_3104 = None
        view_198: "f32[32, 16]" = torch.ops.aten.view.default(clone_97, [32, 16]);  clone_97 = None
        mm_94: "f32[32, 8]" = torch.ops.aten.mm.default(view_198, slice_7)
        view_199: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_94, [2, 16, 8]);  mm_94 = None
        slice_3111: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_562, 1, 752, 768)
        slice_3112: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3111, 2, 0, 16)
        add_96: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3112, view_199);  slice_3112 = view_199 = None
        slice_scatter_564: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3111, add_96, 2, 0, 16);  slice_3111 = add_96 = None
        slice_scatter_565: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_562, slice_scatter_564, 1, 752, 768);  slice_scatter_562 = slice_scatter_564 = None
        slice_3116: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_565, 1, 752, 768)
        slice_3117: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3116, 2, 0, 16)
        slice_scatter_567: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3116, slice_3117, 2, 0, 16);  slice_3116 = slice_3117 = None
        slice_scatter_568: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_565, slice_scatter_567, 1, 752, 768);  slice_scatter_565 = slice_scatter_567 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3137: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_3103, 2, 16, 32);  slice_3103 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_98: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_3137, memory_format = torch.contiguous_format);  slice_3137 = None
        view_200: "f32[32, 11]" = torch.ops.aten.view.default(clone_98, [32, 11]);  clone_98 = None
        mm_95: "f32[32, 8]" = torch.ops.aten.mm.default(view_200, slice_37)
        view_201: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_95, [2, 16, 8]);  mm_95 = None
        slice_3144: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_568, 1, 752, 768)
        slice_3145: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3144, 2, 0, 16)
        add_97: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3145, view_201);  slice_3145 = view_201 = None
        slice_scatter_570: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3144, add_97, 2, 0, 16);  slice_3144 = add_97 = None
        slice_scatter_571: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_568, slice_scatter_570, 1, 752, 768);  slice_scatter_568 = slice_scatter_570 = None
        slice_3149: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_571, 1, 752, 768)
        slice_3150: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3149, 2, 0, 16)
        slice_scatter_573: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3149, slice_3150, 2, 0, 16);  slice_3149 = slice_3150 = None
        slice_scatter_574: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_571, slice_scatter_573, 1, 752, 768);  slice_scatter_571 = slice_scatter_573 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3169: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 768, 784)
        slice_3170: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_3169, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_99: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_3170, memory_format = torch.contiguous_format);  slice_3170 = None
        view_202: "f32[32, 16]" = torch.ops.aten.view.default(clone_99, [32, 16]);  clone_99 = None
        mm_96: "f32[32, 8]" = torch.ops.aten.mm.default(view_202, slice_7)
        view_203: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_96, [2, 16, 8]);  mm_96 = None
        slice_3177: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_574, 1, 768, 784)
        slice_3178: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3177, 2, 0, 16)
        add_98: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3178, view_203);  slice_3178 = view_203 = None
        slice_scatter_576: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3177, add_98, 2, 0, 16);  slice_3177 = add_98 = None
        slice_scatter_577: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_574, slice_scatter_576, 1, 768, 784);  slice_scatter_574 = slice_scatter_576 = None
        slice_3182: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_577, 1, 768, 784)
        slice_3183: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3182, 2, 0, 16)
        slice_scatter_579: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3182, slice_3183, 2, 0, 16);  slice_3182 = slice_3183 = None
        slice_scatter_580: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_577, slice_scatter_579, 1, 768, 784);  slice_scatter_577 = slice_scatter_579 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3203: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_3169, 2, 16, 32);  slice_3169 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_100: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_3203, memory_format = torch.contiguous_format);  slice_3203 = None
        view_204: "f32[32, 11]" = torch.ops.aten.view.default(clone_100, [32, 11]);  clone_100 = None
        mm_97: "f32[32, 8]" = torch.ops.aten.mm.default(view_204, slice_37)
        view_205: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_97, [2, 16, 8]);  mm_97 = None
        slice_3210: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_580, 1, 768, 784)
        slice_3211: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3210, 2, 0, 16)
        add_99: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3211, view_205);  slice_3211 = view_205 = None
        slice_scatter_582: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3210, add_99, 2, 0, 16);  slice_3210 = add_99 = None
        slice_scatter_583: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_580, slice_scatter_582, 1, 768, 784);  slice_scatter_580 = slice_scatter_582 = None
        slice_3215: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_583, 1, 768, 784)
        slice_3216: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3215, 2, 0, 16)
        slice_scatter_585: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3215, slice_3216, 2, 0, 16);  slice_3215 = slice_3216 = None
        slice_scatter_586: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_583, slice_scatter_585, 1, 768, 784);  slice_scatter_583 = slice_scatter_585 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3235: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 784, 800)
        slice_3236: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_3235, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_101: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_3236, memory_format = torch.contiguous_format);  slice_3236 = None
        view_206: "f32[32, 16]" = torch.ops.aten.view.default(clone_101, [32, 16]);  clone_101 = None
        mm_98: "f32[32, 8]" = torch.ops.aten.mm.default(view_206, slice_7)
        view_207: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_98, [2, 16, 8]);  mm_98 = None
        slice_3243: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_586, 1, 784, 800)
        slice_3244: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3243, 2, 0, 16)
        add_100: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3244, view_207);  slice_3244 = view_207 = None
        slice_scatter_588: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3243, add_100, 2, 0, 16);  slice_3243 = add_100 = None
        slice_scatter_589: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_586, slice_scatter_588, 1, 784, 800);  slice_scatter_586 = slice_scatter_588 = None
        slice_3248: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_589, 1, 784, 800)
        slice_3249: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3248, 2, 0, 16)
        slice_scatter_591: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3248, slice_3249, 2, 0, 16);  slice_3248 = slice_3249 = None
        slice_scatter_592: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_589, slice_scatter_591, 1, 784, 800);  slice_scatter_589 = slice_scatter_591 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3269: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_3235, 2, 16, 32);  slice_3235 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_102: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_3269, memory_format = torch.contiguous_format);  slice_3269 = None
        view_208: "f32[32, 11]" = torch.ops.aten.view.default(clone_102, [32, 11]);  clone_102 = None
        mm_99: "f32[32, 8]" = torch.ops.aten.mm.default(view_208, slice_37)
        view_209: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_99, [2, 16, 8]);  mm_99 = None
        slice_3276: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_592, 1, 784, 800)
        slice_3277: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3276, 2, 0, 16)
        add_101: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3277, view_209);  slice_3277 = view_209 = None
        slice_scatter_594: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3276, add_101, 2, 0, 16);  slice_3276 = add_101 = None
        slice_scatter_595: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_592, slice_scatter_594, 1, 784, 800);  slice_scatter_592 = slice_scatter_594 = None
        slice_3281: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_595, 1, 784, 800)
        slice_3282: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3281, 2, 0, 16)
        slice_scatter_597: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3281, slice_3282, 2, 0, 16);  slice_3281 = slice_3282 = None
        slice_scatter_598: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_595, slice_scatter_597, 1, 784, 800);  slice_scatter_595 = slice_scatter_597 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3301: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 800, 816)
        slice_3302: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_3301, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_103: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_3302, memory_format = torch.contiguous_format);  slice_3302 = None
        view_210: "f32[32, 16]" = torch.ops.aten.view.default(clone_103, [32, 16]);  clone_103 = None
        mm_100: "f32[32, 8]" = torch.ops.aten.mm.default(view_210, slice_7)
        view_211: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_100, [2, 16, 8]);  mm_100 = None
        slice_3309: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_598, 1, 800, 816)
        slice_3310: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3309, 2, 0, 16)
        add_102: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3310, view_211);  slice_3310 = view_211 = None
        slice_scatter_600: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3309, add_102, 2, 0, 16);  slice_3309 = add_102 = None
        slice_scatter_601: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_598, slice_scatter_600, 1, 800, 816);  slice_scatter_598 = slice_scatter_600 = None
        slice_3314: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_601, 1, 800, 816)
        slice_3315: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3314, 2, 0, 16)
        slice_scatter_603: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3314, slice_3315, 2, 0, 16);  slice_3314 = slice_3315 = None
        slice_scatter_604: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_601, slice_scatter_603, 1, 800, 816);  slice_scatter_601 = slice_scatter_603 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3335: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_3301, 2, 16, 32);  slice_3301 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_104: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_3335, memory_format = torch.contiguous_format);  slice_3335 = None
        view_212: "f32[32, 11]" = torch.ops.aten.view.default(clone_104, [32, 11]);  clone_104 = None
        mm_101: "f32[32, 8]" = torch.ops.aten.mm.default(view_212, slice_37)
        view_213: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_101, [2, 16, 8]);  mm_101 = None
        slice_3342: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_604, 1, 800, 816)
        slice_3343: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3342, 2, 0, 16)
        add_103: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3343, view_213);  slice_3343 = view_213 = None
        slice_scatter_606: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3342, add_103, 2, 0, 16);  slice_3342 = add_103 = None
        slice_scatter_607: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_604, slice_scatter_606, 1, 800, 816);  slice_scatter_604 = slice_scatter_606 = None
        slice_3347: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_607, 1, 800, 816)
        slice_3348: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3347, 2, 0, 16)
        slice_scatter_609: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3347, slice_3348, 2, 0, 16);  slice_3347 = slice_3348 = None
        slice_scatter_610: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_607, slice_scatter_609, 1, 800, 816);  slice_scatter_607 = slice_scatter_609 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3367: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 816, 832)
        slice_3368: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_3367, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_105: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_3368, memory_format = torch.contiguous_format);  slice_3368 = None
        view_214: "f32[32, 16]" = torch.ops.aten.view.default(clone_105, [32, 16]);  clone_105 = None
        mm_102: "f32[32, 8]" = torch.ops.aten.mm.default(view_214, slice_7)
        view_215: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_102, [2, 16, 8]);  mm_102 = None
        slice_3375: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_610, 1, 816, 832)
        slice_3376: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3375, 2, 0, 16)
        add_104: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3376, view_215);  slice_3376 = view_215 = None
        slice_scatter_612: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3375, add_104, 2, 0, 16);  slice_3375 = add_104 = None
        slice_scatter_613: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_610, slice_scatter_612, 1, 816, 832);  slice_scatter_610 = slice_scatter_612 = None
        slice_3380: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_613, 1, 816, 832)
        slice_3381: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3380, 2, 0, 16)
        slice_scatter_615: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3380, slice_3381, 2, 0, 16);  slice_3380 = slice_3381 = None
        slice_scatter_616: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_613, slice_scatter_615, 1, 816, 832);  slice_scatter_613 = slice_scatter_615 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3401: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_3367, 2, 16, 32);  slice_3367 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_106: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_3401, memory_format = torch.contiguous_format);  slice_3401 = None
        view_216: "f32[32, 11]" = torch.ops.aten.view.default(clone_106, [32, 11]);  clone_106 = None
        mm_103: "f32[32, 8]" = torch.ops.aten.mm.default(view_216, slice_37)
        view_217: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_103, [2, 16, 8]);  mm_103 = None
        slice_3408: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_616, 1, 816, 832)
        slice_3409: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3408, 2, 0, 16)
        add_105: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3409, view_217);  slice_3409 = view_217 = None
        slice_scatter_618: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3408, add_105, 2, 0, 16);  slice_3408 = add_105 = None
        slice_scatter_619: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_616, slice_scatter_618, 1, 816, 832);  slice_scatter_616 = slice_scatter_618 = None
        slice_3413: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_619, 1, 816, 832)
        slice_3414: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3413, 2, 0, 16)
        slice_scatter_621: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3413, slice_3414, 2, 0, 16);  slice_3413 = slice_3414 = None
        slice_scatter_622: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_619, slice_scatter_621, 1, 816, 832);  slice_scatter_619 = slice_scatter_621 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3433: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 832, 848)
        slice_3434: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_3433, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_107: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_3434, memory_format = torch.contiguous_format);  slice_3434 = None
        view_218: "f32[32, 16]" = torch.ops.aten.view.default(clone_107, [32, 16]);  clone_107 = None
        mm_104: "f32[32, 8]" = torch.ops.aten.mm.default(view_218, slice_7)
        view_219: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_104, [2, 16, 8]);  mm_104 = None
        slice_3441: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_622, 1, 832, 848)
        slice_3442: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3441, 2, 0, 16)
        add_106: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3442, view_219);  slice_3442 = view_219 = None
        slice_scatter_624: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3441, add_106, 2, 0, 16);  slice_3441 = add_106 = None
        slice_scatter_625: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_622, slice_scatter_624, 1, 832, 848);  slice_scatter_622 = slice_scatter_624 = None
        slice_3446: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_625, 1, 832, 848)
        slice_3447: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3446, 2, 0, 16)
        slice_scatter_627: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3446, slice_3447, 2, 0, 16);  slice_3446 = slice_3447 = None
        slice_scatter_628: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_625, slice_scatter_627, 1, 832, 848);  slice_scatter_625 = slice_scatter_627 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3467: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_3433, 2, 16, 32);  slice_3433 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_108: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_3467, memory_format = torch.contiguous_format);  slice_3467 = None
        view_220: "f32[32, 11]" = torch.ops.aten.view.default(clone_108, [32, 11]);  clone_108 = None
        mm_105: "f32[32, 8]" = torch.ops.aten.mm.default(view_220, slice_37)
        view_221: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_105, [2, 16, 8]);  mm_105 = None
        slice_3474: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_628, 1, 832, 848)
        slice_3475: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3474, 2, 0, 16)
        add_107: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3475, view_221);  slice_3475 = view_221 = None
        slice_scatter_630: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3474, add_107, 2, 0, 16);  slice_3474 = add_107 = None
        slice_scatter_631: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_628, slice_scatter_630, 1, 832, 848);  slice_scatter_628 = slice_scatter_630 = None
        slice_3479: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_631, 1, 832, 848)
        slice_3480: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3479, 2, 0, 16)
        slice_scatter_633: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3479, slice_3480, 2, 0, 16);  slice_3479 = slice_3480 = None
        slice_scatter_634: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_631, slice_scatter_633, 1, 832, 848);  slice_scatter_631 = slice_scatter_633 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3499: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 848, 864)
        slice_3500: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_3499, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_109: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_3500, memory_format = torch.contiguous_format);  slice_3500 = None
        view_222: "f32[32, 16]" = torch.ops.aten.view.default(clone_109, [32, 16]);  clone_109 = None
        mm_106: "f32[32, 8]" = torch.ops.aten.mm.default(view_222, slice_7)
        view_223: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_106, [2, 16, 8]);  mm_106 = None
        slice_3507: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_634, 1, 848, 864)
        slice_3508: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3507, 2, 0, 16)
        add_108: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3508, view_223);  slice_3508 = view_223 = None
        slice_scatter_636: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3507, add_108, 2, 0, 16);  slice_3507 = add_108 = None
        slice_scatter_637: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_634, slice_scatter_636, 1, 848, 864);  slice_scatter_634 = slice_scatter_636 = None
        slice_3512: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_637, 1, 848, 864)
        slice_3513: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3512, 2, 0, 16)
        slice_scatter_639: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3512, slice_3513, 2, 0, 16);  slice_3512 = slice_3513 = None
        slice_scatter_640: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_637, slice_scatter_639, 1, 848, 864);  slice_scatter_637 = slice_scatter_639 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3533: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_3499, 2, 16, 32);  slice_3499 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_110: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_3533, memory_format = torch.contiguous_format);  slice_3533 = None
        view_224: "f32[32, 11]" = torch.ops.aten.view.default(clone_110, [32, 11]);  clone_110 = None
        mm_107: "f32[32, 8]" = torch.ops.aten.mm.default(view_224, slice_37)
        view_225: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_107, [2, 16, 8]);  mm_107 = None
        slice_3540: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_640, 1, 848, 864)
        slice_3541: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3540, 2, 0, 16)
        add_109: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3541, view_225);  slice_3541 = view_225 = None
        slice_scatter_642: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3540, add_109, 2, 0, 16);  slice_3540 = add_109 = None
        slice_scatter_643: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_640, slice_scatter_642, 1, 848, 864);  slice_scatter_640 = slice_scatter_642 = None
        slice_3545: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_643, 1, 848, 864)
        slice_3546: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3545, 2, 0, 16)
        slice_scatter_645: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3545, slice_3546, 2, 0, 16);  slice_3545 = slice_3546 = None
        slice_scatter_646: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_643, slice_scatter_645, 1, 848, 864);  slice_scatter_643 = slice_scatter_645 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3565: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 864, 880)
        slice_3566: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_3565, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_111: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_3566, memory_format = torch.contiguous_format);  slice_3566 = None
        view_226: "f32[32, 16]" = torch.ops.aten.view.default(clone_111, [32, 16]);  clone_111 = None
        mm_108: "f32[32, 8]" = torch.ops.aten.mm.default(view_226, slice_7)
        view_227: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_108, [2, 16, 8]);  mm_108 = None
        slice_3573: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_646, 1, 864, 880)
        slice_3574: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3573, 2, 0, 16)
        add_110: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3574, view_227);  slice_3574 = view_227 = None
        slice_scatter_648: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3573, add_110, 2, 0, 16);  slice_3573 = add_110 = None
        slice_scatter_649: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_646, slice_scatter_648, 1, 864, 880);  slice_scatter_646 = slice_scatter_648 = None
        slice_3578: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_649, 1, 864, 880)
        slice_3579: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3578, 2, 0, 16)
        slice_scatter_651: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3578, slice_3579, 2, 0, 16);  slice_3578 = slice_3579 = None
        slice_scatter_652: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_649, slice_scatter_651, 1, 864, 880);  slice_scatter_649 = slice_scatter_651 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3599: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_3565, 2, 16, 32);  slice_3565 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_112: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_3599, memory_format = torch.contiguous_format);  slice_3599 = None
        view_228: "f32[32, 11]" = torch.ops.aten.view.default(clone_112, [32, 11]);  clone_112 = None
        mm_109: "f32[32, 8]" = torch.ops.aten.mm.default(view_228, slice_37)
        view_229: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_109, [2, 16, 8]);  mm_109 = None
        slice_3606: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_652, 1, 864, 880)
        slice_3607: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3606, 2, 0, 16)
        add_111: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3607, view_229);  slice_3607 = view_229 = None
        slice_scatter_654: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3606, add_111, 2, 0, 16);  slice_3606 = add_111 = None
        slice_scatter_655: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_652, slice_scatter_654, 1, 864, 880);  slice_scatter_652 = slice_scatter_654 = None
        slice_3611: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_655, 1, 864, 880)
        slice_3612: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3611, 2, 0, 16)
        slice_scatter_657: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3611, slice_3612, 2, 0, 16);  slice_3611 = slice_3612 = None
        slice_scatter_658: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_655, slice_scatter_657, 1, 864, 880);  slice_scatter_655 = slice_scatter_657 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3631: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 880, 896)
        slice_3632: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_3631, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_113: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_3632, memory_format = torch.contiguous_format);  slice_3632 = None
        view_230: "f32[32, 16]" = torch.ops.aten.view.default(clone_113, [32, 16]);  clone_113 = None
        mm_110: "f32[32, 8]" = torch.ops.aten.mm.default(view_230, slice_7)
        view_231: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_110, [2, 16, 8]);  mm_110 = None
        slice_3639: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_658, 1, 880, 896)
        slice_3640: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3639, 2, 0, 16)
        add_112: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3640, view_231);  slice_3640 = view_231 = None
        slice_scatter_660: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3639, add_112, 2, 0, 16);  slice_3639 = add_112 = None
        slice_scatter_661: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_658, slice_scatter_660, 1, 880, 896);  slice_scatter_658 = slice_scatter_660 = None
        slice_3644: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_661, 1, 880, 896)
        slice_3645: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3644, 2, 0, 16)
        slice_scatter_663: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3644, slice_3645, 2, 0, 16);  slice_3644 = slice_3645 = None
        slice_scatter_664: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_661, slice_scatter_663, 1, 880, 896);  slice_scatter_661 = slice_scatter_663 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3665: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_3631, 2, 16, 32);  slice_3631 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_114: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_3665, memory_format = torch.contiguous_format);  slice_3665 = None
        view_232: "f32[32, 11]" = torch.ops.aten.view.default(clone_114, [32, 11]);  clone_114 = None
        mm_111: "f32[32, 8]" = torch.ops.aten.mm.default(view_232, slice_37)
        view_233: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_111, [2, 16, 8]);  mm_111 = None
        slice_3672: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_664, 1, 880, 896)
        slice_3673: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3672, 2, 0, 16)
        add_113: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3673, view_233);  slice_3673 = view_233 = None
        slice_scatter_666: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3672, add_113, 2, 0, 16);  slice_3672 = add_113 = None
        slice_scatter_667: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_664, slice_scatter_666, 1, 880, 896);  slice_scatter_664 = slice_scatter_666 = None
        slice_3677: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_667, 1, 880, 896)
        slice_3678: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3677, 2, 0, 16)
        slice_scatter_669: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3677, slice_3678, 2, 0, 16);  slice_3677 = slice_3678 = None
        slice_scatter_670: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_667, slice_scatter_669, 1, 880, 896);  slice_scatter_667 = slice_scatter_669 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3697: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 896, 912)
        slice_3698: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_3697, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_115: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_3698, memory_format = torch.contiguous_format);  slice_3698 = None
        view_234: "f32[32, 16]" = torch.ops.aten.view.default(clone_115, [32, 16]);  clone_115 = None
        mm_112: "f32[32, 8]" = torch.ops.aten.mm.default(view_234, slice_7)
        view_235: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_112, [2, 16, 8]);  mm_112 = None
        slice_3705: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_670, 1, 896, 912)
        slice_3706: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3705, 2, 0, 16)
        add_114: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3706, view_235);  slice_3706 = view_235 = None
        slice_scatter_672: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3705, add_114, 2, 0, 16);  slice_3705 = add_114 = None
        slice_scatter_673: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_670, slice_scatter_672, 1, 896, 912);  slice_scatter_670 = slice_scatter_672 = None
        slice_3710: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_673, 1, 896, 912)
        slice_3711: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3710, 2, 0, 16)
        slice_scatter_675: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3710, slice_3711, 2, 0, 16);  slice_3710 = slice_3711 = None
        slice_scatter_676: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_673, slice_scatter_675, 1, 896, 912);  slice_scatter_673 = slice_scatter_675 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3731: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_3697, 2, 16, 32);  slice_3697 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_116: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_3731, memory_format = torch.contiguous_format);  slice_3731 = None
        view_236: "f32[32, 11]" = torch.ops.aten.view.default(clone_116, [32, 11]);  clone_116 = None
        mm_113: "f32[32, 8]" = torch.ops.aten.mm.default(view_236, slice_37)
        view_237: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_113, [2, 16, 8]);  mm_113 = None
        slice_3738: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_676, 1, 896, 912)
        slice_3739: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3738, 2, 0, 16)
        add_115: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3739, view_237);  slice_3739 = view_237 = None
        slice_scatter_678: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3738, add_115, 2, 0, 16);  slice_3738 = add_115 = None
        slice_scatter_679: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_676, slice_scatter_678, 1, 896, 912);  slice_scatter_676 = slice_scatter_678 = None
        slice_3743: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_679, 1, 896, 912)
        slice_3744: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3743, 2, 0, 16)
        slice_scatter_681: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3743, slice_3744, 2, 0, 16);  slice_3743 = slice_3744 = None
        slice_scatter_682: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_679, slice_scatter_681, 1, 896, 912);  slice_scatter_679 = slice_scatter_681 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3763: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 912, 928)
        slice_3764: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_3763, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_117: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_3764, memory_format = torch.contiguous_format);  slice_3764 = None
        view_238: "f32[32, 16]" = torch.ops.aten.view.default(clone_117, [32, 16]);  clone_117 = None
        mm_114: "f32[32, 8]" = torch.ops.aten.mm.default(view_238, slice_7)
        view_239: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_114, [2, 16, 8]);  mm_114 = None
        slice_3771: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_682, 1, 912, 928)
        slice_3772: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3771, 2, 0, 16)
        add_116: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3772, view_239);  slice_3772 = view_239 = None
        slice_scatter_684: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3771, add_116, 2, 0, 16);  slice_3771 = add_116 = None
        slice_scatter_685: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_682, slice_scatter_684, 1, 912, 928);  slice_scatter_682 = slice_scatter_684 = None
        slice_3776: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_685, 1, 912, 928)
        slice_3777: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3776, 2, 0, 16)
        slice_scatter_687: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3776, slice_3777, 2, 0, 16);  slice_3776 = slice_3777 = None
        slice_scatter_688: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_685, slice_scatter_687, 1, 912, 928);  slice_scatter_685 = slice_scatter_687 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3797: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_3763, 2, 16, 32);  slice_3763 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_118: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_3797, memory_format = torch.contiguous_format);  slice_3797 = None
        view_240: "f32[32, 11]" = torch.ops.aten.view.default(clone_118, [32, 11]);  clone_118 = None
        mm_115: "f32[32, 8]" = torch.ops.aten.mm.default(view_240, slice_37)
        view_241: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_115, [2, 16, 8]);  mm_115 = None
        slice_3804: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_688, 1, 912, 928)
        slice_3805: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3804, 2, 0, 16)
        add_117: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3805, view_241);  slice_3805 = view_241 = None
        slice_scatter_690: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3804, add_117, 2, 0, 16);  slice_3804 = add_117 = None
        slice_scatter_691: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_688, slice_scatter_690, 1, 912, 928);  slice_scatter_688 = slice_scatter_690 = None
        slice_3809: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_691, 1, 912, 928)
        slice_3810: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3809, 2, 0, 16)
        slice_scatter_693: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3809, slice_3810, 2, 0, 16);  slice_3809 = slice_3810 = None
        slice_scatter_694: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_691, slice_scatter_693, 1, 912, 928);  slice_scatter_691 = slice_scatter_693 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3829: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 928, 944)
        slice_3830: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_3829, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_119: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_3830, memory_format = torch.contiguous_format);  slice_3830 = None
        view_242: "f32[32, 16]" = torch.ops.aten.view.default(clone_119, [32, 16]);  clone_119 = None
        mm_116: "f32[32, 8]" = torch.ops.aten.mm.default(view_242, slice_7)
        view_243: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_116, [2, 16, 8]);  mm_116 = None
        slice_3837: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_694, 1, 928, 944)
        slice_3838: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3837, 2, 0, 16)
        add_118: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3838, view_243);  slice_3838 = view_243 = None
        slice_scatter_696: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3837, add_118, 2, 0, 16);  slice_3837 = add_118 = None
        slice_scatter_697: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_694, slice_scatter_696, 1, 928, 944);  slice_scatter_694 = slice_scatter_696 = None
        slice_3842: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_697, 1, 928, 944)
        slice_3843: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3842, 2, 0, 16)
        slice_scatter_699: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3842, slice_3843, 2, 0, 16);  slice_3842 = slice_3843 = None
        slice_scatter_700: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_697, slice_scatter_699, 1, 928, 944);  slice_scatter_697 = slice_scatter_699 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3863: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_3829, 2, 16, 32);  slice_3829 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_120: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_3863, memory_format = torch.contiguous_format);  slice_3863 = None
        view_244: "f32[32, 11]" = torch.ops.aten.view.default(clone_120, [32, 11]);  clone_120 = None
        mm_117: "f32[32, 8]" = torch.ops.aten.mm.default(view_244, slice_37)
        view_245: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_117, [2, 16, 8]);  mm_117 = None
        slice_3870: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_700, 1, 928, 944)
        slice_3871: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3870, 2, 0, 16)
        add_119: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3871, view_245);  slice_3871 = view_245 = None
        slice_scatter_702: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3870, add_119, 2, 0, 16);  slice_3870 = add_119 = None
        slice_scatter_703: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_700, slice_scatter_702, 1, 928, 944);  slice_scatter_700 = slice_scatter_702 = None
        slice_3875: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_703, 1, 928, 944)
        slice_3876: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3875, 2, 0, 16)
        slice_scatter_705: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3875, slice_3876, 2, 0, 16);  slice_3875 = slice_3876 = None
        slice_scatter_706: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_703, slice_scatter_705, 1, 928, 944);  slice_scatter_703 = slice_scatter_705 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3895: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 944, 960)
        slice_3896: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_3895, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_121: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_3896, memory_format = torch.contiguous_format);  slice_3896 = None
        view_246: "f32[32, 16]" = torch.ops.aten.view.default(clone_121, [32, 16]);  clone_121 = None
        mm_118: "f32[32, 8]" = torch.ops.aten.mm.default(view_246, slice_7)
        view_247: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_118, [2, 16, 8]);  mm_118 = None
        slice_3903: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_706, 1, 944, 960)
        slice_3904: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3903, 2, 0, 16)
        add_120: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3904, view_247);  slice_3904 = view_247 = None
        slice_scatter_708: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3903, add_120, 2, 0, 16);  slice_3903 = add_120 = None
        slice_scatter_709: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_706, slice_scatter_708, 1, 944, 960);  slice_scatter_706 = slice_scatter_708 = None
        slice_3908: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_709, 1, 944, 960)
        slice_3909: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3908, 2, 0, 16)
        slice_scatter_711: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3908, slice_3909, 2, 0, 16);  slice_3908 = slice_3909 = None
        slice_scatter_712: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_709, slice_scatter_711, 1, 944, 960);  slice_scatter_709 = slice_scatter_711 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3929: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_3895, 2, 16, 32);  slice_3895 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_122: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_3929, memory_format = torch.contiguous_format);  slice_3929 = None
        view_248: "f32[32, 11]" = torch.ops.aten.view.default(clone_122, [32, 11]);  clone_122 = None
        mm_119: "f32[32, 8]" = torch.ops.aten.mm.default(view_248, slice_37)
        view_249: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_119, [2, 16, 8]);  mm_119 = None
        slice_3936: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_712, 1, 944, 960)
        slice_3937: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3936, 2, 0, 16)
        add_121: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3937, view_249);  slice_3937 = view_249 = None
        slice_scatter_714: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3936, add_121, 2, 0, 16);  slice_3936 = add_121 = None
        slice_scatter_715: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_712, slice_scatter_714, 1, 944, 960);  slice_scatter_712 = slice_scatter_714 = None
        slice_3941: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_715, 1, 944, 960)
        slice_3942: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3941, 2, 0, 16)
        slice_scatter_717: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3941, slice_3942, 2, 0, 16);  slice_3941 = slice_3942 = None
        slice_scatter_718: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_715, slice_scatter_717, 1, 944, 960);  slice_scatter_715 = slice_scatter_717 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3961: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 960, 976)
        slice_3962: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_3961, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_123: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_3962, memory_format = torch.contiguous_format);  slice_3962 = None
        view_250: "f32[32, 16]" = torch.ops.aten.view.default(clone_123, [32, 16]);  clone_123 = None
        mm_120: "f32[32, 8]" = torch.ops.aten.mm.default(view_250, slice_7)
        view_251: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_120, [2, 16, 8]);  mm_120 = None
        slice_3969: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_718, 1, 960, 976)
        slice_3970: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3969, 2, 0, 16)
        add_122: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_3970, view_251);  slice_3970 = view_251 = None
        slice_scatter_720: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3969, add_122, 2, 0, 16);  slice_3969 = add_122 = None
        slice_scatter_721: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_718, slice_scatter_720, 1, 960, 976);  slice_scatter_718 = slice_scatter_720 = None
        slice_3974: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_721, 1, 960, 976)
        slice_3975: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_3974, 2, 0, 16)
        slice_scatter_723: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_3974, slice_3975, 2, 0, 16);  slice_3974 = slice_3975 = None
        slice_scatter_724: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_721, slice_scatter_723, 1, 960, 976);  slice_scatter_721 = slice_scatter_723 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_3995: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_3961, 2, 16, 32);  slice_3961 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_124: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_3995, memory_format = torch.contiguous_format);  slice_3995 = None
        view_252: "f32[32, 11]" = torch.ops.aten.view.default(clone_124, [32, 11]);  clone_124 = None
        mm_121: "f32[32, 8]" = torch.ops.aten.mm.default(view_252, slice_37)
        view_253: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_121, [2, 16, 8]);  mm_121 = None
        slice_4002: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_724, 1, 960, 976)
        slice_4003: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4002, 2, 0, 16)
        add_123: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4003, view_253);  slice_4003 = view_253 = None
        slice_scatter_726: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4002, add_123, 2, 0, 16);  slice_4002 = add_123 = None
        slice_scatter_727: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_724, slice_scatter_726, 1, 960, 976);  slice_scatter_724 = slice_scatter_726 = None
        slice_4007: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_727, 1, 960, 976)
        slice_4008: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4007, 2, 0, 16)
        slice_scatter_729: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4007, slice_4008, 2, 0, 16);  slice_4007 = slice_4008 = None
        slice_scatter_730: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_727, slice_scatter_729, 1, 960, 976);  slice_scatter_727 = slice_scatter_729 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4027: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 976, 992)
        slice_4028: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_4027, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_125: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_4028, memory_format = torch.contiguous_format);  slice_4028 = None
        view_254: "f32[32, 16]" = torch.ops.aten.view.default(clone_125, [32, 16]);  clone_125 = None
        mm_122: "f32[32, 8]" = torch.ops.aten.mm.default(view_254, slice_7)
        view_255: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_122, [2, 16, 8]);  mm_122 = None
        slice_4035: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_730, 1, 976, 992)
        slice_4036: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4035, 2, 0, 16)
        add_124: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4036, view_255);  slice_4036 = view_255 = None
        slice_scatter_732: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4035, add_124, 2, 0, 16);  slice_4035 = add_124 = None
        slice_scatter_733: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_730, slice_scatter_732, 1, 976, 992);  slice_scatter_730 = slice_scatter_732 = None
        slice_4040: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_733, 1, 976, 992)
        slice_4041: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4040, 2, 0, 16)
        slice_scatter_735: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4040, slice_4041, 2, 0, 16);  slice_4040 = slice_4041 = None
        slice_scatter_736: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_733, slice_scatter_735, 1, 976, 992);  slice_scatter_733 = slice_scatter_735 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4061: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_4027, 2, 16, 32);  slice_4027 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_126: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_4061, memory_format = torch.contiguous_format);  slice_4061 = None
        view_256: "f32[32, 11]" = torch.ops.aten.view.default(clone_126, [32, 11]);  clone_126 = None
        mm_123: "f32[32, 8]" = torch.ops.aten.mm.default(view_256, slice_37)
        view_257: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_123, [2, 16, 8]);  mm_123 = None
        slice_4068: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_736, 1, 976, 992)
        slice_4069: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4068, 2, 0, 16)
        add_125: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4069, view_257);  slice_4069 = view_257 = None
        slice_scatter_738: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4068, add_125, 2, 0, 16);  slice_4068 = add_125 = None
        slice_scatter_739: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_736, slice_scatter_738, 1, 976, 992);  slice_scatter_736 = slice_scatter_738 = None
        slice_4073: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_739, 1, 976, 992)
        slice_4074: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4073, 2, 0, 16)
        slice_scatter_741: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4073, slice_4074, 2, 0, 16);  slice_4073 = slice_4074 = None
        slice_scatter_742: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_739, slice_scatter_741, 1, 976, 992);  slice_scatter_739 = slice_scatter_741 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4093: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 992, 1008)
        slice_4094: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_4093, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_127: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_4094, memory_format = torch.contiguous_format);  slice_4094 = None
        view_258: "f32[32, 16]" = torch.ops.aten.view.default(clone_127, [32, 16]);  clone_127 = None
        mm_124: "f32[32, 8]" = torch.ops.aten.mm.default(view_258, slice_7)
        view_259: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_124, [2, 16, 8]);  mm_124 = None
        slice_4101: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_742, 1, 992, 1008)
        slice_4102: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4101, 2, 0, 16)
        add_126: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4102, view_259);  slice_4102 = view_259 = None
        slice_scatter_744: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4101, add_126, 2, 0, 16);  slice_4101 = add_126 = None
        slice_scatter_745: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_742, slice_scatter_744, 1, 992, 1008);  slice_scatter_742 = slice_scatter_744 = None
        slice_4106: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_745, 1, 992, 1008)
        slice_4107: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4106, 2, 0, 16)
        slice_scatter_747: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4106, slice_4107, 2, 0, 16);  slice_4106 = slice_4107 = None
        slice_scatter_748: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_745, slice_scatter_747, 1, 992, 1008);  slice_scatter_745 = slice_scatter_747 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4127: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_4093, 2, 16, 32);  slice_4093 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_128: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_4127, memory_format = torch.contiguous_format);  slice_4127 = None
        view_260: "f32[32, 11]" = torch.ops.aten.view.default(clone_128, [32, 11]);  clone_128 = None
        mm_125: "f32[32, 8]" = torch.ops.aten.mm.default(view_260, slice_37)
        view_261: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_125, [2, 16, 8]);  mm_125 = None
        slice_4134: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_748, 1, 992, 1008)
        slice_4135: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4134, 2, 0, 16)
        add_127: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4135, view_261);  slice_4135 = view_261 = None
        slice_scatter_750: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4134, add_127, 2, 0, 16);  slice_4134 = add_127 = None
        slice_scatter_751: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_748, slice_scatter_750, 1, 992, 1008);  slice_scatter_748 = slice_scatter_750 = None
        slice_4139: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_751, 1, 992, 1008)
        slice_4140: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4139, 2, 0, 16)
        slice_scatter_753: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4139, slice_4140, 2, 0, 16);  slice_4139 = slice_4140 = None
        slice_scatter_754: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_751, slice_scatter_753, 1, 992, 1008);  slice_scatter_751 = slice_scatter_753 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4159: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1008, 1024)
        slice_4160: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_4159, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_129: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_4160, memory_format = torch.contiguous_format);  slice_4160 = None
        view_262: "f32[32, 16]" = torch.ops.aten.view.default(clone_129, [32, 16]);  clone_129 = None
        mm_126: "f32[32, 8]" = torch.ops.aten.mm.default(view_262, slice_7)
        view_263: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_126, [2, 16, 8]);  mm_126 = None
        slice_4167: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_754, 1, 1008, 1024)
        slice_4168: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4167, 2, 0, 16)
        add_128: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4168, view_263);  slice_4168 = view_263 = None
        slice_scatter_756: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4167, add_128, 2, 0, 16);  slice_4167 = add_128 = None
        slice_scatter_757: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_754, slice_scatter_756, 1, 1008, 1024);  slice_scatter_754 = slice_scatter_756 = None
        slice_4172: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_757, 1, 1008, 1024)
        slice_4173: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4172, 2, 0, 16)
        slice_scatter_759: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4172, slice_4173, 2, 0, 16);  slice_4172 = slice_4173 = None
        slice_scatter_760: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_757, slice_scatter_759, 1, 1008, 1024);  slice_scatter_757 = slice_scatter_759 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4193: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_4159, 2, 16, 32);  slice_4159 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_130: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_4193, memory_format = torch.contiguous_format);  slice_4193 = None
        view_264: "f32[32, 11]" = torch.ops.aten.view.default(clone_130, [32, 11]);  clone_130 = None
        mm_127: "f32[32, 8]" = torch.ops.aten.mm.default(view_264, slice_37)
        view_265: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_127, [2, 16, 8]);  mm_127 = None
        slice_4200: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_760, 1, 1008, 1024)
        slice_4201: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4200, 2, 0, 16)
        add_129: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4201, view_265);  slice_4201 = view_265 = None
        slice_scatter_762: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4200, add_129, 2, 0, 16);  slice_4200 = add_129 = None
        slice_scatter_763: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_760, slice_scatter_762, 1, 1008, 1024);  slice_scatter_760 = slice_scatter_762 = None
        slice_4205: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_763, 1, 1008, 1024)
        slice_4206: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4205, 2, 0, 16)
        slice_scatter_765: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4205, slice_4206, 2, 0, 16);  slice_4205 = slice_4206 = None
        slice_scatter_766: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_763, slice_scatter_765, 1, 1008, 1024);  slice_scatter_763 = slice_scatter_765 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4225: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1024, 1040)
        slice_4226: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_4225, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_131: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_4226, memory_format = torch.contiguous_format);  slice_4226 = None
        view_266: "f32[32, 16]" = torch.ops.aten.view.default(clone_131, [32, 16]);  clone_131 = None
        mm_128: "f32[32, 8]" = torch.ops.aten.mm.default(view_266, slice_7)
        view_267: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_128, [2, 16, 8]);  mm_128 = None
        slice_4233: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_766, 1, 1024, 1040)
        slice_4234: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4233, 2, 0, 16)
        add_130: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4234, view_267);  slice_4234 = view_267 = None
        slice_scatter_768: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4233, add_130, 2, 0, 16);  slice_4233 = add_130 = None
        slice_scatter_769: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_766, slice_scatter_768, 1, 1024, 1040);  slice_scatter_766 = slice_scatter_768 = None
        slice_4238: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_769, 1, 1024, 1040)
        slice_4239: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4238, 2, 0, 16)
        slice_scatter_771: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4238, slice_4239, 2, 0, 16);  slice_4238 = slice_4239 = None
        slice_scatter_772: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_769, slice_scatter_771, 1, 1024, 1040);  slice_scatter_769 = slice_scatter_771 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4259: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_4225, 2, 16, 32);  slice_4225 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_132: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_4259, memory_format = torch.contiguous_format);  slice_4259 = None
        view_268: "f32[32, 11]" = torch.ops.aten.view.default(clone_132, [32, 11]);  clone_132 = None
        mm_129: "f32[32, 8]" = torch.ops.aten.mm.default(view_268, slice_37)
        view_269: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_129, [2, 16, 8]);  mm_129 = None
        slice_4266: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_772, 1, 1024, 1040)
        slice_4267: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4266, 2, 0, 16)
        add_131: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4267, view_269);  slice_4267 = view_269 = None
        slice_scatter_774: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4266, add_131, 2, 0, 16);  slice_4266 = add_131 = None
        slice_scatter_775: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_772, slice_scatter_774, 1, 1024, 1040);  slice_scatter_772 = slice_scatter_774 = None
        slice_4271: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_775, 1, 1024, 1040)
        slice_4272: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4271, 2, 0, 16)
        slice_scatter_777: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4271, slice_4272, 2, 0, 16);  slice_4271 = slice_4272 = None
        slice_scatter_778: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_775, slice_scatter_777, 1, 1024, 1040);  slice_scatter_775 = slice_scatter_777 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4291: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1040, 1056)
        slice_4292: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_4291, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_133: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_4292, memory_format = torch.contiguous_format);  slice_4292 = None
        view_270: "f32[32, 16]" = torch.ops.aten.view.default(clone_133, [32, 16]);  clone_133 = None
        mm_130: "f32[32, 8]" = torch.ops.aten.mm.default(view_270, slice_7)
        view_271: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_130, [2, 16, 8]);  mm_130 = None
        slice_4299: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_778, 1, 1040, 1056)
        slice_4300: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4299, 2, 0, 16)
        add_132: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4300, view_271);  slice_4300 = view_271 = None
        slice_scatter_780: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4299, add_132, 2, 0, 16);  slice_4299 = add_132 = None
        slice_scatter_781: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_778, slice_scatter_780, 1, 1040, 1056);  slice_scatter_778 = slice_scatter_780 = None
        slice_4304: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_781, 1, 1040, 1056)
        slice_4305: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4304, 2, 0, 16)
        slice_scatter_783: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4304, slice_4305, 2, 0, 16);  slice_4304 = slice_4305 = None
        slice_scatter_784: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_781, slice_scatter_783, 1, 1040, 1056);  slice_scatter_781 = slice_scatter_783 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4325: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_4291, 2, 16, 32);  slice_4291 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_134: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_4325, memory_format = torch.contiguous_format);  slice_4325 = None
        view_272: "f32[32, 11]" = torch.ops.aten.view.default(clone_134, [32, 11]);  clone_134 = None
        mm_131: "f32[32, 8]" = torch.ops.aten.mm.default(view_272, slice_37)
        view_273: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_131, [2, 16, 8]);  mm_131 = None
        slice_4332: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_784, 1, 1040, 1056)
        slice_4333: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4332, 2, 0, 16)
        add_133: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4333, view_273);  slice_4333 = view_273 = None
        slice_scatter_786: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4332, add_133, 2, 0, 16);  slice_4332 = add_133 = None
        slice_scatter_787: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_784, slice_scatter_786, 1, 1040, 1056);  slice_scatter_784 = slice_scatter_786 = None
        slice_4337: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_787, 1, 1040, 1056)
        slice_4338: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4337, 2, 0, 16)
        slice_scatter_789: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4337, slice_4338, 2, 0, 16);  slice_4337 = slice_4338 = None
        slice_scatter_790: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_787, slice_scatter_789, 1, 1040, 1056);  slice_scatter_787 = slice_scatter_789 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4357: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1056, 1072)
        slice_4358: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_4357, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_135: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_4358, memory_format = torch.contiguous_format);  slice_4358 = None
        view_274: "f32[32, 16]" = torch.ops.aten.view.default(clone_135, [32, 16]);  clone_135 = None
        mm_132: "f32[32, 8]" = torch.ops.aten.mm.default(view_274, slice_7)
        view_275: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_132, [2, 16, 8]);  mm_132 = None
        slice_4365: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_790, 1, 1056, 1072)
        slice_4366: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4365, 2, 0, 16)
        add_134: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4366, view_275);  slice_4366 = view_275 = None
        slice_scatter_792: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4365, add_134, 2, 0, 16);  slice_4365 = add_134 = None
        slice_scatter_793: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_790, slice_scatter_792, 1, 1056, 1072);  slice_scatter_790 = slice_scatter_792 = None
        slice_4370: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_793, 1, 1056, 1072)
        slice_4371: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4370, 2, 0, 16)
        slice_scatter_795: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4370, slice_4371, 2, 0, 16);  slice_4370 = slice_4371 = None
        slice_scatter_796: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_793, slice_scatter_795, 1, 1056, 1072);  slice_scatter_793 = slice_scatter_795 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4391: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_4357, 2, 16, 32);  slice_4357 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_136: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_4391, memory_format = torch.contiguous_format);  slice_4391 = None
        view_276: "f32[32, 11]" = torch.ops.aten.view.default(clone_136, [32, 11]);  clone_136 = None
        mm_133: "f32[32, 8]" = torch.ops.aten.mm.default(view_276, slice_37)
        view_277: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_133, [2, 16, 8]);  mm_133 = None
        slice_4398: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_796, 1, 1056, 1072)
        slice_4399: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4398, 2, 0, 16)
        add_135: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4399, view_277);  slice_4399 = view_277 = None
        slice_scatter_798: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4398, add_135, 2, 0, 16);  slice_4398 = add_135 = None
        slice_scatter_799: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_796, slice_scatter_798, 1, 1056, 1072);  slice_scatter_796 = slice_scatter_798 = None
        slice_4403: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_799, 1, 1056, 1072)
        slice_4404: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4403, 2, 0, 16)
        slice_scatter_801: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4403, slice_4404, 2, 0, 16);  slice_4403 = slice_4404 = None
        slice_scatter_802: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_799, slice_scatter_801, 1, 1056, 1072);  slice_scatter_799 = slice_scatter_801 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4423: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1072, 1088)
        slice_4424: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_4423, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_137: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_4424, memory_format = torch.contiguous_format);  slice_4424 = None
        view_278: "f32[32, 16]" = torch.ops.aten.view.default(clone_137, [32, 16]);  clone_137 = None
        mm_134: "f32[32, 8]" = torch.ops.aten.mm.default(view_278, slice_7)
        view_279: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_134, [2, 16, 8]);  mm_134 = None
        slice_4431: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_802, 1, 1072, 1088)
        slice_4432: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4431, 2, 0, 16)
        add_136: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4432, view_279);  slice_4432 = view_279 = None
        slice_scatter_804: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4431, add_136, 2, 0, 16);  slice_4431 = add_136 = None
        slice_scatter_805: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_802, slice_scatter_804, 1, 1072, 1088);  slice_scatter_802 = slice_scatter_804 = None
        slice_4436: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_805, 1, 1072, 1088)
        slice_4437: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4436, 2, 0, 16)
        slice_scatter_807: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4436, slice_4437, 2, 0, 16);  slice_4436 = slice_4437 = None
        slice_scatter_808: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_805, slice_scatter_807, 1, 1072, 1088);  slice_scatter_805 = slice_scatter_807 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4457: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_4423, 2, 16, 32);  slice_4423 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_138: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_4457, memory_format = torch.contiguous_format);  slice_4457 = None
        view_280: "f32[32, 11]" = torch.ops.aten.view.default(clone_138, [32, 11]);  clone_138 = None
        mm_135: "f32[32, 8]" = torch.ops.aten.mm.default(view_280, slice_37)
        view_281: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_135, [2, 16, 8]);  mm_135 = None
        slice_4464: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_808, 1, 1072, 1088)
        slice_4465: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4464, 2, 0, 16)
        add_137: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4465, view_281);  slice_4465 = view_281 = None
        slice_scatter_810: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4464, add_137, 2, 0, 16);  slice_4464 = add_137 = None
        slice_scatter_811: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_808, slice_scatter_810, 1, 1072, 1088);  slice_scatter_808 = slice_scatter_810 = None
        slice_4469: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_811, 1, 1072, 1088)
        slice_4470: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4469, 2, 0, 16)
        slice_scatter_813: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4469, slice_4470, 2, 0, 16);  slice_4469 = slice_4470 = None
        slice_scatter_814: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_811, slice_scatter_813, 1, 1072, 1088);  slice_scatter_811 = slice_scatter_813 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4489: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1088, 1104)
        slice_4490: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_4489, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_139: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_4490, memory_format = torch.contiguous_format);  slice_4490 = None
        view_282: "f32[32, 16]" = torch.ops.aten.view.default(clone_139, [32, 16]);  clone_139 = None
        mm_136: "f32[32, 8]" = torch.ops.aten.mm.default(view_282, slice_7)
        view_283: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_136, [2, 16, 8]);  mm_136 = None
        slice_4497: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_814, 1, 1088, 1104)
        slice_4498: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4497, 2, 0, 16)
        add_138: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4498, view_283);  slice_4498 = view_283 = None
        slice_scatter_816: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4497, add_138, 2, 0, 16);  slice_4497 = add_138 = None
        slice_scatter_817: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_814, slice_scatter_816, 1, 1088, 1104);  slice_scatter_814 = slice_scatter_816 = None
        slice_4502: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_817, 1, 1088, 1104)
        slice_4503: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4502, 2, 0, 16)
        slice_scatter_819: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4502, slice_4503, 2, 0, 16);  slice_4502 = slice_4503 = None
        slice_scatter_820: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_817, slice_scatter_819, 1, 1088, 1104);  slice_scatter_817 = slice_scatter_819 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4523: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_4489, 2, 16, 32);  slice_4489 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_140: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_4523, memory_format = torch.contiguous_format);  slice_4523 = None
        view_284: "f32[32, 11]" = torch.ops.aten.view.default(clone_140, [32, 11]);  clone_140 = None
        mm_137: "f32[32, 8]" = torch.ops.aten.mm.default(view_284, slice_37)
        view_285: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_137, [2, 16, 8]);  mm_137 = None
        slice_4530: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_820, 1, 1088, 1104)
        slice_4531: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4530, 2, 0, 16)
        add_139: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4531, view_285);  slice_4531 = view_285 = None
        slice_scatter_822: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4530, add_139, 2, 0, 16);  slice_4530 = add_139 = None
        slice_scatter_823: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_820, slice_scatter_822, 1, 1088, 1104);  slice_scatter_820 = slice_scatter_822 = None
        slice_4535: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_823, 1, 1088, 1104)
        slice_4536: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4535, 2, 0, 16)
        slice_scatter_825: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4535, slice_4536, 2, 0, 16);  slice_4535 = slice_4536 = None
        slice_scatter_826: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_823, slice_scatter_825, 1, 1088, 1104);  slice_scatter_823 = slice_scatter_825 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4555: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1104, 1120)
        slice_4556: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_4555, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_141: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_4556, memory_format = torch.contiguous_format);  slice_4556 = None
        view_286: "f32[32, 16]" = torch.ops.aten.view.default(clone_141, [32, 16]);  clone_141 = None
        mm_138: "f32[32, 8]" = torch.ops.aten.mm.default(view_286, slice_7)
        view_287: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_138, [2, 16, 8]);  mm_138 = None
        slice_4563: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_826, 1, 1104, 1120)
        slice_4564: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4563, 2, 0, 16)
        add_140: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4564, view_287);  slice_4564 = view_287 = None
        slice_scatter_828: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4563, add_140, 2, 0, 16);  slice_4563 = add_140 = None
        slice_scatter_829: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_826, slice_scatter_828, 1, 1104, 1120);  slice_scatter_826 = slice_scatter_828 = None
        slice_4568: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_829, 1, 1104, 1120)
        slice_4569: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4568, 2, 0, 16)
        slice_scatter_831: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4568, slice_4569, 2, 0, 16);  slice_4568 = slice_4569 = None
        slice_scatter_832: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_829, slice_scatter_831, 1, 1104, 1120);  slice_scatter_829 = slice_scatter_831 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4589: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_4555, 2, 16, 32);  slice_4555 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_142: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_4589, memory_format = torch.contiguous_format);  slice_4589 = None
        view_288: "f32[32, 11]" = torch.ops.aten.view.default(clone_142, [32, 11]);  clone_142 = None
        mm_139: "f32[32, 8]" = torch.ops.aten.mm.default(view_288, slice_37)
        view_289: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_139, [2, 16, 8]);  mm_139 = None
        slice_4596: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_832, 1, 1104, 1120)
        slice_4597: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4596, 2, 0, 16)
        add_141: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4597, view_289);  slice_4597 = view_289 = None
        slice_scatter_834: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4596, add_141, 2, 0, 16);  slice_4596 = add_141 = None
        slice_scatter_835: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_832, slice_scatter_834, 1, 1104, 1120);  slice_scatter_832 = slice_scatter_834 = None
        slice_4601: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_835, 1, 1104, 1120)
        slice_4602: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4601, 2, 0, 16)
        slice_scatter_837: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4601, slice_4602, 2, 0, 16);  slice_4601 = slice_4602 = None
        slice_scatter_838: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_835, slice_scatter_837, 1, 1104, 1120);  slice_scatter_835 = slice_scatter_837 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4621: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1120, 1136)
        slice_4622: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_4621, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_143: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_4622, memory_format = torch.contiguous_format);  slice_4622 = None
        view_290: "f32[32, 16]" = torch.ops.aten.view.default(clone_143, [32, 16]);  clone_143 = None
        mm_140: "f32[32, 8]" = torch.ops.aten.mm.default(view_290, slice_7)
        view_291: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_140, [2, 16, 8]);  mm_140 = None
        slice_4629: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_838, 1, 1120, 1136)
        slice_4630: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4629, 2, 0, 16)
        add_142: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4630, view_291);  slice_4630 = view_291 = None
        slice_scatter_840: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4629, add_142, 2, 0, 16);  slice_4629 = add_142 = None
        slice_scatter_841: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_838, slice_scatter_840, 1, 1120, 1136);  slice_scatter_838 = slice_scatter_840 = None
        slice_4634: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_841, 1, 1120, 1136)
        slice_4635: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4634, 2, 0, 16)
        slice_scatter_843: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4634, slice_4635, 2, 0, 16);  slice_4634 = slice_4635 = None
        slice_scatter_844: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_841, slice_scatter_843, 1, 1120, 1136);  slice_scatter_841 = slice_scatter_843 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4655: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_4621, 2, 16, 32);  slice_4621 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_144: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_4655, memory_format = torch.contiguous_format);  slice_4655 = None
        view_292: "f32[32, 11]" = torch.ops.aten.view.default(clone_144, [32, 11]);  clone_144 = None
        mm_141: "f32[32, 8]" = torch.ops.aten.mm.default(view_292, slice_37)
        view_293: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_141, [2, 16, 8]);  mm_141 = None
        slice_4662: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_844, 1, 1120, 1136)
        slice_4663: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4662, 2, 0, 16)
        add_143: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4663, view_293);  slice_4663 = view_293 = None
        slice_scatter_846: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4662, add_143, 2, 0, 16);  slice_4662 = add_143 = None
        slice_scatter_847: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_844, slice_scatter_846, 1, 1120, 1136);  slice_scatter_844 = slice_scatter_846 = None
        slice_4667: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_847, 1, 1120, 1136)
        slice_4668: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4667, 2, 0, 16)
        slice_scatter_849: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4667, slice_4668, 2, 0, 16);  slice_4667 = slice_4668 = None
        slice_scatter_850: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_847, slice_scatter_849, 1, 1120, 1136);  slice_scatter_847 = slice_scatter_849 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4687: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1136, 1152)
        slice_4688: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_4687, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_145: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_4688, memory_format = torch.contiguous_format);  slice_4688 = None
        view_294: "f32[32, 16]" = torch.ops.aten.view.default(clone_145, [32, 16]);  clone_145 = None
        mm_142: "f32[32, 8]" = torch.ops.aten.mm.default(view_294, slice_7)
        view_295: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_142, [2, 16, 8]);  mm_142 = None
        slice_4695: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_850, 1, 1136, 1152)
        slice_4696: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4695, 2, 0, 16)
        add_144: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4696, view_295);  slice_4696 = view_295 = None
        slice_scatter_852: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4695, add_144, 2, 0, 16);  slice_4695 = add_144 = None
        slice_scatter_853: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_850, slice_scatter_852, 1, 1136, 1152);  slice_scatter_850 = slice_scatter_852 = None
        slice_4700: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_853, 1, 1136, 1152)
        slice_4701: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4700, 2, 0, 16)
        slice_scatter_855: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4700, slice_4701, 2, 0, 16);  slice_4700 = slice_4701 = None
        slice_scatter_856: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_853, slice_scatter_855, 1, 1136, 1152);  slice_scatter_853 = slice_scatter_855 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4721: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_4687, 2, 16, 32);  slice_4687 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_146: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_4721, memory_format = torch.contiguous_format);  slice_4721 = None
        view_296: "f32[32, 11]" = torch.ops.aten.view.default(clone_146, [32, 11]);  clone_146 = None
        mm_143: "f32[32, 8]" = torch.ops.aten.mm.default(view_296, slice_37)
        view_297: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_143, [2, 16, 8]);  mm_143 = None
        slice_4728: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_856, 1, 1136, 1152)
        slice_4729: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4728, 2, 0, 16)
        add_145: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4729, view_297);  slice_4729 = view_297 = None
        slice_scatter_858: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4728, add_145, 2, 0, 16);  slice_4728 = add_145 = None
        slice_scatter_859: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_856, slice_scatter_858, 1, 1136, 1152);  slice_scatter_856 = slice_scatter_858 = None
        slice_4733: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_859, 1, 1136, 1152)
        slice_4734: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4733, 2, 0, 16)
        slice_scatter_861: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4733, slice_4734, 2, 0, 16);  slice_4733 = slice_4734 = None
        slice_scatter_862: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_859, slice_scatter_861, 1, 1136, 1152);  slice_scatter_859 = slice_scatter_861 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4753: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1152, 1168)
        slice_4754: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_4753, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_147: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_4754, memory_format = torch.contiguous_format);  slice_4754 = None
        view_298: "f32[32, 16]" = torch.ops.aten.view.default(clone_147, [32, 16]);  clone_147 = None
        mm_144: "f32[32, 8]" = torch.ops.aten.mm.default(view_298, slice_7)
        view_299: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_144, [2, 16, 8]);  mm_144 = None
        slice_4761: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_862, 1, 1152, 1168)
        slice_4762: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4761, 2, 0, 16)
        add_146: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4762, view_299);  slice_4762 = view_299 = None
        slice_scatter_864: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4761, add_146, 2, 0, 16);  slice_4761 = add_146 = None
        slice_scatter_865: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_862, slice_scatter_864, 1, 1152, 1168);  slice_scatter_862 = slice_scatter_864 = None
        slice_4766: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_865, 1, 1152, 1168)
        slice_4767: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4766, 2, 0, 16)
        slice_scatter_867: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4766, slice_4767, 2, 0, 16);  slice_4766 = slice_4767 = None
        slice_scatter_868: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_865, slice_scatter_867, 1, 1152, 1168);  slice_scatter_865 = slice_scatter_867 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4787: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_4753, 2, 16, 32);  slice_4753 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_148: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_4787, memory_format = torch.contiguous_format);  slice_4787 = None
        view_300: "f32[32, 11]" = torch.ops.aten.view.default(clone_148, [32, 11]);  clone_148 = None
        mm_145: "f32[32, 8]" = torch.ops.aten.mm.default(view_300, slice_37)
        view_301: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_145, [2, 16, 8]);  mm_145 = None
        slice_4794: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_868, 1, 1152, 1168)
        slice_4795: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4794, 2, 0, 16)
        add_147: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4795, view_301);  slice_4795 = view_301 = None
        slice_scatter_870: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4794, add_147, 2, 0, 16);  slice_4794 = add_147 = None
        slice_scatter_871: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_868, slice_scatter_870, 1, 1152, 1168);  slice_scatter_868 = slice_scatter_870 = None
        slice_4799: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_871, 1, 1152, 1168)
        slice_4800: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4799, 2, 0, 16)
        slice_scatter_873: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4799, slice_4800, 2, 0, 16);  slice_4799 = slice_4800 = None
        slice_scatter_874: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_871, slice_scatter_873, 1, 1152, 1168);  slice_scatter_871 = slice_scatter_873 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4819: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1168, 1184)
        slice_4820: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_4819, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_149: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_4820, memory_format = torch.contiguous_format);  slice_4820 = None
        view_302: "f32[32, 16]" = torch.ops.aten.view.default(clone_149, [32, 16]);  clone_149 = None
        mm_146: "f32[32, 8]" = torch.ops.aten.mm.default(view_302, slice_7)
        view_303: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_146, [2, 16, 8]);  mm_146 = None
        slice_4827: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_874, 1, 1168, 1184)
        slice_4828: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4827, 2, 0, 16)
        add_148: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4828, view_303);  slice_4828 = view_303 = None
        slice_scatter_876: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4827, add_148, 2, 0, 16);  slice_4827 = add_148 = None
        slice_scatter_877: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_874, slice_scatter_876, 1, 1168, 1184);  slice_scatter_874 = slice_scatter_876 = None
        slice_4832: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_877, 1, 1168, 1184)
        slice_4833: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4832, 2, 0, 16)
        slice_scatter_879: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4832, slice_4833, 2, 0, 16);  slice_4832 = slice_4833 = None
        slice_scatter_880: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_877, slice_scatter_879, 1, 1168, 1184);  slice_scatter_877 = slice_scatter_879 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4853: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_4819, 2, 16, 32);  slice_4819 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_150: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_4853, memory_format = torch.contiguous_format);  slice_4853 = None
        view_304: "f32[32, 11]" = torch.ops.aten.view.default(clone_150, [32, 11]);  clone_150 = None
        mm_147: "f32[32, 8]" = torch.ops.aten.mm.default(view_304, slice_37)
        view_305: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_147, [2, 16, 8]);  mm_147 = None
        slice_4860: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_880, 1, 1168, 1184)
        slice_4861: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4860, 2, 0, 16)
        add_149: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4861, view_305);  slice_4861 = view_305 = None
        slice_scatter_882: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4860, add_149, 2, 0, 16);  slice_4860 = add_149 = None
        slice_scatter_883: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_880, slice_scatter_882, 1, 1168, 1184);  slice_scatter_880 = slice_scatter_882 = None
        slice_4865: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_883, 1, 1168, 1184)
        slice_4866: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4865, 2, 0, 16)
        slice_scatter_885: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4865, slice_4866, 2, 0, 16);  slice_4865 = slice_4866 = None
        slice_scatter_886: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_883, slice_scatter_885, 1, 1168, 1184);  slice_scatter_883 = slice_scatter_885 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4885: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1184, 1200)
        slice_4886: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_4885, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_151: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_4886, memory_format = torch.contiguous_format);  slice_4886 = None
        view_306: "f32[32, 16]" = torch.ops.aten.view.default(clone_151, [32, 16]);  clone_151 = None
        mm_148: "f32[32, 8]" = torch.ops.aten.mm.default(view_306, slice_7)
        view_307: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_148, [2, 16, 8]);  mm_148 = None
        slice_4893: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_886, 1, 1184, 1200)
        slice_4894: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4893, 2, 0, 16)
        add_150: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4894, view_307);  slice_4894 = view_307 = None
        slice_scatter_888: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4893, add_150, 2, 0, 16);  slice_4893 = add_150 = None
        slice_scatter_889: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_886, slice_scatter_888, 1, 1184, 1200);  slice_scatter_886 = slice_scatter_888 = None
        slice_4898: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_889, 1, 1184, 1200)
        slice_4899: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4898, 2, 0, 16)
        slice_scatter_891: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4898, slice_4899, 2, 0, 16);  slice_4898 = slice_4899 = None
        slice_scatter_892: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_889, slice_scatter_891, 1, 1184, 1200);  slice_scatter_889 = slice_scatter_891 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4919: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_4885, 2, 16, 32);  slice_4885 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_152: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_4919, memory_format = torch.contiguous_format);  slice_4919 = None
        view_308: "f32[32, 11]" = torch.ops.aten.view.default(clone_152, [32, 11]);  clone_152 = None
        mm_149: "f32[32, 8]" = torch.ops.aten.mm.default(view_308, slice_37)
        view_309: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_149, [2, 16, 8]);  mm_149 = None
        slice_4926: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_892, 1, 1184, 1200)
        slice_4927: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4926, 2, 0, 16)
        add_151: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4927, view_309);  slice_4927 = view_309 = None
        slice_scatter_894: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4926, add_151, 2, 0, 16);  slice_4926 = add_151 = None
        slice_scatter_895: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_892, slice_scatter_894, 1, 1184, 1200);  slice_scatter_892 = slice_scatter_894 = None
        slice_4931: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_895, 1, 1184, 1200)
        slice_4932: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4931, 2, 0, 16)
        slice_scatter_897: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4931, slice_4932, 2, 0, 16);  slice_4931 = slice_4932 = None
        slice_scatter_898: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_895, slice_scatter_897, 1, 1184, 1200);  slice_scatter_895 = slice_scatter_897 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4951: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1200, 1216)
        slice_4952: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_4951, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_153: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_4952, memory_format = torch.contiguous_format);  slice_4952 = None
        view_310: "f32[32, 16]" = torch.ops.aten.view.default(clone_153, [32, 16]);  clone_153 = None
        mm_150: "f32[32, 8]" = torch.ops.aten.mm.default(view_310, slice_7)
        view_311: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_150, [2, 16, 8]);  mm_150 = None
        slice_4959: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_898, 1, 1200, 1216)
        slice_4960: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4959, 2, 0, 16)
        add_152: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4960, view_311);  slice_4960 = view_311 = None
        slice_scatter_900: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4959, add_152, 2, 0, 16);  slice_4959 = add_152 = None
        slice_scatter_901: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_898, slice_scatter_900, 1, 1200, 1216);  slice_scatter_898 = slice_scatter_900 = None
        slice_4964: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_901, 1, 1200, 1216)
        slice_4965: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4964, 2, 0, 16)
        slice_scatter_903: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4964, slice_4965, 2, 0, 16);  slice_4964 = slice_4965 = None
        slice_scatter_904: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_901, slice_scatter_903, 1, 1200, 1216);  slice_scatter_901 = slice_scatter_903 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_4985: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_4951, 2, 16, 32);  slice_4951 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_154: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_4985, memory_format = torch.contiguous_format);  slice_4985 = None
        view_312: "f32[32, 11]" = torch.ops.aten.view.default(clone_154, [32, 11]);  clone_154 = None
        mm_151: "f32[32, 8]" = torch.ops.aten.mm.default(view_312, slice_37)
        view_313: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_151, [2, 16, 8]);  mm_151 = None
        slice_4992: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_904, 1, 1200, 1216)
        slice_4993: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4992, 2, 0, 16)
        add_153: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_4993, view_313);  slice_4993 = view_313 = None
        slice_scatter_906: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4992, add_153, 2, 0, 16);  slice_4992 = add_153 = None
        slice_scatter_907: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_904, slice_scatter_906, 1, 1200, 1216);  slice_scatter_904 = slice_scatter_906 = None
        slice_4997: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_907, 1, 1200, 1216)
        slice_4998: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_4997, 2, 0, 16)
        slice_scatter_909: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_4997, slice_4998, 2, 0, 16);  slice_4997 = slice_4998 = None
        slice_scatter_910: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_907, slice_scatter_909, 1, 1200, 1216);  slice_scatter_907 = slice_scatter_909 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5017: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1216, 1232)
        slice_5018: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_5017, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_155: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_5018, memory_format = torch.contiguous_format);  slice_5018 = None
        view_314: "f32[32, 16]" = torch.ops.aten.view.default(clone_155, [32, 16]);  clone_155 = None
        mm_152: "f32[32, 8]" = torch.ops.aten.mm.default(view_314, slice_7)
        view_315: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_152, [2, 16, 8]);  mm_152 = None
        slice_5025: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_910, 1, 1216, 1232)
        slice_5026: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5025, 2, 0, 16)
        add_154: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5026, view_315);  slice_5026 = view_315 = None
        slice_scatter_912: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5025, add_154, 2, 0, 16);  slice_5025 = add_154 = None
        slice_scatter_913: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_910, slice_scatter_912, 1, 1216, 1232);  slice_scatter_910 = slice_scatter_912 = None
        slice_5030: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_913, 1, 1216, 1232)
        slice_5031: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5030, 2, 0, 16)
        slice_scatter_915: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5030, slice_5031, 2, 0, 16);  slice_5030 = slice_5031 = None
        slice_scatter_916: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_913, slice_scatter_915, 1, 1216, 1232);  slice_scatter_913 = slice_scatter_915 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5051: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_5017, 2, 16, 32);  slice_5017 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_156: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_5051, memory_format = torch.contiguous_format);  slice_5051 = None
        view_316: "f32[32, 11]" = torch.ops.aten.view.default(clone_156, [32, 11]);  clone_156 = None
        mm_153: "f32[32, 8]" = torch.ops.aten.mm.default(view_316, slice_37)
        view_317: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_153, [2, 16, 8]);  mm_153 = None
        slice_5058: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_916, 1, 1216, 1232)
        slice_5059: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5058, 2, 0, 16)
        add_155: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5059, view_317);  slice_5059 = view_317 = None
        slice_scatter_918: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5058, add_155, 2, 0, 16);  slice_5058 = add_155 = None
        slice_scatter_919: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_916, slice_scatter_918, 1, 1216, 1232);  slice_scatter_916 = slice_scatter_918 = None
        slice_5063: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_919, 1, 1216, 1232)
        slice_5064: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5063, 2, 0, 16)
        slice_scatter_921: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5063, slice_5064, 2, 0, 16);  slice_5063 = slice_5064 = None
        slice_scatter_922: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_919, slice_scatter_921, 1, 1216, 1232);  slice_scatter_919 = slice_scatter_921 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5083: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1232, 1248)
        slice_5084: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_5083, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_157: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_5084, memory_format = torch.contiguous_format);  slice_5084 = None
        view_318: "f32[32, 16]" = torch.ops.aten.view.default(clone_157, [32, 16]);  clone_157 = None
        mm_154: "f32[32, 8]" = torch.ops.aten.mm.default(view_318, slice_7)
        view_319: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_154, [2, 16, 8]);  mm_154 = None
        slice_5091: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_922, 1, 1232, 1248)
        slice_5092: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5091, 2, 0, 16)
        add_156: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5092, view_319);  slice_5092 = view_319 = None
        slice_scatter_924: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5091, add_156, 2, 0, 16);  slice_5091 = add_156 = None
        slice_scatter_925: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_922, slice_scatter_924, 1, 1232, 1248);  slice_scatter_922 = slice_scatter_924 = None
        slice_5096: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_925, 1, 1232, 1248)
        slice_5097: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5096, 2, 0, 16)
        slice_scatter_927: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5096, slice_5097, 2, 0, 16);  slice_5096 = slice_5097 = None
        slice_scatter_928: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_925, slice_scatter_927, 1, 1232, 1248);  slice_scatter_925 = slice_scatter_927 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5117: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_5083, 2, 16, 32);  slice_5083 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_158: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_5117, memory_format = torch.contiguous_format);  slice_5117 = None
        view_320: "f32[32, 11]" = torch.ops.aten.view.default(clone_158, [32, 11]);  clone_158 = None
        mm_155: "f32[32, 8]" = torch.ops.aten.mm.default(view_320, slice_37)
        view_321: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_155, [2, 16, 8]);  mm_155 = None
        slice_5124: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_928, 1, 1232, 1248)
        slice_5125: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5124, 2, 0, 16)
        add_157: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5125, view_321);  slice_5125 = view_321 = None
        slice_scatter_930: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5124, add_157, 2, 0, 16);  slice_5124 = add_157 = None
        slice_scatter_931: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_928, slice_scatter_930, 1, 1232, 1248);  slice_scatter_928 = slice_scatter_930 = None
        slice_5129: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_931, 1, 1232, 1248)
        slice_5130: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5129, 2, 0, 16)
        slice_scatter_933: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5129, slice_5130, 2, 0, 16);  slice_5129 = slice_5130 = None
        slice_scatter_934: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_931, slice_scatter_933, 1, 1232, 1248);  slice_scatter_931 = slice_scatter_933 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5149: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1248, 1264)
        slice_5150: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_5149, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_159: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_5150, memory_format = torch.contiguous_format);  slice_5150 = None
        view_322: "f32[32, 16]" = torch.ops.aten.view.default(clone_159, [32, 16]);  clone_159 = None
        mm_156: "f32[32, 8]" = torch.ops.aten.mm.default(view_322, slice_7)
        view_323: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_156, [2, 16, 8]);  mm_156 = None
        slice_5157: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_934, 1, 1248, 1264)
        slice_5158: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5157, 2, 0, 16)
        add_158: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5158, view_323);  slice_5158 = view_323 = None
        slice_scatter_936: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5157, add_158, 2, 0, 16);  slice_5157 = add_158 = None
        slice_scatter_937: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_934, slice_scatter_936, 1, 1248, 1264);  slice_scatter_934 = slice_scatter_936 = None
        slice_5162: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_937, 1, 1248, 1264)
        slice_5163: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5162, 2, 0, 16)
        slice_scatter_939: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5162, slice_5163, 2, 0, 16);  slice_5162 = slice_5163 = None
        slice_scatter_940: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_937, slice_scatter_939, 1, 1248, 1264);  slice_scatter_937 = slice_scatter_939 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5183: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_5149, 2, 16, 32);  slice_5149 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_160: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_5183, memory_format = torch.contiguous_format);  slice_5183 = None
        view_324: "f32[32, 11]" = torch.ops.aten.view.default(clone_160, [32, 11]);  clone_160 = None
        mm_157: "f32[32, 8]" = torch.ops.aten.mm.default(view_324, slice_37)
        view_325: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_157, [2, 16, 8]);  mm_157 = None
        slice_5190: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_940, 1, 1248, 1264)
        slice_5191: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5190, 2, 0, 16)
        add_159: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5191, view_325);  slice_5191 = view_325 = None
        slice_scatter_942: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5190, add_159, 2, 0, 16);  slice_5190 = add_159 = None
        slice_scatter_943: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_940, slice_scatter_942, 1, 1248, 1264);  slice_scatter_940 = slice_scatter_942 = None
        slice_5195: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_943, 1, 1248, 1264)
        slice_5196: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5195, 2, 0, 16)
        slice_scatter_945: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5195, slice_5196, 2, 0, 16);  slice_5195 = slice_5196 = None
        slice_scatter_946: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_943, slice_scatter_945, 1, 1248, 1264);  slice_scatter_943 = slice_scatter_945 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5215: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1264, 1280)
        slice_5216: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_5215, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_161: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_5216, memory_format = torch.contiguous_format);  slice_5216 = None
        view_326: "f32[32, 16]" = torch.ops.aten.view.default(clone_161, [32, 16]);  clone_161 = None
        mm_158: "f32[32, 8]" = torch.ops.aten.mm.default(view_326, slice_7)
        view_327: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_158, [2, 16, 8]);  mm_158 = None
        slice_5223: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_946, 1, 1264, 1280)
        slice_5224: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5223, 2, 0, 16)
        add_160: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5224, view_327);  slice_5224 = view_327 = None
        slice_scatter_948: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5223, add_160, 2, 0, 16);  slice_5223 = add_160 = None
        slice_scatter_949: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_946, slice_scatter_948, 1, 1264, 1280);  slice_scatter_946 = slice_scatter_948 = None
        slice_5228: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_949, 1, 1264, 1280)
        slice_5229: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5228, 2, 0, 16)
        slice_scatter_951: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5228, slice_5229, 2, 0, 16);  slice_5228 = slice_5229 = None
        slice_scatter_952: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_949, slice_scatter_951, 1, 1264, 1280);  slice_scatter_949 = slice_scatter_951 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5249: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_5215, 2, 16, 32);  slice_5215 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_162: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_5249, memory_format = torch.contiguous_format);  slice_5249 = None
        view_328: "f32[32, 11]" = torch.ops.aten.view.default(clone_162, [32, 11]);  clone_162 = None
        mm_159: "f32[32, 8]" = torch.ops.aten.mm.default(view_328, slice_37)
        view_329: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_159, [2, 16, 8]);  mm_159 = None
        slice_5256: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_952, 1, 1264, 1280)
        slice_5257: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5256, 2, 0, 16)
        add_161: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5257, view_329);  slice_5257 = view_329 = None
        slice_scatter_954: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5256, add_161, 2, 0, 16);  slice_5256 = add_161 = None
        slice_scatter_955: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_952, slice_scatter_954, 1, 1264, 1280);  slice_scatter_952 = slice_scatter_954 = None
        slice_5261: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_955, 1, 1264, 1280)
        slice_5262: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5261, 2, 0, 16)
        slice_scatter_957: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5261, slice_5262, 2, 0, 16);  slice_5261 = slice_5262 = None
        slice_scatter_958: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_955, slice_scatter_957, 1, 1264, 1280);  slice_scatter_955 = slice_scatter_957 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5281: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1280, 1296)
        slice_5282: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_5281, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_163: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_5282, memory_format = torch.contiguous_format);  slice_5282 = None
        view_330: "f32[32, 16]" = torch.ops.aten.view.default(clone_163, [32, 16]);  clone_163 = None
        mm_160: "f32[32, 8]" = torch.ops.aten.mm.default(view_330, slice_7)
        view_331: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_160, [2, 16, 8]);  mm_160 = None
        slice_5289: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_958, 1, 1280, 1296)
        slice_5290: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5289, 2, 0, 16)
        add_162: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5290, view_331);  slice_5290 = view_331 = None
        slice_scatter_960: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5289, add_162, 2, 0, 16);  slice_5289 = add_162 = None
        slice_scatter_961: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_958, slice_scatter_960, 1, 1280, 1296);  slice_scatter_958 = slice_scatter_960 = None
        slice_5294: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_961, 1, 1280, 1296)
        slice_5295: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5294, 2, 0, 16)
        slice_scatter_963: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5294, slice_5295, 2, 0, 16);  slice_5294 = slice_5295 = None
        slice_scatter_964: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_961, slice_scatter_963, 1, 1280, 1296);  slice_scatter_961 = slice_scatter_963 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5315: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_5281, 2, 16, 32);  slice_5281 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_164: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_5315, memory_format = torch.contiguous_format);  slice_5315 = None
        view_332: "f32[32, 11]" = torch.ops.aten.view.default(clone_164, [32, 11]);  clone_164 = None
        mm_161: "f32[32, 8]" = torch.ops.aten.mm.default(view_332, slice_37)
        view_333: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_161, [2, 16, 8]);  mm_161 = None
        slice_5322: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_964, 1, 1280, 1296)
        slice_5323: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5322, 2, 0, 16)
        add_163: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5323, view_333);  slice_5323 = view_333 = None
        slice_scatter_966: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5322, add_163, 2, 0, 16);  slice_5322 = add_163 = None
        slice_scatter_967: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_964, slice_scatter_966, 1, 1280, 1296);  slice_scatter_964 = slice_scatter_966 = None
        slice_5327: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_967, 1, 1280, 1296)
        slice_5328: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5327, 2, 0, 16)
        slice_scatter_969: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5327, slice_5328, 2, 0, 16);  slice_5327 = slice_5328 = None
        slice_scatter_970: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_967, slice_scatter_969, 1, 1280, 1296);  slice_scatter_967 = slice_scatter_969 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5347: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1296, 1312)
        slice_5348: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_5347, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_165: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_5348, memory_format = torch.contiguous_format);  slice_5348 = None
        view_334: "f32[32, 16]" = torch.ops.aten.view.default(clone_165, [32, 16]);  clone_165 = None
        mm_162: "f32[32, 8]" = torch.ops.aten.mm.default(view_334, slice_7)
        view_335: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_162, [2, 16, 8]);  mm_162 = None
        slice_5355: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_970, 1, 1296, 1312)
        slice_5356: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5355, 2, 0, 16)
        add_164: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5356, view_335);  slice_5356 = view_335 = None
        slice_scatter_972: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5355, add_164, 2, 0, 16);  slice_5355 = add_164 = None
        slice_scatter_973: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_970, slice_scatter_972, 1, 1296, 1312);  slice_scatter_970 = slice_scatter_972 = None
        slice_5360: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_973, 1, 1296, 1312)
        slice_5361: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5360, 2, 0, 16)
        slice_scatter_975: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5360, slice_5361, 2, 0, 16);  slice_5360 = slice_5361 = None
        slice_scatter_976: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_973, slice_scatter_975, 1, 1296, 1312);  slice_scatter_973 = slice_scatter_975 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5381: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_5347, 2, 16, 32);  slice_5347 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_166: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_5381, memory_format = torch.contiguous_format);  slice_5381 = None
        view_336: "f32[32, 11]" = torch.ops.aten.view.default(clone_166, [32, 11]);  clone_166 = None
        mm_163: "f32[32, 8]" = torch.ops.aten.mm.default(view_336, slice_37)
        view_337: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_163, [2, 16, 8]);  mm_163 = None
        slice_5388: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_976, 1, 1296, 1312)
        slice_5389: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5388, 2, 0, 16)
        add_165: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5389, view_337);  slice_5389 = view_337 = None
        slice_scatter_978: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5388, add_165, 2, 0, 16);  slice_5388 = add_165 = None
        slice_scatter_979: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_976, slice_scatter_978, 1, 1296, 1312);  slice_scatter_976 = slice_scatter_978 = None
        slice_5393: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_979, 1, 1296, 1312)
        slice_5394: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5393, 2, 0, 16)
        slice_scatter_981: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5393, slice_5394, 2, 0, 16);  slice_5393 = slice_5394 = None
        slice_scatter_982: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_979, slice_scatter_981, 1, 1296, 1312);  slice_scatter_979 = slice_scatter_981 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5413: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1312, 1328)
        slice_5414: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_5413, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_167: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_5414, memory_format = torch.contiguous_format);  slice_5414 = None
        view_338: "f32[32, 16]" = torch.ops.aten.view.default(clone_167, [32, 16]);  clone_167 = None
        mm_164: "f32[32, 8]" = torch.ops.aten.mm.default(view_338, slice_7)
        view_339: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_164, [2, 16, 8]);  mm_164 = None
        slice_5421: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_982, 1, 1312, 1328)
        slice_5422: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5421, 2, 0, 16)
        add_166: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5422, view_339);  slice_5422 = view_339 = None
        slice_scatter_984: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5421, add_166, 2, 0, 16);  slice_5421 = add_166 = None
        slice_scatter_985: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_982, slice_scatter_984, 1, 1312, 1328);  slice_scatter_982 = slice_scatter_984 = None
        slice_5426: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_985, 1, 1312, 1328)
        slice_5427: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5426, 2, 0, 16)
        slice_scatter_987: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5426, slice_5427, 2, 0, 16);  slice_5426 = slice_5427 = None
        slice_scatter_988: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_985, slice_scatter_987, 1, 1312, 1328);  slice_scatter_985 = slice_scatter_987 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5447: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_5413, 2, 16, 32);  slice_5413 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_168: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_5447, memory_format = torch.contiguous_format);  slice_5447 = None
        view_340: "f32[32, 11]" = torch.ops.aten.view.default(clone_168, [32, 11]);  clone_168 = None
        mm_165: "f32[32, 8]" = torch.ops.aten.mm.default(view_340, slice_37)
        view_341: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_165, [2, 16, 8]);  mm_165 = None
        slice_5454: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_988, 1, 1312, 1328)
        slice_5455: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5454, 2, 0, 16)
        add_167: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5455, view_341);  slice_5455 = view_341 = None
        slice_scatter_990: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5454, add_167, 2, 0, 16);  slice_5454 = add_167 = None
        slice_scatter_991: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_988, slice_scatter_990, 1, 1312, 1328);  slice_scatter_988 = slice_scatter_990 = None
        slice_5459: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_991, 1, 1312, 1328)
        slice_5460: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5459, 2, 0, 16)
        slice_scatter_993: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5459, slice_5460, 2, 0, 16);  slice_5459 = slice_5460 = None
        slice_scatter_994: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_991, slice_scatter_993, 1, 1312, 1328);  slice_scatter_991 = slice_scatter_993 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5479: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1328, 1344)
        slice_5480: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_5479, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_169: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_5480, memory_format = torch.contiguous_format);  slice_5480 = None
        view_342: "f32[32, 16]" = torch.ops.aten.view.default(clone_169, [32, 16]);  clone_169 = None
        mm_166: "f32[32, 8]" = torch.ops.aten.mm.default(view_342, slice_7)
        view_343: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_166, [2, 16, 8]);  mm_166 = None
        slice_5487: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_994, 1, 1328, 1344)
        slice_5488: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5487, 2, 0, 16)
        add_168: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5488, view_343);  slice_5488 = view_343 = None
        slice_scatter_996: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5487, add_168, 2, 0, 16);  slice_5487 = add_168 = None
        slice_scatter_997: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_994, slice_scatter_996, 1, 1328, 1344);  slice_scatter_994 = slice_scatter_996 = None
        slice_5492: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_997, 1, 1328, 1344)
        slice_5493: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5492, 2, 0, 16)
        slice_scatter_999: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5492, slice_5493, 2, 0, 16);  slice_5492 = slice_5493 = None
        slice_scatter_1000: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_997, slice_scatter_999, 1, 1328, 1344);  slice_scatter_997 = slice_scatter_999 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5513: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_5479, 2, 16, 32);  slice_5479 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_170: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_5513, memory_format = torch.contiguous_format);  slice_5513 = None
        view_344: "f32[32, 11]" = torch.ops.aten.view.default(clone_170, [32, 11]);  clone_170 = None
        mm_167: "f32[32, 8]" = torch.ops.aten.mm.default(view_344, slice_37)
        view_345: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_167, [2, 16, 8]);  mm_167 = None
        slice_5520: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1000, 1, 1328, 1344)
        slice_5521: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5520, 2, 0, 16)
        add_169: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5521, view_345);  slice_5521 = view_345 = None
        slice_scatter_1002: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5520, add_169, 2, 0, 16);  slice_5520 = add_169 = None
        slice_scatter_1003: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1000, slice_scatter_1002, 1, 1328, 1344);  slice_scatter_1000 = slice_scatter_1002 = None
        slice_5525: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1003, 1, 1328, 1344)
        slice_5526: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5525, 2, 0, 16)
        slice_scatter_1005: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5525, slice_5526, 2, 0, 16);  slice_5525 = slice_5526 = None
        slice_scatter_1006: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1003, slice_scatter_1005, 1, 1328, 1344);  slice_scatter_1003 = slice_scatter_1005 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5545: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1344, 1360)
        slice_5546: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_5545, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_171: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_5546, memory_format = torch.contiguous_format);  slice_5546 = None
        view_346: "f32[32, 16]" = torch.ops.aten.view.default(clone_171, [32, 16]);  clone_171 = None
        mm_168: "f32[32, 8]" = torch.ops.aten.mm.default(view_346, slice_7)
        view_347: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_168, [2, 16, 8]);  mm_168 = None
        slice_5553: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1006, 1, 1344, 1360)
        slice_5554: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5553, 2, 0, 16)
        add_170: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5554, view_347);  slice_5554 = view_347 = None
        slice_scatter_1008: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5553, add_170, 2, 0, 16);  slice_5553 = add_170 = None
        slice_scatter_1009: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1006, slice_scatter_1008, 1, 1344, 1360);  slice_scatter_1006 = slice_scatter_1008 = None
        slice_5558: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1009, 1, 1344, 1360)
        slice_5559: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5558, 2, 0, 16)
        slice_scatter_1011: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5558, slice_5559, 2, 0, 16);  slice_5558 = slice_5559 = None
        slice_scatter_1012: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1009, slice_scatter_1011, 1, 1344, 1360);  slice_scatter_1009 = slice_scatter_1011 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5579: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_5545, 2, 16, 32);  slice_5545 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_172: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_5579, memory_format = torch.contiguous_format);  slice_5579 = None
        view_348: "f32[32, 11]" = torch.ops.aten.view.default(clone_172, [32, 11]);  clone_172 = None
        mm_169: "f32[32, 8]" = torch.ops.aten.mm.default(view_348, slice_37)
        view_349: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_169, [2, 16, 8]);  mm_169 = None
        slice_5586: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1012, 1, 1344, 1360)
        slice_5587: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5586, 2, 0, 16)
        add_171: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5587, view_349);  slice_5587 = view_349 = None
        slice_scatter_1014: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5586, add_171, 2, 0, 16);  slice_5586 = add_171 = None
        slice_scatter_1015: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1012, slice_scatter_1014, 1, 1344, 1360);  slice_scatter_1012 = slice_scatter_1014 = None
        slice_5591: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1015, 1, 1344, 1360)
        slice_5592: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5591, 2, 0, 16)
        slice_scatter_1017: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5591, slice_5592, 2, 0, 16);  slice_5591 = slice_5592 = None
        slice_scatter_1018: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1015, slice_scatter_1017, 1, 1344, 1360);  slice_scatter_1015 = slice_scatter_1017 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5611: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1360, 1376)
        slice_5612: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_5611, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_173: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_5612, memory_format = torch.contiguous_format);  slice_5612 = None
        view_350: "f32[32, 16]" = torch.ops.aten.view.default(clone_173, [32, 16]);  clone_173 = None
        mm_170: "f32[32, 8]" = torch.ops.aten.mm.default(view_350, slice_7)
        view_351: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_170, [2, 16, 8]);  mm_170 = None
        slice_5619: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1018, 1, 1360, 1376)
        slice_5620: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5619, 2, 0, 16)
        add_172: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5620, view_351);  slice_5620 = view_351 = None
        slice_scatter_1020: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5619, add_172, 2, 0, 16);  slice_5619 = add_172 = None
        slice_scatter_1021: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1018, slice_scatter_1020, 1, 1360, 1376);  slice_scatter_1018 = slice_scatter_1020 = None
        slice_5624: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1021, 1, 1360, 1376)
        slice_5625: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5624, 2, 0, 16)
        slice_scatter_1023: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5624, slice_5625, 2, 0, 16);  slice_5624 = slice_5625 = None
        slice_scatter_1024: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1021, slice_scatter_1023, 1, 1360, 1376);  slice_scatter_1021 = slice_scatter_1023 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5645: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_5611, 2, 16, 32);  slice_5611 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_174: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_5645, memory_format = torch.contiguous_format);  slice_5645 = None
        view_352: "f32[32, 11]" = torch.ops.aten.view.default(clone_174, [32, 11]);  clone_174 = None
        mm_171: "f32[32, 8]" = torch.ops.aten.mm.default(view_352, slice_37)
        view_353: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_171, [2, 16, 8]);  mm_171 = None
        slice_5652: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1024, 1, 1360, 1376)
        slice_5653: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5652, 2, 0, 16)
        add_173: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5653, view_353);  slice_5653 = view_353 = None
        slice_scatter_1026: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5652, add_173, 2, 0, 16);  slice_5652 = add_173 = None
        slice_scatter_1027: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1024, slice_scatter_1026, 1, 1360, 1376);  slice_scatter_1024 = slice_scatter_1026 = None
        slice_5657: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1027, 1, 1360, 1376)
        slice_5658: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5657, 2, 0, 16)
        slice_scatter_1029: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5657, slice_5658, 2, 0, 16);  slice_5657 = slice_5658 = None
        slice_scatter_1030: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1027, slice_scatter_1029, 1, 1360, 1376);  slice_scatter_1027 = slice_scatter_1029 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5677: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1376, 1392)
        slice_5678: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_5677, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_175: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_5678, memory_format = torch.contiguous_format);  slice_5678 = None
        view_354: "f32[32, 16]" = torch.ops.aten.view.default(clone_175, [32, 16]);  clone_175 = None
        mm_172: "f32[32, 8]" = torch.ops.aten.mm.default(view_354, slice_7)
        view_355: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_172, [2, 16, 8]);  mm_172 = None
        slice_5685: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1030, 1, 1376, 1392)
        slice_5686: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5685, 2, 0, 16)
        add_174: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5686, view_355);  slice_5686 = view_355 = None
        slice_scatter_1032: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5685, add_174, 2, 0, 16);  slice_5685 = add_174 = None
        slice_scatter_1033: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1030, slice_scatter_1032, 1, 1376, 1392);  slice_scatter_1030 = slice_scatter_1032 = None
        slice_5690: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1033, 1, 1376, 1392)
        slice_5691: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5690, 2, 0, 16)
        slice_scatter_1035: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5690, slice_5691, 2, 0, 16);  slice_5690 = slice_5691 = None
        slice_scatter_1036: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1033, slice_scatter_1035, 1, 1376, 1392);  slice_scatter_1033 = slice_scatter_1035 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5711: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_5677, 2, 16, 32);  slice_5677 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_176: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_5711, memory_format = torch.contiguous_format);  slice_5711 = None
        view_356: "f32[32, 11]" = torch.ops.aten.view.default(clone_176, [32, 11]);  clone_176 = None
        mm_173: "f32[32, 8]" = torch.ops.aten.mm.default(view_356, slice_37)
        view_357: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_173, [2, 16, 8]);  mm_173 = None
        slice_5718: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1036, 1, 1376, 1392)
        slice_5719: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5718, 2, 0, 16)
        add_175: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5719, view_357);  slice_5719 = view_357 = None
        slice_scatter_1038: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5718, add_175, 2, 0, 16);  slice_5718 = add_175 = None
        slice_scatter_1039: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1036, slice_scatter_1038, 1, 1376, 1392);  slice_scatter_1036 = slice_scatter_1038 = None
        slice_5723: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1039, 1, 1376, 1392)
        slice_5724: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5723, 2, 0, 16)
        slice_scatter_1041: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5723, slice_5724, 2, 0, 16);  slice_5723 = slice_5724 = None
        slice_scatter_1042: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1039, slice_scatter_1041, 1, 1376, 1392);  slice_scatter_1039 = slice_scatter_1041 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5743: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1392, 1408)
        slice_5744: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_5743, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_177: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_5744, memory_format = torch.contiguous_format);  slice_5744 = None
        view_358: "f32[32, 16]" = torch.ops.aten.view.default(clone_177, [32, 16]);  clone_177 = None
        mm_174: "f32[32, 8]" = torch.ops.aten.mm.default(view_358, slice_7)
        view_359: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_174, [2, 16, 8]);  mm_174 = None
        slice_5751: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1042, 1, 1392, 1408)
        slice_5752: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5751, 2, 0, 16)
        add_176: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5752, view_359);  slice_5752 = view_359 = None
        slice_scatter_1044: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5751, add_176, 2, 0, 16);  slice_5751 = add_176 = None
        slice_scatter_1045: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1042, slice_scatter_1044, 1, 1392, 1408);  slice_scatter_1042 = slice_scatter_1044 = None
        slice_5756: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1045, 1, 1392, 1408)
        slice_5757: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5756, 2, 0, 16)
        slice_scatter_1047: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5756, slice_5757, 2, 0, 16);  slice_5756 = slice_5757 = None
        slice_scatter_1048: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1045, slice_scatter_1047, 1, 1392, 1408);  slice_scatter_1045 = slice_scatter_1047 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5777: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_5743, 2, 16, 32);  slice_5743 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_178: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_5777, memory_format = torch.contiguous_format);  slice_5777 = None
        view_360: "f32[32, 11]" = torch.ops.aten.view.default(clone_178, [32, 11]);  clone_178 = None
        mm_175: "f32[32, 8]" = torch.ops.aten.mm.default(view_360, slice_37)
        view_361: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_175, [2, 16, 8]);  mm_175 = None
        slice_5784: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1048, 1, 1392, 1408)
        slice_5785: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5784, 2, 0, 16)
        add_177: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5785, view_361);  slice_5785 = view_361 = None
        slice_scatter_1050: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5784, add_177, 2, 0, 16);  slice_5784 = add_177 = None
        slice_scatter_1051: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1048, slice_scatter_1050, 1, 1392, 1408);  slice_scatter_1048 = slice_scatter_1050 = None
        slice_5789: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1051, 1, 1392, 1408)
        slice_5790: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5789, 2, 0, 16)
        slice_scatter_1053: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5789, slice_5790, 2, 0, 16);  slice_5789 = slice_5790 = None
        slice_scatter_1054: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1051, slice_scatter_1053, 1, 1392, 1408);  slice_scatter_1051 = slice_scatter_1053 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5809: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1408, 1424)
        slice_5810: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_5809, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_179: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_5810, memory_format = torch.contiguous_format);  slice_5810 = None
        view_362: "f32[32, 16]" = torch.ops.aten.view.default(clone_179, [32, 16]);  clone_179 = None
        mm_176: "f32[32, 8]" = torch.ops.aten.mm.default(view_362, slice_7)
        view_363: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_176, [2, 16, 8]);  mm_176 = None
        slice_5817: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1054, 1, 1408, 1424)
        slice_5818: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5817, 2, 0, 16)
        add_178: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5818, view_363);  slice_5818 = view_363 = None
        slice_scatter_1056: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5817, add_178, 2, 0, 16);  slice_5817 = add_178 = None
        slice_scatter_1057: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1054, slice_scatter_1056, 1, 1408, 1424);  slice_scatter_1054 = slice_scatter_1056 = None
        slice_5822: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1057, 1, 1408, 1424)
        slice_5823: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5822, 2, 0, 16)
        slice_scatter_1059: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5822, slice_5823, 2, 0, 16);  slice_5822 = slice_5823 = None
        slice_scatter_1060: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1057, slice_scatter_1059, 1, 1408, 1424);  slice_scatter_1057 = slice_scatter_1059 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5843: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_5809, 2, 16, 32);  slice_5809 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_180: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_5843, memory_format = torch.contiguous_format);  slice_5843 = None
        view_364: "f32[32, 11]" = torch.ops.aten.view.default(clone_180, [32, 11]);  clone_180 = None
        mm_177: "f32[32, 8]" = torch.ops.aten.mm.default(view_364, slice_37)
        view_365: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_177, [2, 16, 8]);  mm_177 = None
        slice_5850: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1060, 1, 1408, 1424)
        slice_5851: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5850, 2, 0, 16)
        add_179: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5851, view_365);  slice_5851 = view_365 = None
        slice_scatter_1062: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5850, add_179, 2, 0, 16);  slice_5850 = add_179 = None
        slice_scatter_1063: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1060, slice_scatter_1062, 1, 1408, 1424);  slice_scatter_1060 = slice_scatter_1062 = None
        slice_5855: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1063, 1, 1408, 1424)
        slice_5856: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5855, 2, 0, 16)
        slice_scatter_1065: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5855, slice_5856, 2, 0, 16);  slice_5855 = slice_5856 = None
        slice_scatter_1066: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1063, slice_scatter_1065, 1, 1408, 1424);  slice_scatter_1063 = slice_scatter_1065 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5875: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1424, 1440)
        slice_5876: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_5875, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_181: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_5876, memory_format = torch.contiguous_format);  slice_5876 = None
        view_366: "f32[32, 16]" = torch.ops.aten.view.default(clone_181, [32, 16]);  clone_181 = None
        mm_178: "f32[32, 8]" = torch.ops.aten.mm.default(view_366, slice_7)
        view_367: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_178, [2, 16, 8]);  mm_178 = None
        slice_5883: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1066, 1, 1424, 1440)
        slice_5884: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5883, 2, 0, 16)
        add_180: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5884, view_367);  slice_5884 = view_367 = None
        slice_scatter_1068: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5883, add_180, 2, 0, 16);  slice_5883 = add_180 = None
        slice_scatter_1069: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1066, slice_scatter_1068, 1, 1424, 1440);  slice_scatter_1066 = slice_scatter_1068 = None
        slice_5888: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1069, 1, 1424, 1440)
        slice_5889: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5888, 2, 0, 16)
        slice_scatter_1071: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5888, slice_5889, 2, 0, 16);  slice_5888 = slice_5889 = None
        slice_scatter_1072: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1069, slice_scatter_1071, 1, 1424, 1440);  slice_scatter_1069 = slice_scatter_1071 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5909: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_5875, 2, 16, 32);  slice_5875 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_182: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_5909, memory_format = torch.contiguous_format);  slice_5909 = None
        view_368: "f32[32, 11]" = torch.ops.aten.view.default(clone_182, [32, 11]);  clone_182 = None
        mm_179: "f32[32, 8]" = torch.ops.aten.mm.default(view_368, slice_37)
        view_369: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_179, [2, 16, 8]);  mm_179 = None
        slice_5916: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1072, 1, 1424, 1440)
        slice_5917: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5916, 2, 0, 16)
        add_181: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5917, view_369);  slice_5917 = view_369 = None
        slice_scatter_1074: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5916, add_181, 2, 0, 16);  slice_5916 = add_181 = None
        slice_scatter_1075: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1072, slice_scatter_1074, 1, 1424, 1440);  slice_scatter_1072 = slice_scatter_1074 = None
        slice_5921: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1075, 1, 1424, 1440)
        slice_5922: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5921, 2, 0, 16)
        slice_scatter_1077: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5921, slice_5922, 2, 0, 16);  slice_5921 = slice_5922 = None
        slice_scatter_1078: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1075, slice_scatter_1077, 1, 1424, 1440);  slice_scatter_1075 = slice_scatter_1077 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5941: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1440, 1456)
        slice_5942: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_5941, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_183: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_5942, memory_format = torch.contiguous_format);  slice_5942 = None
        view_370: "f32[32, 16]" = torch.ops.aten.view.default(clone_183, [32, 16]);  clone_183 = None
        mm_180: "f32[32, 8]" = torch.ops.aten.mm.default(view_370, slice_7)
        view_371: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_180, [2, 16, 8]);  mm_180 = None
        slice_5949: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1078, 1, 1440, 1456)
        slice_5950: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5949, 2, 0, 16)
        add_182: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5950, view_371);  slice_5950 = view_371 = None
        slice_scatter_1080: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5949, add_182, 2, 0, 16);  slice_5949 = add_182 = None
        slice_scatter_1081: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1078, slice_scatter_1080, 1, 1440, 1456);  slice_scatter_1078 = slice_scatter_1080 = None
        slice_5954: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1081, 1, 1440, 1456)
        slice_5955: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5954, 2, 0, 16)
        slice_scatter_1083: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5954, slice_5955, 2, 0, 16);  slice_5954 = slice_5955 = None
        slice_scatter_1084: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1081, slice_scatter_1083, 1, 1440, 1456);  slice_scatter_1081 = slice_scatter_1083 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_5975: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_5941, 2, 16, 32);  slice_5941 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_184: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_5975, memory_format = torch.contiguous_format);  slice_5975 = None
        view_372: "f32[32, 11]" = torch.ops.aten.view.default(clone_184, [32, 11]);  clone_184 = None
        mm_181: "f32[32, 8]" = torch.ops.aten.mm.default(view_372, slice_37)
        view_373: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_181, [2, 16, 8]);  mm_181 = None
        slice_5982: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1084, 1, 1440, 1456)
        slice_5983: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5982, 2, 0, 16)
        add_183: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_5983, view_373);  slice_5983 = view_373 = None
        slice_scatter_1086: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5982, add_183, 2, 0, 16);  slice_5982 = add_183 = None
        slice_scatter_1087: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1084, slice_scatter_1086, 1, 1440, 1456);  slice_scatter_1084 = slice_scatter_1086 = None
        slice_5987: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1087, 1, 1440, 1456)
        slice_5988: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_5987, 2, 0, 16)
        slice_scatter_1089: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_5987, slice_5988, 2, 0, 16);  slice_5987 = slice_5988 = None
        slice_scatter_1090: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1087, slice_scatter_1089, 1, 1440, 1456);  slice_scatter_1087 = slice_scatter_1089 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6007: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1456, 1472)
        slice_6008: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_6007, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_185: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_6008, memory_format = torch.contiguous_format);  slice_6008 = None
        view_374: "f32[32, 16]" = torch.ops.aten.view.default(clone_185, [32, 16]);  clone_185 = None
        mm_182: "f32[32, 8]" = torch.ops.aten.mm.default(view_374, slice_7)
        view_375: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_182, [2, 16, 8]);  mm_182 = None
        slice_6015: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1090, 1, 1456, 1472)
        slice_6016: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6015, 2, 0, 16)
        add_184: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6016, view_375);  slice_6016 = view_375 = None
        slice_scatter_1092: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6015, add_184, 2, 0, 16);  slice_6015 = add_184 = None
        slice_scatter_1093: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1090, slice_scatter_1092, 1, 1456, 1472);  slice_scatter_1090 = slice_scatter_1092 = None
        slice_6020: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1093, 1, 1456, 1472)
        slice_6021: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6020, 2, 0, 16)
        slice_scatter_1095: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6020, slice_6021, 2, 0, 16);  slice_6020 = slice_6021 = None
        slice_scatter_1096: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1093, slice_scatter_1095, 1, 1456, 1472);  slice_scatter_1093 = slice_scatter_1095 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6041: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_6007, 2, 16, 32);  slice_6007 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_186: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_6041, memory_format = torch.contiguous_format);  slice_6041 = None
        view_376: "f32[32, 11]" = torch.ops.aten.view.default(clone_186, [32, 11]);  clone_186 = None
        mm_183: "f32[32, 8]" = torch.ops.aten.mm.default(view_376, slice_37)
        view_377: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_183, [2, 16, 8]);  mm_183 = None
        slice_6048: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1096, 1, 1456, 1472)
        slice_6049: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6048, 2, 0, 16)
        add_185: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6049, view_377);  slice_6049 = view_377 = None
        slice_scatter_1098: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6048, add_185, 2, 0, 16);  slice_6048 = add_185 = None
        slice_scatter_1099: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1096, slice_scatter_1098, 1, 1456, 1472);  slice_scatter_1096 = slice_scatter_1098 = None
        slice_6053: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1099, 1, 1456, 1472)
        slice_6054: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6053, 2, 0, 16)
        slice_scatter_1101: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6053, slice_6054, 2, 0, 16);  slice_6053 = slice_6054 = None
        slice_scatter_1102: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1099, slice_scatter_1101, 1, 1456, 1472);  slice_scatter_1099 = slice_scatter_1101 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6073: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1472, 1488)
        slice_6074: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_6073, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_187: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_6074, memory_format = torch.contiguous_format);  slice_6074 = None
        view_378: "f32[32, 16]" = torch.ops.aten.view.default(clone_187, [32, 16]);  clone_187 = None
        mm_184: "f32[32, 8]" = torch.ops.aten.mm.default(view_378, slice_7)
        view_379: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_184, [2, 16, 8]);  mm_184 = None
        slice_6081: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1102, 1, 1472, 1488)
        slice_6082: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6081, 2, 0, 16)
        add_186: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6082, view_379);  slice_6082 = view_379 = None
        slice_scatter_1104: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6081, add_186, 2, 0, 16);  slice_6081 = add_186 = None
        slice_scatter_1105: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1102, slice_scatter_1104, 1, 1472, 1488);  slice_scatter_1102 = slice_scatter_1104 = None
        slice_6086: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1105, 1, 1472, 1488)
        slice_6087: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6086, 2, 0, 16)
        slice_scatter_1107: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6086, slice_6087, 2, 0, 16);  slice_6086 = slice_6087 = None
        slice_scatter_1108: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1105, slice_scatter_1107, 1, 1472, 1488);  slice_scatter_1105 = slice_scatter_1107 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6107: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_6073, 2, 16, 32);  slice_6073 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_188: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_6107, memory_format = torch.contiguous_format);  slice_6107 = None
        view_380: "f32[32, 11]" = torch.ops.aten.view.default(clone_188, [32, 11]);  clone_188 = None
        mm_185: "f32[32, 8]" = torch.ops.aten.mm.default(view_380, slice_37)
        view_381: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_185, [2, 16, 8]);  mm_185 = None
        slice_6114: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1108, 1, 1472, 1488)
        slice_6115: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6114, 2, 0, 16)
        add_187: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6115, view_381);  slice_6115 = view_381 = None
        slice_scatter_1110: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6114, add_187, 2, 0, 16);  slice_6114 = add_187 = None
        slice_scatter_1111: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1108, slice_scatter_1110, 1, 1472, 1488);  slice_scatter_1108 = slice_scatter_1110 = None
        slice_6119: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1111, 1, 1472, 1488)
        slice_6120: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6119, 2, 0, 16)
        slice_scatter_1113: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6119, slice_6120, 2, 0, 16);  slice_6119 = slice_6120 = None
        slice_scatter_1114: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1111, slice_scatter_1113, 1, 1472, 1488);  slice_scatter_1111 = slice_scatter_1113 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6139: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1488, 1504)
        slice_6140: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_6139, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_189: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_6140, memory_format = torch.contiguous_format);  slice_6140 = None
        view_382: "f32[32, 16]" = torch.ops.aten.view.default(clone_189, [32, 16]);  clone_189 = None
        mm_186: "f32[32, 8]" = torch.ops.aten.mm.default(view_382, slice_7)
        view_383: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_186, [2, 16, 8]);  mm_186 = None
        slice_6147: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1114, 1, 1488, 1504)
        slice_6148: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6147, 2, 0, 16)
        add_188: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6148, view_383);  slice_6148 = view_383 = None
        slice_scatter_1116: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6147, add_188, 2, 0, 16);  slice_6147 = add_188 = None
        slice_scatter_1117: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1114, slice_scatter_1116, 1, 1488, 1504);  slice_scatter_1114 = slice_scatter_1116 = None
        slice_6152: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1117, 1, 1488, 1504)
        slice_6153: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6152, 2, 0, 16)
        slice_scatter_1119: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6152, slice_6153, 2, 0, 16);  slice_6152 = slice_6153 = None
        slice_scatter_1120: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1117, slice_scatter_1119, 1, 1488, 1504);  slice_scatter_1117 = slice_scatter_1119 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6173: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_6139, 2, 16, 32);  slice_6139 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_190: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_6173, memory_format = torch.contiguous_format);  slice_6173 = None
        view_384: "f32[32, 11]" = torch.ops.aten.view.default(clone_190, [32, 11]);  clone_190 = None
        mm_187: "f32[32, 8]" = torch.ops.aten.mm.default(view_384, slice_37)
        view_385: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_187, [2, 16, 8]);  mm_187 = None
        slice_6180: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1120, 1, 1488, 1504)
        slice_6181: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6180, 2, 0, 16)
        add_189: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6181, view_385);  slice_6181 = view_385 = None
        slice_scatter_1122: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6180, add_189, 2, 0, 16);  slice_6180 = add_189 = None
        slice_scatter_1123: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1120, slice_scatter_1122, 1, 1488, 1504);  slice_scatter_1120 = slice_scatter_1122 = None
        slice_6185: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1123, 1, 1488, 1504)
        slice_6186: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6185, 2, 0, 16)
        slice_scatter_1125: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6185, slice_6186, 2, 0, 16);  slice_6185 = slice_6186 = None
        slice_scatter_1126: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1123, slice_scatter_1125, 1, 1488, 1504);  slice_scatter_1123 = slice_scatter_1125 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6205: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1504, 1520)
        slice_6206: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_6205, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_191: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_6206, memory_format = torch.contiguous_format);  slice_6206 = None
        view_386: "f32[32, 16]" = torch.ops.aten.view.default(clone_191, [32, 16]);  clone_191 = None
        mm_188: "f32[32, 8]" = torch.ops.aten.mm.default(view_386, slice_7)
        view_387: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_188, [2, 16, 8]);  mm_188 = None
        slice_6213: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1126, 1, 1504, 1520)
        slice_6214: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6213, 2, 0, 16)
        add_190: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6214, view_387);  slice_6214 = view_387 = None
        slice_scatter_1128: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6213, add_190, 2, 0, 16);  slice_6213 = add_190 = None
        slice_scatter_1129: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1126, slice_scatter_1128, 1, 1504, 1520);  slice_scatter_1126 = slice_scatter_1128 = None
        slice_6218: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1129, 1, 1504, 1520)
        slice_6219: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6218, 2, 0, 16)
        slice_scatter_1131: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6218, slice_6219, 2, 0, 16);  slice_6218 = slice_6219 = None
        slice_scatter_1132: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1129, slice_scatter_1131, 1, 1504, 1520);  slice_scatter_1129 = slice_scatter_1131 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6239: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_6205, 2, 16, 32);  slice_6205 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_192: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_6239, memory_format = torch.contiguous_format);  slice_6239 = None
        view_388: "f32[32, 11]" = torch.ops.aten.view.default(clone_192, [32, 11]);  clone_192 = None
        mm_189: "f32[32, 8]" = torch.ops.aten.mm.default(view_388, slice_37)
        view_389: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_189, [2, 16, 8]);  mm_189 = None
        slice_6246: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1132, 1, 1504, 1520)
        slice_6247: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6246, 2, 0, 16)
        add_191: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6247, view_389);  slice_6247 = view_389 = None
        slice_scatter_1134: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6246, add_191, 2, 0, 16);  slice_6246 = add_191 = None
        slice_scatter_1135: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1132, slice_scatter_1134, 1, 1504, 1520);  slice_scatter_1132 = slice_scatter_1134 = None
        slice_6251: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1135, 1, 1504, 1520)
        slice_6252: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6251, 2, 0, 16)
        slice_scatter_1137: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6251, slice_6252, 2, 0, 16);  slice_6251 = slice_6252 = None
        slice_scatter_1138: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1135, slice_scatter_1137, 1, 1504, 1520);  slice_scatter_1135 = slice_scatter_1137 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6271: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1520, 1536)
        slice_6272: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_6271, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_193: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_6272, memory_format = torch.contiguous_format);  slice_6272 = None
        view_390: "f32[32, 16]" = torch.ops.aten.view.default(clone_193, [32, 16]);  clone_193 = None
        mm_190: "f32[32, 8]" = torch.ops.aten.mm.default(view_390, slice_7)
        view_391: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_190, [2, 16, 8]);  mm_190 = None
        slice_6279: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1138, 1, 1520, 1536)
        slice_6280: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6279, 2, 0, 16)
        add_192: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6280, view_391);  slice_6280 = view_391 = None
        slice_scatter_1140: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6279, add_192, 2, 0, 16);  slice_6279 = add_192 = None
        slice_scatter_1141: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1138, slice_scatter_1140, 1, 1520, 1536);  slice_scatter_1138 = slice_scatter_1140 = None
        slice_6284: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1141, 1, 1520, 1536)
        slice_6285: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6284, 2, 0, 16)
        slice_scatter_1143: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6284, slice_6285, 2, 0, 16);  slice_6284 = slice_6285 = None
        slice_scatter_1144: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1141, slice_scatter_1143, 1, 1520, 1536);  slice_scatter_1141 = slice_scatter_1143 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6305: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_6271, 2, 16, 32);  slice_6271 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_194: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_6305, memory_format = torch.contiguous_format);  slice_6305 = None
        view_392: "f32[32, 11]" = torch.ops.aten.view.default(clone_194, [32, 11]);  clone_194 = None
        mm_191: "f32[32, 8]" = torch.ops.aten.mm.default(view_392, slice_37)
        view_393: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_191, [2, 16, 8]);  mm_191 = None
        slice_6312: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1144, 1, 1520, 1536)
        slice_6313: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6312, 2, 0, 16)
        add_193: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6313, view_393);  slice_6313 = view_393 = None
        slice_scatter_1146: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6312, add_193, 2, 0, 16);  slice_6312 = add_193 = None
        slice_scatter_1147: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1144, slice_scatter_1146, 1, 1520, 1536);  slice_scatter_1144 = slice_scatter_1146 = None
        slice_6317: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1147, 1, 1520, 1536)
        slice_6318: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6317, 2, 0, 16)
        slice_scatter_1149: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6317, slice_6318, 2, 0, 16);  slice_6317 = slice_6318 = None
        slice_scatter_1150: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1147, slice_scatter_1149, 1, 1520, 1536);  slice_scatter_1147 = slice_scatter_1149 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6337: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1536, 1552)
        slice_6338: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_6337, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_195: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_6338, memory_format = torch.contiguous_format);  slice_6338 = None
        view_394: "f32[32, 16]" = torch.ops.aten.view.default(clone_195, [32, 16]);  clone_195 = None
        mm_192: "f32[32, 8]" = torch.ops.aten.mm.default(view_394, slice_7)
        view_395: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_192, [2, 16, 8]);  mm_192 = None
        slice_6345: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1150, 1, 1536, 1552)
        slice_6346: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6345, 2, 0, 16)
        add_194: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6346, view_395);  slice_6346 = view_395 = None
        slice_scatter_1152: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6345, add_194, 2, 0, 16);  slice_6345 = add_194 = None
        slice_scatter_1153: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1150, slice_scatter_1152, 1, 1536, 1552);  slice_scatter_1150 = slice_scatter_1152 = None
        slice_6350: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1153, 1, 1536, 1552)
        slice_6351: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6350, 2, 0, 16)
        slice_scatter_1155: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6350, slice_6351, 2, 0, 16);  slice_6350 = slice_6351 = None
        slice_scatter_1156: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1153, slice_scatter_1155, 1, 1536, 1552);  slice_scatter_1153 = slice_scatter_1155 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6371: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_6337, 2, 16, 32);  slice_6337 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_196: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_6371, memory_format = torch.contiguous_format);  slice_6371 = None
        view_396: "f32[32, 11]" = torch.ops.aten.view.default(clone_196, [32, 11]);  clone_196 = None
        mm_193: "f32[32, 8]" = torch.ops.aten.mm.default(view_396, slice_37)
        view_397: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_193, [2, 16, 8]);  mm_193 = None
        slice_6378: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1156, 1, 1536, 1552)
        slice_6379: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6378, 2, 0, 16)
        add_195: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6379, view_397);  slice_6379 = view_397 = None
        slice_scatter_1158: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6378, add_195, 2, 0, 16);  slice_6378 = add_195 = None
        slice_scatter_1159: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1156, slice_scatter_1158, 1, 1536, 1552);  slice_scatter_1156 = slice_scatter_1158 = None
        slice_6383: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1159, 1, 1536, 1552)
        slice_6384: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6383, 2, 0, 16)
        slice_scatter_1161: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6383, slice_6384, 2, 0, 16);  slice_6383 = slice_6384 = None
        slice_scatter_1162: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1159, slice_scatter_1161, 1, 1536, 1552);  slice_scatter_1159 = slice_scatter_1161 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6403: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1552, 1568)
        slice_6404: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_6403, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_197: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_6404, memory_format = torch.contiguous_format);  slice_6404 = None
        view_398: "f32[32, 16]" = torch.ops.aten.view.default(clone_197, [32, 16]);  clone_197 = None
        mm_194: "f32[32, 8]" = torch.ops.aten.mm.default(view_398, slice_7)
        view_399: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_194, [2, 16, 8]);  mm_194 = None
        slice_6411: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1162, 1, 1552, 1568)
        slice_6412: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6411, 2, 0, 16)
        add_196: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6412, view_399);  slice_6412 = view_399 = None
        slice_scatter_1164: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6411, add_196, 2, 0, 16);  slice_6411 = add_196 = None
        slice_scatter_1165: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1162, slice_scatter_1164, 1, 1552, 1568);  slice_scatter_1162 = slice_scatter_1164 = None
        slice_6416: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1165, 1, 1552, 1568)
        slice_6417: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6416, 2, 0, 16)
        slice_scatter_1167: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6416, slice_6417, 2, 0, 16);  slice_6416 = slice_6417 = None
        slice_scatter_1168: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1165, slice_scatter_1167, 1, 1552, 1568);  slice_scatter_1165 = slice_scatter_1167 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6437: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_6403, 2, 16, 32);  slice_6403 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_198: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_6437, memory_format = torch.contiguous_format);  slice_6437 = None
        view_400: "f32[32, 11]" = torch.ops.aten.view.default(clone_198, [32, 11]);  clone_198 = None
        mm_195: "f32[32, 8]" = torch.ops.aten.mm.default(view_400, slice_37)
        view_401: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_195, [2, 16, 8]);  mm_195 = None
        slice_6444: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1168, 1, 1552, 1568)
        slice_6445: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6444, 2, 0, 16)
        add_197: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6445, view_401);  slice_6445 = view_401 = None
        slice_scatter_1170: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6444, add_197, 2, 0, 16);  slice_6444 = add_197 = None
        slice_scatter_1171: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1168, slice_scatter_1170, 1, 1552, 1568);  slice_scatter_1168 = slice_scatter_1170 = None
        slice_6449: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1171, 1, 1552, 1568)
        slice_6450: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6449, 2, 0, 16)
        slice_scatter_1173: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6449, slice_6450, 2, 0, 16);  slice_6449 = slice_6450 = None
        slice_scatter_1174: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1171, slice_scatter_1173, 1, 1552, 1568);  slice_scatter_1171 = slice_scatter_1173 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6469: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1568, 1584)
        slice_6470: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_6469, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_199: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_6470, memory_format = torch.contiguous_format);  slice_6470 = None
        view_402: "f32[32, 16]" = torch.ops.aten.view.default(clone_199, [32, 16]);  clone_199 = None
        mm_196: "f32[32, 8]" = torch.ops.aten.mm.default(view_402, slice_7)
        view_403: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_196, [2, 16, 8]);  mm_196 = None
        slice_6477: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1174, 1, 1568, 1584)
        slice_6478: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6477, 2, 0, 16)
        add_198: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6478, view_403);  slice_6478 = view_403 = None
        slice_scatter_1176: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6477, add_198, 2, 0, 16);  slice_6477 = add_198 = None
        slice_scatter_1177: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1174, slice_scatter_1176, 1, 1568, 1584);  slice_scatter_1174 = slice_scatter_1176 = None
        slice_6482: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1177, 1, 1568, 1584)
        slice_6483: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6482, 2, 0, 16)
        slice_scatter_1179: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6482, slice_6483, 2, 0, 16);  slice_6482 = slice_6483 = None
        slice_scatter_1180: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1177, slice_scatter_1179, 1, 1568, 1584);  slice_scatter_1177 = slice_scatter_1179 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6503: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_6469, 2, 16, 32);  slice_6469 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_200: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_6503, memory_format = torch.contiguous_format);  slice_6503 = None
        view_404: "f32[32, 11]" = torch.ops.aten.view.default(clone_200, [32, 11]);  clone_200 = None
        mm_197: "f32[32, 8]" = torch.ops.aten.mm.default(view_404, slice_37)
        view_405: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_197, [2, 16, 8]);  mm_197 = None
        slice_6510: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1180, 1, 1568, 1584)
        slice_6511: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6510, 2, 0, 16)
        add_199: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6511, view_405);  slice_6511 = view_405 = None
        slice_scatter_1182: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6510, add_199, 2, 0, 16);  slice_6510 = add_199 = None
        slice_scatter_1183: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1180, slice_scatter_1182, 1, 1568, 1584);  slice_scatter_1180 = slice_scatter_1182 = None
        slice_6515: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1183, 1, 1568, 1584)
        slice_6516: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6515, 2, 0, 16)
        slice_scatter_1185: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6515, slice_6516, 2, 0, 16);  slice_6515 = slice_6516 = None
        slice_scatter_1186: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1183, slice_scatter_1185, 1, 1568, 1584);  slice_scatter_1183 = slice_scatter_1185 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6535: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1584, 1600)
        slice_6536: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_6535, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_201: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_6536, memory_format = torch.contiguous_format);  slice_6536 = None
        view_406: "f32[32, 16]" = torch.ops.aten.view.default(clone_201, [32, 16]);  clone_201 = None
        mm_198: "f32[32, 8]" = torch.ops.aten.mm.default(view_406, slice_7)
        view_407: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_198, [2, 16, 8]);  mm_198 = None
        slice_6543: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1186, 1, 1584, 1600)
        slice_6544: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6543, 2, 0, 16)
        add_200: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6544, view_407);  slice_6544 = view_407 = None
        slice_scatter_1188: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6543, add_200, 2, 0, 16);  slice_6543 = add_200 = None
        slice_scatter_1189: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1186, slice_scatter_1188, 1, 1584, 1600);  slice_scatter_1186 = slice_scatter_1188 = None
        slice_6548: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1189, 1, 1584, 1600)
        slice_6549: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6548, 2, 0, 16)
        slice_scatter_1191: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6548, slice_6549, 2, 0, 16);  slice_6548 = slice_6549 = None
        slice_scatter_1192: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1189, slice_scatter_1191, 1, 1584, 1600);  slice_scatter_1189 = slice_scatter_1191 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6569: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_6535, 2, 16, 32);  slice_6535 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_202: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_6569, memory_format = torch.contiguous_format);  slice_6569 = None
        view_408: "f32[32, 11]" = torch.ops.aten.view.default(clone_202, [32, 11]);  clone_202 = None
        mm_199: "f32[32, 8]" = torch.ops.aten.mm.default(view_408, slice_37)
        view_409: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_199, [2, 16, 8]);  mm_199 = None
        slice_6576: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1192, 1, 1584, 1600)
        slice_6577: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6576, 2, 0, 16)
        add_201: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6577, view_409);  slice_6577 = view_409 = None
        slice_scatter_1194: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6576, add_201, 2, 0, 16);  slice_6576 = add_201 = None
        slice_scatter_1195: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1192, slice_scatter_1194, 1, 1584, 1600);  slice_scatter_1192 = slice_scatter_1194 = None
        slice_6581: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1195, 1, 1584, 1600)
        slice_6582: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6581, 2, 0, 16)
        slice_scatter_1197: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6581, slice_6582, 2, 0, 16);  slice_6581 = slice_6582 = None
        slice_scatter_1198: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1195, slice_scatter_1197, 1, 1584, 1600);  slice_scatter_1195 = slice_scatter_1197 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6601: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1600, 1616)
        slice_6602: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_6601, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_203: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_6602, memory_format = torch.contiguous_format);  slice_6602 = None
        view_410: "f32[32, 16]" = torch.ops.aten.view.default(clone_203, [32, 16]);  clone_203 = None
        mm_200: "f32[32, 8]" = torch.ops.aten.mm.default(view_410, slice_7)
        view_411: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_200, [2, 16, 8]);  mm_200 = None
        slice_6609: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1198, 1, 1600, 1616)
        slice_6610: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6609, 2, 0, 16)
        add_202: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6610, view_411);  slice_6610 = view_411 = None
        slice_scatter_1200: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6609, add_202, 2, 0, 16);  slice_6609 = add_202 = None
        slice_scatter_1201: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1198, slice_scatter_1200, 1, 1600, 1616);  slice_scatter_1198 = slice_scatter_1200 = None
        slice_6614: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1201, 1, 1600, 1616)
        slice_6615: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6614, 2, 0, 16)
        slice_scatter_1203: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6614, slice_6615, 2, 0, 16);  slice_6614 = slice_6615 = None
        slice_scatter_1204: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1201, slice_scatter_1203, 1, 1600, 1616);  slice_scatter_1201 = slice_scatter_1203 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6635: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_6601, 2, 16, 32);  slice_6601 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_204: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_6635, memory_format = torch.contiguous_format);  slice_6635 = None
        view_412: "f32[32, 11]" = torch.ops.aten.view.default(clone_204, [32, 11]);  clone_204 = None
        mm_201: "f32[32, 8]" = torch.ops.aten.mm.default(view_412, slice_37)
        view_413: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_201, [2, 16, 8]);  mm_201 = None
        slice_6642: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1204, 1, 1600, 1616)
        slice_6643: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6642, 2, 0, 16)
        add_203: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6643, view_413);  slice_6643 = view_413 = None
        slice_scatter_1206: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6642, add_203, 2, 0, 16);  slice_6642 = add_203 = None
        slice_scatter_1207: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1204, slice_scatter_1206, 1, 1600, 1616);  slice_scatter_1204 = slice_scatter_1206 = None
        slice_6647: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1207, 1, 1600, 1616)
        slice_6648: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6647, 2, 0, 16)
        slice_scatter_1209: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6647, slice_6648, 2, 0, 16);  slice_6647 = slice_6648 = None
        slice_scatter_1210: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1207, slice_scatter_1209, 1, 1600, 1616);  slice_scatter_1207 = slice_scatter_1209 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6667: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1616, 1632)
        slice_6668: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_6667, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_205: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_6668, memory_format = torch.contiguous_format);  slice_6668 = None
        view_414: "f32[32, 16]" = torch.ops.aten.view.default(clone_205, [32, 16]);  clone_205 = None
        mm_202: "f32[32, 8]" = torch.ops.aten.mm.default(view_414, slice_7)
        view_415: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_202, [2, 16, 8]);  mm_202 = None
        slice_6675: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1210, 1, 1616, 1632)
        slice_6676: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6675, 2, 0, 16)
        add_204: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6676, view_415);  slice_6676 = view_415 = None
        slice_scatter_1212: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6675, add_204, 2, 0, 16);  slice_6675 = add_204 = None
        slice_scatter_1213: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1210, slice_scatter_1212, 1, 1616, 1632);  slice_scatter_1210 = slice_scatter_1212 = None
        slice_6680: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1213, 1, 1616, 1632)
        slice_6681: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6680, 2, 0, 16)
        slice_scatter_1215: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6680, slice_6681, 2, 0, 16);  slice_6680 = slice_6681 = None
        slice_scatter_1216: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1213, slice_scatter_1215, 1, 1616, 1632);  slice_scatter_1213 = slice_scatter_1215 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6701: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_6667, 2, 16, 32);  slice_6667 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_206: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_6701, memory_format = torch.contiguous_format);  slice_6701 = None
        view_416: "f32[32, 11]" = torch.ops.aten.view.default(clone_206, [32, 11]);  clone_206 = None
        mm_203: "f32[32, 8]" = torch.ops.aten.mm.default(view_416, slice_37)
        view_417: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_203, [2, 16, 8]);  mm_203 = None
        slice_6708: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1216, 1, 1616, 1632)
        slice_6709: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6708, 2, 0, 16)
        add_205: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6709, view_417);  slice_6709 = view_417 = None
        slice_scatter_1218: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6708, add_205, 2, 0, 16);  slice_6708 = add_205 = None
        slice_scatter_1219: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1216, slice_scatter_1218, 1, 1616, 1632);  slice_scatter_1216 = slice_scatter_1218 = None
        slice_6713: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1219, 1, 1616, 1632)
        slice_6714: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6713, 2, 0, 16)
        slice_scatter_1221: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6713, slice_6714, 2, 0, 16);  slice_6713 = slice_6714 = None
        slice_scatter_1222: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1219, slice_scatter_1221, 1, 1616, 1632);  slice_scatter_1219 = slice_scatter_1221 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6733: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1632, 1648)
        slice_6734: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_6733, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_207: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_6734, memory_format = torch.contiguous_format);  slice_6734 = None
        view_418: "f32[32, 16]" = torch.ops.aten.view.default(clone_207, [32, 16]);  clone_207 = None
        mm_204: "f32[32, 8]" = torch.ops.aten.mm.default(view_418, slice_7)
        view_419: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_204, [2, 16, 8]);  mm_204 = None
        slice_6741: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1222, 1, 1632, 1648)
        slice_6742: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6741, 2, 0, 16)
        add_206: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6742, view_419);  slice_6742 = view_419 = None
        slice_scatter_1224: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6741, add_206, 2, 0, 16);  slice_6741 = add_206 = None
        slice_scatter_1225: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1222, slice_scatter_1224, 1, 1632, 1648);  slice_scatter_1222 = slice_scatter_1224 = None
        slice_6746: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1225, 1, 1632, 1648)
        slice_6747: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6746, 2, 0, 16)
        slice_scatter_1227: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6746, slice_6747, 2, 0, 16);  slice_6746 = slice_6747 = None
        slice_scatter_1228: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1225, slice_scatter_1227, 1, 1632, 1648);  slice_scatter_1225 = slice_scatter_1227 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6767: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_6733, 2, 16, 32);  slice_6733 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_208: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_6767, memory_format = torch.contiguous_format);  slice_6767 = None
        view_420: "f32[32, 11]" = torch.ops.aten.view.default(clone_208, [32, 11]);  clone_208 = None
        mm_205: "f32[32, 8]" = torch.ops.aten.mm.default(view_420, slice_37)
        view_421: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_205, [2, 16, 8]);  mm_205 = None
        slice_6774: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1228, 1, 1632, 1648)
        slice_6775: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6774, 2, 0, 16)
        add_207: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6775, view_421);  slice_6775 = view_421 = None
        slice_scatter_1230: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6774, add_207, 2, 0, 16);  slice_6774 = add_207 = None
        slice_scatter_1231: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1228, slice_scatter_1230, 1, 1632, 1648);  slice_scatter_1228 = slice_scatter_1230 = None
        slice_6779: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1231, 1, 1632, 1648)
        slice_6780: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6779, 2, 0, 16)
        slice_scatter_1233: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6779, slice_6780, 2, 0, 16);  slice_6779 = slice_6780 = None
        slice_scatter_1234: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1231, slice_scatter_1233, 1, 1632, 1648);  slice_scatter_1231 = slice_scatter_1233 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6799: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1648, 1664)
        slice_6800: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_6799, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_209: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_6800, memory_format = torch.contiguous_format);  slice_6800 = None
        view_422: "f32[32, 16]" = torch.ops.aten.view.default(clone_209, [32, 16]);  clone_209 = None
        mm_206: "f32[32, 8]" = torch.ops.aten.mm.default(view_422, slice_7)
        view_423: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_206, [2, 16, 8]);  mm_206 = None
        slice_6807: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1234, 1, 1648, 1664)
        slice_6808: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6807, 2, 0, 16)
        add_208: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6808, view_423);  slice_6808 = view_423 = None
        slice_scatter_1236: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6807, add_208, 2, 0, 16);  slice_6807 = add_208 = None
        slice_scatter_1237: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1234, slice_scatter_1236, 1, 1648, 1664);  slice_scatter_1234 = slice_scatter_1236 = None
        slice_6812: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1237, 1, 1648, 1664)
        slice_6813: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6812, 2, 0, 16)
        slice_scatter_1239: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6812, slice_6813, 2, 0, 16);  slice_6812 = slice_6813 = None
        slice_scatter_1240: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1237, slice_scatter_1239, 1, 1648, 1664);  slice_scatter_1237 = slice_scatter_1239 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6833: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_6799, 2, 16, 32);  slice_6799 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_210: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_6833, memory_format = torch.contiguous_format);  slice_6833 = None
        view_424: "f32[32, 11]" = torch.ops.aten.view.default(clone_210, [32, 11]);  clone_210 = None
        mm_207: "f32[32, 8]" = torch.ops.aten.mm.default(view_424, slice_37)
        view_425: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_207, [2, 16, 8]);  mm_207 = None
        slice_6840: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1240, 1, 1648, 1664)
        slice_6841: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6840, 2, 0, 16)
        add_209: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6841, view_425);  slice_6841 = view_425 = None
        slice_scatter_1242: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6840, add_209, 2, 0, 16);  slice_6840 = add_209 = None
        slice_scatter_1243: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1240, slice_scatter_1242, 1, 1648, 1664);  slice_scatter_1240 = slice_scatter_1242 = None
        slice_6845: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1243, 1, 1648, 1664)
        slice_6846: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6845, 2, 0, 16)
        slice_scatter_1245: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6845, slice_6846, 2, 0, 16);  slice_6845 = slice_6846 = None
        slice_scatter_1246: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1243, slice_scatter_1245, 1, 1648, 1664);  slice_scatter_1243 = slice_scatter_1245 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6865: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1664, 1680)
        slice_6866: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_6865, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_211: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_6866, memory_format = torch.contiguous_format);  slice_6866 = None
        view_426: "f32[32, 16]" = torch.ops.aten.view.default(clone_211, [32, 16]);  clone_211 = None
        mm_208: "f32[32, 8]" = torch.ops.aten.mm.default(view_426, slice_7)
        view_427: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_208, [2, 16, 8]);  mm_208 = None
        slice_6873: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1246, 1, 1664, 1680)
        slice_6874: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6873, 2, 0, 16)
        add_210: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6874, view_427);  slice_6874 = view_427 = None
        slice_scatter_1248: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6873, add_210, 2, 0, 16);  slice_6873 = add_210 = None
        slice_scatter_1249: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1246, slice_scatter_1248, 1, 1664, 1680);  slice_scatter_1246 = slice_scatter_1248 = None
        slice_6878: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1249, 1, 1664, 1680)
        slice_6879: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6878, 2, 0, 16)
        slice_scatter_1251: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6878, slice_6879, 2, 0, 16);  slice_6878 = slice_6879 = None
        slice_scatter_1252: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1249, slice_scatter_1251, 1, 1664, 1680);  slice_scatter_1249 = slice_scatter_1251 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6899: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_6865, 2, 16, 32);  slice_6865 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_212: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_6899, memory_format = torch.contiguous_format);  slice_6899 = None
        view_428: "f32[32, 11]" = torch.ops.aten.view.default(clone_212, [32, 11]);  clone_212 = None
        mm_209: "f32[32, 8]" = torch.ops.aten.mm.default(view_428, slice_37)
        view_429: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_209, [2, 16, 8]);  mm_209 = None
        slice_6906: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1252, 1, 1664, 1680)
        slice_6907: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6906, 2, 0, 16)
        add_211: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6907, view_429);  slice_6907 = view_429 = None
        slice_scatter_1254: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6906, add_211, 2, 0, 16);  slice_6906 = add_211 = None
        slice_scatter_1255: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1252, slice_scatter_1254, 1, 1664, 1680);  slice_scatter_1252 = slice_scatter_1254 = None
        slice_6911: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1255, 1, 1664, 1680)
        slice_6912: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6911, 2, 0, 16)
        slice_scatter_1257: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6911, slice_6912, 2, 0, 16);  slice_6911 = slice_6912 = None
        slice_scatter_1258: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1255, slice_scatter_1257, 1, 1664, 1680);  slice_scatter_1255 = slice_scatter_1257 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6931: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1680, 1696)
        slice_6932: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_6931, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_213: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_6932, memory_format = torch.contiguous_format);  slice_6932 = None
        view_430: "f32[32, 16]" = torch.ops.aten.view.default(clone_213, [32, 16]);  clone_213 = None
        mm_210: "f32[32, 8]" = torch.ops.aten.mm.default(view_430, slice_7)
        view_431: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_210, [2, 16, 8]);  mm_210 = None
        slice_6939: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1258, 1, 1680, 1696)
        slice_6940: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6939, 2, 0, 16)
        add_212: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6940, view_431);  slice_6940 = view_431 = None
        slice_scatter_1260: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6939, add_212, 2, 0, 16);  slice_6939 = add_212 = None
        slice_scatter_1261: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1258, slice_scatter_1260, 1, 1680, 1696);  slice_scatter_1258 = slice_scatter_1260 = None
        slice_6944: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1261, 1, 1680, 1696)
        slice_6945: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6944, 2, 0, 16)
        slice_scatter_1263: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6944, slice_6945, 2, 0, 16);  slice_6944 = slice_6945 = None
        slice_scatter_1264: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1261, slice_scatter_1263, 1, 1680, 1696);  slice_scatter_1261 = slice_scatter_1263 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6965: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_6931, 2, 16, 32);  slice_6931 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_214: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_6965, memory_format = torch.contiguous_format);  slice_6965 = None
        view_432: "f32[32, 11]" = torch.ops.aten.view.default(clone_214, [32, 11]);  clone_214 = None
        mm_211: "f32[32, 8]" = torch.ops.aten.mm.default(view_432, slice_37)
        view_433: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_211, [2, 16, 8]);  mm_211 = None
        slice_6972: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1264, 1, 1680, 1696)
        slice_6973: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6972, 2, 0, 16)
        add_213: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_6973, view_433);  slice_6973 = view_433 = None
        slice_scatter_1266: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6972, add_213, 2, 0, 16);  slice_6972 = add_213 = None
        slice_scatter_1267: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1264, slice_scatter_1266, 1, 1680, 1696);  slice_scatter_1264 = slice_scatter_1266 = None
        slice_6977: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1267, 1, 1680, 1696)
        slice_6978: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_6977, 2, 0, 16)
        slice_scatter_1269: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_6977, slice_6978, 2, 0, 16);  slice_6977 = slice_6978 = None
        slice_scatter_1270: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1267, slice_scatter_1269, 1, 1680, 1696);  slice_scatter_1267 = slice_scatter_1269 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_6997: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1696, 1712)
        slice_6998: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_6997, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_215: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_6998, memory_format = torch.contiguous_format);  slice_6998 = None
        view_434: "f32[32, 16]" = torch.ops.aten.view.default(clone_215, [32, 16]);  clone_215 = None
        mm_212: "f32[32, 8]" = torch.ops.aten.mm.default(view_434, slice_7)
        view_435: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_212, [2, 16, 8]);  mm_212 = None
        slice_7005: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1270, 1, 1696, 1712)
        slice_7006: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7005, 2, 0, 16)
        add_214: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7006, view_435);  slice_7006 = view_435 = None
        slice_scatter_1272: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7005, add_214, 2, 0, 16);  slice_7005 = add_214 = None
        slice_scatter_1273: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1270, slice_scatter_1272, 1, 1696, 1712);  slice_scatter_1270 = slice_scatter_1272 = None
        slice_7010: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1273, 1, 1696, 1712)
        slice_7011: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7010, 2, 0, 16)
        slice_scatter_1275: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7010, slice_7011, 2, 0, 16);  slice_7010 = slice_7011 = None
        slice_scatter_1276: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1273, slice_scatter_1275, 1, 1696, 1712);  slice_scatter_1273 = slice_scatter_1275 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7031: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_6997, 2, 16, 32);  slice_6997 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_216: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_7031, memory_format = torch.contiguous_format);  slice_7031 = None
        view_436: "f32[32, 11]" = torch.ops.aten.view.default(clone_216, [32, 11]);  clone_216 = None
        mm_213: "f32[32, 8]" = torch.ops.aten.mm.default(view_436, slice_37)
        view_437: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_213, [2, 16, 8]);  mm_213 = None
        slice_7038: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1276, 1, 1696, 1712)
        slice_7039: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7038, 2, 0, 16)
        add_215: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7039, view_437);  slice_7039 = view_437 = None
        slice_scatter_1278: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7038, add_215, 2, 0, 16);  slice_7038 = add_215 = None
        slice_scatter_1279: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1276, slice_scatter_1278, 1, 1696, 1712);  slice_scatter_1276 = slice_scatter_1278 = None
        slice_7043: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1279, 1, 1696, 1712)
        slice_7044: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7043, 2, 0, 16)
        slice_scatter_1281: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7043, slice_7044, 2, 0, 16);  slice_7043 = slice_7044 = None
        slice_scatter_1282: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1279, slice_scatter_1281, 1, 1696, 1712);  slice_scatter_1279 = slice_scatter_1281 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7063: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1712, 1728)
        slice_7064: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_7063, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_217: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_7064, memory_format = torch.contiguous_format);  slice_7064 = None
        view_438: "f32[32, 16]" = torch.ops.aten.view.default(clone_217, [32, 16]);  clone_217 = None
        mm_214: "f32[32, 8]" = torch.ops.aten.mm.default(view_438, slice_7)
        view_439: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_214, [2, 16, 8]);  mm_214 = None
        slice_7071: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1282, 1, 1712, 1728)
        slice_7072: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7071, 2, 0, 16)
        add_216: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7072, view_439);  slice_7072 = view_439 = None
        slice_scatter_1284: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7071, add_216, 2, 0, 16);  slice_7071 = add_216 = None
        slice_scatter_1285: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1282, slice_scatter_1284, 1, 1712, 1728);  slice_scatter_1282 = slice_scatter_1284 = None
        slice_7076: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1285, 1, 1712, 1728)
        slice_7077: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7076, 2, 0, 16)
        slice_scatter_1287: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7076, slice_7077, 2, 0, 16);  slice_7076 = slice_7077 = None
        slice_scatter_1288: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1285, slice_scatter_1287, 1, 1712, 1728);  slice_scatter_1285 = slice_scatter_1287 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7097: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_7063, 2, 16, 32);  slice_7063 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_218: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_7097, memory_format = torch.contiguous_format);  slice_7097 = None
        view_440: "f32[32, 11]" = torch.ops.aten.view.default(clone_218, [32, 11]);  clone_218 = None
        mm_215: "f32[32, 8]" = torch.ops.aten.mm.default(view_440, slice_37)
        view_441: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_215, [2, 16, 8]);  mm_215 = None
        slice_7104: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1288, 1, 1712, 1728)
        slice_7105: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7104, 2, 0, 16)
        add_217: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7105, view_441);  slice_7105 = view_441 = None
        slice_scatter_1290: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7104, add_217, 2, 0, 16);  slice_7104 = add_217 = None
        slice_scatter_1291: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1288, slice_scatter_1290, 1, 1712, 1728);  slice_scatter_1288 = slice_scatter_1290 = None
        slice_7109: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1291, 1, 1712, 1728)
        slice_7110: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7109, 2, 0, 16)
        slice_scatter_1293: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7109, slice_7110, 2, 0, 16);  slice_7109 = slice_7110 = None
        slice_scatter_1294: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1291, slice_scatter_1293, 1, 1712, 1728);  slice_scatter_1291 = slice_scatter_1293 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7129: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1728, 1744)
        slice_7130: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_7129, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_219: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_7130, memory_format = torch.contiguous_format);  slice_7130 = None
        view_442: "f32[32, 16]" = torch.ops.aten.view.default(clone_219, [32, 16]);  clone_219 = None
        mm_216: "f32[32, 8]" = torch.ops.aten.mm.default(view_442, slice_7)
        view_443: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_216, [2, 16, 8]);  mm_216 = None
        slice_7137: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1294, 1, 1728, 1744)
        slice_7138: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7137, 2, 0, 16)
        add_218: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7138, view_443);  slice_7138 = view_443 = None
        slice_scatter_1296: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7137, add_218, 2, 0, 16);  slice_7137 = add_218 = None
        slice_scatter_1297: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1294, slice_scatter_1296, 1, 1728, 1744);  slice_scatter_1294 = slice_scatter_1296 = None
        slice_7142: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1297, 1, 1728, 1744)
        slice_7143: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7142, 2, 0, 16)
        slice_scatter_1299: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7142, slice_7143, 2, 0, 16);  slice_7142 = slice_7143 = None
        slice_scatter_1300: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1297, slice_scatter_1299, 1, 1728, 1744);  slice_scatter_1297 = slice_scatter_1299 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7163: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_7129, 2, 16, 32);  slice_7129 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_220: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_7163, memory_format = torch.contiguous_format);  slice_7163 = None
        view_444: "f32[32, 11]" = torch.ops.aten.view.default(clone_220, [32, 11]);  clone_220 = None
        mm_217: "f32[32, 8]" = torch.ops.aten.mm.default(view_444, slice_37)
        view_445: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_217, [2, 16, 8]);  mm_217 = None
        slice_7170: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1300, 1, 1728, 1744)
        slice_7171: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7170, 2, 0, 16)
        add_219: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7171, view_445);  slice_7171 = view_445 = None
        slice_scatter_1302: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7170, add_219, 2, 0, 16);  slice_7170 = add_219 = None
        slice_scatter_1303: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1300, slice_scatter_1302, 1, 1728, 1744);  slice_scatter_1300 = slice_scatter_1302 = None
        slice_7175: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1303, 1, 1728, 1744)
        slice_7176: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7175, 2, 0, 16)
        slice_scatter_1305: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7175, slice_7176, 2, 0, 16);  slice_7175 = slice_7176 = None
        slice_scatter_1306: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1303, slice_scatter_1305, 1, 1728, 1744);  slice_scatter_1303 = slice_scatter_1305 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7195: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1744, 1760)
        slice_7196: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_7195, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_221: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_7196, memory_format = torch.contiguous_format);  slice_7196 = None
        view_446: "f32[32, 16]" = torch.ops.aten.view.default(clone_221, [32, 16]);  clone_221 = None
        mm_218: "f32[32, 8]" = torch.ops.aten.mm.default(view_446, slice_7)
        view_447: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_218, [2, 16, 8]);  mm_218 = None
        slice_7203: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1306, 1, 1744, 1760)
        slice_7204: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7203, 2, 0, 16)
        add_220: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7204, view_447);  slice_7204 = view_447 = None
        slice_scatter_1308: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7203, add_220, 2, 0, 16);  slice_7203 = add_220 = None
        slice_scatter_1309: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1306, slice_scatter_1308, 1, 1744, 1760);  slice_scatter_1306 = slice_scatter_1308 = None
        slice_7208: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1309, 1, 1744, 1760)
        slice_7209: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7208, 2, 0, 16)
        slice_scatter_1311: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7208, slice_7209, 2, 0, 16);  slice_7208 = slice_7209 = None
        slice_scatter_1312: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1309, slice_scatter_1311, 1, 1744, 1760);  slice_scatter_1309 = slice_scatter_1311 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7229: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_7195, 2, 16, 32);  slice_7195 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_222: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_7229, memory_format = torch.contiguous_format);  slice_7229 = None
        view_448: "f32[32, 11]" = torch.ops.aten.view.default(clone_222, [32, 11]);  clone_222 = None
        mm_219: "f32[32, 8]" = torch.ops.aten.mm.default(view_448, slice_37)
        view_449: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_219, [2, 16, 8]);  mm_219 = None
        slice_7236: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1312, 1, 1744, 1760)
        slice_7237: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7236, 2, 0, 16)
        add_221: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7237, view_449);  slice_7237 = view_449 = None
        slice_scatter_1314: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7236, add_221, 2, 0, 16);  slice_7236 = add_221 = None
        slice_scatter_1315: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1312, slice_scatter_1314, 1, 1744, 1760);  slice_scatter_1312 = slice_scatter_1314 = None
        slice_7241: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1315, 1, 1744, 1760)
        slice_7242: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7241, 2, 0, 16)
        slice_scatter_1317: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7241, slice_7242, 2, 0, 16);  slice_7241 = slice_7242 = None
        slice_scatter_1318: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1315, slice_scatter_1317, 1, 1744, 1760);  slice_scatter_1315 = slice_scatter_1317 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7261: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1760, 1776)
        slice_7262: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_7261, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_223: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_7262, memory_format = torch.contiguous_format);  slice_7262 = None
        view_450: "f32[32, 16]" = torch.ops.aten.view.default(clone_223, [32, 16]);  clone_223 = None
        mm_220: "f32[32, 8]" = torch.ops.aten.mm.default(view_450, slice_7)
        view_451: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_220, [2, 16, 8]);  mm_220 = None
        slice_7269: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1318, 1, 1760, 1776)
        slice_7270: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7269, 2, 0, 16)
        add_222: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7270, view_451);  slice_7270 = view_451 = None
        slice_scatter_1320: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7269, add_222, 2, 0, 16);  slice_7269 = add_222 = None
        slice_scatter_1321: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1318, slice_scatter_1320, 1, 1760, 1776);  slice_scatter_1318 = slice_scatter_1320 = None
        slice_7274: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1321, 1, 1760, 1776)
        slice_7275: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7274, 2, 0, 16)
        slice_scatter_1323: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7274, slice_7275, 2, 0, 16);  slice_7274 = slice_7275 = None
        slice_scatter_1324: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1321, slice_scatter_1323, 1, 1760, 1776);  slice_scatter_1321 = slice_scatter_1323 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7295: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_7261, 2, 16, 32);  slice_7261 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_224: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_7295, memory_format = torch.contiguous_format);  slice_7295 = None
        view_452: "f32[32, 11]" = torch.ops.aten.view.default(clone_224, [32, 11]);  clone_224 = None
        mm_221: "f32[32, 8]" = torch.ops.aten.mm.default(view_452, slice_37)
        view_453: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_221, [2, 16, 8]);  mm_221 = None
        slice_7302: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1324, 1, 1760, 1776)
        slice_7303: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7302, 2, 0, 16)
        add_223: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7303, view_453);  slice_7303 = view_453 = None
        slice_scatter_1326: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7302, add_223, 2, 0, 16);  slice_7302 = add_223 = None
        slice_scatter_1327: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1324, slice_scatter_1326, 1, 1760, 1776);  slice_scatter_1324 = slice_scatter_1326 = None
        slice_7307: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1327, 1, 1760, 1776)
        slice_7308: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7307, 2, 0, 16)
        slice_scatter_1329: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7307, slice_7308, 2, 0, 16);  slice_7307 = slice_7308 = None
        slice_scatter_1330: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1327, slice_scatter_1329, 1, 1760, 1776);  slice_scatter_1327 = slice_scatter_1329 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7327: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1776, 1792)
        slice_7328: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_7327, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_225: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_7328, memory_format = torch.contiguous_format);  slice_7328 = None
        view_454: "f32[32, 16]" = torch.ops.aten.view.default(clone_225, [32, 16]);  clone_225 = None
        mm_222: "f32[32, 8]" = torch.ops.aten.mm.default(view_454, slice_7)
        view_455: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_222, [2, 16, 8]);  mm_222 = None
        slice_7335: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1330, 1, 1776, 1792)
        slice_7336: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7335, 2, 0, 16)
        add_224: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7336, view_455);  slice_7336 = view_455 = None
        slice_scatter_1332: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7335, add_224, 2, 0, 16);  slice_7335 = add_224 = None
        slice_scatter_1333: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1330, slice_scatter_1332, 1, 1776, 1792);  slice_scatter_1330 = slice_scatter_1332 = None
        slice_7340: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1333, 1, 1776, 1792)
        slice_7341: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7340, 2, 0, 16)
        slice_scatter_1335: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7340, slice_7341, 2, 0, 16);  slice_7340 = slice_7341 = None
        slice_scatter_1336: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1333, slice_scatter_1335, 1, 1776, 1792);  slice_scatter_1333 = slice_scatter_1335 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7361: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_7327, 2, 16, 32);  slice_7327 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_226: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_7361, memory_format = torch.contiguous_format);  slice_7361 = None
        view_456: "f32[32, 11]" = torch.ops.aten.view.default(clone_226, [32, 11]);  clone_226 = None
        mm_223: "f32[32, 8]" = torch.ops.aten.mm.default(view_456, slice_37)
        view_457: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_223, [2, 16, 8]);  mm_223 = None
        slice_7368: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1336, 1, 1776, 1792)
        slice_7369: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7368, 2, 0, 16)
        add_225: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7369, view_457);  slice_7369 = view_457 = None
        slice_scatter_1338: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7368, add_225, 2, 0, 16);  slice_7368 = add_225 = None
        slice_scatter_1339: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1336, slice_scatter_1338, 1, 1776, 1792);  slice_scatter_1336 = slice_scatter_1338 = None
        slice_7373: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1339, 1, 1776, 1792)
        slice_7374: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7373, 2, 0, 16)
        slice_scatter_1341: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7373, slice_7374, 2, 0, 16);  slice_7373 = slice_7374 = None
        slice_scatter_1342: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1339, slice_scatter_1341, 1, 1776, 1792);  slice_scatter_1339 = slice_scatter_1341 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7393: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1792, 1808)
        slice_7394: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_7393, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_227: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_7394, memory_format = torch.contiguous_format);  slice_7394 = None
        view_458: "f32[32, 16]" = torch.ops.aten.view.default(clone_227, [32, 16]);  clone_227 = None
        mm_224: "f32[32, 8]" = torch.ops.aten.mm.default(view_458, slice_7)
        view_459: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_224, [2, 16, 8]);  mm_224 = None
        slice_7401: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1342, 1, 1792, 1808)
        slice_7402: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7401, 2, 0, 16)
        add_226: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7402, view_459);  slice_7402 = view_459 = None
        slice_scatter_1344: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7401, add_226, 2, 0, 16);  slice_7401 = add_226 = None
        slice_scatter_1345: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1342, slice_scatter_1344, 1, 1792, 1808);  slice_scatter_1342 = slice_scatter_1344 = None
        slice_7406: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1345, 1, 1792, 1808)
        slice_7407: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7406, 2, 0, 16)
        slice_scatter_1347: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7406, slice_7407, 2, 0, 16);  slice_7406 = slice_7407 = None
        slice_scatter_1348: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1345, slice_scatter_1347, 1, 1792, 1808);  slice_scatter_1345 = slice_scatter_1347 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7427: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_7393, 2, 16, 32);  slice_7393 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_228: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_7427, memory_format = torch.contiguous_format);  slice_7427 = None
        view_460: "f32[32, 11]" = torch.ops.aten.view.default(clone_228, [32, 11]);  clone_228 = None
        mm_225: "f32[32, 8]" = torch.ops.aten.mm.default(view_460, slice_37)
        view_461: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_225, [2, 16, 8]);  mm_225 = None
        slice_7434: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1348, 1, 1792, 1808)
        slice_7435: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7434, 2, 0, 16)
        add_227: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7435, view_461);  slice_7435 = view_461 = None
        slice_scatter_1350: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7434, add_227, 2, 0, 16);  slice_7434 = add_227 = None
        slice_scatter_1351: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1348, slice_scatter_1350, 1, 1792, 1808);  slice_scatter_1348 = slice_scatter_1350 = None
        slice_7439: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1351, 1, 1792, 1808)
        slice_7440: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7439, 2, 0, 16)
        slice_scatter_1353: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7439, slice_7440, 2, 0, 16);  slice_7439 = slice_7440 = None
        slice_scatter_1354: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1351, slice_scatter_1353, 1, 1792, 1808);  slice_scatter_1351 = slice_scatter_1353 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7459: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1808, 1824)
        slice_7460: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_7459, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_229: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_7460, memory_format = torch.contiguous_format);  slice_7460 = None
        view_462: "f32[32, 16]" = torch.ops.aten.view.default(clone_229, [32, 16]);  clone_229 = None
        mm_226: "f32[32, 8]" = torch.ops.aten.mm.default(view_462, slice_7)
        view_463: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_226, [2, 16, 8]);  mm_226 = None
        slice_7467: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1354, 1, 1808, 1824)
        slice_7468: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7467, 2, 0, 16)
        add_228: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7468, view_463);  slice_7468 = view_463 = None
        slice_scatter_1356: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7467, add_228, 2, 0, 16);  slice_7467 = add_228 = None
        slice_scatter_1357: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1354, slice_scatter_1356, 1, 1808, 1824);  slice_scatter_1354 = slice_scatter_1356 = None
        slice_7472: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1357, 1, 1808, 1824)
        slice_7473: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7472, 2, 0, 16)
        slice_scatter_1359: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7472, slice_7473, 2, 0, 16);  slice_7472 = slice_7473 = None
        slice_scatter_1360: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1357, slice_scatter_1359, 1, 1808, 1824);  slice_scatter_1357 = slice_scatter_1359 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7493: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_7459, 2, 16, 32);  slice_7459 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_230: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_7493, memory_format = torch.contiguous_format);  slice_7493 = None
        view_464: "f32[32, 11]" = torch.ops.aten.view.default(clone_230, [32, 11]);  clone_230 = None
        mm_227: "f32[32, 8]" = torch.ops.aten.mm.default(view_464, slice_37)
        view_465: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_227, [2, 16, 8]);  mm_227 = None
        slice_7500: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1360, 1, 1808, 1824)
        slice_7501: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7500, 2, 0, 16)
        add_229: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7501, view_465);  slice_7501 = view_465 = None
        slice_scatter_1362: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7500, add_229, 2, 0, 16);  slice_7500 = add_229 = None
        slice_scatter_1363: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1360, slice_scatter_1362, 1, 1808, 1824);  slice_scatter_1360 = slice_scatter_1362 = None
        slice_7505: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1363, 1, 1808, 1824)
        slice_7506: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7505, 2, 0, 16)
        slice_scatter_1365: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7505, slice_7506, 2, 0, 16);  slice_7505 = slice_7506 = None
        slice_scatter_1366: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1363, slice_scatter_1365, 1, 1808, 1824);  slice_scatter_1363 = slice_scatter_1365 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7525: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1824, 1840)
        slice_7526: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_7525, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_231: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_7526, memory_format = torch.contiguous_format);  slice_7526 = None
        view_466: "f32[32, 16]" = torch.ops.aten.view.default(clone_231, [32, 16]);  clone_231 = None
        mm_228: "f32[32, 8]" = torch.ops.aten.mm.default(view_466, slice_7)
        view_467: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_228, [2, 16, 8]);  mm_228 = None
        slice_7533: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1366, 1, 1824, 1840)
        slice_7534: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7533, 2, 0, 16)
        add_230: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7534, view_467);  slice_7534 = view_467 = None
        slice_scatter_1368: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7533, add_230, 2, 0, 16);  slice_7533 = add_230 = None
        slice_scatter_1369: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1366, slice_scatter_1368, 1, 1824, 1840);  slice_scatter_1366 = slice_scatter_1368 = None
        slice_7538: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1369, 1, 1824, 1840)
        slice_7539: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7538, 2, 0, 16)
        slice_scatter_1371: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7538, slice_7539, 2, 0, 16);  slice_7538 = slice_7539 = None
        slice_scatter_1372: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1369, slice_scatter_1371, 1, 1824, 1840);  slice_scatter_1369 = slice_scatter_1371 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7559: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_7525, 2, 16, 32);  slice_7525 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_232: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_7559, memory_format = torch.contiguous_format);  slice_7559 = None
        view_468: "f32[32, 11]" = torch.ops.aten.view.default(clone_232, [32, 11]);  clone_232 = None
        mm_229: "f32[32, 8]" = torch.ops.aten.mm.default(view_468, slice_37)
        view_469: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_229, [2, 16, 8]);  mm_229 = None
        slice_7566: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1372, 1, 1824, 1840)
        slice_7567: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7566, 2, 0, 16)
        add_231: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7567, view_469);  slice_7567 = view_469 = None
        slice_scatter_1374: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7566, add_231, 2, 0, 16);  slice_7566 = add_231 = None
        slice_scatter_1375: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1372, slice_scatter_1374, 1, 1824, 1840);  slice_scatter_1372 = slice_scatter_1374 = None
        slice_7571: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1375, 1, 1824, 1840)
        slice_7572: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7571, 2, 0, 16)
        slice_scatter_1377: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7571, slice_7572, 2, 0, 16);  slice_7571 = slice_7572 = None
        slice_scatter_1378: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1375, slice_scatter_1377, 1, 1824, 1840);  slice_scatter_1375 = slice_scatter_1377 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7591: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1840, 1856)
        slice_7592: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_7591, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_233: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_7592, memory_format = torch.contiguous_format);  slice_7592 = None
        view_470: "f32[32, 16]" = torch.ops.aten.view.default(clone_233, [32, 16]);  clone_233 = None
        mm_230: "f32[32, 8]" = torch.ops.aten.mm.default(view_470, slice_7)
        view_471: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_230, [2, 16, 8]);  mm_230 = None
        slice_7599: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1378, 1, 1840, 1856)
        slice_7600: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7599, 2, 0, 16)
        add_232: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7600, view_471);  slice_7600 = view_471 = None
        slice_scatter_1380: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7599, add_232, 2, 0, 16);  slice_7599 = add_232 = None
        slice_scatter_1381: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1378, slice_scatter_1380, 1, 1840, 1856);  slice_scatter_1378 = slice_scatter_1380 = None
        slice_7604: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1381, 1, 1840, 1856)
        slice_7605: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7604, 2, 0, 16)
        slice_scatter_1383: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7604, slice_7605, 2, 0, 16);  slice_7604 = slice_7605 = None
        slice_scatter_1384: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1381, slice_scatter_1383, 1, 1840, 1856);  slice_scatter_1381 = slice_scatter_1383 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7625: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_7591, 2, 16, 32);  slice_7591 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_234: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_7625, memory_format = torch.contiguous_format);  slice_7625 = None
        view_472: "f32[32, 11]" = torch.ops.aten.view.default(clone_234, [32, 11]);  clone_234 = None
        mm_231: "f32[32, 8]" = torch.ops.aten.mm.default(view_472, slice_37)
        view_473: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_231, [2, 16, 8]);  mm_231 = None
        slice_7632: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1384, 1, 1840, 1856)
        slice_7633: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7632, 2, 0, 16)
        add_233: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7633, view_473);  slice_7633 = view_473 = None
        slice_scatter_1386: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7632, add_233, 2, 0, 16);  slice_7632 = add_233 = None
        slice_scatter_1387: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1384, slice_scatter_1386, 1, 1840, 1856);  slice_scatter_1384 = slice_scatter_1386 = None
        slice_7637: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1387, 1, 1840, 1856)
        slice_7638: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7637, 2, 0, 16)
        slice_scatter_1389: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7637, slice_7638, 2, 0, 16);  slice_7637 = slice_7638 = None
        slice_scatter_1390: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1387, slice_scatter_1389, 1, 1840, 1856);  slice_scatter_1387 = slice_scatter_1389 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7657: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1856, 1872)
        slice_7658: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_7657, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_235: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_7658, memory_format = torch.contiguous_format);  slice_7658 = None
        view_474: "f32[32, 16]" = torch.ops.aten.view.default(clone_235, [32, 16]);  clone_235 = None
        mm_232: "f32[32, 8]" = torch.ops.aten.mm.default(view_474, slice_7)
        view_475: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_232, [2, 16, 8]);  mm_232 = None
        slice_7665: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1390, 1, 1856, 1872)
        slice_7666: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7665, 2, 0, 16)
        add_234: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7666, view_475);  slice_7666 = view_475 = None
        slice_scatter_1392: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7665, add_234, 2, 0, 16);  slice_7665 = add_234 = None
        slice_scatter_1393: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1390, slice_scatter_1392, 1, 1856, 1872);  slice_scatter_1390 = slice_scatter_1392 = None
        slice_7670: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1393, 1, 1856, 1872)
        slice_7671: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7670, 2, 0, 16)
        slice_scatter_1395: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7670, slice_7671, 2, 0, 16);  slice_7670 = slice_7671 = None
        slice_scatter_1396: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1393, slice_scatter_1395, 1, 1856, 1872);  slice_scatter_1393 = slice_scatter_1395 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7691: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_7657, 2, 16, 32);  slice_7657 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_236: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_7691, memory_format = torch.contiguous_format);  slice_7691 = None
        view_476: "f32[32, 11]" = torch.ops.aten.view.default(clone_236, [32, 11]);  clone_236 = None
        mm_233: "f32[32, 8]" = torch.ops.aten.mm.default(view_476, slice_37)
        view_477: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_233, [2, 16, 8]);  mm_233 = None
        slice_7698: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1396, 1, 1856, 1872)
        slice_7699: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7698, 2, 0, 16)
        add_235: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7699, view_477);  slice_7699 = view_477 = None
        slice_scatter_1398: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7698, add_235, 2, 0, 16);  slice_7698 = add_235 = None
        slice_scatter_1399: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1396, slice_scatter_1398, 1, 1856, 1872);  slice_scatter_1396 = slice_scatter_1398 = None
        slice_7703: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1399, 1, 1856, 1872)
        slice_7704: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7703, 2, 0, 16)
        slice_scatter_1401: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7703, slice_7704, 2, 0, 16);  slice_7703 = slice_7704 = None
        slice_scatter_1402: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1399, slice_scatter_1401, 1, 1856, 1872);  slice_scatter_1399 = slice_scatter_1401 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7723: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1872, 1888)
        slice_7724: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_7723, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_237: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_7724, memory_format = torch.contiguous_format);  slice_7724 = None
        view_478: "f32[32, 16]" = torch.ops.aten.view.default(clone_237, [32, 16]);  clone_237 = None
        mm_234: "f32[32, 8]" = torch.ops.aten.mm.default(view_478, slice_7)
        view_479: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_234, [2, 16, 8]);  mm_234 = None
        slice_7731: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1402, 1, 1872, 1888)
        slice_7732: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7731, 2, 0, 16)
        add_236: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7732, view_479);  slice_7732 = view_479 = None
        slice_scatter_1404: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7731, add_236, 2, 0, 16);  slice_7731 = add_236 = None
        slice_scatter_1405: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1402, slice_scatter_1404, 1, 1872, 1888);  slice_scatter_1402 = slice_scatter_1404 = None
        slice_7736: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1405, 1, 1872, 1888)
        slice_7737: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7736, 2, 0, 16)
        slice_scatter_1407: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7736, slice_7737, 2, 0, 16);  slice_7736 = slice_7737 = None
        slice_scatter_1408: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1405, slice_scatter_1407, 1, 1872, 1888);  slice_scatter_1405 = slice_scatter_1407 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7757: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_7723, 2, 16, 32);  slice_7723 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_238: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_7757, memory_format = torch.contiguous_format);  slice_7757 = None
        view_480: "f32[32, 11]" = torch.ops.aten.view.default(clone_238, [32, 11]);  clone_238 = None
        mm_235: "f32[32, 8]" = torch.ops.aten.mm.default(view_480, slice_37)
        view_481: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_235, [2, 16, 8]);  mm_235 = None
        slice_7764: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1408, 1, 1872, 1888)
        slice_7765: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7764, 2, 0, 16)
        add_237: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7765, view_481);  slice_7765 = view_481 = None
        slice_scatter_1410: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7764, add_237, 2, 0, 16);  slice_7764 = add_237 = None
        slice_scatter_1411: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1408, slice_scatter_1410, 1, 1872, 1888);  slice_scatter_1408 = slice_scatter_1410 = None
        slice_7769: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1411, 1, 1872, 1888)
        slice_7770: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7769, 2, 0, 16)
        slice_scatter_1413: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7769, slice_7770, 2, 0, 16);  slice_7769 = slice_7770 = None
        slice_scatter_1414: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1411, slice_scatter_1413, 1, 1872, 1888);  slice_scatter_1411 = slice_scatter_1413 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7789: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1888, 1904)
        slice_7790: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_7789, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_239: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_7790, memory_format = torch.contiguous_format);  slice_7790 = None
        view_482: "f32[32, 16]" = torch.ops.aten.view.default(clone_239, [32, 16]);  clone_239 = None
        mm_236: "f32[32, 8]" = torch.ops.aten.mm.default(view_482, slice_7)
        view_483: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_236, [2, 16, 8]);  mm_236 = None
        slice_7797: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1414, 1, 1888, 1904)
        slice_7798: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7797, 2, 0, 16)
        add_238: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7798, view_483);  slice_7798 = view_483 = None
        slice_scatter_1416: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7797, add_238, 2, 0, 16);  slice_7797 = add_238 = None
        slice_scatter_1417: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1414, slice_scatter_1416, 1, 1888, 1904);  slice_scatter_1414 = slice_scatter_1416 = None
        slice_7802: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1417, 1, 1888, 1904)
        slice_7803: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7802, 2, 0, 16)
        slice_scatter_1419: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7802, slice_7803, 2, 0, 16);  slice_7802 = slice_7803 = None
        slice_scatter_1420: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1417, slice_scatter_1419, 1, 1888, 1904);  slice_scatter_1417 = slice_scatter_1419 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7823: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_7789, 2, 16, 32);  slice_7789 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_240: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_7823, memory_format = torch.contiguous_format);  slice_7823 = None
        view_484: "f32[32, 11]" = torch.ops.aten.view.default(clone_240, [32, 11]);  clone_240 = None
        mm_237: "f32[32, 8]" = torch.ops.aten.mm.default(view_484, slice_37)
        view_485: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_237, [2, 16, 8]);  mm_237 = None
        slice_7830: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1420, 1, 1888, 1904)
        slice_7831: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7830, 2, 0, 16)
        add_239: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7831, view_485);  slice_7831 = view_485 = None
        slice_scatter_1422: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7830, add_239, 2, 0, 16);  slice_7830 = add_239 = None
        slice_scatter_1423: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1420, slice_scatter_1422, 1, 1888, 1904);  slice_scatter_1420 = slice_scatter_1422 = None
        slice_7835: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1423, 1, 1888, 1904)
        slice_7836: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7835, 2, 0, 16)
        slice_scatter_1425: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7835, slice_7836, 2, 0, 16);  slice_7835 = slice_7836 = None
        slice_scatter_1426: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1423, slice_scatter_1425, 1, 1888, 1904);  slice_scatter_1423 = slice_scatter_1425 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7855: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1904, 1920)
        slice_7856: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_7855, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_241: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_7856, memory_format = torch.contiguous_format);  slice_7856 = None
        view_486: "f32[32, 16]" = torch.ops.aten.view.default(clone_241, [32, 16]);  clone_241 = None
        mm_238: "f32[32, 8]" = torch.ops.aten.mm.default(view_486, slice_7)
        view_487: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_238, [2, 16, 8]);  mm_238 = None
        slice_7863: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1426, 1, 1904, 1920)
        slice_7864: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7863, 2, 0, 16)
        add_240: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7864, view_487);  slice_7864 = view_487 = None
        slice_scatter_1428: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7863, add_240, 2, 0, 16);  slice_7863 = add_240 = None
        slice_scatter_1429: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1426, slice_scatter_1428, 1, 1904, 1920);  slice_scatter_1426 = slice_scatter_1428 = None
        slice_7868: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1429, 1, 1904, 1920)
        slice_7869: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7868, 2, 0, 16)
        slice_scatter_1431: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7868, slice_7869, 2, 0, 16);  slice_7868 = slice_7869 = None
        slice_scatter_1432: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1429, slice_scatter_1431, 1, 1904, 1920);  slice_scatter_1429 = slice_scatter_1431 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7889: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_7855, 2, 16, 32);  slice_7855 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_242: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_7889, memory_format = torch.contiguous_format);  slice_7889 = None
        view_488: "f32[32, 11]" = torch.ops.aten.view.default(clone_242, [32, 11]);  clone_242 = None
        mm_239: "f32[32, 8]" = torch.ops.aten.mm.default(view_488, slice_37)
        view_489: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_239, [2, 16, 8]);  mm_239 = None
        slice_7896: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1432, 1, 1904, 1920)
        slice_7897: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7896, 2, 0, 16)
        add_241: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7897, view_489);  slice_7897 = view_489 = None
        slice_scatter_1434: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7896, add_241, 2, 0, 16);  slice_7896 = add_241 = None
        slice_scatter_1435: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1432, slice_scatter_1434, 1, 1904, 1920);  slice_scatter_1432 = slice_scatter_1434 = None
        slice_7901: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1435, 1, 1904, 1920)
        slice_7902: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7901, 2, 0, 16)
        slice_scatter_1437: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7901, slice_7902, 2, 0, 16);  slice_7901 = slice_7902 = None
        slice_scatter_1438: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1435, slice_scatter_1437, 1, 1904, 1920);  slice_scatter_1435 = slice_scatter_1437 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7921: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1920, 1936)
        slice_7922: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_7921, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_243: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_7922, memory_format = torch.contiguous_format);  slice_7922 = None
        view_490: "f32[32, 16]" = torch.ops.aten.view.default(clone_243, [32, 16]);  clone_243 = None
        mm_240: "f32[32, 8]" = torch.ops.aten.mm.default(view_490, slice_7)
        view_491: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_240, [2, 16, 8]);  mm_240 = None
        slice_7929: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1438, 1, 1920, 1936)
        slice_7930: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7929, 2, 0, 16)
        add_242: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7930, view_491);  slice_7930 = view_491 = None
        slice_scatter_1440: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7929, add_242, 2, 0, 16);  slice_7929 = add_242 = None
        slice_scatter_1441: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1438, slice_scatter_1440, 1, 1920, 1936);  slice_scatter_1438 = slice_scatter_1440 = None
        slice_7934: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1441, 1, 1920, 1936)
        slice_7935: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7934, 2, 0, 16)
        slice_scatter_1443: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7934, slice_7935, 2, 0, 16);  slice_7934 = slice_7935 = None
        slice_scatter_1444: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1441, slice_scatter_1443, 1, 1920, 1936);  slice_scatter_1441 = slice_scatter_1443 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7955: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_7921, 2, 16, 32);  slice_7921 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_244: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_7955, memory_format = torch.contiguous_format);  slice_7955 = None
        view_492: "f32[32, 11]" = torch.ops.aten.view.default(clone_244, [32, 11]);  clone_244 = None
        mm_241: "f32[32, 8]" = torch.ops.aten.mm.default(view_492, slice_37)
        view_493: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_241, [2, 16, 8]);  mm_241 = None
        slice_7962: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1444, 1, 1920, 1936)
        slice_7963: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7962, 2, 0, 16)
        add_243: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7963, view_493);  slice_7963 = view_493 = None
        slice_scatter_1446: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7962, add_243, 2, 0, 16);  slice_7962 = add_243 = None
        slice_scatter_1447: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1444, slice_scatter_1446, 1, 1920, 1936);  slice_scatter_1444 = slice_scatter_1446 = None
        slice_7967: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1447, 1, 1920, 1936)
        slice_7968: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7967, 2, 0, 16)
        slice_scatter_1449: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7967, slice_7968, 2, 0, 16);  slice_7967 = slice_7968 = None
        slice_scatter_1450: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1447, slice_scatter_1449, 1, 1920, 1936);  slice_scatter_1447 = slice_scatter_1449 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_7987: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1936, 1952)
        slice_7988: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_7987, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_245: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_7988, memory_format = torch.contiguous_format);  slice_7988 = None
        view_494: "f32[32, 16]" = torch.ops.aten.view.default(clone_245, [32, 16]);  clone_245 = None
        mm_242: "f32[32, 8]" = torch.ops.aten.mm.default(view_494, slice_7)
        view_495: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_242, [2, 16, 8]);  mm_242 = None
        slice_7995: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1450, 1, 1936, 1952)
        slice_7996: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_7995, 2, 0, 16)
        add_244: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_7996, view_495);  slice_7996 = view_495 = None
        slice_scatter_1452: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_7995, add_244, 2, 0, 16);  slice_7995 = add_244 = None
        slice_scatter_1453: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1450, slice_scatter_1452, 1, 1936, 1952);  slice_scatter_1450 = slice_scatter_1452 = None
        slice_8000: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1453, 1, 1936, 1952)
        slice_8001: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8000, 2, 0, 16)
        slice_scatter_1455: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8000, slice_8001, 2, 0, 16);  slice_8000 = slice_8001 = None
        slice_scatter_1456: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1453, slice_scatter_1455, 1, 1936, 1952);  slice_scatter_1453 = slice_scatter_1455 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8021: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_7987, 2, 16, 32);  slice_7987 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_246: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_8021, memory_format = torch.contiguous_format);  slice_8021 = None
        view_496: "f32[32, 11]" = torch.ops.aten.view.default(clone_246, [32, 11]);  clone_246 = None
        mm_243: "f32[32, 8]" = torch.ops.aten.mm.default(view_496, slice_37)
        view_497: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_243, [2, 16, 8]);  mm_243 = None
        slice_8028: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1456, 1, 1936, 1952)
        slice_8029: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8028, 2, 0, 16)
        add_245: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8029, view_497);  slice_8029 = view_497 = None
        slice_scatter_1458: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8028, add_245, 2, 0, 16);  slice_8028 = add_245 = None
        slice_scatter_1459: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1456, slice_scatter_1458, 1, 1936, 1952);  slice_scatter_1456 = slice_scatter_1458 = None
        slice_8033: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1459, 1, 1936, 1952)
        slice_8034: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8033, 2, 0, 16)
        slice_scatter_1461: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8033, slice_8034, 2, 0, 16);  slice_8033 = slice_8034 = None
        slice_scatter_1462: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1459, slice_scatter_1461, 1, 1936, 1952);  slice_scatter_1459 = slice_scatter_1461 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8053: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1952, 1968)
        slice_8054: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_8053, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_247: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_8054, memory_format = torch.contiguous_format);  slice_8054 = None
        view_498: "f32[32, 16]" = torch.ops.aten.view.default(clone_247, [32, 16]);  clone_247 = None
        mm_244: "f32[32, 8]" = torch.ops.aten.mm.default(view_498, slice_7)
        view_499: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_244, [2, 16, 8]);  mm_244 = None
        slice_8061: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1462, 1, 1952, 1968)
        slice_8062: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8061, 2, 0, 16)
        add_246: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8062, view_499);  slice_8062 = view_499 = None
        slice_scatter_1464: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8061, add_246, 2, 0, 16);  slice_8061 = add_246 = None
        slice_scatter_1465: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1462, slice_scatter_1464, 1, 1952, 1968);  slice_scatter_1462 = slice_scatter_1464 = None
        slice_8066: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1465, 1, 1952, 1968)
        slice_8067: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8066, 2, 0, 16)
        slice_scatter_1467: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8066, slice_8067, 2, 0, 16);  slice_8066 = slice_8067 = None
        slice_scatter_1468: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1465, slice_scatter_1467, 1, 1952, 1968);  slice_scatter_1465 = slice_scatter_1467 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8087: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_8053, 2, 16, 32);  slice_8053 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_248: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_8087, memory_format = torch.contiguous_format);  slice_8087 = None
        view_500: "f32[32, 11]" = torch.ops.aten.view.default(clone_248, [32, 11]);  clone_248 = None
        mm_245: "f32[32, 8]" = torch.ops.aten.mm.default(view_500, slice_37)
        view_501: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_245, [2, 16, 8]);  mm_245 = None
        slice_8094: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1468, 1, 1952, 1968)
        slice_8095: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8094, 2, 0, 16)
        add_247: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8095, view_501);  slice_8095 = view_501 = None
        slice_scatter_1470: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8094, add_247, 2, 0, 16);  slice_8094 = add_247 = None
        slice_scatter_1471: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1468, slice_scatter_1470, 1, 1952, 1968);  slice_scatter_1468 = slice_scatter_1470 = None
        slice_8099: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1471, 1, 1952, 1968)
        slice_8100: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8099, 2, 0, 16)
        slice_scatter_1473: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8099, slice_8100, 2, 0, 16);  slice_8099 = slice_8100 = None
        slice_scatter_1474: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1471, slice_scatter_1473, 1, 1952, 1968);  slice_scatter_1471 = slice_scatter_1473 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8119: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1968, 1984)
        slice_8120: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_8119, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_249: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_8120, memory_format = torch.contiguous_format);  slice_8120 = None
        view_502: "f32[32, 16]" = torch.ops.aten.view.default(clone_249, [32, 16]);  clone_249 = None
        mm_246: "f32[32, 8]" = torch.ops.aten.mm.default(view_502, slice_7)
        view_503: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_246, [2, 16, 8]);  mm_246 = None
        slice_8127: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1474, 1, 1968, 1984)
        slice_8128: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8127, 2, 0, 16)
        add_248: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8128, view_503);  slice_8128 = view_503 = None
        slice_scatter_1476: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8127, add_248, 2, 0, 16);  slice_8127 = add_248 = None
        slice_scatter_1477: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1474, slice_scatter_1476, 1, 1968, 1984);  slice_scatter_1474 = slice_scatter_1476 = None
        slice_8132: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1477, 1, 1968, 1984)
        slice_8133: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8132, 2, 0, 16)
        slice_scatter_1479: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8132, slice_8133, 2, 0, 16);  slice_8132 = slice_8133 = None
        slice_scatter_1480: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1477, slice_scatter_1479, 1, 1968, 1984);  slice_scatter_1477 = slice_scatter_1479 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8153: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_8119, 2, 16, 32);  slice_8119 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_250: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_8153, memory_format = torch.contiguous_format);  slice_8153 = None
        view_504: "f32[32, 11]" = torch.ops.aten.view.default(clone_250, [32, 11]);  clone_250 = None
        mm_247: "f32[32, 8]" = torch.ops.aten.mm.default(view_504, slice_37)
        view_505: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_247, [2, 16, 8]);  mm_247 = None
        slice_8160: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1480, 1, 1968, 1984)
        slice_8161: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8160, 2, 0, 16)
        add_249: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8161, view_505);  slice_8161 = view_505 = None
        slice_scatter_1482: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8160, add_249, 2, 0, 16);  slice_8160 = add_249 = None
        slice_scatter_1483: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1480, slice_scatter_1482, 1, 1968, 1984);  slice_scatter_1480 = slice_scatter_1482 = None
        slice_8165: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1483, 1, 1968, 1984)
        slice_8166: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8165, 2, 0, 16)
        slice_scatter_1485: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8165, slice_8166, 2, 0, 16);  slice_8165 = slice_8166 = None
        slice_scatter_1486: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1483, slice_scatter_1485, 1, 1968, 1984);  slice_scatter_1483 = slice_scatter_1485 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8185: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 1984, 2000)
        slice_8186: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_8185, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_251: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_8186, memory_format = torch.contiguous_format);  slice_8186 = None
        view_506: "f32[32, 16]" = torch.ops.aten.view.default(clone_251, [32, 16]);  clone_251 = None
        mm_248: "f32[32, 8]" = torch.ops.aten.mm.default(view_506, slice_7)
        view_507: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_248, [2, 16, 8]);  mm_248 = None
        slice_8193: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1486, 1, 1984, 2000)
        slice_8194: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8193, 2, 0, 16)
        add_250: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8194, view_507);  slice_8194 = view_507 = None
        slice_scatter_1488: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8193, add_250, 2, 0, 16);  slice_8193 = add_250 = None
        slice_scatter_1489: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1486, slice_scatter_1488, 1, 1984, 2000);  slice_scatter_1486 = slice_scatter_1488 = None
        slice_8198: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1489, 1, 1984, 2000)
        slice_8199: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8198, 2, 0, 16)
        slice_scatter_1491: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8198, slice_8199, 2, 0, 16);  slice_8198 = slice_8199 = None
        slice_scatter_1492: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1489, slice_scatter_1491, 1, 1984, 2000);  slice_scatter_1489 = slice_scatter_1491 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8219: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_8185, 2, 16, 32);  slice_8185 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_252: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_8219, memory_format = torch.contiguous_format);  slice_8219 = None
        view_508: "f32[32, 11]" = torch.ops.aten.view.default(clone_252, [32, 11]);  clone_252 = None
        mm_249: "f32[32, 8]" = torch.ops.aten.mm.default(view_508, slice_37)
        view_509: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_249, [2, 16, 8]);  mm_249 = None
        slice_8226: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1492, 1, 1984, 2000)
        slice_8227: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8226, 2, 0, 16)
        add_251: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8227, view_509);  slice_8227 = view_509 = None
        slice_scatter_1494: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8226, add_251, 2, 0, 16);  slice_8226 = add_251 = None
        slice_scatter_1495: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1492, slice_scatter_1494, 1, 1984, 2000);  slice_scatter_1492 = slice_scatter_1494 = None
        slice_8231: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1495, 1, 1984, 2000)
        slice_8232: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8231, 2, 0, 16)
        slice_scatter_1497: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8231, slice_8232, 2, 0, 16);  slice_8231 = slice_8232 = None
        slice_scatter_1498: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1495, slice_scatter_1497, 1, 1984, 2000);  slice_scatter_1495 = slice_scatter_1497 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8251: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2000, 2016)
        slice_8252: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_8251, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_253: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_8252, memory_format = torch.contiguous_format);  slice_8252 = None
        view_510: "f32[32, 16]" = torch.ops.aten.view.default(clone_253, [32, 16]);  clone_253 = None
        mm_250: "f32[32, 8]" = torch.ops.aten.mm.default(view_510, slice_7)
        view_511: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_250, [2, 16, 8]);  mm_250 = None
        slice_8259: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1498, 1, 2000, 2016)
        slice_8260: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8259, 2, 0, 16)
        add_252: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8260, view_511);  slice_8260 = view_511 = None
        slice_scatter_1500: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8259, add_252, 2, 0, 16);  slice_8259 = add_252 = None
        slice_scatter_1501: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1498, slice_scatter_1500, 1, 2000, 2016);  slice_scatter_1498 = slice_scatter_1500 = None
        slice_8264: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1501, 1, 2000, 2016)
        slice_8265: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8264, 2, 0, 16)
        slice_scatter_1503: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8264, slice_8265, 2, 0, 16);  slice_8264 = slice_8265 = None
        slice_scatter_1504: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1501, slice_scatter_1503, 1, 2000, 2016);  slice_scatter_1501 = slice_scatter_1503 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8285: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_8251, 2, 16, 32);  slice_8251 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_254: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_8285, memory_format = torch.contiguous_format);  slice_8285 = None
        view_512: "f32[32, 11]" = torch.ops.aten.view.default(clone_254, [32, 11]);  clone_254 = None
        mm_251: "f32[32, 8]" = torch.ops.aten.mm.default(view_512, slice_37)
        view_513: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_251, [2, 16, 8]);  mm_251 = None
        slice_8292: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1504, 1, 2000, 2016)
        slice_8293: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8292, 2, 0, 16)
        add_253: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8293, view_513);  slice_8293 = view_513 = None
        slice_scatter_1506: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8292, add_253, 2, 0, 16);  slice_8292 = add_253 = None
        slice_scatter_1507: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1504, slice_scatter_1506, 1, 2000, 2016);  slice_scatter_1504 = slice_scatter_1506 = None
        slice_8297: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1507, 1, 2000, 2016)
        slice_8298: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8297, 2, 0, 16)
        slice_scatter_1509: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8297, slice_8298, 2, 0, 16);  slice_8297 = slice_8298 = None
        slice_scatter_1510: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1507, slice_scatter_1509, 1, 2000, 2016);  slice_scatter_1507 = slice_scatter_1509 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8317: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2016, 2032)
        slice_8318: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_8317, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_255: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_8318, memory_format = torch.contiguous_format);  slice_8318 = None
        view_514: "f32[32, 16]" = torch.ops.aten.view.default(clone_255, [32, 16]);  clone_255 = None
        mm_252: "f32[32, 8]" = torch.ops.aten.mm.default(view_514, slice_7)
        view_515: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_252, [2, 16, 8]);  mm_252 = None
        slice_8325: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1510, 1, 2016, 2032)
        slice_8326: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8325, 2, 0, 16)
        add_254: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8326, view_515);  slice_8326 = view_515 = None
        slice_scatter_1512: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8325, add_254, 2, 0, 16);  slice_8325 = add_254 = None
        slice_scatter_1513: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1510, slice_scatter_1512, 1, 2016, 2032);  slice_scatter_1510 = slice_scatter_1512 = None
        slice_8330: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1513, 1, 2016, 2032)
        slice_8331: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8330, 2, 0, 16)
        slice_scatter_1515: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8330, slice_8331, 2, 0, 16);  slice_8330 = slice_8331 = None
        slice_scatter_1516: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1513, slice_scatter_1515, 1, 2016, 2032);  slice_scatter_1513 = slice_scatter_1515 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8351: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_8317, 2, 16, 32);  slice_8317 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_256: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_8351, memory_format = torch.contiguous_format);  slice_8351 = None
        view_516: "f32[32, 11]" = torch.ops.aten.view.default(clone_256, [32, 11]);  clone_256 = None
        mm_253: "f32[32, 8]" = torch.ops.aten.mm.default(view_516, slice_37)
        view_517: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_253, [2, 16, 8]);  mm_253 = None
        slice_8358: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1516, 1, 2016, 2032)
        slice_8359: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8358, 2, 0, 16)
        add_255: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8359, view_517);  slice_8359 = view_517 = None
        slice_scatter_1518: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8358, add_255, 2, 0, 16);  slice_8358 = add_255 = None
        slice_scatter_1519: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1516, slice_scatter_1518, 1, 2016, 2032);  slice_scatter_1516 = slice_scatter_1518 = None
        slice_8363: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1519, 1, 2016, 2032)
        slice_8364: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8363, 2, 0, 16)
        slice_scatter_1521: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8363, slice_8364, 2, 0, 16);  slice_8363 = slice_8364 = None
        slice_scatter_1522: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1519, slice_scatter_1521, 1, 2016, 2032);  slice_scatter_1519 = slice_scatter_1521 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8383: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2032, 2048)
        slice_8384: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_8383, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_257: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_8384, memory_format = torch.contiguous_format);  slice_8384 = None
        view_518: "f32[32, 16]" = torch.ops.aten.view.default(clone_257, [32, 16]);  clone_257 = None
        mm_254: "f32[32, 8]" = torch.ops.aten.mm.default(view_518, slice_7)
        view_519: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_254, [2, 16, 8]);  mm_254 = None
        slice_8391: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1522, 1, 2032, 2048)
        slice_8392: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8391, 2, 0, 16)
        add_256: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8392, view_519);  slice_8392 = view_519 = None
        slice_scatter_1524: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8391, add_256, 2, 0, 16);  slice_8391 = add_256 = None
        slice_scatter_1525: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1522, slice_scatter_1524, 1, 2032, 2048);  slice_scatter_1522 = slice_scatter_1524 = None
        slice_8396: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1525, 1, 2032, 2048)
        slice_8397: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8396, 2, 0, 16)
        slice_scatter_1527: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8396, slice_8397, 2, 0, 16);  slice_8396 = slice_8397 = None
        slice_scatter_1528: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1525, slice_scatter_1527, 1, 2032, 2048);  slice_scatter_1525 = slice_scatter_1527 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8417: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_8383, 2, 16, 32);  slice_8383 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_258: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_8417, memory_format = torch.contiguous_format);  slice_8417 = None
        view_520: "f32[32, 11]" = torch.ops.aten.view.default(clone_258, [32, 11]);  clone_258 = None
        mm_255: "f32[32, 8]" = torch.ops.aten.mm.default(view_520, slice_37)
        view_521: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_255, [2, 16, 8]);  mm_255 = None
        slice_8424: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1528, 1, 2032, 2048)
        slice_8425: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8424, 2, 0, 16)
        add_257: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8425, view_521);  slice_8425 = view_521 = None
        slice_scatter_1530: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8424, add_257, 2, 0, 16);  slice_8424 = add_257 = None
        slice_scatter_1531: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1528, slice_scatter_1530, 1, 2032, 2048);  slice_scatter_1528 = slice_scatter_1530 = None
        slice_8429: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1531, 1, 2032, 2048)
        slice_8430: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8429, 2, 0, 16)
        slice_scatter_1533: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8429, slice_8430, 2, 0, 16);  slice_8429 = slice_8430 = None
        slice_scatter_1534: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1531, slice_scatter_1533, 1, 2032, 2048);  slice_scatter_1531 = slice_scatter_1533 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8449: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2048, 2064)
        slice_8450: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_8449, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_259: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_8450, memory_format = torch.contiguous_format);  slice_8450 = None
        view_522: "f32[32, 16]" = torch.ops.aten.view.default(clone_259, [32, 16]);  clone_259 = None
        mm_256: "f32[32, 8]" = torch.ops.aten.mm.default(view_522, slice_7)
        view_523: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_256, [2, 16, 8]);  mm_256 = None
        slice_8457: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1534, 1, 2048, 2064)
        slice_8458: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8457, 2, 0, 16)
        add_258: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8458, view_523);  slice_8458 = view_523 = None
        slice_scatter_1536: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8457, add_258, 2, 0, 16);  slice_8457 = add_258 = None
        slice_scatter_1537: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1534, slice_scatter_1536, 1, 2048, 2064);  slice_scatter_1534 = slice_scatter_1536 = None
        slice_8462: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1537, 1, 2048, 2064)
        slice_8463: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8462, 2, 0, 16)
        slice_scatter_1539: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8462, slice_8463, 2, 0, 16);  slice_8462 = slice_8463 = None
        slice_scatter_1540: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1537, slice_scatter_1539, 1, 2048, 2064);  slice_scatter_1537 = slice_scatter_1539 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8483: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_8449, 2, 16, 32);  slice_8449 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_260: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_8483, memory_format = torch.contiguous_format);  slice_8483 = None
        view_524: "f32[32, 11]" = torch.ops.aten.view.default(clone_260, [32, 11]);  clone_260 = None
        mm_257: "f32[32, 8]" = torch.ops.aten.mm.default(view_524, slice_37)
        view_525: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_257, [2, 16, 8]);  mm_257 = None
        slice_8490: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1540, 1, 2048, 2064)
        slice_8491: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8490, 2, 0, 16)
        add_259: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8491, view_525);  slice_8491 = view_525 = None
        slice_scatter_1542: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8490, add_259, 2, 0, 16);  slice_8490 = add_259 = None
        slice_scatter_1543: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1540, slice_scatter_1542, 1, 2048, 2064);  slice_scatter_1540 = slice_scatter_1542 = None
        slice_8495: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1543, 1, 2048, 2064)
        slice_8496: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8495, 2, 0, 16)
        slice_scatter_1545: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8495, slice_8496, 2, 0, 16);  slice_8495 = slice_8496 = None
        slice_scatter_1546: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1543, slice_scatter_1545, 1, 2048, 2064);  slice_scatter_1543 = slice_scatter_1545 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8515: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2064, 2080)
        slice_8516: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_8515, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_261: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_8516, memory_format = torch.contiguous_format);  slice_8516 = None
        view_526: "f32[32, 16]" = torch.ops.aten.view.default(clone_261, [32, 16]);  clone_261 = None
        mm_258: "f32[32, 8]" = torch.ops.aten.mm.default(view_526, slice_7)
        view_527: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_258, [2, 16, 8]);  mm_258 = None
        slice_8523: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1546, 1, 2064, 2080)
        slice_8524: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8523, 2, 0, 16)
        add_260: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8524, view_527);  slice_8524 = view_527 = None
        slice_scatter_1548: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8523, add_260, 2, 0, 16);  slice_8523 = add_260 = None
        slice_scatter_1549: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1546, slice_scatter_1548, 1, 2064, 2080);  slice_scatter_1546 = slice_scatter_1548 = None
        slice_8528: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1549, 1, 2064, 2080)
        slice_8529: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8528, 2, 0, 16)
        slice_scatter_1551: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8528, slice_8529, 2, 0, 16);  slice_8528 = slice_8529 = None
        slice_scatter_1552: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1549, slice_scatter_1551, 1, 2064, 2080);  slice_scatter_1549 = slice_scatter_1551 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8549: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_8515, 2, 16, 32);  slice_8515 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_262: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_8549, memory_format = torch.contiguous_format);  slice_8549 = None
        view_528: "f32[32, 11]" = torch.ops.aten.view.default(clone_262, [32, 11]);  clone_262 = None
        mm_259: "f32[32, 8]" = torch.ops.aten.mm.default(view_528, slice_37)
        view_529: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_259, [2, 16, 8]);  mm_259 = None
        slice_8556: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1552, 1, 2064, 2080)
        slice_8557: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8556, 2, 0, 16)
        add_261: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8557, view_529);  slice_8557 = view_529 = None
        slice_scatter_1554: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8556, add_261, 2, 0, 16);  slice_8556 = add_261 = None
        slice_scatter_1555: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1552, slice_scatter_1554, 1, 2064, 2080);  slice_scatter_1552 = slice_scatter_1554 = None
        slice_8561: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1555, 1, 2064, 2080)
        slice_8562: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8561, 2, 0, 16)
        slice_scatter_1557: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8561, slice_8562, 2, 0, 16);  slice_8561 = slice_8562 = None
        slice_scatter_1558: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1555, slice_scatter_1557, 1, 2064, 2080);  slice_scatter_1555 = slice_scatter_1557 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8581: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2080, 2096)
        slice_8582: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_8581, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_263: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_8582, memory_format = torch.contiguous_format);  slice_8582 = None
        view_530: "f32[32, 16]" = torch.ops.aten.view.default(clone_263, [32, 16]);  clone_263 = None
        mm_260: "f32[32, 8]" = torch.ops.aten.mm.default(view_530, slice_7)
        view_531: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_260, [2, 16, 8]);  mm_260 = None
        slice_8589: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1558, 1, 2080, 2096)
        slice_8590: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8589, 2, 0, 16)
        add_262: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8590, view_531);  slice_8590 = view_531 = None
        slice_scatter_1560: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8589, add_262, 2, 0, 16);  slice_8589 = add_262 = None
        slice_scatter_1561: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1558, slice_scatter_1560, 1, 2080, 2096);  slice_scatter_1558 = slice_scatter_1560 = None
        slice_8594: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1561, 1, 2080, 2096)
        slice_8595: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8594, 2, 0, 16)
        slice_scatter_1563: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8594, slice_8595, 2, 0, 16);  slice_8594 = slice_8595 = None
        slice_scatter_1564: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1561, slice_scatter_1563, 1, 2080, 2096);  slice_scatter_1561 = slice_scatter_1563 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8615: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_8581, 2, 16, 32);  slice_8581 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_264: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_8615, memory_format = torch.contiguous_format);  slice_8615 = None
        view_532: "f32[32, 11]" = torch.ops.aten.view.default(clone_264, [32, 11]);  clone_264 = None
        mm_261: "f32[32, 8]" = torch.ops.aten.mm.default(view_532, slice_37)
        view_533: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_261, [2, 16, 8]);  mm_261 = None
        slice_8622: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1564, 1, 2080, 2096)
        slice_8623: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8622, 2, 0, 16)
        add_263: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8623, view_533);  slice_8623 = view_533 = None
        slice_scatter_1566: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8622, add_263, 2, 0, 16);  slice_8622 = add_263 = None
        slice_scatter_1567: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1564, slice_scatter_1566, 1, 2080, 2096);  slice_scatter_1564 = slice_scatter_1566 = None
        slice_8627: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1567, 1, 2080, 2096)
        slice_8628: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8627, 2, 0, 16)
        slice_scatter_1569: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8627, slice_8628, 2, 0, 16);  slice_8627 = slice_8628 = None
        slice_scatter_1570: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1567, slice_scatter_1569, 1, 2080, 2096);  slice_scatter_1567 = slice_scatter_1569 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8647: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2096, 2112)
        slice_8648: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_8647, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_265: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_8648, memory_format = torch.contiguous_format);  slice_8648 = None
        view_534: "f32[32, 16]" = torch.ops.aten.view.default(clone_265, [32, 16]);  clone_265 = None
        mm_262: "f32[32, 8]" = torch.ops.aten.mm.default(view_534, slice_7)
        view_535: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_262, [2, 16, 8]);  mm_262 = None
        slice_8655: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1570, 1, 2096, 2112)
        slice_8656: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8655, 2, 0, 16)
        add_264: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8656, view_535);  slice_8656 = view_535 = None
        slice_scatter_1572: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8655, add_264, 2, 0, 16);  slice_8655 = add_264 = None
        slice_scatter_1573: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1570, slice_scatter_1572, 1, 2096, 2112);  slice_scatter_1570 = slice_scatter_1572 = None
        slice_8660: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1573, 1, 2096, 2112)
        slice_8661: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8660, 2, 0, 16)
        slice_scatter_1575: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8660, slice_8661, 2, 0, 16);  slice_8660 = slice_8661 = None
        slice_scatter_1576: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1573, slice_scatter_1575, 1, 2096, 2112);  slice_scatter_1573 = slice_scatter_1575 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8681: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_8647, 2, 16, 32);  slice_8647 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_266: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_8681, memory_format = torch.contiguous_format);  slice_8681 = None
        view_536: "f32[32, 11]" = torch.ops.aten.view.default(clone_266, [32, 11]);  clone_266 = None
        mm_263: "f32[32, 8]" = torch.ops.aten.mm.default(view_536, slice_37)
        view_537: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_263, [2, 16, 8]);  mm_263 = None
        slice_8688: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1576, 1, 2096, 2112)
        slice_8689: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8688, 2, 0, 16)
        add_265: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8689, view_537);  slice_8689 = view_537 = None
        slice_scatter_1578: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8688, add_265, 2, 0, 16);  slice_8688 = add_265 = None
        slice_scatter_1579: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1576, slice_scatter_1578, 1, 2096, 2112);  slice_scatter_1576 = slice_scatter_1578 = None
        slice_8693: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1579, 1, 2096, 2112)
        slice_8694: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8693, 2, 0, 16)
        slice_scatter_1581: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8693, slice_8694, 2, 0, 16);  slice_8693 = slice_8694 = None
        slice_scatter_1582: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1579, slice_scatter_1581, 1, 2096, 2112);  slice_scatter_1579 = slice_scatter_1581 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8713: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2112, 2128)
        slice_8714: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_8713, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_267: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_8714, memory_format = torch.contiguous_format);  slice_8714 = None
        view_538: "f32[32, 16]" = torch.ops.aten.view.default(clone_267, [32, 16]);  clone_267 = None
        mm_264: "f32[32, 8]" = torch.ops.aten.mm.default(view_538, slice_7)
        view_539: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_264, [2, 16, 8]);  mm_264 = None
        slice_8721: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1582, 1, 2112, 2128)
        slice_8722: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8721, 2, 0, 16)
        add_266: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8722, view_539);  slice_8722 = view_539 = None
        slice_scatter_1584: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8721, add_266, 2, 0, 16);  slice_8721 = add_266 = None
        slice_scatter_1585: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1582, slice_scatter_1584, 1, 2112, 2128);  slice_scatter_1582 = slice_scatter_1584 = None
        slice_8726: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1585, 1, 2112, 2128)
        slice_8727: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8726, 2, 0, 16)
        slice_scatter_1587: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8726, slice_8727, 2, 0, 16);  slice_8726 = slice_8727 = None
        slice_scatter_1588: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1585, slice_scatter_1587, 1, 2112, 2128);  slice_scatter_1585 = slice_scatter_1587 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8747: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_8713, 2, 16, 32);  slice_8713 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_268: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_8747, memory_format = torch.contiguous_format);  slice_8747 = None
        view_540: "f32[32, 11]" = torch.ops.aten.view.default(clone_268, [32, 11]);  clone_268 = None
        mm_265: "f32[32, 8]" = torch.ops.aten.mm.default(view_540, slice_37)
        view_541: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_265, [2, 16, 8]);  mm_265 = None
        slice_8754: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1588, 1, 2112, 2128)
        slice_8755: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8754, 2, 0, 16)
        add_267: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8755, view_541);  slice_8755 = view_541 = None
        slice_scatter_1590: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8754, add_267, 2, 0, 16);  slice_8754 = add_267 = None
        slice_scatter_1591: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1588, slice_scatter_1590, 1, 2112, 2128);  slice_scatter_1588 = slice_scatter_1590 = None
        slice_8759: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1591, 1, 2112, 2128)
        slice_8760: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8759, 2, 0, 16)
        slice_scatter_1593: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8759, slice_8760, 2, 0, 16);  slice_8759 = slice_8760 = None
        slice_scatter_1594: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1591, slice_scatter_1593, 1, 2112, 2128);  slice_scatter_1591 = slice_scatter_1593 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8779: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2128, 2144)
        slice_8780: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_8779, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_269: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_8780, memory_format = torch.contiguous_format);  slice_8780 = None
        view_542: "f32[32, 16]" = torch.ops.aten.view.default(clone_269, [32, 16]);  clone_269 = None
        mm_266: "f32[32, 8]" = torch.ops.aten.mm.default(view_542, slice_7)
        view_543: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_266, [2, 16, 8]);  mm_266 = None
        slice_8787: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1594, 1, 2128, 2144)
        slice_8788: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8787, 2, 0, 16)
        add_268: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8788, view_543);  slice_8788 = view_543 = None
        slice_scatter_1596: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8787, add_268, 2, 0, 16);  slice_8787 = add_268 = None
        slice_scatter_1597: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1594, slice_scatter_1596, 1, 2128, 2144);  slice_scatter_1594 = slice_scatter_1596 = None
        slice_8792: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1597, 1, 2128, 2144)
        slice_8793: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8792, 2, 0, 16)
        slice_scatter_1599: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8792, slice_8793, 2, 0, 16);  slice_8792 = slice_8793 = None
        slice_scatter_1600: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1597, slice_scatter_1599, 1, 2128, 2144);  slice_scatter_1597 = slice_scatter_1599 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8813: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_8779, 2, 16, 32);  slice_8779 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_270: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_8813, memory_format = torch.contiguous_format);  slice_8813 = None
        view_544: "f32[32, 11]" = torch.ops.aten.view.default(clone_270, [32, 11]);  clone_270 = None
        mm_267: "f32[32, 8]" = torch.ops.aten.mm.default(view_544, slice_37)
        view_545: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_267, [2, 16, 8]);  mm_267 = None
        slice_8820: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1600, 1, 2128, 2144)
        slice_8821: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8820, 2, 0, 16)
        add_269: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8821, view_545);  slice_8821 = view_545 = None
        slice_scatter_1602: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8820, add_269, 2, 0, 16);  slice_8820 = add_269 = None
        slice_scatter_1603: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1600, slice_scatter_1602, 1, 2128, 2144);  slice_scatter_1600 = slice_scatter_1602 = None
        slice_8825: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1603, 1, 2128, 2144)
        slice_8826: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8825, 2, 0, 16)
        slice_scatter_1605: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8825, slice_8826, 2, 0, 16);  slice_8825 = slice_8826 = None
        slice_scatter_1606: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1603, slice_scatter_1605, 1, 2128, 2144);  slice_scatter_1603 = slice_scatter_1605 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8845: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2144, 2160)
        slice_8846: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_8845, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_271: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_8846, memory_format = torch.contiguous_format);  slice_8846 = None
        view_546: "f32[32, 16]" = torch.ops.aten.view.default(clone_271, [32, 16]);  clone_271 = None
        mm_268: "f32[32, 8]" = torch.ops.aten.mm.default(view_546, slice_7)
        view_547: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_268, [2, 16, 8]);  mm_268 = None
        slice_8853: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1606, 1, 2144, 2160)
        slice_8854: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8853, 2, 0, 16)
        add_270: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8854, view_547);  slice_8854 = view_547 = None
        slice_scatter_1608: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8853, add_270, 2, 0, 16);  slice_8853 = add_270 = None
        slice_scatter_1609: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1606, slice_scatter_1608, 1, 2144, 2160);  slice_scatter_1606 = slice_scatter_1608 = None
        slice_8858: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1609, 1, 2144, 2160)
        slice_8859: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8858, 2, 0, 16)
        slice_scatter_1611: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8858, slice_8859, 2, 0, 16);  slice_8858 = slice_8859 = None
        slice_scatter_1612: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1609, slice_scatter_1611, 1, 2144, 2160);  slice_scatter_1609 = slice_scatter_1611 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8879: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_8845, 2, 16, 32);  slice_8845 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_272: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_8879, memory_format = torch.contiguous_format);  slice_8879 = None
        view_548: "f32[32, 11]" = torch.ops.aten.view.default(clone_272, [32, 11]);  clone_272 = None
        mm_269: "f32[32, 8]" = torch.ops.aten.mm.default(view_548, slice_37)
        view_549: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_269, [2, 16, 8]);  mm_269 = None
        slice_8886: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1612, 1, 2144, 2160)
        slice_8887: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8886, 2, 0, 16)
        add_271: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8887, view_549);  slice_8887 = view_549 = None
        slice_scatter_1614: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8886, add_271, 2, 0, 16);  slice_8886 = add_271 = None
        slice_scatter_1615: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1612, slice_scatter_1614, 1, 2144, 2160);  slice_scatter_1612 = slice_scatter_1614 = None
        slice_8891: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1615, 1, 2144, 2160)
        slice_8892: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8891, 2, 0, 16)
        slice_scatter_1617: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8891, slice_8892, 2, 0, 16);  slice_8891 = slice_8892 = None
        slice_scatter_1618: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1615, slice_scatter_1617, 1, 2144, 2160);  slice_scatter_1615 = slice_scatter_1617 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8911: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2160, 2176)
        slice_8912: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_8911, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_273: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_8912, memory_format = torch.contiguous_format);  slice_8912 = None
        view_550: "f32[32, 16]" = torch.ops.aten.view.default(clone_273, [32, 16]);  clone_273 = None
        mm_270: "f32[32, 8]" = torch.ops.aten.mm.default(view_550, slice_7)
        view_551: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_270, [2, 16, 8]);  mm_270 = None
        slice_8919: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1618, 1, 2160, 2176)
        slice_8920: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8919, 2, 0, 16)
        add_272: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8920, view_551);  slice_8920 = view_551 = None
        slice_scatter_1620: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8919, add_272, 2, 0, 16);  slice_8919 = add_272 = None
        slice_scatter_1621: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1618, slice_scatter_1620, 1, 2160, 2176);  slice_scatter_1618 = slice_scatter_1620 = None
        slice_8924: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1621, 1, 2160, 2176)
        slice_8925: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8924, 2, 0, 16)
        slice_scatter_1623: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8924, slice_8925, 2, 0, 16);  slice_8924 = slice_8925 = None
        slice_scatter_1624: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1621, slice_scatter_1623, 1, 2160, 2176);  slice_scatter_1621 = slice_scatter_1623 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8945: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_8911, 2, 16, 32);  slice_8911 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_274: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_8945, memory_format = torch.contiguous_format);  slice_8945 = None
        view_552: "f32[32, 11]" = torch.ops.aten.view.default(clone_274, [32, 11]);  clone_274 = None
        mm_271: "f32[32, 8]" = torch.ops.aten.mm.default(view_552, slice_37)
        view_553: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_271, [2, 16, 8]);  mm_271 = None
        slice_8952: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1624, 1, 2160, 2176)
        slice_8953: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8952, 2, 0, 16)
        add_273: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8953, view_553);  slice_8953 = view_553 = None
        slice_scatter_1626: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8952, add_273, 2, 0, 16);  slice_8952 = add_273 = None
        slice_scatter_1627: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1624, slice_scatter_1626, 1, 2160, 2176);  slice_scatter_1624 = slice_scatter_1626 = None
        slice_8957: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1627, 1, 2160, 2176)
        slice_8958: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8957, 2, 0, 16)
        slice_scatter_1629: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8957, slice_8958, 2, 0, 16);  slice_8957 = slice_8958 = None
        slice_scatter_1630: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1627, slice_scatter_1629, 1, 2160, 2176);  slice_scatter_1627 = slice_scatter_1629 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_8977: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2176, 2192)
        slice_8978: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_8977, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_275: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_8978, memory_format = torch.contiguous_format);  slice_8978 = None
        view_554: "f32[32, 16]" = torch.ops.aten.view.default(clone_275, [32, 16]);  clone_275 = None
        mm_272: "f32[32, 8]" = torch.ops.aten.mm.default(view_554, slice_7)
        view_555: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_272, [2, 16, 8]);  mm_272 = None
        slice_8985: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1630, 1, 2176, 2192)
        slice_8986: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8985, 2, 0, 16)
        add_274: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_8986, view_555);  slice_8986 = view_555 = None
        slice_scatter_1632: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8985, add_274, 2, 0, 16);  slice_8985 = add_274 = None
        slice_scatter_1633: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1630, slice_scatter_1632, 1, 2176, 2192);  slice_scatter_1630 = slice_scatter_1632 = None
        slice_8990: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1633, 1, 2176, 2192)
        slice_8991: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_8990, 2, 0, 16)
        slice_scatter_1635: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_8990, slice_8991, 2, 0, 16);  slice_8990 = slice_8991 = None
        slice_scatter_1636: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1633, slice_scatter_1635, 1, 2176, 2192);  slice_scatter_1633 = slice_scatter_1635 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9011: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_8977, 2, 16, 32);  slice_8977 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_276: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_9011, memory_format = torch.contiguous_format);  slice_9011 = None
        view_556: "f32[32, 11]" = torch.ops.aten.view.default(clone_276, [32, 11]);  clone_276 = None
        mm_273: "f32[32, 8]" = torch.ops.aten.mm.default(view_556, slice_37)
        view_557: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_273, [2, 16, 8]);  mm_273 = None
        slice_9018: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1636, 1, 2176, 2192)
        slice_9019: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9018, 2, 0, 16)
        add_275: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9019, view_557);  slice_9019 = view_557 = None
        slice_scatter_1638: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9018, add_275, 2, 0, 16);  slice_9018 = add_275 = None
        slice_scatter_1639: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1636, slice_scatter_1638, 1, 2176, 2192);  slice_scatter_1636 = slice_scatter_1638 = None
        slice_9023: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1639, 1, 2176, 2192)
        slice_9024: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9023, 2, 0, 16)
        slice_scatter_1641: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9023, slice_9024, 2, 0, 16);  slice_9023 = slice_9024 = None
        slice_scatter_1642: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1639, slice_scatter_1641, 1, 2176, 2192);  slice_scatter_1639 = slice_scatter_1641 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9043: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2192, 2208)
        slice_9044: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_9043, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_277: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_9044, memory_format = torch.contiguous_format);  slice_9044 = None
        view_558: "f32[32, 16]" = torch.ops.aten.view.default(clone_277, [32, 16]);  clone_277 = None
        mm_274: "f32[32, 8]" = torch.ops.aten.mm.default(view_558, slice_7)
        view_559: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_274, [2, 16, 8]);  mm_274 = None
        slice_9051: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1642, 1, 2192, 2208)
        slice_9052: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9051, 2, 0, 16)
        add_276: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9052, view_559);  slice_9052 = view_559 = None
        slice_scatter_1644: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9051, add_276, 2, 0, 16);  slice_9051 = add_276 = None
        slice_scatter_1645: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1642, slice_scatter_1644, 1, 2192, 2208);  slice_scatter_1642 = slice_scatter_1644 = None
        slice_9056: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1645, 1, 2192, 2208)
        slice_9057: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9056, 2, 0, 16)
        slice_scatter_1647: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9056, slice_9057, 2, 0, 16);  slice_9056 = slice_9057 = None
        slice_scatter_1648: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1645, slice_scatter_1647, 1, 2192, 2208);  slice_scatter_1645 = slice_scatter_1647 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9077: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_9043, 2, 16, 32);  slice_9043 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_278: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_9077, memory_format = torch.contiguous_format);  slice_9077 = None
        view_560: "f32[32, 11]" = torch.ops.aten.view.default(clone_278, [32, 11]);  clone_278 = None
        mm_275: "f32[32, 8]" = torch.ops.aten.mm.default(view_560, slice_37)
        view_561: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_275, [2, 16, 8]);  mm_275 = None
        slice_9084: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1648, 1, 2192, 2208)
        slice_9085: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9084, 2, 0, 16)
        add_277: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9085, view_561);  slice_9085 = view_561 = None
        slice_scatter_1650: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9084, add_277, 2, 0, 16);  slice_9084 = add_277 = None
        slice_scatter_1651: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1648, slice_scatter_1650, 1, 2192, 2208);  slice_scatter_1648 = slice_scatter_1650 = None
        slice_9089: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1651, 1, 2192, 2208)
        slice_9090: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9089, 2, 0, 16)
        slice_scatter_1653: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9089, slice_9090, 2, 0, 16);  slice_9089 = slice_9090 = None
        slice_scatter_1654: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1651, slice_scatter_1653, 1, 2192, 2208);  slice_scatter_1651 = slice_scatter_1653 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9109: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2208, 2224)
        slice_9110: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_9109, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_279: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_9110, memory_format = torch.contiguous_format);  slice_9110 = None
        view_562: "f32[32, 16]" = torch.ops.aten.view.default(clone_279, [32, 16]);  clone_279 = None
        mm_276: "f32[32, 8]" = torch.ops.aten.mm.default(view_562, slice_7)
        view_563: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_276, [2, 16, 8]);  mm_276 = None
        slice_9117: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1654, 1, 2208, 2224)
        slice_9118: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9117, 2, 0, 16)
        add_278: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9118, view_563);  slice_9118 = view_563 = None
        slice_scatter_1656: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9117, add_278, 2, 0, 16);  slice_9117 = add_278 = None
        slice_scatter_1657: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1654, slice_scatter_1656, 1, 2208, 2224);  slice_scatter_1654 = slice_scatter_1656 = None
        slice_9122: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1657, 1, 2208, 2224)
        slice_9123: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9122, 2, 0, 16)
        slice_scatter_1659: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9122, slice_9123, 2, 0, 16);  slice_9122 = slice_9123 = None
        slice_scatter_1660: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1657, slice_scatter_1659, 1, 2208, 2224);  slice_scatter_1657 = slice_scatter_1659 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9143: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_9109, 2, 16, 32);  slice_9109 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_280: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_9143, memory_format = torch.contiguous_format);  slice_9143 = None
        view_564: "f32[32, 11]" = torch.ops.aten.view.default(clone_280, [32, 11]);  clone_280 = None
        mm_277: "f32[32, 8]" = torch.ops.aten.mm.default(view_564, slice_37)
        view_565: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_277, [2, 16, 8]);  mm_277 = None
        slice_9150: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1660, 1, 2208, 2224)
        slice_9151: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9150, 2, 0, 16)
        add_279: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9151, view_565);  slice_9151 = view_565 = None
        slice_scatter_1662: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9150, add_279, 2, 0, 16);  slice_9150 = add_279 = None
        slice_scatter_1663: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1660, slice_scatter_1662, 1, 2208, 2224);  slice_scatter_1660 = slice_scatter_1662 = None
        slice_9155: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1663, 1, 2208, 2224)
        slice_9156: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9155, 2, 0, 16)
        slice_scatter_1665: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9155, slice_9156, 2, 0, 16);  slice_9155 = slice_9156 = None
        slice_scatter_1666: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1663, slice_scatter_1665, 1, 2208, 2224);  slice_scatter_1663 = slice_scatter_1665 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9175: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2224, 2240)
        slice_9176: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_9175, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_281: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_9176, memory_format = torch.contiguous_format);  slice_9176 = None
        view_566: "f32[32, 16]" = torch.ops.aten.view.default(clone_281, [32, 16]);  clone_281 = None
        mm_278: "f32[32, 8]" = torch.ops.aten.mm.default(view_566, slice_7)
        view_567: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_278, [2, 16, 8]);  mm_278 = None
        slice_9183: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1666, 1, 2224, 2240)
        slice_9184: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9183, 2, 0, 16)
        add_280: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9184, view_567);  slice_9184 = view_567 = None
        slice_scatter_1668: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9183, add_280, 2, 0, 16);  slice_9183 = add_280 = None
        slice_scatter_1669: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1666, slice_scatter_1668, 1, 2224, 2240);  slice_scatter_1666 = slice_scatter_1668 = None
        slice_9188: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1669, 1, 2224, 2240)
        slice_9189: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9188, 2, 0, 16)
        slice_scatter_1671: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9188, slice_9189, 2, 0, 16);  slice_9188 = slice_9189 = None
        slice_scatter_1672: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1669, slice_scatter_1671, 1, 2224, 2240);  slice_scatter_1669 = slice_scatter_1671 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9209: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_9175, 2, 16, 32);  slice_9175 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_282: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_9209, memory_format = torch.contiguous_format);  slice_9209 = None
        view_568: "f32[32, 11]" = torch.ops.aten.view.default(clone_282, [32, 11]);  clone_282 = None
        mm_279: "f32[32, 8]" = torch.ops.aten.mm.default(view_568, slice_37)
        view_569: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_279, [2, 16, 8]);  mm_279 = None
        slice_9216: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1672, 1, 2224, 2240)
        slice_9217: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9216, 2, 0, 16)
        add_281: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9217, view_569);  slice_9217 = view_569 = None
        slice_scatter_1674: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9216, add_281, 2, 0, 16);  slice_9216 = add_281 = None
        slice_scatter_1675: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1672, slice_scatter_1674, 1, 2224, 2240);  slice_scatter_1672 = slice_scatter_1674 = None
        slice_9221: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1675, 1, 2224, 2240)
        slice_9222: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9221, 2, 0, 16)
        slice_scatter_1677: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9221, slice_9222, 2, 0, 16);  slice_9221 = slice_9222 = None
        slice_scatter_1678: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1675, slice_scatter_1677, 1, 2224, 2240);  slice_scatter_1675 = slice_scatter_1677 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9241: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2240, 2256)
        slice_9242: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_9241, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_283: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_9242, memory_format = torch.contiguous_format);  slice_9242 = None
        view_570: "f32[32, 16]" = torch.ops.aten.view.default(clone_283, [32, 16]);  clone_283 = None
        mm_280: "f32[32, 8]" = torch.ops.aten.mm.default(view_570, slice_7)
        view_571: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_280, [2, 16, 8]);  mm_280 = None
        slice_9249: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1678, 1, 2240, 2256)
        slice_9250: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9249, 2, 0, 16)
        add_282: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9250, view_571);  slice_9250 = view_571 = None
        slice_scatter_1680: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9249, add_282, 2, 0, 16);  slice_9249 = add_282 = None
        slice_scatter_1681: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1678, slice_scatter_1680, 1, 2240, 2256);  slice_scatter_1678 = slice_scatter_1680 = None
        slice_9254: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1681, 1, 2240, 2256)
        slice_9255: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9254, 2, 0, 16)
        slice_scatter_1683: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9254, slice_9255, 2, 0, 16);  slice_9254 = slice_9255 = None
        slice_scatter_1684: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1681, slice_scatter_1683, 1, 2240, 2256);  slice_scatter_1681 = slice_scatter_1683 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9275: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_9241, 2, 16, 32);  slice_9241 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_284: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_9275, memory_format = torch.contiguous_format);  slice_9275 = None
        view_572: "f32[32, 11]" = torch.ops.aten.view.default(clone_284, [32, 11]);  clone_284 = None
        mm_281: "f32[32, 8]" = torch.ops.aten.mm.default(view_572, slice_37)
        view_573: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_281, [2, 16, 8]);  mm_281 = None
        slice_9282: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1684, 1, 2240, 2256)
        slice_9283: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9282, 2, 0, 16)
        add_283: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9283, view_573);  slice_9283 = view_573 = None
        slice_scatter_1686: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9282, add_283, 2, 0, 16);  slice_9282 = add_283 = None
        slice_scatter_1687: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1684, slice_scatter_1686, 1, 2240, 2256);  slice_scatter_1684 = slice_scatter_1686 = None
        slice_9287: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1687, 1, 2240, 2256)
        slice_9288: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9287, 2, 0, 16)
        slice_scatter_1689: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9287, slice_9288, 2, 0, 16);  slice_9287 = slice_9288 = None
        slice_scatter_1690: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1687, slice_scatter_1689, 1, 2240, 2256);  slice_scatter_1687 = slice_scatter_1689 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9307: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2256, 2272)
        slice_9308: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_9307, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_285: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_9308, memory_format = torch.contiguous_format);  slice_9308 = None
        view_574: "f32[32, 16]" = torch.ops.aten.view.default(clone_285, [32, 16]);  clone_285 = None
        mm_282: "f32[32, 8]" = torch.ops.aten.mm.default(view_574, slice_7)
        view_575: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_282, [2, 16, 8]);  mm_282 = None
        slice_9315: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1690, 1, 2256, 2272)
        slice_9316: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9315, 2, 0, 16)
        add_284: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9316, view_575);  slice_9316 = view_575 = None
        slice_scatter_1692: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9315, add_284, 2, 0, 16);  slice_9315 = add_284 = None
        slice_scatter_1693: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1690, slice_scatter_1692, 1, 2256, 2272);  slice_scatter_1690 = slice_scatter_1692 = None
        slice_9320: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1693, 1, 2256, 2272)
        slice_9321: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9320, 2, 0, 16)
        slice_scatter_1695: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9320, slice_9321, 2, 0, 16);  slice_9320 = slice_9321 = None
        slice_scatter_1696: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1693, slice_scatter_1695, 1, 2256, 2272);  slice_scatter_1693 = slice_scatter_1695 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9341: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_9307, 2, 16, 32);  slice_9307 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_286: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_9341, memory_format = torch.contiguous_format);  slice_9341 = None
        view_576: "f32[32, 11]" = torch.ops.aten.view.default(clone_286, [32, 11]);  clone_286 = None
        mm_283: "f32[32, 8]" = torch.ops.aten.mm.default(view_576, slice_37)
        view_577: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_283, [2, 16, 8]);  mm_283 = None
        slice_9348: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1696, 1, 2256, 2272)
        slice_9349: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9348, 2, 0, 16)
        add_285: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9349, view_577);  slice_9349 = view_577 = None
        slice_scatter_1698: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9348, add_285, 2, 0, 16);  slice_9348 = add_285 = None
        slice_scatter_1699: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1696, slice_scatter_1698, 1, 2256, 2272);  slice_scatter_1696 = slice_scatter_1698 = None
        slice_9353: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1699, 1, 2256, 2272)
        slice_9354: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9353, 2, 0, 16)
        slice_scatter_1701: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9353, slice_9354, 2, 0, 16);  slice_9353 = slice_9354 = None
        slice_scatter_1702: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1699, slice_scatter_1701, 1, 2256, 2272);  slice_scatter_1699 = slice_scatter_1701 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9373: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2272, 2288)
        slice_9374: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_9373, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_287: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_9374, memory_format = torch.contiguous_format);  slice_9374 = None
        view_578: "f32[32, 16]" = torch.ops.aten.view.default(clone_287, [32, 16]);  clone_287 = None
        mm_284: "f32[32, 8]" = torch.ops.aten.mm.default(view_578, slice_7)
        view_579: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_284, [2, 16, 8]);  mm_284 = None
        slice_9381: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1702, 1, 2272, 2288)
        slice_9382: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9381, 2, 0, 16)
        add_286: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9382, view_579);  slice_9382 = view_579 = None
        slice_scatter_1704: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9381, add_286, 2, 0, 16);  slice_9381 = add_286 = None
        slice_scatter_1705: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1702, slice_scatter_1704, 1, 2272, 2288);  slice_scatter_1702 = slice_scatter_1704 = None
        slice_9386: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1705, 1, 2272, 2288)
        slice_9387: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9386, 2, 0, 16)
        slice_scatter_1707: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9386, slice_9387, 2, 0, 16);  slice_9386 = slice_9387 = None
        slice_scatter_1708: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1705, slice_scatter_1707, 1, 2272, 2288);  slice_scatter_1705 = slice_scatter_1707 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9407: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_9373, 2, 16, 32);  slice_9373 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_288: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_9407, memory_format = torch.contiguous_format);  slice_9407 = None
        view_580: "f32[32, 11]" = torch.ops.aten.view.default(clone_288, [32, 11]);  clone_288 = None
        mm_285: "f32[32, 8]" = torch.ops.aten.mm.default(view_580, slice_37)
        view_581: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_285, [2, 16, 8]);  mm_285 = None
        slice_9414: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1708, 1, 2272, 2288)
        slice_9415: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9414, 2, 0, 16)
        add_287: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9415, view_581);  slice_9415 = view_581 = None
        slice_scatter_1710: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9414, add_287, 2, 0, 16);  slice_9414 = add_287 = None
        slice_scatter_1711: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1708, slice_scatter_1710, 1, 2272, 2288);  slice_scatter_1708 = slice_scatter_1710 = None
        slice_9419: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1711, 1, 2272, 2288)
        slice_9420: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9419, 2, 0, 16)
        slice_scatter_1713: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9419, slice_9420, 2, 0, 16);  slice_9419 = slice_9420 = None
        slice_scatter_1714: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1711, slice_scatter_1713, 1, 2272, 2288);  slice_scatter_1711 = slice_scatter_1713 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9439: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2288, 2304)
        slice_9440: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_9439, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_289: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_9440, memory_format = torch.contiguous_format);  slice_9440 = None
        view_582: "f32[32, 16]" = torch.ops.aten.view.default(clone_289, [32, 16]);  clone_289 = None
        mm_286: "f32[32, 8]" = torch.ops.aten.mm.default(view_582, slice_7)
        view_583: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_286, [2, 16, 8]);  mm_286 = None
        slice_9447: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1714, 1, 2288, 2304)
        slice_9448: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9447, 2, 0, 16)
        add_288: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9448, view_583);  slice_9448 = view_583 = None
        slice_scatter_1716: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9447, add_288, 2, 0, 16);  slice_9447 = add_288 = None
        slice_scatter_1717: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1714, slice_scatter_1716, 1, 2288, 2304);  slice_scatter_1714 = slice_scatter_1716 = None
        slice_9452: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1717, 1, 2288, 2304)
        slice_9453: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9452, 2, 0, 16)
        slice_scatter_1719: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9452, slice_9453, 2, 0, 16);  slice_9452 = slice_9453 = None
        slice_scatter_1720: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1717, slice_scatter_1719, 1, 2288, 2304);  slice_scatter_1717 = slice_scatter_1719 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9473: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_9439, 2, 16, 32);  slice_9439 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_290: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_9473, memory_format = torch.contiguous_format);  slice_9473 = None
        view_584: "f32[32, 11]" = torch.ops.aten.view.default(clone_290, [32, 11]);  clone_290 = None
        mm_287: "f32[32, 8]" = torch.ops.aten.mm.default(view_584, slice_37)
        view_585: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_287, [2, 16, 8]);  mm_287 = None
        slice_9480: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1720, 1, 2288, 2304)
        slice_9481: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9480, 2, 0, 16)
        add_289: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9481, view_585);  slice_9481 = view_585 = None
        slice_scatter_1722: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9480, add_289, 2, 0, 16);  slice_9480 = add_289 = None
        slice_scatter_1723: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1720, slice_scatter_1722, 1, 2288, 2304);  slice_scatter_1720 = slice_scatter_1722 = None
        slice_9485: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1723, 1, 2288, 2304)
        slice_9486: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9485, 2, 0, 16)
        slice_scatter_1725: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9485, slice_9486, 2, 0, 16);  slice_9485 = slice_9486 = None
        slice_scatter_1726: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1723, slice_scatter_1725, 1, 2288, 2304);  slice_scatter_1723 = slice_scatter_1725 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9505: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2304, 2320)
        slice_9506: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_9505, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_291: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_9506, memory_format = torch.contiguous_format);  slice_9506 = None
        view_586: "f32[32, 16]" = torch.ops.aten.view.default(clone_291, [32, 16]);  clone_291 = None
        mm_288: "f32[32, 8]" = torch.ops.aten.mm.default(view_586, slice_7)
        view_587: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_288, [2, 16, 8]);  mm_288 = None
        slice_9513: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1726, 1, 2304, 2320)
        slice_9514: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9513, 2, 0, 16)
        add_290: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9514, view_587);  slice_9514 = view_587 = None
        slice_scatter_1728: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9513, add_290, 2, 0, 16);  slice_9513 = add_290 = None
        slice_scatter_1729: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1726, slice_scatter_1728, 1, 2304, 2320);  slice_scatter_1726 = slice_scatter_1728 = None
        slice_9518: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1729, 1, 2304, 2320)
        slice_9519: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9518, 2, 0, 16)
        slice_scatter_1731: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9518, slice_9519, 2, 0, 16);  slice_9518 = slice_9519 = None
        slice_scatter_1732: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1729, slice_scatter_1731, 1, 2304, 2320);  slice_scatter_1729 = slice_scatter_1731 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9539: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_9505, 2, 16, 32);  slice_9505 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_292: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_9539, memory_format = torch.contiguous_format);  slice_9539 = None
        view_588: "f32[32, 11]" = torch.ops.aten.view.default(clone_292, [32, 11]);  clone_292 = None
        mm_289: "f32[32, 8]" = torch.ops.aten.mm.default(view_588, slice_37)
        view_589: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_289, [2, 16, 8]);  mm_289 = None
        slice_9546: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1732, 1, 2304, 2320)
        slice_9547: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9546, 2, 0, 16)
        add_291: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9547, view_589);  slice_9547 = view_589 = None
        slice_scatter_1734: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9546, add_291, 2, 0, 16);  slice_9546 = add_291 = None
        slice_scatter_1735: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1732, slice_scatter_1734, 1, 2304, 2320);  slice_scatter_1732 = slice_scatter_1734 = None
        slice_9551: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1735, 1, 2304, 2320)
        slice_9552: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9551, 2, 0, 16)
        slice_scatter_1737: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9551, slice_9552, 2, 0, 16);  slice_9551 = slice_9552 = None
        slice_scatter_1738: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1735, slice_scatter_1737, 1, 2304, 2320);  slice_scatter_1735 = slice_scatter_1737 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9571: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2320, 2336)
        slice_9572: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_9571, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_293: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_9572, memory_format = torch.contiguous_format);  slice_9572 = None
        view_590: "f32[32, 16]" = torch.ops.aten.view.default(clone_293, [32, 16]);  clone_293 = None
        mm_290: "f32[32, 8]" = torch.ops.aten.mm.default(view_590, slice_7)
        view_591: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_290, [2, 16, 8]);  mm_290 = None
        slice_9579: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1738, 1, 2320, 2336)
        slice_9580: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9579, 2, 0, 16)
        add_292: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9580, view_591);  slice_9580 = view_591 = None
        slice_scatter_1740: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9579, add_292, 2, 0, 16);  slice_9579 = add_292 = None
        slice_scatter_1741: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1738, slice_scatter_1740, 1, 2320, 2336);  slice_scatter_1738 = slice_scatter_1740 = None
        slice_9584: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1741, 1, 2320, 2336)
        slice_9585: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9584, 2, 0, 16)
        slice_scatter_1743: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9584, slice_9585, 2, 0, 16);  slice_9584 = slice_9585 = None
        slice_scatter_1744: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1741, slice_scatter_1743, 1, 2320, 2336);  slice_scatter_1741 = slice_scatter_1743 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9605: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_9571, 2, 16, 32);  slice_9571 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_294: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_9605, memory_format = torch.contiguous_format);  slice_9605 = None
        view_592: "f32[32, 11]" = torch.ops.aten.view.default(clone_294, [32, 11]);  clone_294 = None
        mm_291: "f32[32, 8]" = torch.ops.aten.mm.default(view_592, slice_37)
        view_593: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_291, [2, 16, 8]);  mm_291 = None
        slice_9612: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1744, 1, 2320, 2336)
        slice_9613: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9612, 2, 0, 16)
        add_293: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9613, view_593);  slice_9613 = view_593 = None
        slice_scatter_1746: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9612, add_293, 2, 0, 16);  slice_9612 = add_293 = None
        slice_scatter_1747: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1744, slice_scatter_1746, 1, 2320, 2336);  slice_scatter_1744 = slice_scatter_1746 = None
        slice_9617: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1747, 1, 2320, 2336)
        slice_9618: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9617, 2, 0, 16)
        slice_scatter_1749: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9617, slice_9618, 2, 0, 16);  slice_9617 = slice_9618 = None
        slice_scatter_1750: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1747, slice_scatter_1749, 1, 2320, 2336);  slice_scatter_1747 = slice_scatter_1749 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9637: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2336, 2352)
        slice_9638: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_9637, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_295: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_9638, memory_format = torch.contiguous_format);  slice_9638 = None
        view_594: "f32[32, 16]" = torch.ops.aten.view.default(clone_295, [32, 16]);  clone_295 = None
        mm_292: "f32[32, 8]" = torch.ops.aten.mm.default(view_594, slice_7)
        view_595: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_292, [2, 16, 8]);  mm_292 = None
        slice_9645: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1750, 1, 2336, 2352)
        slice_9646: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9645, 2, 0, 16)
        add_294: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9646, view_595);  slice_9646 = view_595 = None
        slice_scatter_1752: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9645, add_294, 2, 0, 16);  slice_9645 = add_294 = None
        slice_scatter_1753: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1750, slice_scatter_1752, 1, 2336, 2352);  slice_scatter_1750 = slice_scatter_1752 = None
        slice_9650: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1753, 1, 2336, 2352)
        slice_9651: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9650, 2, 0, 16)
        slice_scatter_1755: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9650, slice_9651, 2, 0, 16);  slice_9650 = slice_9651 = None
        slice_scatter_1756: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1753, slice_scatter_1755, 1, 2336, 2352);  slice_scatter_1753 = slice_scatter_1755 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9671: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_9637, 2, 16, 32);  slice_9637 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_296: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_9671, memory_format = torch.contiguous_format);  slice_9671 = None
        view_596: "f32[32, 11]" = torch.ops.aten.view.default(clone_296, [32, 11]);  clone_296 = None
        mm_293: "f32[32, 8]" = torch.ops.aten.mm.default(view_596, slice_37)
        view_597: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_293, [2, 16, 8]);  mm_293 = None
        slice_9678: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1756, 1, 2336, 2352)
        slice_9679: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9678, 2, 0, 16)
        add_295: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9679, view_597);  slice_9679 = view_597 = None
        slice_scatter_1758: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9678, add_295, 2, 0, 16);  slice_9678 = add_295 = None
        slice_scatter_1759: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1756, slice_scatter_1758, 1, 2336, 2352);  slice_scatter_1756 = slice_scatter_1758 = None
        slice_9683: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1759, 1, 2336, 2352)
        slice_9684: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9683, 2, 0, 16)
        slice_scatter_1761: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9683, slice_9684, 2, 0, 16);  slice_9683 = slice_9684 = None
        slice_scatter_1762: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1759, slice_scatter_1761, 1, 2336, 2352);  slice_scatter_1759 = slice_scatter_1761 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9703: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2352, 2368)
        slice_9704: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_9703, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_297: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_9704, memory_format = torch.contiguous_format);  slice_9704 = None
        view_598: "f32[32, 16]" = torch.ops.aten.view.default(clone_297, [32, 16]);  clone_297 = None
        mm_294: "f32[32, 8]" = torch.ops.aten.mm.default(view_598, slice_7)
        view_599: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_294, [2, 16, 8]);  mm_294 = None
        slice_9711: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1762, 1, 2352, 2368)
        slice_9712: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9711, 2, 0, 16)
        add_296: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9712, view_599);  slice_9712 = view_599 = None
        slice_scatter_1764: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9711, add_296, 2, 0, 16);  slice_9711 = add_296 = None
        slice_scatter_1765: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1762, slice_scatter_1764, 1, 2352, 2368);  slice_scatter_1762 = slice_scatter_1764 = None
        slice_9716: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1765, 1, 2352, 2368)
        slice_9717: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9716, 2, 0, 16)
        slice_scatter_1767: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9716, slice_9717, 2, 0, 16);  slice_9716 = slice_9717 = None
        slice_scatter_1768: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1765, slice_scatter_1767, 1, 2352, 2368);  slice_scatter_1765 = slice_scatter_1767 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9737: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_9703, 2, 16, 32);  slice_9703 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_298: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_9737, memory_format = torch.contiguous_format);  slice_9737 = None
        view_600: "f32[32, 11]" = torch.ops.aten.view.default(clone_298, [32, 11]);  clone_298 = None
        mm_295: "f32[32, 8]" = torch.ops.aten.mm.default(view_600, slice_37)
        view_601: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_295, [2, 16, 8]);  mm_295 = None
        slice_9744: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1768, 1, 2352, 2368)
        slice_9745: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9744, 2, 0, 16)
        add_297: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9745, view_601);  slice_9745 = view_601 = None
        slice_scatter_1770: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9744, add_297, 2, 0, 16);  slice_9744 = add_297 = None
        slice_scatter_1771: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1768, slice_scatter_1770, 1, 2352, 2368);  slice_scatter_1768 = slice_scatter_1770 = None
        slice_9749: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1771, 1, 2352, 2368)
        slice_9750: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9749, 2, 0, 16)
        slice_scatter_1773: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9749, slice_9750, 2, 0, 16);  slice_9749 = slice_9750 = None
        slice_scatter_1774: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1771, slice_scatter_1773, 1, 2352, 2368);  slice_scatter_1771 = slice_scatter_1773 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9769: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2368, 2384)
        slice_9770: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_9769, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_299: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_9770, memory_format = torch.contiguous_format);  slice_9770 = None
        view_602: "f32[32, 16]" = torch.ops.aten.view.default(clone_299, [32, 16]);  clone_299 = None
        mm_296: "f32[32, 8]" = torch.ops.aten.mm.default(view_602, slice_7)
        view_603: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_296, [2, 16, 8]);  mm_296 = None
        slice_9777: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1774, 1, 2368, 2384)
        slice_9778: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9777, 2, 0, 16)
        add_298: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9778, view_603);  slice_9778 = view_603 = None
        slice_scatter_1776: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9777, add_298, 2, 0, 16);  slice_9777 = add_298 = None
        slice_scatter_1777: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1774, slice_scatter_1776, 1, 2368, 2384);  slice_scatter_1774 = slice_scatter_1776 = None
        slice_9782: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1777, 1, 2368, 2384)
        slice_9783: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9782, 2, 0, 16)
        slice_scatter_1779: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9782, slice_9783, 2, 0, 16);  slice_9782 = slice_9783 = None
        slice_scatter_1780: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1777, slice_scatter_1779, 1, 2368, 2384);  slice_scatter_1777 = slice_scatter_1779 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9803: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_9769, 2, 16, 32);  slice_9769 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_300: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_9803, memory_format = torch.contiguous_format);  slice_9803 = None
        view_604: "f32[32, 11]" = torch.ops.aten.view.default(clone_300, [32, 11]);  clone_300 = None
        mm_297: "f32[32, 8]" = torch.ops.aten.mm.default(view_604, slice_37)
        view_605: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_297, [2, 16, 8]);  mm_297 = None
        slice_9810: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1780, 1, 2368, 2384)
        slice_9811: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9810, 2, 0, 16)
        add_299: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9811, view_605);  slice_9811 = view_605 = None
        slice_scatter_1782: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9810, add_299, 2, 0, 16);  slice_9810 = add_299 = None
        slice_scatter_1783: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1780, slice_scatter_1782, 1, 2368, 2384);  slice_scatter_1780 = slice_scatter_1782 = None
        slice_9815: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1783, 1, 2368, 2384)
        slice_9816: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9815, 2, 0, 16)
        slice_scatter_1785: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9815, slice_9816, 2, 0, 16);  slice_9815 = slice_9816 = None
        slice_scatter_1786: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1783, slice_scatter_1785, 1, 2368, 2384);  slice_scatter_1783 = slice_scatter_1785 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9835: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2384, 2400)
        slice_9836: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_9835, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_301: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_9836, memory_format = torch.contiguous_format);  slice_9836 = None
        view_606: "f32[32, 16]" = torch.ops.aten.view.default(clone_301, [32, 16]);  clone_301 = None
        mm_298: "f32[32, 8]" = torch.ops.aten.mm.default(view_606, slice_7)
        view_607: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_298, [2, 16, 8]);  mm_298 = None
        slice_9843: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1786, 1, 2384, 2400)
        slice_9844: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9843, 2, 0, 16)
        add_300: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9844, view_607);  slice_9844 = view_607 = None
        slice_scatter_1788: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9843, add_300, 2, 0, 16);  slice_9843 = add_300 = None
        slice_scatter_1789: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1786, slice_scatter_1788, 1, 2384, 2400);  slice_scatter_1786 = slice_scatter_1788 = None
        slice_9848: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1789, 1, 2384, 2400)
        slice_9849: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9848, 2, 0, 16)
        slice_scatter_1791: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9848, slice_9849, 2, 0, 16);  slice_9848 = slice_9849 = None
        slice_scatter_1792: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1789, slice_scatter_1791, 1, 2384, 2400);  slice_scatter_1789 = slice_scatter_1791 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9869: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_9835, 2, 16, 32);  slice_9835 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_302: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_9869, memory_format = torch.contiguous_format);  slice_9869 = None
        view_608: "f32[32, 11]" = torch.ops.aten.view.default(clone_302, [32, 11]);  clone_302 = None
        mm_299: "f32[32, 8]" = torch.ops.aten.mm.default(view_608, slice_37)
        view_609: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_299, [2, 16, 8]);  mm_299 = None
        slice_9876: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1792, 1, 2384, 2400)
        slice_9877: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9876, 2, 0, 16)
        add_301: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9877, view_609);  slice_9877 = view_609 = None
        slice_scatter_1794: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9876, add_301, 2, 0, 16);  slice_9876 = add_301 = None
        slice_scatter_1795: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1792, slice_scatter_1794, 1, 2384, 2400);  slice_scatter_1792 = slice_scatter_1794 = None
        slice_9881: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1795, 1, 2384, 2400)
        slice_9882: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9881, 2, 0, 16)
        slice_scatter_1797: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9881, slice_9882, 2, 0, 16);  slice_9881 = slice_9882 = None
        slice_scatter_1798: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1795, slice_scatter_1797, 1, 2384, 2400);  slice_scatter_1795 = slice_scatter_1797 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9901: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2400, 2416)
        slice_9902: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_9901, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_303: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_9902, memory_format = torch.contiguous_format);  slice_9902 = None
        view_610: "f32[32, 16]" = torch.ops.aten.view.default(clone_303, [32, 16]);  clone_303 = None
        mm_300: "f32[32, 8]" = torch.ops.aten.mm.default(view_610, slice_7)
        view_611: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_300, [2, 16, 8]);  mm_300 = None
        slice_9909: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1798, 1, 2400, 2416)
        slice_9910: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9909, 2, 0, 16)
        add_302: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9910, view_611);  slice_9910 = view_611 = None
        slice_scatter_1800: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9909, add_302, 2, 0, 16);  slice_9909 = add_302 = None
        slice_scatter_1801: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1798, slice_scatter_1800, 1, 2400, 2416);  slice_scatter_1798 = slice_scatter_1800 = None
        slice_9914: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1801, 1, 2400, 2416)
        slice_9915: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9914, 2, 0, 16)
        slice_scatter_1803: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9914, slice_9915, 2, 0, 16);  slice_9914 = slice_9915 = None
        slice_scatter_1804: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1801, slice_scatter_1803, 1, 2400, 2416);  slice_scatter_1801 = slice_scatter_1803 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9935: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_9901, 2, 16, 32);  slice_9901 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_304: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_9935, memory_format = torch.contiguous_format);  slice_9935 = None
        view_612: "f32[32, 11]" = torch.ops.aten.view.default(clone_304, [32, 11]);  clone_304 = None
        mm_301: "f32[32, 8]" = torch.ops.aten.mm.default(view_612, slice_37)
        view_613: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_301, [2, 16, 8]);  mm_301 = None
        slice_9942: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1804, 1, 2400, 2416)
        slice_9943: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9942, 2, 0, 16)
        add_303: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9943, view_613);  slice_9943 = view_613 = None
        slice_scatter_1806: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9942, add_303, 2, 0, 16);  slice_9942 = add_303 = None
        slice_scatter_1807: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1804, slice_scatter_1806, 1, 2400, 2416);  slice_scatter_1804 = slice_scatter_1806 = None
        slice_9947: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1807, 1, 2400, 2416)
        slice_9948: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9947, 2, 0, 16)
        slice_scatter_1809: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9947, slice_9948, 2, 0, 16);  slice_9947 = slice_9948 = None
        slice_scatter_1810: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1807, slice_scatter_1809, 1, 2400, 2416);  slice_scatter_1807 = slice_scatter_1809 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_9967: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2416, 2432)
        slice_9968: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_9967, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_305: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_9968, memory_format = torch.contiguous_format);  slice_9968 = None
        view_614: "f32[32, 16]" = torch.ops.aten.view.default(clone_305, [32, 16]);  clone_305 = None
        mm_302: "f32[32, 8]" = torch.ops.aten.mm.default(view_614, slice_7)
        view_615: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_302, [2, 16, 8]);  mm_302 = None
        slice_9975: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1810, 1, 2416, 2432)
        slice_9976: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9975, 2, 0, 16)
        add_304: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_9976, view_615);  slice_9976 = view_615 = None
        slice_scatter_1812: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9975, add_304, 2, 0, 16);  slice_9975 = add_304 = None
        slice_scatter_1813: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1810, slice_scatter_1812, 1, 2416, 2432);  slice_scatter_1810 = slice_scatter_1812 = None
        slice_9980: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1813, 1, 2416, 2432)
        slice_9981: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_9980, 2, 0, 16)
        slice_scatter_1815: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_9980, slice_9981, 2, 0, 16);  slice_9980 = slice_9981 = None
        slice_scatter_1816: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1813, slice_scatter_1815, 1, 2416, 2432);  slice_scatter_1813 = slice_scatter_1815 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10001: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_9967, 2, 16, 32);  slice_9967 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_306: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_10001, memory_format = torch.contiguous_format);  slice_10001 = None
        view_616: "f32[32, 11]" = torch.ops.aten.view.default(clone_306, [32, 11]);  clone_306 = None
        mm_303: "f32[32, 8]" = torch.ops.aten.mm.default(view_616, slice_37)
        view_617: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_303, [2, 16, 8]);  mm_303 = None
        slice_10008: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1816, 1, 2416, 2432)
        slice_10009: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10008, 2, 0, 16)
        add_305: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10009, view_617);  slice_10009 = view_617 = None
        slice_scatter_1818: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10008, add_305, 2, 0, 16);  slice_10008 = add_305 = None
        slice_scatter_1819: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1816, slice_scatter_1818, 1, 2416, 2432);  slice_scatter_1816 = slice_scatter_1818 = None
        slice_10013: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1819, 1, 2416, 2432)
        slice_10014: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10013, 2, 0, 16)
        slice_scatter_1821: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10013, slice_10014, 2, 0, 16);  slice_10013 = slice_10014 = None
        slice_scatter_1822: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1819, slice_scatter_1821, 1, 2416, 2432);  slice_scatter_1819 = slice_scatter_1821 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10033: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2432, 2448)
        slice_10034: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_10033, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_307: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_10034, memory_format = torch.contiguous_format);  slice_10034 = None
        view_618: "f32[32, 16]" = torch.ops.aten.view.default(clone_307, [32, 16]);  clone_307 = None
        mm_304: "f32[32, 8]" = torch.ops.aten.mm.default(view_618, slice_7)
        view_619: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_304, [2, 16, 8]);  mm_304 = None
        slice_10041: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1822, 1, 2432, 2448)
        slice_10042: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10041, 2, 0, 16)
        add_306: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10042, view_619);  slice_10042 = view_619 = None
        slice_scatter_1824: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10041, add_306, 2, 0, 16);  slice_10041 = add_306 = None
        slice_scatter_1825: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1822, slice_scatter_1824, 1, 2432, 2448);  slice_scatter_1822 = slice_scatter_1824 = None
        slice_10046: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1825, 1, 2432, 2448)
        slice_10047: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10046, 2, 0, 16)
        slice_scatter_1827: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10046, slice_10047, 2, 0, 16);  slice_10046 = slice_10047 = None
        slice_scatter_1828: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1825, slice_scatter_1827, 1, 2432, 2448);  slice_scatter_1825 = slice_scatter_1827 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10067: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_10033, 2, 16, 32);  slice_10033 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_308: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_10067, memory_format = torch.contiguous_format);  slice_10067 = None
        view_620: "f32[32, 11]" = torch.ops.aten.view.default(clone_308, [32, 11]);  clone_308 = None
        mm_305: "f32[32, 8]" = torch.ops.aten.mm.default(view_620, slice_37)
        view_621: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_305, [2, 16, 8]);  mm_305 = None
        slice_10074: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1828, 1, 2432, 2448)
        slice_10075: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10074, 2, 0, 16)
        add_307: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10075, view_621);  slice_10075 = view_621 = None
        slice_scatter_1830: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10074, add_307, 2, 0, 16);  slice_10074 = add_307 = None
        slice_scatter_1831: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1828, slice_scatter_1830, 1, 2432, 2448);  slice_scatter_1828 = slice_scatter_1830 = None
        slice_10079: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1831, 1, 2432, 2448)
        slice_10080: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10079, 2, 0, 16)
        slice_scatter_1833: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10079, slice_10080, 2, 0, 16);  slice_10079 = slice_10080 = None
        slice_scatter_1834: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1831, slice_scatter_1833, 1, 2432, 2448);  slice_scatter_1831 = slice_scatter_1833 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10099: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2448, 2464)
        slice_10100: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_10099, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_309: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_10100, memory_format = torch.contiguous_format);  slice_10100 = None
        view_622: "f32[32, 16]" = torch.ops.aten.view.default(clone_309, [32, 16]);  clone_309 = None
        mm_306: "f32[32, 8]" = torch.ops.aten.mm.default(view_622, slice_7)
        view_623: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_306, [2, 16, 8]);  mm_306 = None
        slice_10107: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1834, 1, 2448, 2464)
        slice_10108: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10107, 2, 0, 16)
        add_308: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10108, view_623);  slice_10108 = view_623 = None
        slice_scatter_1836: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10107, add_308, 2, 0, 16);  slice_10107 = add_308 = None
        slice_scatter_1837: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1834, slice_scatter_1836, 1, 2448, 2464);  slice_scatter_1834 = slice_scatter_1836 = None
        slice_10112: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1837, 1, 2448, 2464)
        slice_10113: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10112, 2, 0, 16)
        slice_scatter_1839: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10112, slice_10113, 2, 0, 16);  slice_10112 = slice_10113 = None
        slice_scatter_1840: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1837, slice_scatter_1839, 1, 2448, 2464);  slice_scatter_1837 = slice_scatter_1839 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10133: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_10099, 2, 16, 32);  slice_10099 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_310: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_10133, memory_format = torch.contiguous_format);  slice_10133 = None
        view_624: "f32[32, 11]" = torch.ops.aten.view.default(clone_310, [32, 11]);  clone_310 = None
        mm_307: "f32[32, 8]" = torch.ops.aten.mm.default(view_624, slice_37)
        view_625: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_307, [2, 16, 8]);  mm_307 = None
        slice_10140: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1840, 1, 2448, 2464)
        slice_10141: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10140, 2, 0, 16)
        add_309: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10141, view_625);  slice_10141 = view_625 = None
        slice_scatter_1842: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10140, add_309, 2, 0, 16);  slice_10140 = add_309 = None
        slice_scatter_1843: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1840, slice_scatter_1842, 1, 2448, 2464);  slice_scatter_1840 = slice_scatter_1842 = None
        slice_10145: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1843, 1, 2448, 2464)
        slice_10146: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10145, 2, 0, 16)
        slice_scatter_1845: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10145, slice_10146, 2, 0, 16);  slice_10145 = slice_10146 = None
        slice_scatter_1846: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1843, slice_scatter_1845, 1, 2448, 2464);  slice_scatter_1843 = slice_scatter_1845 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10165: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2464, 2480)
        slice_10166: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_10165, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_311: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_10166, memory_format = torch.contiguous_format);  slice_10166 = None
        view_626: "f32[32, 16]" = torch.ops.aten.view.default(clone_311, [32, 16]);  clone_311 = None
        mm_308: "f32[32, 8]" = torch.ops.aten.mm.default(view_626, slice_7)
        view_627: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_308, [2, 16, 8]);  mm_308 = None
        slice_10173: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1846, 1, 2464, 2480)
        slice_10174: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10173, 2, 0, 16)
        add_310: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10174, view_627);  slice_10174 = view_627 = None
        slice_scatter_1848: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10173, add_310, 2, 0, 16);  slice_10173 = add_310 = None
        slice_scatter_1849: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1846, slice_scatter_1848, 1, 2464, 2480);  slice_scatter_1846 = slice_scatter_1848 = None
        slice_10178: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1849, 1, 2464, 2480)
        slice_10179: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10178, 2, 0, 16)
        slice_scatter_1851: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10178, slice_10179, 2, 0, 16);  slice_10178 = slice_10179 = None
        slice_scatter_1852: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1849, slice_scatter_1851, 1, 2464, 2480);  slice_scatter_1849 = slice_scatter_1851 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10199: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_10165, 2, 16, 32);  slice_10165 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_312: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_10199, memory_format = torch.contiguous_format);  slice_10199 = None
        view_628: "f32[32, 11]" = torch.ops.aten.view.default(clone_312, [32, 11]);  clone_312 = None
        mm_309: "f32[32, 8]" = torch.ops.aten.mm.default(view_628, slice_37)
        view_629: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_309, [2, 16, 8]);  mm_309 = None
        slice_10206: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1852, 1, 2464, 2480)
        slice_10207: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10206, 2, 0, 16)
        add_311: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10207, view_629);  slice_10207 = view_629 = None
        slice_scatter_1854: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10206, add_311, 2, 0, 16);  slice_10206 = add_311 = None
        slice_scatter_1855: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1852, slice_scatter_1854, 1, 2464, 2480);  slice_scatter_1852 = slice_scatter_1854 = None
        slice_10211: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1855, 1, 2464, 2480)
        slice_10212: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10211, 2, 0, 16)
        slice_scatter_1857: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10211, slice_10212, 2, 0, 16);  slice_10211 = slice_10212 = None
        slice_scatter_1858: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1855, slice_scatter_1857, 1, 2464, 2480);  slice_scatter_1855 = slice_scatter_1857 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10231: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2480, 2496)
        slice_10232: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_10231, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_313: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_10232, memory_format = torch.contiguous_format);  slice_10232 = None
        view_630: "f32[32, 16]" = torch.ops.aten.view.default(clone_313, [32, 16]);  clone_313 = None
        mm_310: "f32[32, 8]" = torch.ops.aten.mm.default(view_630, slice_7)
        view_631: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_310, [2, 16, 8]);  mm_310 = None
        slice_10239: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1858, 1, 2480, 2496)
        slice_10240: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10239, 2, 0, 16)
        add_312: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10240, view_631);  slice_10240 = view_631 = None
        slice_scatter_1860: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10239, add_312, 2, 0, 16);  slice_10239 = add_312 = None
        slice_scatter_1861: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1858, slice_scatter_1860, 1, 2480, 2496);  slice_scatter_1858 = slice_scatter_1860 = None
        slice_10244: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1861, 1, 2480, 2496)
        slice_10245: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10244, 2, 0, 16)
        slice_scatter_1863: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10244, slice_10245, 2, 0, 16);  slice_10244 = slice_10245 = None
        slice_scatter_1864: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1861, slice_scatter_1863, 1, 2480, 2496);  slice_scatter_1861 = slice_scatter_1863 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10265: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_10231, 2, 16, 32);  slice_10231 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_314: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_10265, memory_format = torch.contiguous_format);  slice_10265 = None
        view_632: "f32[32, 11]" = torch.ops.aten.view.default(clone_314, [32, 11]);  clone_314 = None
        mm_311: "f32[32, 8]" = torch.ops.aten.mm.default(view_632, slice_37)
        view_633: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_311, [2, 16, 8]);  mm_311 = None
        slice_10272: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1864, 1, 2480, 2496)
        slice_10273: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10272, 2, 0, 16)
        add_313: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10273, view_633);  slice_10273 = view_633 = None
        slice_scatter_1866: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10272, add_313, 2, 0, 16);  slice_10272 = add_313 = None
        slice_scatter_1867: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1864, slice_scatter_1866, 1, 2480, 2496);  slice_scatter_1864 = slice_scatter_1866 = None
        slice_10277: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1867, 1, 2480, 2496)
        slice_10278: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10277, 2, 0, 16)
        slice_scatter_1869: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10277, slice_10278, 2, 0, 16);  slice_10277 = slice_10278 = None
        slice_scatter_1870: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1867, slice_scatter_1869, 1, 2480, 2496);  slice_scatter_1867 = slice_scatter_1869 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10297: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2496, 2512)
        slice_10298: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_10297, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_315: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_10298, memory_format = torch.contiguous_format);  slice_10298 = None
        view_634: "f32[32, 16]" = torch.ops.aten.view.default(clone_315, [32, 16]);  clone_315 = None
        mm_312: "f32[32, 8]" = torch.ops.aten.mm.default(view_634, slice_7)
        view_635: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_312, [2, 16, 8]);  mm_312 = None
        slice_10305: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1870, 1, 2496, 2512)
        slice_10306: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10305, 2, 0, 16)
        add_314: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10306, view_635);  slice_10306 = view_635 = None
        slice_scatter_1872: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10305, add_314, 2, 0, 16);  slice_10305 = add_314 = None
        slice_scatter_1873: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1870, slice_scatter_1872, 1, 2496, 2512);  slice_scatter_1870 = slice_scatter_1872 = None
        slice_10310: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1873, 1, 2496, 2512)
        slice_10311: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10310, 2, 0, 16)
        slice_scatter_1875: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10310, slice_10311, 2, 0, 16);  slice_10310 = slice_10311 = None
        slice_scatter_1876: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1873, slice_scatter_1875, 1, 2496, 2512);  slice_scatter_1873 = slice_scatter_1875 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10331: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_10297, 2, 16, 32);  slice_10297 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_316: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_10331, memory_format = torch.contiguous_format);  slice_10331 = None
        view_636: "f32[32, 11]" = torch.ops.aten.view.default(clone_316, [32, 11]);  clone_316 = None
        mm_313: "f32[32, 8]" = torch.ops.aten.mm.default(view_636, slice_37)
        view_637: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_313, [2, 16, 8]);  mm_313 = None
        slice_10338: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1876, 1, 2496, 2512)
        slice_10339: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10338, 2, 0, 16)
        add_315: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10339, view_637);  slice_10339 = view_637 = None
        slice_scatter_1878: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10338, add_315, 2, 0, 16);  slice_10338 = add_315 = None
        slice_scatter_1879: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1876, slice_scatter_1878, 1, 2496, 2512);  slice_scatter_1876 = slice_scatter_1878 = None
        slice_10343: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1879, 1, 2496, 2512)
        slice_10344: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10343, 2, 0, 16)
        slice_scatter_1881: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10343, slice_10344, 2, 0, 16);  slice_10343 = slice_10344 = None
        slice_scatter_1882: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1879, slice_scatter_1881, 1, 2496, 2512);  slice_scatter_1879 = slice_scatter_1881 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10363: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2512, 2528)
        slice_10364: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_10363, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_317: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_10364, memory_format = torch.contiguous_format);  slice_10364 = None
        view_638: "f32[32, 16]" = torch.ops.aten.view.default(clone_317, [32, 16]);  clone_317 = None
        mm_314: "f32[32, 8]" = torch.ops.aten.mm.default(view_638, slice_7)
        view_639: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_314, [2, 16, 8]);  mm_314 = None
        slice_10371: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1882, 1, 2512, 2528)
        slice_10372: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10371, 2, 0, 16)
        add_316: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10372, view_639);  slice_10372 = view_639 = None
        slice_scatter_1884: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10371, add_316, 2, 0, 16);  slice_10371 = add_316 = None
        slice_scatter_1885: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1882, slice_scatter_1884, 1, 2512, 2528);  slice_scatter_1882 = slice_scatter_1884 = None
        slice_10376: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1885, 1, 2512, 2528)
        slice_10377: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10376, 2, 0, 16)
        slice_scatter_1887: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10376, slice_10377, 2, 0, 16);  slice_10376 = slice_10377 = None
        slice_scatter_1888: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1885, slice_scatter_1887, 1, 2512, 2528);  slice_scatter_1885 = slice_scatter_1887 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10397: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_10363, 2, 16, 32);  slice_10363 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_318: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_10397, memory_format = torch.contiguous_format);  slice_10397 = None
        view_640: "f32[32, 11]" = torch.ops.aten.view.default(clone_318, [32, 11]);  clone_318 = None
        mm_315: "f32[32, 8]" = torch.ops.aten.mm.default(view_640, slice_37)
        view_641: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_315, [2, 16, 8]);  mm_315 = None
        slice_10404: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1888, 1, 2512, 2528)
        slice_10405: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10404, 2, 0, 16)
        add_317: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10405, view_641);  slice_10405 = view_641 = None
        slice_scatter_1890: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10404, add_317, 2, 0, 16);  slice_10404 = add_317 = None
        slice_scatter_1891: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1888, slice_scatter_1890, 1, 2512, 2528);  slice_scatter_1888 = slice_scatter_1890 = None
        slice_10409: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1891, 1, 2512, 2528)
        slice_10410: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10409, 2, 0, 16)
        slice_scatter_1893: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10409, slice_10410, 2, 0, 16);  slice_10409 = slice_10410 = None
        slice_scatter_1894: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1891, slice_scatter_1893, 1, 2512, 2528);  slice_scatter_1891 = slice_scatter_1893 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10429: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2528, 2544)
        slice_10430: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_10429, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_319: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_10430, memory_format = torch.contiguous_format);  slice_10430 = None
        view_642: "f32[32, 16]" = torch.ops.aten.view.default(clone_319, [32, 16]);  clone_319 = None
        mm_316: "f32[32, 8]" = torch.ops.aten.mm.default(view_642, slice_7)
        view_643: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_316, [2, 16, 8]);  mm_316 = None
        slice_10437: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1894, 1, 2528, 2544)
        slice_10438: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10437, 2, 0, 16)
        add_318: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10438, view_643);  slice_10438 = view_643 = None
        slice_scatter_1896: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10437, add_318, 2, 0, 16);  slice_10437 = add_318 = None
        slice_scatter_1897: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1894, slice_scatter_1896, 1, 2528, 2544);  slice_scatter_1894 = slice_scatter_1896 = None
        slice_10442: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1897, 1, 2528, 2544)
        slice_10443: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10442, 2, 0, 16)
        slice_scatter_1899: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10442, slice_10443, 2, 0, 16);  slice_10442 = slice_10443 = None
        slice_scatter_1900: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1897, slice_scatter_1899, 1, 2528, 2544);  slice_scatter_1897 = slice_scatter_1899 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10463: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_10429, 2, 16, 32);  slice_10429 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_320: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_10463, memory_format = torch.contiguous_format);  slice_10463 = None
        view_644: "f32[32, 11]" = torch.ops.aten.view.default(clone_320, [32, 11]);  clone_320 = None
        mm_317: "f32[32, 8]" = torch.ops.aten.mm.default(view_644, slice_37)
        view_645: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_317, [2, 16, 8]);  mm_317 = None
        slice_10470: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1900, 1, 2528, 2544)
        slice_10471: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10470, 2, 0, 16)
        add_319: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10471, view_645);  slice_10471 = view_645 = None
        slice_scatter_1902: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10470, add_319, 2, 0, 16);  slice_10470 = add_319 = None
        slice_scatter_1903: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1900, slice_scatter_1902, 1, 2528, 2544);  slice_scatter_1900 = slice_scatter_1902 = None
        slice_10475: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1903, 1, 2528, 2544)
        slice_10476: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10475, 2, 0, 16)
        slice_scatter_1905: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10475, slice_10476, 2, 0, 16);  slice_10475 = slice_10476 = None
        slice_scatter_1906: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1903, slice_scatter_1905, 1, 2528, 2544);  slice_scatter_1903 = slice_scatter_1905 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10495: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2544, 2560)
        slice_10496: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_10495, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_321: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_10496, memory_format = torch.contiguous_format);  slice_10496 = None
        view_646: "f32[32, 16]" = torch.ops.aten.view.default(clone_321, [32, 16]);  clone_321 = None
        mm_318: "f32[32, 8]" = torch.ops.aten.mm.default(view_646, slice_7)
        view_647: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_318, [2, 16, 8]);  mm_318 = None
        slice_10503: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1906, 1, 2544, 2560)
        slice_10504: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10503, 2, 0, 16)
        add_320: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10504, view_647);  slice_10504 = view_647 = None
        slice_scatter_1908: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10503, add_320, 2, 0, 16);  slice_10503 = add_320 = None
        slice_scatter_1909: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1906, slice_scatter_1908, 1, 2544, 2560);  slice_scatter_1906 = slice_scatter_1908 = None
        slice_10508: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1909, 1, 2544, 2560)
        slice_10509: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10508, 2, 0, 16)
        slice_scatter_1911: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10508, slice_10509, 2, 0, 16);  slice_10508 = slice_10509 = None
        slice_scatter_1912: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1909, slice_scatter_1911, 1, 2544, 2560);  slice_scatter_1909 = slice_scatter_1911 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10529: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_10495, 2, 16, 32);  slice_10495 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_322: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_10529, memory_format = torch.contiguous_format);  slice_10529 = None
        view_648: "f32[32, 11]" = torch.ops.aten.view.default(clone_322, [32, 11]);  clone_322 = None
        mm_319: "f32[32, 8]" = torch.ops.aten.mm.default(view_648, slice_37)
        view_649: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_319, [2, 16, 8]);  mm_319 = None
        slice_10536: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1912, 1, 2544, 2560)
        slice_10537: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10536, 2, 0, 16)
        add_321: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10537, view_649);  slice_10537 = view_649 = None
        slice_scatter_1914: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10536, add_321, 2, 0, 16);  slice_10536 = add_321 = None
        slice_scatter_1915: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1912, slice_scatter_1914, 1, 2544, 2560);  slice_scatter_1912 = slice_scatter_1914 = None
        slice_10541: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1915, 1, 2544, 2560)
        slice_10542: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10541, 2, 0, 16)
        slice_scatter_1917: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10541, slice_10542, 2, 0, 16);  slice_10541 = slice_10542 = None
        slice_scatter_1918: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1915, slice_scatter_1917, 1, 2544, 2560);  slice_scatter_1915 = slice_scatter_1917 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10561: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2560, 2576)
        slice_10562: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_10561, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_323: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_10562, memory_format = torch.contiguous_format);  slice_10562 = None
        view_650: "f32[32, 16]" = torch.ops.aten.view.default(clone_323, [32, 16]);  clone_323 = None
        mm_320: "f32[32, 8]" = torch.ops.aten.mm.default(view_650, slice_7)
        view_651: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_320, [2, 16, 8]);  mm_320 = None
        slice_10569: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1918, 1, 2560, 2576)
        slice_10570: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10569, 2, 0, 16)
        add_322: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10570, view_651);  slice_10570 = view_651 = None
        slice_scatter_1920: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10569, add_322, 2, 0, 16);  slice_10569 = add_322 = None
        slice_scatter_1921: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1918, slice_scatter_1920, 1, 2560, 2576);  slice_scatter_1918 = slice_scatter_1920 = None
        slice_10574: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1921, 1, 2560, 2576)
        slice_10575: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10574, 2, 0, 16)
        slice_scatter_1923: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10574, slice_10575, 2, 0, 16);  slice_10574 = slice_10575 = None
        slice_scatter_1924: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1921, slice_scatter_1923, 1, 2560, 2576);  slice_scatter_1921 = slice_scatter_1923 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10595: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_10561, 2, 16, 32);  slice_10561 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_324: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_10595, memory_format = torch.contiguous_format);  slice_10595 = None
        view_652: "f32[32, 11]" = torch.ops.aten.view.default(clone_324, [32, 11]);  clone_324 = None
        mm_321: "f32[32, 8]" = torch.ops.aten.mm.default(view_652, slice_37)
        view_653: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_321, [2, 16, 8]);  mm_321 = None
        slice_10602: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1924, 1, 2560, 2576)
        slice_10603: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10602, 2, 0, 16)
        add_323: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10603, view_653);  slice_10603 = view_653 = None
        slice_scatter_1926: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10602, add_323, 2, 0, 16);  slice_10602 = add_323 = None
        slice_scatter_1927: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1924, slice_scatter_1926, 1, 2560, 2576);  slice_scatter_1924 = slice_scatter_1926 = None
        slice_10607: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1927, 1, 2560, 2576)
        slice_10608: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10607, 2, 0, 16)
        slice_scatter_1929: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10607, slice_10608, 2, 0, 16);  slice_10607 = slice_10608 = None
        slice_scatter_1930: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1927, slice_scatter_1929, 1, 2560, 2576);  slice_scatter_1927 = slice_scatter_1929 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10627: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2576, 2592)
        slice_10628: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_10627, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_325: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_10628, memory_format = torch.contiguous_format);  slice_10628 = None
        view_654: "f32[32, 16]" = torch.ops.aten.view.default(clone_325, [32, 16]);  clone_325 = None
        mm_322: "f32[32, 8]" = torch.ops.aten.mm.default(view_654, slice_7)
        view_655: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_322, [2, 16, 8]);  mm_322 = None
        slice_10635: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1930, 1, 2576, 2592)
        slice_10636: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10635, 2, 0, 16)
        add_324: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10636, view_655);  slice_10636 = view_655 = None
        slice_scatter_1932: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10635, add_324, 2, 0, 16);  slice_10635 = add_324 = None
        slice_scatter_1933: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1930, slice_scatter_1932, 1, 2576, 2592);  slice_scatter_1930 = slice_scatter_1932 = None
        slice_10640: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1933, 1, 2576, 2592)
        slice_10641: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10640, 2, 0, 16)
        slice_scatter_1935: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10640, slice_10641, 2, 0, 16);  slice_10640 = slice_10641 = None
        slice_scatter_1936: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1933, slice_scatter_1935, 1, 2576, 2592);  slice_scatter_1933 = slice_scatter_1935 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10661: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_10627, 2, 16, 32);  slice_10627 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_326: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_10661, memory_format = torch.contiguous_format);  slice_10661 = None
        view_656: "f32[32, 11]" = torch.ops.aten.view.default(clone_326, [32, 11]);  clone_326 = None
        mm_323: "f32[32, 8]" = torch.ops.aten.mm.default(view_656, slice_37)
        view_657: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_323, [2, 16, 8]);  mm_323 = None
        slice_10668: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1936, 1, 2576, 2592)
        slice_10669: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10668, 2, 0, 16)
        add_325: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10669, view_657);  slice_10669 = view_657 = None
        slice_scatter_1938: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10668, add_325, 2, 0, 16);  slice_10668 = add_325 = None
        slice_scatter_1939: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1936, slice_scatter_1938, 1, 2576, 2592);  slice_scatter_1936 = slice_scatter_1938 = None
        slice_10673: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1939, 1, 2576, 2592)
        slice_10674: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10673, 2, 0, 16)
        slice_scatter_1941: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10673, slice_10674, 2, 0, 16);  slice_10673 = slice_10674 = None
        slice_scatter_1942: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1939, slice_scatter_1941, 1, 2576, 2592);  slice_scatter_1939 = slice_scatter_1941 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10693: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2592, 2608)
        slice_10694: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_10693, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_327: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_10694, memory_format = torch.contiguous_format);  slice_10694 = None
        view_658: "f32[32, 16]" = torch.ops.aten.view.default(clone_327, [32, 16]);  clone_327 = None
        mm_324: "f32[32, 8]" = torch.ops.aten.mm.default(view_658, slice_7)
        view_659: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_324, [2, 16, 8]);  mm_324 = None
        slice_10701: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1942, 1, 2592, 2608)
        slice_10702: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10701, 2, 0, 16)
        add_326: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10702, view_659);  slice_10702 = view_659 = None
        slice_scatter_1944: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10701, add_326, 2, 0, 16);  slice_10701 = add_326 = None
        slice_scatter_1945: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1942, slice_scatter_1944, 1, 2592, 2608);  slice_scatter_1942 = slice_scatter_1944 = None
        slice_10706: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1945, 1, 2592, 2608)
        slice_10707: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10706, 2, 0, 16)
        slice_scatter_1947: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10706, slice_10707, 2, 0, 16);  slice_10706 = slice_10707 = None
        slice_scatter_1948: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1945, slice_scatter_1947, 1, 2592, 2608);  slice_scatter_1945 = slice_scatter_1947 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10727: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_10693, 2, 16, 32);  slice_10693 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_328: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_10727, memory_format = torch.contiguous_format);  slice_10727 = None
        view_660: "f32[32, 11]" = torch.ops.aten.view.default(clone_328, [32, 11]);  clone_328 = None
        mm_325: "f32[32, 8]" = torch.ops.aten.mm.default(view_660, slice_37)
        view_661: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_325, [2, 16, 8]);  mm_325 = None
        slice_10734: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1948, 1, 2592, 2608)
        slice_10735: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10734, 2, 0, 16)
        add_327: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10735, view_661);  slice_10735 = view_661 = None
        slice_scatter_1950: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10734, add_327, 2, 0, 16);  slice_10734 = add_327 = None
        slice_scatter_1951: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1948, slice_scatter_1950, 1, 2592, 2608);  slice_scatter_1948 = slice_scatter_1950 = None
        slice_10739: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1951, 1, 2592, 2608)
        slice_10740: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10739, 2, 0, 16)
        slice_scatter_1953: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10739, slice_10740, 2, 0, 16);  slice_10739 = slice_10740 = None
        slice_scatter_1954: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1951, slice_scatter_1953, 1, 2592, 2608);  slice_scatter_1951 = slice_scatter_1953 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10759: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2608, 2624)
        slice_10760: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_10759, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_329: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_10760, memory_format = torch.contiguous_format);  slice_10760 = None
        view_662: "f32[32, 16]" = torch.ops.aten.view.default(clone_329, [32, 16]);  clone_329 = None
        mm_326: "f32[32, 8]" = torch.ops.aten.mm.default(view_662, slice_7)
        view_663: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_326, [2, 16, 8]);  mm_326 = None
        slice_10767: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1954, 1, 2608, 2624)
        slice_10768: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10767, 2, 0, 16)
        add_328: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10768, view_663);  slice_10768 = view_663 = None
        slice_scatter_1956: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10767, add_328, 2, 0, 16);  slice_10767 = add_328 = None
        slice_scatter_1957: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1954, slice_scatter_1956, 1, 2608, 2624);  slice_scatter_1954 = slice_scatter_1956 = None
        slice_10772: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1957, 1, 2608, 2624)
        slice_10773: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10772, 2, 0, 16)
        slice_scatter_1959: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10772, slice_10773, 2, 0, 16);  slice_10772 = slice_10773 = None
        slice_scatter_1960: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1957, slice_scatter_1959, 1, 2608, 2624);  slice_scatter_1957 = slice_scatter_1959 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10793: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_10759, 2, 16, 32);  slice_10759 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_330: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_10793, memory_format = torch.contiguous_format);  slice_10793 = None
        view_664: "f32[32, 11]" = torch.ops.aten.view.default(clone_330, [32, 11]);  clone_330 = None
        mm_327: "f32[32, 8]" = torch.ops.aten.mm.default(view_664, slice_37)
        view_665: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_327, [2, 16, 8]);  mm_327 = None
        slice_10800: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1960, 1, 2608, 2624)
        slice_10801: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10800, 2, 0, 16)
        add_329: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10801, view_665);  slice_10801 = view_665 = None
        slice_scatter_1962: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10800, add_329, 2, 0, 16);  slice_10800 = add_329 = None
        slice_scatter_1963: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1960, slice_scatter_1962, 1, 2608, 2624);  slice_scatter_1960 = slice_scatter_1962 = None
        slice_10805: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1963, 1, 2608, 2624)
        slice_10806: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10805, 2, 0, 16)
        slice_scatter_1965: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10805, slice_10806, 2, 0, 16);  slice_10805 = slice_10806 = None
        slice_scatter_1966: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1963, slice_scatter_1965, 1, 2608, 2624);  slice_scatter_1963 = slice_scatter_1965 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10825: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2624, 2640)
        slice_10826: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_10825, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_331: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_10826, memory_format = torch.contiguous_format);  slice_10826 = None
        view_666: "f32[32, 16]" = torch.ops.aten.view.default(clone_331, [32, 16]);  clone_331 = None
        mm_328: "f32[32, 8]" = torch.ops.aten.mm.default(view_666, slice_7)
        view_667: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_328, [2, 16, 8]);  mm_328 = None
        slice_10833: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1966, 1, 2624, 2640)
        slice_10834: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10833, 2, 0, 16)
        add_330: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10834, view_667);  slice_10834 = view_667 = None
        slice_scatter_1968: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10833, add_330, 2, 0, 16);  slice_10833 = add_330 = None
        slice_scatter_1969: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1966, slice_scatter_1968, 1, 2624, 2640);  slice_scatter_1966 = slice_scatter_1968 = None
        slice_10838: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1969, 1, 2624, 2640)
        slice_10839: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10838, 2, 0, 16)
        slice_scatter_1971: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10838, slice_10839, 2, 0, 16);  slice_10838 = slice_10839 = None
        slice_scatter_1972: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1969, slice_scatter_1971, 1, 2624, 2640);  slice_scatter_1969 = slice_scatter_1971 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10859: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_10825, 2, 16, 32);  slice_10825 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_332: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_10859, memory_format = torch.contiguous_format);  slice_10859 = None
        view_668: "f32[32, 11]" = torch.ops.aten.view.default(clone_332, [32, 11]);  clone_332 = None
        mm_329: "f32[32, 8]" = torch.ops.aten.mm.default(view_668, slice_37)
        view_669: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_329, [2, 16, 8]);  mm_329 = None
        slice_10866: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1972, 1, 2624, 2640)
        slice_10867: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10866, 2, 0, 16)
        add_331: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10867, view_669);  slice_10867 = view_669 = None
        slice_scatter_1974: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10866, add_331, 2, 0, 16);  slice_10866 = add_331 = None
        slice_scatter_1975: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1972, slice_scatter_1974, 1, 2624, 2640);  slice_scatter_1972 = slice_scatter_1974 = None
        slice_10871: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1975, 1, 2624, 2640)
        slice_10872: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10871, 2, 0, 16)
        slice_scatter_1977: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10871, slice_10872, 2, 0, 16);  slice_10871 = slice_10872 = None
        slice_scatter_1978: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1975, slice_scatter_1977, 1, 2624, 2640);  slice_scatter_1975 = slice_scatter_1977 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10891: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2640, 2656)
        slice_10892: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_10891, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_333: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_10892, memory_format = torch.contiguous_format);  slice_10892 = None
        view_670: "f32[32, 16]" = torch.ops.aten.view.default(clone_333, [32, 16]);  clone_333 = None
        mm_330: "f32[32, 8]" = torch.ops.aten.mm.default(view_670, slice_7)
        view_671: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_330, [2, 16, 8]);  mm_330 = None
        slice_10899: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1978, 1, 2640, 2656)
        slice_10900: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10899, 2, 0, 16)
        add_332: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10900, view_671);  slice_10900 = view_671 = None
        slice_scatter_1980: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10899, add_332, 2, 0, 16);  slice_10899 = add_332 = None
        slice_scatter_1981: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1978, slice_scatter_1980, 1, 2640, 2656);  slice_scatter_1978 = slice_scatter_1980 = None
        slice_10904: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1981, 1, 2640, 2656)
        slice_10905: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10904, 2, 0, 16)
        slice_scatter_1983: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10904, slice_10905, 2, 0, 16);  slice_10904 = slice_10905 = None
        slice_scatter_1984: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1981, slice_scatter_1983, 1, 2640, 2656);  slice_scatter_1981 = slice_scatter_1983 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10925: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_10891, 2, 16, 32);  slice_10891 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_334: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_10925, memory_format = torch.contiguous_format);  slice_10925 = None
        view_672: "f32[32, 11]" = torch.ops.aten.view.default(clone_334, [32, 11]);  clone_334 = None
        mm_331: "f32[32, 8]" = torch.ops.aten.mm.default(view_672, slice_37)
        view_673: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_331, [2, 16, 8]);  mm_331 = None
        slice_10932: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1984, 1, 2640, 2656)
        slice_10933: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10932, 2, 0, 16)
        add_333: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10933, view_673);  slice_10933 = view_673 = None
        slice_scatter_1986: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10932, add_333, 2, 0, 16);  slice_10932 = add_333 = None
        slice_scatter_1987: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1984, slice_scatter_1986, 1, 2640, 2656);  slice_scatter_1984 = slice_scatter_1986 = None
        slice_10937: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1987, 1, 2640, 2656)
        slice_10938: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10937, 2, 0, 16)
        slice_scatter_1989: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10937, slice_10938, 2, 0, 16);  slice_10937 = slice_10938 = None
        slice_scatter_1990: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1987, slice_scatter_1989, 1, 2640, 2656);  slice_scatter_1987 = slice_scatter_1989 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10957: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2656, 2672)
        slice_10958: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_10957, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_335: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_10958, memory_format = torch.contiguous_format);  slice_10958 = None
        view_674: "f32[32, 16]" = torch.ops.aten.view.default(clone_335, [32, 16]);  clone_335 = None
        mm_332: "f32[32, 8]" = torch.ops.aten.mm.default(view_674, slice_7)
        view_675: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_332, [2, 16, 8]);  mm_332 = None
        slice_10965: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1990, 1, 2656, 2672)
        slice_10966: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10965, 2, 0, 16)
        add_334: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10966, view_675);  slice_10966 = view_675 = None
        slice_scatter_1992: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10965, add_334, 2, 0, 16);  slice_10965 = add_334 = None
        slice_scatter_1993: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1990, slice_scatter_1992, 1, 2656, 2672);  slice_scatter_1990 = slice_scatter_1992 = None
        slice_10970: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1993, 1, 2656, 2672)
        slice_10971: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10970, 2, 0, 16)
        slice_scatter_1995: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10970, slice_10971, 2, 0, 16);  slice_10970 = slice_10971 = None
        slice_scatter_1996: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1993, slice_scatter_1995, 1, 2656, 2672);  slice_scatter_1993 = slice_scatter_1995 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_10991: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_10957, 2, 16, 32);  slice_10957 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_336: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_10991, memory_format = torch.contiguous_format);  slice_10991 = None
        view_676: "f32[32, 11]" = torch.ops.aten.view.default(clone_336, [32, 11]);  clone_336 = None
        mm_333: "f32[32, 8]" = torch.ops.aten.mm.default(view_676, slice_37)
        view_677: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_333, [2, 16, 8]);  mm_333 = None
        slice_10998: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1996, 1, 2656, 2672)
        slice_10999: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_10998, 2, 0, 16)
        add_335: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_10999, view_677);  slice_10999 = view_677 = None
        slice_scatter_1998: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_10998, add_335, 2, 0, 16);  slice_10998 = add_335 = None
        slice_scatter_1999: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1996, slice_scatter_1998, 1, 2656, 2672);  slice_scatter_1996 = slice_scatter_1998 = None
        slice_11003: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_1999, 1, 2656, 2672)
        slice_11004: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11003, 2, 0, 16)
        slice_scatter_2001: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11003, slice_11004, 2, 0, 16);  slice_11003 = slice_11004 = None
        slice_scatter_2002: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_1999, slice_scatter_2001, 1, 2656, 2672);  slice_scatter_1999 = slice_scatter_2001 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11023: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2672, 2688)
        slice_11024: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_11023, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_337: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_11024, memory_format = torch.contiguous_format);  slice_11024 = None
        view_678: "f32[32, 16]" = torch.ops.aten.view.default(clone_337, [32, 16]);  clone_337 = None
        mm_334: "f32[32, 8]" = torch.ops.aten.mm.default(view_678, slice_7)
        view_679: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_334, [2, 16, 8]);  mm_334 = None
        slice_11031: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2002, 1, 2672, 2688)
        slice_11032: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11031, 2, 0, 16)
        add_336: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11032, view_679);  slice_11032 = view_679 = None
        slice_scatter_2004: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11031, add_336, 2, 0, 16);  slice_11031 = add_336 = None
        slice_scatter_2005: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2002, slice_scatter_2004, 1, 2672, 2688);  slice_scatter_2002 = slice_scatter_2004 = None
        slice_11036: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2005, 1, 2672, 2688)
        slice_11037: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11036, 2, 0, 16)
        slice_scatter_2007: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11036, slice_11037, 2, 0, 16);  slice_11036 = slice_11037 = None
        slice_scatter_2008: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2005, slice_scatter_2007, 1, 2672, 2688);  slice_scatter_2005 = slice_scatter_2007 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11057: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_11023, 2, 16, 32);  slice_11023 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_338: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_11057, memory_format = torch.contiguous_format);  slice_11057 = None
        view_680: "f32[32, 11]" = torch.ops.aten.view.default(clone_338, [32, 11]);  clone_338 = None
        mm_335: "f32[32, 8]" = torch.ops.aten.mm.default(view_680, slice_37)
        view_681: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_335, [2, 16, 8]);  mm_335 = None
        slice_11064: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2008, 1, 2672, 2688)
        slice_11065: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11064, 2, 0, 16)
        add_337: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11065, view_681);  slice_11065 = view_681 = None
        slice_scatter_2010: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11064, add_337, 2, 0, 16);  slice_11064 = add_337 = None
        slice_scatter_2011: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2008, slice_scatter_2010, 1, 2672, 2688);  slice_scatter_2008 = slice_scatter_2010 = None
        slice_11069: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2011, 1, 2672, 2688)
        slice_11070: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11069, 2, 0, 16)
        slice_scatter_2013: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11069, slice_11070, 2, 0, 16);  slice_11069 = slice_11070 = None
        slice_scatter_2014: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2011, slice_scatter_2013, 1, 2672, 2688);  slice_scatter_2011 = slice_scatter_2013 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11089: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2688, 2704)
        slice_11090: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_11089, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_339: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_11090, memory_format = torch.contiguous_format);  slice_11090 = None
        view_682: "f32[32, 16]" = torch.ops.aten.view.default(clone_339, [32, 16]);  clone_339 = None
        mm_336: "f32[32, 8]" = torch.ops.aten.mm.default(view_682, slice_7)
        view_683: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_336, [2, 16, 8]);  mm_336 = None
        slice_11097: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2014, 1, 2688, 2704)
        slice_11098: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11097, 2, 0, 16)
        add_338: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11098, view_683);  slice_11098 = view_683 = None
        slice_scatter_2016: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11097, add_338, 2, 0, 16);  slice_11097 = add_338 = None
        slice_scatter_2017: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2014, slice_scatter_2016, 1, 2688, 2704);  slice_scatter_2014 = slice_scatter_2016 = None
        slice_11102: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2017, 1, 2688, 2704)
        slice_11103: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11102, 2, 0, 16)
        slice_scatter_2019: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11102, slice_11103, 2, 0, 16);  slice_11102 = slice_11103 = None
        slice_scatter_2020: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2017, slice_scatter_2019, 1, 2688, 2704);  slice_scatter_2017 = slice_scatter_2019 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11123: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_11089, 2, 16, 32);  slice_11089 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_340: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_11123, memory_format = torch.contiguous_format);  slice_11123 = None
        view_684: "f32[32, 11]" = torch.ops.aten.view.default(clone_340, [32, 11]);  clone_340 = None
        mm_337: "f32[32, 8]" = torch.ops.aten.mm.default(view_684, slice_37)
        view_685: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_337, [2, 16, 8]);  mm_337 = None
        slice_11130: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2020, 1, 2688, 2704)
        slice_11131: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11130, 2, 0, 16)
        add_339: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11131, view_685);  slice_11131 = view_685 = None
        slice_scatter_2022: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11130, add_339, 2, 0, 16);  slice_11130 = add_339 = None
        slice_scatter_2023: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2020, slice_scatter_2022, 1, 2688, 2704);  slice_scatter_2020 = slice_scatter_2022 = None
        slice_11135: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2023, 1, 2688, 2704)
        slice_11136: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11135, 2, 0, 16)
        slice_scatter_2025: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11135, slice_11136, 2, 0, 16);  slice_11135 = slice_11136 = None
        slice_scatter_2026: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2023, slice_scatter_2025, 1, 2688, 2704);  slice_scatter_2023 = slice_scatter_2025 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11155: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2704, 2720)
        slice_11156: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_11155, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_341: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_11156, memory_format = torch.contiguous_format);  slice_11156 = None
        view_686: "f32[32, 16]" = torch.ops.aten.view.default(clone_341, [32, 16]);  clone_341 = None
        mm_338: "f32[32, 8]" = torch.ops.aten.mm.default(view_686, slice_7)
        view_687: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_338, [2, 16, 8]);  mm_338 = None
        slice_11163: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2026, 1, 2704, 2720)
        slice_11164: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11163, 2, 0, 16)
        add_340: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11164, view_687);  slice_11164 = view_687 = None
        slice_scatter_2028: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11163, add_340, 2, 0, 16);  slice_11163 = add_340 = None
        slice_scatter_2029: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2026, slice_scatter_2028, 1, 2704, 2720);  slice_scatter_2026 = slice_scatter_2028 = None
        slice_11168: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2029, 1, 2704, 2720)
        slice_11169: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11168, 2, 0, 16)
        slice_scatter_2031: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11168, slice_11169, 2, 0, 16);  slice_11168 = slice_11169 = None
        slice_scatter_2032: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2029, slice_scatter_2031, 1, 2704, 2720);  slice_scatter_2029 = slice_scatter_2031 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11189: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_11155, 2, 16, 32);  slice_11155 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_342: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_11189, memory_format = torch.contiguous_format);  slice_11189 = None
        view_688: "f32[32, 11]" = torch.ops.aten.view.default(clone_342, [32, 11]);  clone_342 = None
        mm_339: "f32[32, 8]" = torch.ops.aten.mm.default(view_688, slice_37)
        view_689: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_339, [2, 16, 8]);  mm_339 = None
        slice_11196: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2032, 1, 2704, 2720)
        slice_11197: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11196, 2, 0, 16)
        add_341: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11197, view_689);  slice_11197 = view_689 = None
        slice_scatter_2034: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11196, add_341, 2, 0, 16);  slice_11196 = add_341 = None
        slice_scatter_2035: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2032, slice_scatter_2034, 1, 2704, 2720);  slice_scatter_2032 = slice_scatter_2034 = None
        slice_11201: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2035, 1, 2704, 2720)
        slice_11202: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11201, 2, 0, 16)
        slice_scatter_2037: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11201, slice_11202, 2, 0, 16);  slice_11201 = slice_11202 = None
        slice_scatter_2038: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2035, slice_scatter_2037, 1, 2704, 2720);  slice_scatter_2035 = slice_scatter_2037 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11221: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2720, 2736)
        slice_11222: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_11221, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_343: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_11222, memory_format = torch.contiguous_format);  slice_11222 = None
        view_690: "f32[32, 16]" = torch.ops.aten.view.default(clone_343, [32, 16]);  clone_343 = None
        mm_340: "f32[32, 8]" = torch.ops.aten.mm.default(view_690, slice_7)
        view_691: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_340, [2, 16, 8]);  mm_340 = None
        slice_11229: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2038, 1, 2720, 2736)
        slice_11230: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11229, 2, 0, 16)
        add_342: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11230, view_691);  slice_11230 = view_691 = None
        slice_scatter_2040: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11229, add_342, 2, 0, 16);  slice_11229 = add_342 = None
        slice_scatter_2041: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2038, slice_scatter_2040, 1, 2720, 2736);  slice_scatter_2038 = slice_scatter_2040 = None
        slice_11234: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2041, 1, 2720, 2736)
        slice_11235: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11234, 2, 0, 16)
        slice_scatter_2043: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11234, slice_11235, 2, 0, 16);  slice_11234 = slice_11235 = None
        slice_scatter_2044: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2041, slice_scatter_2043, 1, 2720, 2736);  slice_scatter_2041 = slice_scatter_2043 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11255: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_11221, 2, 16, 32);  slice_11221 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_344: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_11255, memory_format = torch.contiguous_format);  slice_11255 = None
        view_692: "f32[32, 11]" = torch.ops.aten.view.default(clone_344, [32, 11]);  clone_344 = None
        mm_341: "f32[32, 8]" = torch.ops.aten.mm.default(view_692, slice_37)
        view_693: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_341, [2, 16, 8]);  mm_341 = None
        slice_11262: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2044, 1, 2720, 2736)
        slice_11263: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11262, 2, 0, 16)
        add_343: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11263, view_693);  slice_11263 = view_693 = None
        slice_scatter_2046: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11262, add_343, 2, 0, 16);  slice_11262 = add_343 = None
        slice_scatter_2047: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2044, slice_scatter_2046, 1, 2720, 2736);  slice_scatter_2044 = slice_scatter_2046 = None
        slice_11267: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2047, 1, 2720, 2736)
        slice_11268: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11267, 2, 0, 16)
        slice_scatter_2049: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11267, slice_11268, 2, 0, 16);  slice_11267 = slice_11268 = None
        slice_scatter_2050: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2047, slice_scatter_2049, 1, 2720, 2736);  slice_scatter_2047 = slice_scatter_2049 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11287: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2736, 2752)
        slice_11288: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_11287, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_345: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_11288, memory_format = torch.contiguous_format);  slice_11288 = None
        view_694: "f32[32, 16]" = torch.ops.aten.view.default(clone_345, [32, 16]);  clone_345 = None
        mm_342: "f32[32, 8]" = torch.ops.aten.mm.default(view_694, slice_7)
        view_695: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_342, [2, 16, 8]);  mm_342 = None
        slice_11295: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2050, 1, 2736, 2752)
        slice_11296: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11295, 2, 0, 16)
        add_344: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11296, view_695);  slice_11296 = view_695 = None
        slice_scatter_2052: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11295, add_344, 2, 0, 16);  slice_11295 = add_344 = None
        slice_scatter_2053: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2050, slice_scatter_2052, 1, 2736, 2752);  slice_scatter_2050 = slice_scatter_2052 = None
        slice_11300: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2053, 1, 2736, 2752)
        slice_11301: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11300, 2, 0, 16)
        slice_scatter_2055: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11300, slice_11301, 2, 0, 16);  slice_11300 = slice_11301 = None
        slice_scatter_2056: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2053, slice_scatter_2055, 1, 2736, 2752);  slice_scatter_2053 = slice_scatter_2055 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11321: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_11287, 2, 16, 32);  slice_11287 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_346: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_11321, memory_format = torch.contiguous_format);  slice_11321 = None
        view_696: "f32[32, 11]" = torch.ops.aten.view.default(clone_346, [32, 11]);  clone_346 = None
        mm_343: "f32[32, 8]" = torch.ops.aten.mm.default(view_696, slice_37)
        view_697: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_343, [2, 16, 8]);  mm_343 = None
        slice_11328: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2056, 1, 2736, 2752)
        slice_11329: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11328, 2, 0, 16)
        add_345: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11329, view_697);  slice_11329 = view_697 = None
        slice_scatter_2058: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11328, add_345, 2, 0, 16);  slice_11328 = add_345 = None
        slice_scatter_2059: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2056, slice_scatter_2058, 1, 2736, 2752);  slice_scatter_2056 = slice_scatter_2058 = None
        slice_11333: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2059, 1, 2736, 2752)
        slice_11334: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11333, 2, 0, 16)
        slice_scatter_2061: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11333, slice_11334, 2, 0, 16);  slice_11333 = slice_11334 = None
        slice_scatter_2062: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2059, slice_scatter_2061, 1, 2736, 2752);  slice_scatter_2059 = slice_scatter_2061 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11353: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2752, 2768)
        slice_11354: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_11353, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_347: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_11354, memory_format = torch.contiguous_format);  slice_11354 = None
        view_698: "f32[32, 16]" = torch.ops.aten.view.default(clone_347, [32, 16]);  clone_347 = None
        mm_344: "f32[32, 8]" = torch.ops.aten.mm.default(view_698, slice_7)
        view_699: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_344, [2, 16, 8]);  mm_344 = None
        slice_11361: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2062, 1, 2752, 2768)
        slice_11362: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11361, 2, 0, 16)
        add_346: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11362, view_699);  slice_11362 = view_699 = None
        slice_scatter_2064: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11361, add_346, 2, 0, 16);  slice_11361 = add_346 = None
        slice_scatter_2065: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2062, slice_scatter_2064, 1, 2752, 2768);  slice_scatter_2062 = slice_scatter_2064 = None
        slice_11366: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2065, 1, 2752, 2768)
        slice_11367: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11366, 2, 0, 16)
        slice_scatter_2067: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11366, slice_11367, 2, 0, 16);  slice_11366 = slice_11367 = None
        slice_scatter_2068: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2065, slice_scatter_2067, 1, 2752, 2768);  slice_scatter_2065 = slice_scatter_2067 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11387: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_11353, 2, 16, 32);  slice_11353 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_348: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_11387, memory_format = torch.contiguous_format);  slice_11387 = None
        view_700: "f32[32, 11]" = torch.ops.aten.view.default(clone_348, [32, 11]);  clone_348 = None
        mm_345: "f32[32, 8]" = torch.ops.aten.mm.default(view_700, slice_37)
        view_701: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_345, [2, 16, 8]);  mm_345 = None
        slice_11394: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2068, 1, 2752, 2768)
        slice_11395: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11394, 2, 0, 16)
        add_347: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11395, view_701);  slice_11395 = view_701 = None
        slice_scatter_2070: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11394, add_347, 2, 0, 16);  slice_11394 = add_347 = None
        slice_scatter_2071: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2068, slice_scatter_2070, 1, 2752, 2768);  slice_scatter_2068 = slice_scatter_2070 = None
        slice_11399: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2071, 1, 2752, 2768)
        slice_11400: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11399, 2, 0, 16)
        slice_scatter_2073: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11399, slice_11400, 2, 0, 16);  slice_11399 = slice_11400 = None
        slice_scatter_2074: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2071, slice_scatter_2073, 1, 2752, 2768);  slice_scatter_2071 = slice_scatter_2073 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11419: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2768, 2784)
        slice_11420: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_11419, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_349: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_11420, memory_format = torch.contiguous_format);  slice_11420 = None
        view_702: "f32[32, 16]" = torch.ops.aten.view.default(clone_349, [32, 16]);  clone_349 = None
        mm_346: "f32[32, 8]" = torch.ops.aten.mm.default(view_702, slice_7)
        view_703: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_346, [2, 16, 8]);  mm_346 = None
        slice_11427: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2074, 1, 2768, 2784)
        slice_11428: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11427, 2, 0, 16)
        add_348: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11428, view_703);  slice_11428 = view_703 = None
        slice_scatter_2076: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11427, add_348, 2, 0, 16);  slice_11427 = add_348 = None
        slice_scatter_2077: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2074, slice_scatter_2076, 1, 2768, 2784);  slice_scatter_2074 = slice_scatter_2076 = None
        slice_11432: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2077, 1, 2768, 2784)
        slice_11433: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11432, 2, 0, 16)
        slice_scatter_2079: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11432, slice_11433, 2, 0, 16);  slice_11432 = slice_11433 = None
        slice_scatter_2080: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2077, slice_scatter_2079, 1, 2768, 2784);  slice_scatter_2077 = slice_scatter_2079 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11453: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_11419, 2, 16, 32);  slice_11419 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_350: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_11453, memory_format = torch.contiguous_format);  slice_11453 = None
        view_704: "f32[32, 11]" = torch.ops.aten.view.default(clone_350, [32, 11]);  clone_350 = None
        mm_347: "f32[32, 8]" = torch.ops.aten.mm.default(view_704, slice_37)
        view_705: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_347, [2, 16, 8]);  mm_347 = None
        slice_11460: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2080, 1, 2768, 2784)
        slice_11461: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11460, 2, 0, 16)
        add_349: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11461, view_705);  slice_11461 = view_705 = None
        slice_scatter_2082: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11460, add_349, 2, 0, 16);  slice_11460 = add_349 = None
        slice_scatter_2083: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2080, slice_scatter_2082, 1, 2768, 2784);  slice_scatter_2080 = slice_scatter_2082 = None
        slice_11465: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2083, 1, 2768, 2784)
        slice_11466: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11465, 2, 0, 16)
        slice_scatter_2085: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11465, slice_11466, 2, 0, 16);  slice_11465 = slice_11466 = None
        slice_scatter_2086: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2083, slice_scatter_2085, 1, 2768, 2784);  slice_scatter_2083 = slice_scatter_2085 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11485: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2784, 2800)
        slice_11486: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_11485, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_351: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_11486, memory_format = torch.contiguous_format);  slice_11486 = None
        view_706: "f32[32, 16]" = torch.ops.aten.view.default(clone_351, [32, 16]);  clone_351 = None
        mm_348: "f32[32, 8]" = torch.ops.aten.mm.default(view_706, slice_7)
        view_707: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_348, [2, 16, 8]);  mm_348 = None
        slice_11493: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2086, 1, 2784, 2800)
        slice_11494: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11493, 2, 0, 16)
        add_350: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11494, view_707);  slice_11494 = view_707 = None
        slice_scatter_2088: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11493, add_350, 2, 0, 16);  slice_11493 = add_350 = None
        slice_scatter_2089: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2086, slice_scatter_2088, 1, 2784, 2800);  slice_scatter_2086 = slice_scatter_2088 = None
        slice_11498: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2089, 1, 2784, 2800)
        slice_11499: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11498, 2, 0, 16)
        slice_scatter_2091: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11498, slice_11499, 2, 0, 16);  slice_11498 = slice_11499 = None
        slice_scatter_2092: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2089, slice_scatter_2091, 1, 2784, 2800);  slice_scatter_2089 = slice_scatter_2091 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11519: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_11485, 2, 16, 32);  slice_11485 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_352: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_11519, memory_format = torch.contiguous_format);  slice_11519 = None
        view_708: "f32[32, 11]" = torch.ops.aten.view.default(clone_352, [32, 11]);  clone_352 = None
        mm_349: "f32[32, 8]" = torch.ops.aten.mm.default(view_708, slice_37)
        view_709: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_349, [2, 16, 8]);  mm_349 = None
        slice_11526: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2092, 1, 2784, 2800)
        slice_11527: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11526, 2, 0, 16)
        add_351: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11527, view_709);  slice_11527 = view_709 = None
        slice_scatter_2094: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11526, add_351, 2, 0, 16);  slice_11526 = add_351 = None
        slice_scatter_2095: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2092, slice_scatter_2094, 1, 2784, 2800);  slice_scatter_2092 = slice_scatter_2094 = None
        slice_11531: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2095, 1, 2784, 2800)
        slice_11532: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11531, 2, 0, 16)
        slice_scatter_2097: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11531, slice_11532, 2, 0, 16);  slice_11531 = slice_11532 = None
        slice_scatter_2098: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2095, slice_scatter_2097, 1, 2784, 2800);  slice_scatter_2095 = slice_scatter_2097 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11551: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2800, 2816)
        slice_11552: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_11551, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_353: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_11552, memory_format = torch.contiguous_format);  slice_11552 = None
        view_710: "f32[32, 16]" = torch.ops.aten.view.default(clone_353, [32, 16]);  clone_353 = None
        mm_350: "f32[32, 8]" = torch.ops.aten.mm.default(view_710, slice_7)
        view_711: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_350, [2, 16, 8]);  mm_350 = None
        slice_11559: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2098, 1, 2800, 2816)
        slice_11560: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11559, 2, 0, 16)
        add_352: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11560, view_711);  slice_11560 = view_711 = None
        slice_scatter_2100: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11559, add_352, 2, 0, 16);  slice_11559 = add_352 = None
        slice_scatter_2101: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2098, slice_scatter_2100, 1, 2800, 2816);  slice_scatter_2098 = slice_scatter_2100 = None
        slice_11564: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2101, 1, 2800, 2816)
        slice_11565: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11564, 2, 0, 16)
        slice_scatter_2103: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11564, slice_11565, 2, 0, 16);  slice_11564 = slice_11565 = None
        slice_scatter_2104: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2101, slice_scatter_2103, 1, 2800, 2816);  slice_scatter_2101 = slice_scatter_2103 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11585: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_11551, 2, 16, 32);  slice_11551 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_354: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_11585, memory_format = torch.contiguous_format);  slice_11585 = None
        view_712: "f32[32, 11]" = torch.ops.aten.view.default(clone_354, [32, 11]);  clone_354 = None
        mm_351: "f32[32, 8]" = torch.ops.aten.mm.default(view_712, slice_37)
        view_713: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_351, [2, 16, 8]);  mm_351 = None
        slice_11592: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2104, 1, 2800, 2816)
        slice_11593: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11592, 2, 0, 16)
        add_353: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11593, view_713);  slice_11593 = view_713 = None
        slice_scatter_2106: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11592, add_353, 2, 0, 16);  slice_11592 = add_353 = None
        slice_scatter_2107: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2104, slice_scatter_2106, 1, 2800, 2816);  slice_scatter_2104 = slice_scatter_2106 = None
        slice_11597: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2107, 1, 2800, 2816)
        slice_11598: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11597, 2, 0, 16)
        slice_scatter_2109: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11597, slice_11598, 2, 0, 16);  slice_11597 = slice_11598 = None
        slice_scatter_2110: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2107, slice_scatter_2109, 1, 2800, 2816);  slice_scatter_2107 = slice_scatter_2109 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11617: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2816, 2832)
        slice_11618: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_11617, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_355: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_11618, memory_format = torch.contiguous_format);  slice_11618 = None
        view_714: "f32[32, 16]" = torch.ops.aten.view.default(clone_355, [32, 16]);  clone_355 = None
        mm_352: "f32[32, 8]" = torch.ops.aten.mm.default(view_714, slice_7)
        view_715: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_352, [2, 16, 8]);  mm_352 = None
        slice_11625: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2110, 1, 2816, 2832)
        slice_11626: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11625, 2, 0, 16)
        add_354: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11626, view_715);  slice_11626 = view_715 = None
        slice_scatter_2112: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11625, add_354, 2, 0, 16);  slice_11625 = add_354 = None
        slice_scatter_2113: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2110, slice_scatter_2112, 1, 2816, 2832);  slice_scatter_2110 = slice_scatter_2112 = None
        slice_11630: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2113, 1, 2816, 2832)
        slice_11631: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11630, 2, 0, 16)
        slice_scatter_2115: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11630, slice_11631, 2, 0, 16);  slice_11630 = slice_11631 = None
        slice_scatter_2116: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2113, slice_scatter_2115, 1, 2816, 2832);  slice_scatter_2113 = slice_scatter_2115 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11651: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_11617, 2, 16, 32);  slice_11617 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_356: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_11651, memory_format = torch.contiguous_format);  slice_11651 = None
        view_716: "f32[32, 11]" = torch.ops.aten.view.default(clone_356, [32, 11]);  clone_356 = None
        mm_353: "f32[32, 8]" = torch.ops.aten.mm.default(view_716, slice_37)
        view_717: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_353, [2, 16, 8]);  mm_353 = None
        slice_11658: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2116, 1, 2816, 2832)
        slice_11659: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11658, 2, 0, 16)
        add_355: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11659, view_717);  slice_11659 = view_717 = None
        slice_scatter_2118: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11658, add_355, 2, 0, 16);  slice_11658 = add_355 = None
        slice_scatter_2119: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2116, slice_scatter_2118, 1, 2816, 2832);  slice_scatter_2116 = slice_scatter_2118 = None
        slice_11663: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2119, 1, 2816, 2832)
        slice_11664: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11663, 2, 0, 16)
        slice_scatter_2121: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11663, slice_11664, 2, 0, 16);  slice_11663 = slice_11664 = None
        slice_scatter_2122: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2119, slice_scatter_2121, 1, 2816, 2832);  slice_scatter_2119 = slice_scatter_2121 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11683: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2832, 2848)
        slice_11684: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_11683, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_357: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_11684, memory_format = torch.contiguous_format);  slice_11684 = None
        view_718: "f32[32, 16]" = torch.ops.aten.view.default(clone_357, [32, 16]);  clone_357 = None
        mm_354: "f32[32, 8]" = torch.ops.aten.mm.default(view_718, slice_7)
        view_719: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_354, [2, 16, 8]);  mm_354 = None
        slice_11691: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2122, 1, 2832, 2848)
        slice_11692: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11691, 2, 0, 16)
        add_356: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11692, view_719);  slice_11692 = view_719 = None
        slice_scatter_2124: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11691, add_356, 2, 0, 16);  slice_11691 = add_356 = None
        slice_scatter_2125: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2122, slice_scatter_2124, 1, 2832, 2848);  slice_scatter_2122 = slice_scatter_2124 = None
        slice_11696: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2125, 1, 2832, 2848)
        slice_11697: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11696, 2, 0, 16)
        slice_scatter_2127: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11696, slice_11697, 2, 0, 16);  slice_11696 = slice_11697 = None
        slice_scatter_2128: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2125, slice_scatter_2127, 1, 2832, 2848);  slice_scatter_2125 = slice_scatter_2127 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11717: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_11683, 2, 16, 32);  slice_11683 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_358: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_11717, memory_format = torch.contiguous_format);  slice_11717 = None
        view_720: "f32[32, 11]" = torch.ops.aten.view.default(clone_358, [32, 11]);  clone_358 = None
        mm_355: "f32[32, 8]" = torch.ops.aten.mm.default(view_720, slice_37)
        view_721: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_355, [2, 16, 8]);  mm_355 = None
        slice_11724: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2128, 1, 2832, 2848)
        slice_11725: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11724, 2, 0, 16)
        add_357: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11725, view_721);  slice_11725 = view_721 = None
        slice_scatter_2130: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11724, add_357, 2, 0, 16);  slice_11724 = add_357 = None
        slice_scatter_2131: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2128, slice_scatter_2130, 1, 2832, 2848);  slice_scatter_2128 = slice_scatter_2130 = None
        slice_11729: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2131, 1, 2832, 2848)
        slice_11730: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11729, 2, 0, 16)
        slice_scatter_2133: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11729, slice_11730, 2, 0, 16);  slice_11729 = slice_11730 = None
        slice_scatter_2134: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2131, slice_scatter_2133, 1, 2832, 2848);  slice_scatter_2131 = slice_scatter_2133 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11749: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2848, 2864)
        slice_11750: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_11749, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_359: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_11750, memory_format = torch.contiguous_format);  slice_11750 = None
        view_722: "f32[32, 16]" = torch.ops.aten.view.default(clone_359, [32, 16]);  clone_359 = None
        mm_356: "f32[32, 8]" = torch.ops.aten.mm.default(view_722, slice_7)
        view_723: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_356, [2, 16, 8]);  mm_356 = None
        slice_11757: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2134, 1, 2848, 2864)
        slice_11758: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11757, 2, 0, 16)
        add_358: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11758, view_723);  slice_11758 = view_723 = None
        slice_scatter_2136: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11757, add_358, 2, 0, 16);  slice_11757 = add_358 = None
        slice_scatter_2137: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2134, slice_scatter_2136, 1, 2848, 2864);  slice_scatter_2134 = slice_scatter_2136 = None
        slice_11762: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2137, 1, 2848, 2864)
        slice_11763: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11762, 2, 0, 16)
        slice_scatter_2139: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11762, slice_11763, 2, 0, 16);  slice_11762 = slice_11763 = None
        slice_scatter_2140: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2137, slice_scatter_2139, 1, 2848, 2864);  slice_scatter_2137 = slice_scatter_2139 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11783: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_11749, 2, 16, 32);  slice_11749 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_360: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_11783, memory_format = torch.contiguous_format);  slice_11783 = None
        view_724: "f32[32, 11]" = torch.ops.aten.view.default(clone_360, [32, 11]);  clone_360 = None
        mm_357: "f32[32, 8]" = torch.ops.aten.mm.default(view_724, slice_37)
        view_725: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_357, [2, 16, 8]);  mm_357 = None
        slice_11790: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2140, 1, 2848, 2864)
        slice_11791: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11790, 2, 0, 16)
        add_359: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11791, view_725);  slice_11791 = view_725 = None
        slice_scatter_2142: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11790, add_359, 2, 0, 16);  slice_11790 = add_359 = None
        slice_scatter_2143: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2140, slice_scatter_2142, 1, 2848, 2864);  slice_scatter_2140 = slice_scatter_2142 = None
        slice_11795: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2143, 1, 2848, 2864)
        slice_11796: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11795, 2, 0, 16)
        slice_scatter_2145: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11795, slice_11796, 2, 0, 16);  slice_11795 = slice_11796 = None
        slice_scatter_2146: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2143, slice_scatter_2145, 1, 2848, 2864);  slice_scatter_2143 = slice_scatter_2145 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11815: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2864, 2880)
        slice_11816: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_11815, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_361: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_11816, memory_format = torch.contiguous_format);  slice_11816 = None
        view_726: "f32[32, 16]" = torch.ops.aten.view.default(clone_361, [32, 16]);  clone_361 = None
        mm_358: "f32[32, 8]" = torch.ops.aten.mm.default(view_726, slice_7)
        view_727: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_358, [2, 16, 8]);  mm_358 = None
        slice_11823: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2146, 1, 2864, 2880)
        slice_11824: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11823, 2, 0, 16)
        add_360: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11824, view_727);  slice_11824 = view_727 = None
        slice_scatter_2148: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11823, add_360, 2, 0, 16);  slice_11823 = add_360 = None
        slice_scatter_2149: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2146, slice_scatter_2148, 1, 2864, 2880);  slice_scatter_2146 = slice_scatter_2148 = None
        slice_11828: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2149, 1, 2864, 2880)
        slice_11829: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11828, 2, 0, 16)
        slice_scatter_2151: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11828, slice_11829, 2, 0, 16);  slice_11828 = slice_11829 = None
        slice_scatter_2152: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2149, slice_scatter_2151, 1, 2864, 2880);  slice_scatter_2149 = slice_scatter_2151 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11849: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_11815, 2, 16, 32);  slice_11815 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_362: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_11849, memory_format = torch.contiguous_format);  slice_11849 = None
        view_728: "f32[32, 11]" = torch.ops.aten.view.default(clone_362, [32, 11]);  clone_362 = None
        mm_359: "f32[32, 8]" = torch.ops.aten.mm.default(view_728, slice_37)
        view_729: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_359, [2, 16, 8]);  mm_359 = None
        slice_11856: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2152, 1, 2864, 2880)
        slice_11857: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11856, 2, 0, 16)
        add_361: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11857, view_729);  slice_11857 = view_729 = None
        slice_scatter_2154: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11856, add_361, 2, 0, 16);  slice_11856 = add_361 = None
        slice_scatter_2155: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2152, slice_scatter_2154, 1, 2864, 2880);  slice_scatter_2152 = slice_scatter_2154 = None
        slice_11861: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2155, 1, 2864, 2880)
        slice_11862: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11861, 2, 0, 16)
        slice_scatter_2157: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11861, slice_11862, 2, 0, 16);  slice_11861 = slice_11862 = None
        slice_scatter_2158: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2155, slice_scatter_2157, 1, 2864, 2880);  slice_scatter_2155 = slice_scatter_2157 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11881: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2880, 2896)
        slice_11882: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_11881, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_363: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_11882, memory_format = torch.contiguous_format);  slice_11882 = None
        view_730: "f32[32, 16]" = torch.ops.aten.view.default(clone_363, [32, 16]);  clone_363 = None
        mm_360: "f32[32, 8]" = torch.ops.aten.mm.default(view_730, slice_7)
        view_731: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_360, [2, 16, 8]);  mm_360 = None
        slice_11889: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2158, 1, 2880, 2896)
        slice_11890: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11889, 2, 0, 16)
        add_362: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11890, view_731);  slice_11890 = view_731 = None
        slice_scatter_2160: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11889, add_362, 2, 0, 16);  slice_11889 = add_362 = None
        slice_scatter_2161: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2158, slice_scatter_2160, 1, 2880, 2896);  slice_scatter_2158 = slice_scatter_2160 = None
        slice_11894: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2161, 1, 2880, 2896)
        slice_11895: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11894, 2, 0, 16)
        slice_scatter_2163: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11894, slice_11895, 2, 0, 16);  slice_11894 = slice_11895 = None
        slice_scatter_2164: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2161, slice_scatter_2163, 1, 2880, 2896);  slice_scatter_2161 = slice_scatter_2163 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11915: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_11881, 2, 16, 32);  slice_11881 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_364: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_11915, memory_format = torch.contiguous_format);  slice_11915 = None
        view_732: "f32[32, 11]" = torch.ops.aten.view.default(clone_364, [32, 11]);  clone_364 = None
        mm_361: "f32[32, 8]" = torch.ops.aten.mm.default(view_732, slice_37)
        view_733: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_361, [2, 16, 8]);  mm_361 = None
        slice_11922: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2164, 1, 2880, 2896)
        slice_11923: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11922, 2, 0, 16)
        add_363: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11923, view_733);  slice_11923 = view_733 = None
        slice_scatter_2166: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11922, add_363, 2, 0, 16);  slice_11922 = add_363 = None
        slice_scatter_2167: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2164, slice_scatter_2166, 1, 2880, 2896);  slice_scatter_2164 = slice_scatter_2166 = None
        slice_11927: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2167, 1, 2880, 2896)
        slice_11928: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11927, 2, 0, 16)
        slice_scatter_2169: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11927, slice_11928, 2, 0, 16);  slice_11927 = slice_11928 = None
        slice_scatter_2170: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2167, slice_scatter_2169, 1, 2880, 2896);  slice_scatter_2167 = slice_scatter_2169 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11947: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2896, 2912)
        slice_11948: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_11947, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_365: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_11948, memory_format = torch.contiguous_format);  slice_11948 = None
        view_734: "f32[32, 16]" = torch.ops.aten.view.default(clone_365, [32, 16]);  clone_365 = None
        mm_362: "f32[32, 8]" = torch.ops.aten.mm.default(view_734, slice_7)
        view_735: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_362, [2, 16, 8]);  mm_362 = None
        slice_11955: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2170, 1, 2896, 2912)
        slice_11956: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11955, 2, 0, 16)
        add_364: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11956, view_735);  slice_11956 = view_735 = None
        slice_scatter_2172: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11955, add_364, 2, 0, 16);  slice_11955 = add_364 = None
        slice_scatter_2173: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2170, slice_scatter_2172, 1, 2896, 2912);  slice_scatter_2170 = slice_scatter_2172 = None
        slice_11960: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2173, 1, 2896, 2912)
        slice_11961: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11960, 2, 0, 16)
        slice_scatter_2175: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11960, slice_11961, 2, 0, 16);  slice_11960 = slice_11961 = None
        slice_scatter_2176: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2173, slice_scatter_2175, 1, 2896, 2912);  slice_scatter_2173 = slice_scatter_2175 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_11981: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_11947, 2, 16, 32);  slice_11947 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_366: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_11981, memory_format = torch.contiguous_format);  slice_11981 = None
        view_736: "f32[32, 11]" = torch.ops.aten.view.default(clone_366, [32, 11]);  clone_366 = None
        mm_363: "f32[32, 8]" = torch.ops.aten.mm.default(view_736, slice_37)
        view_737: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_363, [2, 16, 8]);  mm_363 = None
        slice_11988: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2176, 1, 2896, 2912)
        slice_11989: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11988, 2, 0, 16)
        add_365: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_11989, view_737);  slice_11989 = view_737 = None
        slice_scatter_2178: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11988, add_365, 2, 0, 16);  slice_11988 = add_365 = None
        slice_scatter_2179: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2176, slice_scatter_2178, 1, 2896, 2912);  slice_scatter_2176 = slice_scatter_2178 = None
        slice_11993: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2179, 1, 2896, 2912)
        slice_11994: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_11993, 2, 0, 16)
        slice_scatter_2181: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_11993, slice_11994, 2, 0, 16);  slice_11993 = slice_11994 = None
        slice_scatter_2182: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2179, slice_scatter_2181, 1, 2896, 2912);  slice_scatter_2179 = slice_scatter_2181 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12013: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2912, 2928)
        slice_12014: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_12013, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_367: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_12014, memory_format = torch.contiguous_format);  slice_12014 = None
        view_738: "f32[32, 16]" = torch.ops.aten.view.default(clone_367, [32, 16]);  clone_367 = None
        mm_364: "f32[32, 8]" = torch.ops.aten.mm.default(view_738, slice_7)
        view_739: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_364, [2, 16, 8]);  mm_364 = None
        slice_12021: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2182, 1, 2912, 2928)
        slice_12022: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12021, 2, 0, 16)
        add_366: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12022, view_739);  slice_12022 = view_739 = None
        slice_scatter_2184: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12021, add_366, 2, 0, 16);  slice_12021 = add_366 = None
        slice_scatter_2185: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2182, slice_scatter_2184, 1, 2912, 2928);  slice_scatter_2182 = slice_scatter_2184 = None
        slice_12026: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2185, 1, 2912, 2928)
        slice_12027: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12026, 2, 0, 16)
        slice_scatter_2187: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12026, slice_12027, 2, 0, 16);  slice_12026 = slice_12027 = None
        slice_scatter_2188: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2185, slice_scatter_2187, 1, 2912, 2928);  slice_scatter_2185 = slice_scatter_2187 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12047: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_12013, 2, 16, 32);  slice_12013 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_368: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_12047, memory_format = torch.contiguous_format);  slice_12047 = None
        view_740: "f32[32, 11]" = torch.ops.aten.view.default(clone_368, [32, 11]);  clone_368 = None
        mm_365: "f32[32, 8]" = torch.ops.aten.mm.default(view_740, slice_37)
        view_741: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_365, [2, 16, 8]);  mm_365 = None
        slice_12054: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2188, 1, 2912, 2928)
        slice_12055: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12054, 2, 0, 16)
        add_367: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12055, view_741);  slice_12055 = view_741 = None
        slice_scatter_2190: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12054, add_367, 2, 0, 16);  slice_12054 = add_367 = None
        slice_scatter_2191: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2188, slice_scatter_2190, 1, 2912, 2928);  slice_scatter_2188 = slice_scatter_2190 = None
        slice_12059: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2191, 1, 2912, 2928)
        slice_12060: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12059, 2, 0, 16)
        slice_scatter_2193: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12059, slice_12060, 2, 0, 16);  slice_12059 = slice_12060 = None
        slice_scatter_2194: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2191, slice_scatter_2193, 1, 2912, 2928);  slice_scatter_2191 = slice_scatter_2193 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12079: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2928, 2944)
        slice_12080: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_12079, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_369: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_12080, memory_format = torch.contiguous_format);  slice_12080 = None
        view_742: "f32[32, 16]" = torch.ops.aten.view.default(clone_369, [32, 16]);  clone_369 = None
        mm_366: "f32[32, 8]" = torch.ops.aten.mm.default(view_742, slice_7)
        view_743: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_366, [2, 16, 8]);  mm_366 = None
        slice_12087: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2194, 1, 2928, 2944)
        slice_12088: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12087, 2, 0, 16)
        add_368: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12088, view_743);  slice_12088 = view_743 = None
        slice_scatter_2196: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12087, add_368, 2, 0, 16);  slice_12087 = add_368 = None
        slice_scatter_2197: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2194, slice_scatter_2196, 1, 2928, 2944);  slice_scatter_2194 = slice_scatter_2196 = None
        slice_12092: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2197, 1, 2928, 2944)
        slice_12093: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12092, 2, 0, 16)
        slice_scatter_2199: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12092, slice_12093, 2, 0, 16);  slice_12092 = slice_12093 = None
        slice_scatter_2200: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2197, slice_scatter_2199, 1, 2928, 2944);  slice_scatter_2197 = slice_scatter_2199 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12113: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_12079, 2, 16, 32);  slice_12079 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_370: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_12113, memory_format = torch.contiguous_format);  slice_12113 = None
        view_744: "f32[32, 11]" = torch.ops.aten.view.default(clone_370, [32, 11]);  clone_370 = None
        mm_367: "f32[32, 8]" = torch.ops.aten.mm.default(view_744, slice_37)
        view_745: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_367, [2, 16, 8]);  mm_367 = None
        slice_12120: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2200, 1, 2928, 2944)
        slice_12121: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12120, 2, 0, 16)
        add_369: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12121, view_745);  slice_12121 = view_745 = None
        slice_scatter_2202: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12120, add_369, 2, 0, 16);  slice_12120 = add_369 = None
        slice_scatter_2203: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2200, slice_scatter_2202, 1, 2928, 2944);  slice_scatter_2200 = slice_scatter_2202 = None
        slice_12125: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2203, 1, 2928, 2944)
        slice_12126: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12125, 2, 0, 16)
        slice_scatter_2205: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12125, slice_12126, 2, 0, 16);  slice_12125 = slice_12126 = None
        slice_scatter_2206: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2203, slice_scatter_2205, 1, 2928, 2944);  slice_scatter_2203 = slice_scatter_2205 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12145: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2944, 2960)
        slice_12146: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_12145, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_371: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_12146, memory_format = torch.contiguous_format);  slice_12146 = None
        view_746: "f32[32, 16]" = torch.ops.aten.view.default(clone_371, [32, 16]);  clone_371 = None
        mm_368: "f32[32, 8]" = torch.ops.aten.mm.default(view_746, slice_7)
        view_747: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_368, [2, 16, 8]);  mm_368 = None
        slice_12153: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2206, 1, 2944, 2960)
        slice_12154: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12153, 2, 0, 16)
        add_370: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12154, view_747);  slice_12154 = view_747 = None
        slice_scatter_2208: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12153, add_370, 2, 0, 16);  slice_12153 = add_370 = None
        slice_scatter_2209: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2206, slice_scatter_2208, 1, 2944, 2960);  slice_scatter_2206 = slice_scatter_2208 = None
        slice_12158: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2209, 1, 2944, 2960)
        slice_12159: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12158, 2, 0, 16)
        slice_scatter_2211: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12158, slice_12159, 2, 0, 16);  slice_12158 = slice_12159 = None
        slice_scatter_2212: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2209, slice_scatter_2211, 1, 2944, 2960);  slice_scatter_2209 = slice_scatter_2211 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12179: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_12145, 2, 16, 32);  slice_12145 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_372: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_12179, memory_format = torch.contiguous_format);  slice_12179 = None
        view_748: "f32[32, 11]" = torch.ops.aten.view.default(clone_372, [32, 11]);  clone_372 = None
        mm_369: "f32[32, 8]" = torch.ops.aten.mm.default(view_748, slice_37)
        view_749: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_369, [2, 16, 8]);  mm_369 = None
        slice_12186: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2212, 1, 2944, 2960)
        slice_12187: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12186, 2, 0, 16)
        add_371: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12187, view_749);  slice_12187 = view_749 = None
        slice_scatter_2214: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12186, add_371, 2, 0, 16);  slice_12186 = add_371 = None
        slice_scatter_2215: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2212, slice_scatter_2214, 1, 2944, 2960);  slice_scatter_2212 = slice_scatter_2214 = None
        slice_12191: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2215, 1, 2944, 2960)
        slice_12192: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12191, 2, 0, 16)
        slice_scatter_2217: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12191, slice_12192, 2, 0, 16);  slice_12191 = slice_12192 = None
        slice_scatter_2218: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2215, slice_scatter_2217, 1, 2944, 2960);  slice_scatter_2215 = slice_scatter_2217 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12211: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2960, 2976)
        slice_12212: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_12211, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_373: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_12212, memory_format = torch.contiguous_format);  slice_12212 = None
        view_750: "f32[32, 16]" = torch.ops.aten.view.default(clone_373, [32, 16]);  clone_373 = None
        mm_370: "f32[32, 8]" = torch.ops.aten.mm.default(view_750, slice_7)
        view_751: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_370, [2, 16, 8]);  mm_370 = None
        slice_12219: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2218, 1, 2960, 2976)
        slice_12220: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12219, 2, 0, 16)
        add_372: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12220, view_751);  slice_12220 = view_751 = None
        slice_scatter_2220: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12219, add_372, 2, 0, 16);  slice_12219 = add_372 = None
        slice_scatter_2221: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2218, slice_scatter_2220, 1, 2960, 2976);  slice_scatter_2218 = slice_scatter_2220 = None
        slice_12224: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2221, 1, 2960, 2976)
        slice_12225: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12224, 2, 0, 16)
        slice_scatter_2223: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12224, slice_12225, 2, 0, 16);  slice_12224 = slice_12225 = None
        slice_scatter_2224: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2221, slice_scatter_2223, 1, 2960, 2976);  slice_scatter_2221 = slice_scatter_2223 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12245: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_12211, 2, 16, 32);  slice_12211 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_374: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_12245, memory_format = torch.contiguous_format);  slice_12245 = None
        view_752: "f32[32, 11]" = torch.ops.aten.view.default(clone_374, [32, 11]);  clone_374 = None
        mm_371: "f32[32, 8]" = torch.ops.aten.mm.default(view_752, slice_37)
        view_753: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_371, [2, 16, 8]);  mm_371 = None
        slice_12252: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2224, 1, 2960, 2976)
        slice_12253: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12252, 2, 0, 16)
        add_373: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12253, view_753);  slice_12253 = view_753 = None
        slice_scatter_2226: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12252, add_373, 2, 0, 16);  slice_12252 = add_373 = None
        slice_scatter_2227: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2224, slice_scatter_2226, 1, 2960, 2976);  slice_scatter_2224 = slice_scatter_2226 = None
        slice_12257: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2227, 1, 2960, 2976)
        slice_12258: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12257, 2, 0, 16)
        slice_scatter_2229: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12257, slice_12258, 2, 0, 16);  slice_12257 = slice_12258 = None
        slice_scatter_2230: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2227, slice_scatter_2229, 1, 2960, 2976);  slice_scatter_2227 = slice_scatter_2229 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12277: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2976, 2992)
        slice_12278: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_12277, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_375: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_12278, memory_format = torch.contiguous_format);  slice_12278 = None
        view_754: "f32[32, 16]" = torch.ops.aten.view.default(clone_375, [32, 16]);  clone_375 = None
        mm_372: "f32[32, 8]" = torch.ops.aten.mm.default(view_754, slice_7)
        view_755: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_372, [2, 16, 8]);  mm_372 = None
        slice_12285: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2230, 1, 2976, 2992)
        slice_12286: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12285, 2, 0, 16)
        add_374: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12286, view_755);  slice_12286 = view_755 = None
        slice_scatter_2232: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12285, add_374, 2, 0, 16);  slice_12285 = add_374 = None
        slice_scatter_2233: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2230, slice_scatter_2232, 1, 2976, 2992);  slice_scatter_2230 = slice_scatter_2232 = None
        slice_12290: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2233, 1, 2976, 2992)
        slice_12291: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12290, 2, 0, 16)
        slice_scatter_2235: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12290, slice_12291, 2, 0, 16);  slice_12290 = slice_12291 = None
        slice_scatter_2236: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2233, slice_scatter_2235, 1, 2976, 2992);  slice_scatter_2233 = slice_scatter_2235 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12311: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_12277, 2, 16, 32);  slice_12277 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_376: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_12311, memory_format = torch.contiguous_format);  slice_12311 = None
        view_756: "f32[32, 11]" = torch.ops.aten.view.default(clone_376, [32, 11]);  clone_376 = None
        mm_373: "f32[32, 8]" = torch.ops.aten.mm.default(view_756, slice_37)
        view_757: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_373, [2, 16, 8]);  mm_373 = None
        slice_12318: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2236, 1, 2976, 2992)
        slice_12319: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12318, 2, 0, 16)
        add_375: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12319, view_757);  slice_12319 = view_757 = None
        slice_scatter_2238: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12318, add_375, 2, 0, 16);  slice_12318 = add_375 = None
        slice_scatter_2239: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2236, slice_scatter_2238, 1, 2976, 2992);  slice_scatter_2236 = slice_scatter_2238 = None
        slice_12323: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2239, 1, 2976, 2992)
        slice_12324: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12323, 2, 0, 16)
        slice_scatter_2241: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12323, slice_12324, 2, 0, 16);  slice_12323 = slice_12324 = None
        slice_scatter_2242: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2239, slice_scatter_2241, 1, 2976, 2992);  slice_scatter_2239 = slice_scatter_2241 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12343: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 2992, 3008)
        slice_12344: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_12343, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_377: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_12344, memory_format = torch.contiguous_format);  slice_12344 = None
        view_758: "f32[32, 16]" = torch.ops.aten.view.default(clone_377, [32, 16]);  clone_377 = None
        mm_374: "f32[32, 8]" = torch.ops.aten.mm.default(view_758, slice_7)
        view_759: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_374, [2, 16, 8]);  mm_374 = None
        slice_12351: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2242, 1, 2992, 3008)
        slice_12352: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12351, 2, 0, 16)
        add_376: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12352, view_759);  slice_12352 = view_759 = None
        slice_scatter_2244: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12351, add_376, 2, 0, 16);  slice_12351 = add_376 = None
        slice_scatter_2245: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2242, slice_scatter_2244, 1, 2992, 3008);  slice_scatter_2242 = slice_scatter_2244 = None
        slice_12356: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2245, 1, 2992, 3008)
        slice_12357: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12356, 2, 0, 16)
        slice_scatter_2247: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12356, slice_12357, 2, 0, 16);  slice_12356 = slice_12357 = None
        slice_scatter_2248: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2245, slice_scatter_2247, 1, 2992, 3008);  slice_scatter_2245 = slice_scatter_2247 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12377: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_12343, 2, 16, 32);  slice_12343 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_378: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_12377, memory_format = torch.contiguous_format);  slice_12377 = None
        view_760: "f32[32, 11]" = torch.ops.aten.view.default(clone_378, [32, 11]);  clone_378 = None
        mm_375: "f32[32, 8]" = torch.ops.aten.mm.default(view_760, slice_37)
        view_761: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_375, [2, 16, 8]);  mm_375 = None
        slice_12384: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2248, 1, 2992, 3008)
        slice_12385: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12384, 2, 0, 16)
        add_377: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12385, view_761);  slice_12385 = view_761 = None
        slice_scatter_2250: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12384, add_377, 2, 0, 16);  slice_12384 = add_377 = None
        slice_scatter_2251: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2248, slice_scatter_2250, 1, 2992, 3008);  slice_scatter_2248 = slice_scatter_2250 = None
        slice_12389: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2251, 1, 2992, 3008)
        slice_12390: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12389, 2, 0, 16)
        slice_scatter_2253: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12389, slice_12390, 2, 0, 16);  slice_12389 = slice_12390 = None
        slice_scatter_2254: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2251, slice_scatter_2253, 1, 2992, 3008);  slice_scatter_2251 = slice_scatter_2253 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12409: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3008, 3024)
        slice_12410: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_12409, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_379: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_12410, memory_format = torch.contiguous_format);  slice_12410 = None
        view_762: "f32[32, 16]" = torch.ops.aten.view.default(clone_379, [32, 16]);  clone_379 = None
        mm_376: "f32[32, 8]" = torch.ops.aten.mm.default(view_762, slice_7)
        view_763: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_376, [2, 16, 8]);  mm_376 = None
        slice_12417: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2254, 1, 3008, 3024)
        slice_12418: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12417, 2, 0, 16)
        add_378: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12418, view_763);  slice_12418 = view_763 = None
        slice_scatter_2256: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12417, add_378, 2, 0, 16);  slice_12417 = add_378 = None
        slice_scatter_2257: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2254, slice_scatter_2256, 1, 3008, 3024);  slice_scatter_2254 = slice_scatter_2256 = None
        slice_12422: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2257, 1, 3008, 3024)
        slice_12423: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12422, 2, 0, 16)
        slice_scatter_2259: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12422, slice_12423, 2, 0, 16);  slice_12422 = slice_12423 = None
        slice_scatter_2260: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2257, slice_scatter_2259, 1, 3008, 3024);  slice_scatter_2257 = slice_scatter_2259 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12443: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_12409, 2, 16, 32);  slice_12409 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_380: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_12443, memory_format = torch.contiguous_format);  slice_12443 = None
        view_764: "f32[32, 11]" = torch.ops.aten.view.default(clone_380, [32, 11]);  clone_380 = None
        mm_377: "f32[32, 8]" = torch.ops.aten.mm.default(view_764, slice_37)
        view_765: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_377, [2, 16, 8]);  mm_377 = None
        slice_12450: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2260, 1, 3008, 3024)
        slice_12451: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12450, 2, 0, 16)
        add_379: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12451, view_765);  slice_12451 = view_765 = None
        slice_scatter_2262: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12450, add_379, 2, 0, 16);  slice_12450 = add_379 = None
        slice_scatter_2263: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2260, slice_scatter_2262, 1, 3008, 3024);  slice_scatter_2260 = slice_scatter_2262 = None
        slice_12455: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2263, 1, 3008, 3024)
        slice_12456: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12455, 2, 0, 16)
        slice_scatter_2265: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12455, slice_12456, 2, 0, 16);  slice_12455 = slice_12456 = None
        slice_scatter_2266: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2263, slice_scatter_2265, 1, 3008, 3024);  slice_scatter_2263 = slice_scatter_2265 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12475: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3024, 3040)
        slice_12476: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_12475, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_381: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_12476, memory_format = torch.contiguous_format);  slice_12476 = None
        view_766: "f32[32, 16]" = torch.ops.aten.view.default(clone_381, [32, 16]);  clone_381 = None
        mm_378: "f32[32, 8]" = torch.ops.aten.mm.default(view_766, slice_7)
        view_767: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_378, [2, 16, 8]);  mm_378 = None
        slice_12483: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2266, 1, 3024, 3040)
        slice_12484: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12483, 2, 0, 16)
        add_380: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12484, view_767);  slice_12484 = view_767 = None
        slice_scatter_2268: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12483, add_380, 2, 0, 16);  slice_12483 = add_380 = None
        slice_scatter_2269: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2266, slice_scatter_2268, 1, 3024, 3040);  slice_scatter_2266 = slice_scatter_2268 = None
        slice_12488: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2269, 1, 3024, 3040)
        slice_12489: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12488, 2, 0, 16)
        slice_scatter_2271: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12488, slice_12489, 2, 0, 16);  slice_12488 = slice_12489 = None
        slice_scatter_2272: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2269, slice_scatter_2271, 1, 3024, 3040);  slice_scatter_2269 = slice_scatter_2271 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12509: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_12475, 2, 16, 32);  slice_12475 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_382: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_12509, memory_format = torch.contiguous_format);  slice_12509 = None
        view_768: "f32[32, 11]" = torch.ops.aten.view.default(clone_382, [32, 11]);  clone_382 = None
        mm_379: "f32[32, 8]" = torch.ops.aten.mm.default(view_768, slice_37)
        view_769: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_379, [2, 16, 8]);  mm_379 = None
        slice_12516: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2272, 1, 3024, 3040)
        slice_12517: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12516, 2, 0, 16)
        add_381: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12517, view_769);  slice_12517 = view_769 = None
        slice_scatter_2274: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12516, add_381, 2, 0, 16);  slice_12516 = add_381 = None
        slice_scatter_2275: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2272, slice_scatter_2274, 1, 3024, 3040);  slice_scatter_2272 = slice_scatter_2274 = None
        slice_12521: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2275, 1, 3024, 3040)
        slice_12522: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12521, 2, 0, 16)
        slice_scatter_2277: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12521, slice_12522, 2, 0, 16);  slice_12521 = slice_12522 = None
        slice_scatter_2278: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2275, slice_scatter_2277, 1, 3024, 3040);  slice_scatter_2275 = slice_scatter_2277 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12541: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3040, 3056)
        slice_12542: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_12541, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_383: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_12542, memory_format = torch.contiguous_format);  slice_12542 = None
        view_770: "f32[32, 16]" = torch.ops.aten.view.default(clone_383, [32, 16]);  clone_383 = None
        mm_380: "f32[32, 8]" = torch.ops.aten.mm.default(view_770, slice_7)
        view_771: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_380, [2, 16, 8]);  mm_380 = None
        slice_12549: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2278, 1, 3040, 3056)
        slice_12550: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12549, 2, 0, 16)
        add_382: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12550, view_771);  slice_12550 = view_771 = None
        slice_scatter_2280: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12549, add_382, 2, 0, 16);  slice_12549 = add_382 = None
        slice_scatter_2281: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2278, slice_scatter_2280, 1, 3040, 3056);  slice_scatter_2278 = slice_scatter_2280 = None
        slice_12554: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2281, 1, 3040, 3056)
        slice_12555: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12554, 2, 0, 16)
        slice_scatter_2283: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12554, slice_12555, 2, 0, 16);  slice_12554 = slice_12555 = None
        slice_scatter_2284: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2281, slice_scatter_2283, 1, 3040, 3056);  slice_scatter_2281 = slice_scatter_2283 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12575: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_12541, 2, 16, 32);  slice_12541 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_384: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_12575, memory_format = torch.contiguous_format);  slice_12575 = None
        view_772: "f32[32, 11]" = torch.ops.aten.view.default(clone_384, [32, 11]);  clone_384 = None
        mm_381: "f32[32, 8]" = torch.ops.aten.mm.default(view_772, slice_37)
        view_773: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_381, [2, 16, 8]);  mm_381 = None
        slice_12582: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2284, 1, 3040, 3056)
        slice_12583: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12582, 2, 0, 16)
        add_383: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12583, view_773);  slice_12583 = view_773 = None
        slice_scatter_2286: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12582, add_383, 2, 0, 16);  slice_12582 = add_383 = None
        slice_scatter_2287: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2284, slice_scatter_2286, 1, 3040, 3056);  slice_scatter_2284 = slice_scatter_2286 = None
        slice_12587: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2287, 1, 3040, 3056)
        slice_12588: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12587, 2, 0, 16)
        slice_scatter_2289: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12587, slice_12588, 2, 0, 16);  slice_12587 = slice_12588 = None
        slice_scatter_2290: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2287, slice_scatter_2289, 1, 3040, 3056);  slice_scatter_2287 = slice_scatter_2289 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12607: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3056, 3072)
        slice_12608: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_12607, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_385: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_12608, memory_format = torch.contiguous_format);  slice_12608 = None
        view_774: "f32[32, 16]" = torch.ops.aten.view.default(clone_385, [32, 16]);  clone_385 = None
        mm_382: "f32[32, 8]" = torch.ops.aten.mm.default(view_774, slice_7)
        view_775: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_382, [2, 16, 8]);  mm_382 = None
        slice_12615: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2290, 1, 3056, 3072)
        slice_12616: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12615, 2, 0, 16)
        add_384: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12616, view_775);  slice_12616 = view_775 = None
        slice_scatter_2292: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12615, add_384, 2, 0, 16);  slice_12615 = add_384 = None
        slice_scatter_2293: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2290, slice_scatter_2292, 1, 3056, 3072);  slice_scatter_2290 = slice_scatter_2292 = None
        slice_12620: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2293, 1, 3056, 3072)
        slice_12621: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12620, 2, 0, 16)
        slice_scatter_2295: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12620, slice_12621, 2, 0, 16);  slice_12620 = slice_12621 = None
        slice_scatter_2296: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2293, slice_scatter_2295, 1, 3056, 3072);  slice_scatter_2293 = slice_scatter_2295 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12641: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_12607, 2, 16, 32);  slice_12607 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_386: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_12641, memory_format = torch.contiguous_format);  slice_12641 = None
        view_776: "f32[32, 11]" = torch.ops.aten.view.default(clone_386, [32, 11]);  clone_386 = None
        mm_383: "f32[32, 8]" = torch.ops.aten.mm.default(view_776, slice_37)
        view_777: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_383, [2, 16, 8]);  mm_383 = None
        slice_12648: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2296, 1, 3056, 3072)
        slice_12649: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12648, 2, 0, 16)
        add_385: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12649, view_777);  slice_12649 = view_777 = None
        slice_scatter_2298: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12648, add_385, 2, 0, 16);  slice_12648 = add_385 = None
        slice_scatter_2299: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2296, slice_scatter_2298, 1, 3056, 3072);  slice_scatter_2296 = slice_scatter_2298 = None
        slice_12653: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2299, 1, 3056, 3072)
        slice_12654: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12653, 2, 0, 16)
        slice_scatter_2301: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12653, slice_12654, 2, 0, 16);  slice_12653 = slice_12654 = None
        slice_scatter_2302: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2299, slice_scatter_2301, 1, 3056, 3072);  slice_scatter_2299 = slice_scatter_2301 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12673: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3072, 3088)
        slice_12674: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_12673, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_387: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_12674, memory_format = torch.contiguous_format);  slice_12674 = None
        view_778: "f32[32, 16]" = torch.ops.aten.view.default(clone_387, [32, 16]);  clone_387 = None
        mm_384: "f32[32, 8]" = torch.ops.aten.mm.default(view_778, slice_7)
        view_779: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_384, [2, 16, 8]);  mm_384 = None
        slice_12681: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2302, 1, 3072, 3088)
        slice_12682: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12681, 2, 0, 16)
        add_386: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12682, view_779);  slice_12682 = view_779 = None
        slice_scatter_2304: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12681, add_386, 2, 0, 16);  slice_12681 = add_386 = None
        slice_scatter_2305: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2302, slice_scatter_2304, 1, 3072, 3088);  slice_scatter_2302 = slice_scatter_2304 = None
        slice_12686: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2305, 1, 3072, 3088)
        slice_12687: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12686, 2, 0, 16)
        slice_scatter_2307: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12686, slice_12687, 2, 0, 16);  slice_12686 = slice_12687 = None
        slice_scatter_2308: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2305, slice_scatter_2307, 1, 3072, 3088);  slice_scatter_2305 = slice_scatter_2307 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12707: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_12673, 2, 16, 32);  slice_12673 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_388: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_12707, memory_format = torch.contiguous_format);  slice_12707 = None
        view_780: "f32[32, 11]" = torch.ops.aten.view.default(clone_388, [32, 11]);  clone_388 = None
        mm_385: "f32[32, 8]" = torch.ops.aten.mm.default(view_780, slice_37)
        view_781: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_385, [2, 16, 8]);  mm_385 = None
        slice_12714: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2308, 1, 3072, 3088)
        slice_12715: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12714, 2, 0, 16)
        add_387: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12715, view_781);  slice_12715 = view_781 = None
        slice_scatter_2310: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12714, add_387, 2, 0, 16);  slice_12714 = add_387 = None
        slice_scatter_2311: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2308, slice_scatter_2310, 1, 3072, 3088);  slice_scatter_2308 = slice_scatter_2310 = None
        slice_12719: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2311, 1, 3072, 3088)
        slice_12720: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12719, 2, 0, 16)
        slice_scatter_2313: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12719, slice_12720, 2, 0, 16);  slice_12719 = slice_12720 = None
        slice_scatter_2314: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2311, slice_scatter_2313, 1, 3072, 3088);  slice_scatter_2311 = slice_scatter_2313 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12739: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3088, 3104)
        slice_12740: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_12739, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_389: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_12740, memory_format = torch.contiguous_format);  slice_12740 = None
        view_782: "f32[32, 16]" = torch.ops.aten.view.default(clone_389, [32, 16]);  clone_389 = None
        mm_386: "f32[32, 8]" = torch.ops.aten.mm.default(view_782, slice_7)
        view_783: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_386, [2, 16, 8]);  mm_386 = None
        slice_12747: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2314, 1, 3088, 3104)
        slice_12748: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12747, 2, 0, 16)
        add_388: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12748, view_783);  slice_12748 = view_783 = None
        slice_scatter_2316: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12747, add_388, 2, 0, 16);  slice_12747 = add_388 = None
        slice_scatter_2317: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2314, slice_scatter_2316, 1, 3088, 3104);  slice_scatter_2314 = slice_scatter_2316 = None
        slice_12752: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2317, 1, 3088, 3104)
        slice_12753: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12752, 2, 0, 16)
        slice_scatter_2319: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12752, slice_12753, 2, 0, 16);  slice_12752 = slice_12753 = None
        slice_scatter_2320: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2317, slice_scatter_2319, 1, 3088, 3104);  slice_scatter_2317 = slice_scatter_2319 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12773: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_12739, 2, 16, 32);  slice_12739 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_390: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_12773, memory_format = torch.contiguous_format);  slice_12773 = None
        view_784: "f32[32, 11]" = torch.ops.aten.view.default(clone_390, [32, 11]);  clone_390 = None
        mm_387: "f32[32, 8]" = torch.ops.aten.mm.default(view_784, slice_37)
        view_785: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_387, [2, 16, 8]);  mm_387 = None
        slice_12780: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2320, 1, 3088, 3104)
        slice_12781: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12780, 2, 0, 16)
        add_389: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12781, view_785);  slice_12781 = view_785 = None
        slice_scatter_2322: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12780, add_389, 2, 0, 16);  slice_12780 = add_389 = None
        slice_scatter_2323: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2320, slice_scatter_2322, 1, 3088, 3104);  slice_scatter_2320 = slice_scatter_2322 = None
        slice_12785: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2323, 1, 3088, 3104)
        slice_12786: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12785, 2, 0, 16)
        slice_scatter_2325: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12785, slice_12786, 2, 0, 16);  slice_12785 = slice_12786 = None
        slice_scatter_2326: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2323, slice_scatter_2325, 1, 3088, 3104);  slice_scatter_2323 = slice_scatter_2325 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12805: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3104, 3120)
        slice_12806: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_12805, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_391: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_12806, memory_format = torch.contiguous_format);  slice_12806 = None
        view_786: "f32[32, 16]" = torch.ops.aten.view.default(clone_391, [32, 16]);  clone_391 = None
        mm_388: "f32[32, 8]" = torch.ops.aten.mm.default(view_786, slice_7)
        view_787: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_388, [2, 16, 8]);  mm_388 = None
        slice_12813: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2326, 1, 3104, 3120)
        slice_12814: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12813, 2, 0, 16)
        add_390: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12814, view_787);  slice_12814 = view_787 = None
        slice_scatter_2328: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12813, add_390, 2, 0, 16);  slice_12813 = add_390 = None
        slice_scatter_2329: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2326, slice_scatter_2328, 1, 3104, 3120);  slice_scatter_2326 = slice_scatter_2328 = None
        slice_12818: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2329, 1, 3104, 3120)
        slice_12819: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12818, 2, 0, 16)
        slice_scatter_2331: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12818, slice_12819, 2, 0, 16);  slice_12818 = slice_12819 = None
        slice_scatter_2332: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2329, slice_scatter_2331, 1, 3104, 3120);  slice_scatter_2329 = slice_scatter_2331 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12839: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_12805, 2, 16, 32);  slice_12805 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_392: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_12839, memory_format = torch.contiguous_format);  slice_12839 = None
        view_788: "f32[32, 11]" = torch.ops.aten.view.default(clone_392, [32, 11]);  clone_392 = None
        mm_389: "f32[32, 8]" = torch.ops.aten.mm.default(view_788, slice_37)
        view_789: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_389, [2, 16, 8]);  mm_389 = None
        slice_12846: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2332, 1, 3104, 3120)
        slice_12847: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12846, 2, 0, 16)
        add_391: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12847, view_789);  slice_12847 = view_789 = None
        slice_scatter_2334: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12846, add_391, 2, 0, 16);  slice_12846 = add_391 = None
        slice_scatter_2335: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2332, slice_scatter_2334, 1, 3104, 3120);  slice_scatter_2332 = slice_scatter_2334 = None
        slice_12851: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2335, 1, 3104, 3120)
        slice_12852: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12851, 2, 0, 16)
        slice_scatter_2337: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12851, slice_12852, 2, 0, 16);  slice_12851 = slice_12852 = None
        slice_scatter_2338: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2335, slice_scatter_2337, 1, 3104, 3120);  slice_scatter_2335 = slice_scatter_2337 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12871: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3120, 3136)
        slice_12872: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_12871, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_393: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_12872, memory_format = torch.contiguous_format);  slice_12872 = None
        view_790: "f32[32, 16]" = torch.ops.aten.view.default(clone_393, [32, 16]);  clone_393 = None
        mm_390: "f32[32, 8]" = torch.ops.aten.mm.default(view_790, slice_7)
        view_791: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_390, [2, 16, 8]);  mm_390 = None
        slice_12879: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2338, 1, 3120, 3136)
        slice_12880: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12879, 2, 0, 16)
        add_392: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12880, view_791);  slice_12880 = view_791 = None
        slice_scatter_2340: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12879, add_392, 2, 0, 16);  slice_12879 = add_392 = None
        slice_scatter_2341: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2338, slice_scatter_2340, 1, 3120, 3136);  slice_scatter_2338 = slice_scatter_2340 = None
        slice_12884: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2341, 1, 3120, 3136)
        slice_12885: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12884, 2, 0, 16)
        slice_scatter_2343: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12884, slice_12885, 2, 0, 16);  slice_12884 = slice_12885 = None
        slice_scatter_2344: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2341, slice_scatter_2343, 1, 3120, 3136);  slice_scatter_2341 = slice_scatter_2343 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12905: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_12871, 2, 16, 32);  slice_12871 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_394: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_12905, memory_format = torch.contiguous_format);  slice_12905 = None
        view_792: "f32[32, 11]" = torch.ops.aten.view.default(clone_394, [32, 11]);  clone_394 = None
        mm_391: "f32[32, 8]" = torch.ops.aten.mm.default(view_792, slice_37)
        view_793: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_391, [2, 16, 8]);  mm_391 = None
        slice_12912: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2344, 1, 3120, 3136)
        slice_12913: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12912, 2, 0, 16)
        add_393: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12913, view_793);  slice_12913 = view_793 = None
        slice_scatter_2346: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12912, add_393, 2, 0, 16);  slice_12912 = add_393 = None
        slice_scatter_2347: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2344, slice_scatter_2346, 1, 3120, 3136);  slice_scatter_2344 = slice_scatter_2346 = None
        slice_12917: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2347, 1, 3120, 3136)
        slice_12918: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12917, 2, 0, 16)
        slice_scatter_2349: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12917, slice_12918, 2, 0, 16);  slice_12917 = slice_12918 = None
        slice_scatter_2350: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2347, slice_scatter_2349, 1, 3120, 3136);  slice_scatter_2347 = slice_scatter_2349 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12937: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3136, 3152)
        slice_12938: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_12937, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_395: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_12938, memory_format = torch.contiguous_format);  slice_12938 = None
        view_794: "f32[32, 16]" = torch.ops.aten.view.default(clone_395, [32, 16]);  clone_395 = None
        mm_392: "f32[32, 8]" = torch.ops.aten.mm.default(view_794, slice_7)
        view_795: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_392, [2, 16, 8]);  mm_392 = None
        slice_12945: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2350, 1, 3136, 3152)
        slice_12946: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12945, 2, 0, 16)
        add_394: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12946, view_795);  slice_12946 = view_795 = None
        slice_scatter_2352: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12945, add_394, 2, 0, 16);  slice_12945 = add_394 = None
        slice_scatter_2353: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2350, slice_scatter_2352, 1, 3136, 3152);  slice_scatter_2350 = slice_scatter_2352 = None
        slice_12950: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2353, 1, 3136, 3152)
        slice_12951: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12950, 2, 0, 16)
        slice_scatter_2355: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12950, slice_12951, 2, 0, 16);  slice_12950 = slice_12951 = None
        slice_scatter_2356: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2353, slice_scatter_2355, 1, 3136, 3152);  slice_scatter_2353 = slice_scatter_2355 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_12971: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_12937, 2, 16, 32);  slice_12937 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_396: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_12971, memory_format = torch.contiguous_format);  slice_12971 = None
        view_796: "f32[32, 11]" = torch.ops.aten.view.default(clone_396, [32, 11]);  clone_396 = None
        mm_393: "f32[32, 8]" = torch.ops.aten.mm.default(view_796, slice_37)
        view_797: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_393, [2, 16, 8]);  mm_393 = None
        slice_12978: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2356, 1, 3136, 3152)
        slice_12979: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12978, 2, 0, 16)
        add_395: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_12979, view_797);  slice_12979 = view_797 = None
        slice_scatter_2358: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12978, add_395, 2, 0, 16);  slice_12978 = add_395 = None
        slice_scatter_2359: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2356, slice_scatter_2358, 1, 3136, 3152);  slice_scatter_2356 = slice_scatter_2358 = None
        slice_12983: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2359, 1, 3136, 3152)
        slice_12984: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_12983, 2, 0, 16)
        slice_scatter_2361: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_12983, slice_12984, 2, 0, 16);  slice_12983 = slice_12984 = None
        slice_scatter_2362: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2359, slice_scatter_2361, 1, 3136, 3152);  slice_scatter_2359 = slice_scatter_2361 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13003: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3152, 3168)
        slice_13004: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_13003, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_397: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_13004, memory_format = torch.contiguous_format);  slice_13004 = None
        view_798: "f32[32, 16]" = torch.ops.aten.view.default(clone_397, [32, 16]);  clone_397 = None
        mm_394: "f32[32, 8]" = torch.ops.aten.mm.default(view_798, slice_7)
        view_799: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_394, [2, 16, 8]);  mm_394 = None
        slice_13011: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2362, 1, 3152, 3168)
        slice_13012: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13011, 2, 0, 16)
        add_396: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13012, view_799);  slice_13012 = view_799 = None
        slice_scatter_2364: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13011, add_396, 2, 0, 16);  slice_13011 = add_396 = None
        slice_scatter_2365: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2362, slice_scatter_2364, 1, 3152, 3168);  slice_scatter_2362 = slice_scatter_2364 = None
        slice_13016: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2365, 1, 3152, 3168)
        slice_13017: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13016, 2, 0, 16)
        slice_scatter_2367: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13016, slice_13017, 2, 0, 16);  slice_13016 = slice_13017 = None
        slice_scatter_2368: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2365, slice_scatter_2367, 1, 3152, 3168);  slice_scatter_2365 = slice_scatter_2367 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13037: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_13003, 2, 16, 32);  slice_13003 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_398: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_13037, memory_format = torch.contiguous_format);  slice_13037 = None
        view_800: "f32[32, 11]" = torch.ops.aten.view.default(clone_398, [32, 11]);  clone_398 = None
        mm_395: "f32[32, 8]" = torch.ops.aten.mm.default(view_800, slice_37)
        view_801: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_395, [2, 16, 8]);  mm_395 = None
        slice_13044: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2368, 1, 3152, 3168)
        slice_13045: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13044, 2, 0, 16)
        add_397: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13045, view_801);  slice_13045 = view_801 = None
        slice_scatter_2370: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13044, add_397, 2, 0, 16);  slice_13044 = add_397 = None
        slice_scatter_2371: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2368, slice_scatter_2370, 1, 3152, 3168);  slice_scatter_2368 = slice_scatter_2370 = None
        slice_13049: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2371, 1, 3152, 3168)
        slice_13050: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13049, 2, 0, 16)
        slice_scatter_2373: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13049, slice_13050, 2, 0, 16);  slice_13049 = slice_13050 = None
        slice_scatter_2374: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2371, slice_scatter_2373, 1, 3152, 3168);  slice_scatter_2371 = slice_scatter_2373 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13069: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3168, 3184)
        slice_13070: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_13069, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_399: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_13070, memory_format = torch.contiguous_format);  slice_13070 = None
        view_802: "f32[32, 16]" = torch.ops.aten.view.default(clone_399, [32, 16]);  clone_399 = None
        mm_396: "f32[32, 8]" = torch.ops.aten.mm.default(view_802, slice_7)
        view_803: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_396, [2, 16, 8]);  mm_396 = None
        slice_13077: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2374, 1, 3168, 3184)
        slice_13078: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13077, 2, 0, 16)
        add_398: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13078, view_803);  slice_13078 = view_803 = None
        slice_scatter_2376: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13077, add_398, 2, 0, 16);  slice_13077 = add_398 = None
        slice_scatter_2377: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2374, slice_scatter_2376, 1, 3168, 3184);  slice_scatter_2374 = slice_scatter_2376 = None
        slice_13082: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2377, 1, 3168, 3184)
        slice_13083: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13082, 2, 0, 16)
        slice_scatter_2379: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13082, slice_13083, 2, 0, 16);  slice_13082 = slice_13083 = None
        slice_scatter_2380: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2377, slice_scatter_2379, 1, 3168, 3184);  slice_scatter_2377 = slice_scatter_2379 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13103: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_13069, 2, 16, 32);  slice_13069 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_400: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_13103, memory_format = torch.contiguous_format);  slice_13103 = None
        view_804: "f32[32, 11]" = torch.ops.aten.view.default(clone_400, [32, 11]);  clone_400 = None
        mm_397: "f32[32, 8]" = torch.ops.aten.mm.default(view_804, slice_37)
        view_805: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_397, [2, 16, 8]);  mm_397 = None
        slice_13110: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2380, 1, 3168, 3184)
        slice_13111: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13110, 2, 0, 16)
        add_399: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13111, view_805);  slice_13111 = view_805 = None
        slice_scatter_2382: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13110, add_399, 2, 0, 16);  slice_13110 = add_399 = None
        slice_scatter_2383: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2380, slice_scatter_2382, 1, 3168, 3184);  slice_scatter_2380 = slice_scatter_2382 = None
        slice_13115: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2383, 1, 3168, 3184)
        slice_13116: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13115, 2, 0, 16)
        slice_scatter_2385: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13115, slice_13116, 2, 0, 16);  slice_13115 = slice_13116 = None
        slice_scatter_2386: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2383, slice_scatter_2385, 1, 3168, 3184);  slice_scatter_2383 = slice_scatter_2385 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13135: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3184, 3200)
        slice_13136: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_13135, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_401: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_13136, memory_format = torch.contiguous_format);  slice_13136 = None
        view_806: "f32[32, 16]" = torch.ops.aten.view.default(clone_401, [32, 16]);  clone_401 = None
        mm_398: "f32[32, 8]" = torch.ops.aten.mm.default(view_806, slice_7)
        view_807: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_398, [2, 16, 8]);  mm_398 = None
        slice_13143: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2386, 1, 3184, 3200)
        slice_13144: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13143, 2, 0, 16)
        add_400: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13144, view_807);  slice_13144 = view_807 = None
        slice_scatter_2388: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13143, add_400, 2, 0, 16);  slice_13143 = add_400 = None
        slice_scatter_2389: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2386, slice_scatter_2388, 1, 3184, 3200);  slice_scatter_2386 = slice_scatter_2388 = None
        slice_13148: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2389, 1, 3184, 3200)
        slice_13149: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13148, 2, 0, 16)
        slice_scatter_2391: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13148, slice_13149, 2, 0, 16);  slice_13148 = slice_13149 = None
        slice_scatter_2392: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2389, slice_scatter_2391, 1, 3184, 3200);  slice_scatter_2389 = slice_scatter_2391 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13169: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_13135, 2, 16, 32);  slice_13135 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_402: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_13169, memory_format = torch.contiguous_format);  slice_13169 = None
        view_808: "f32[32, 11]" = torch.ops.aten.view.default(clone_402, [32, 11]);  clone_402 = None
        mm_399: "f32[32, 8]" = torch.ops.aten.mm.default(view_808, slice_37)
        view_809: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_399, [2, 16, 8]);  mm_399 = None
        slice_13176: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2392, 1, 3184, 3200)
        slice_13177: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13176, 2, 0, 16)
        add_401: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13177, view_809);  slice_13177 = view_809 = None
        slice_scatter_2394: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13176, add_401, 2, 0, 16);  slice_13176 = add_401 = None
        slice_scatter_2395: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2392, slice_scatter_2394, 1, 3184, 3200);  slice_scatter_2392 = slice_scatter_2394 = None
        slice_13181: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2395, 1, 3184, 3200)
        slice_13182: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13181, 2, 0, 16)
        slice_scatter_2397: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13181, slice_13182, 2, 0, 16);  slice_13181 = slice_13182 = None
        slice_scatter_2398: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2395, slice_scatter_2397, 1, 3184, 3200);  slice_scatter_2395 = slice_scatter_2397 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13201: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3200, 3216)
        slice_13202: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_13201, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_403: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_13202, memory_format = torch.contiguous_format);  slice_13202 = None
        view_810: "f32[32, 16]" = torch.ops.aten.view.default(clone_403, [32, 16]);  clone_403 = None
        mm_400: "f32[32, 8]" = torch.ops.aten.mm.default(view_810, slice_7)
        view_811: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_400, [2, 16, 8]);  mm_400 = None
        slice_13209: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2398, 1, 3200, 3216)
        slice_13210: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13209, 2, 0, 16)
        add_402: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13210, view_811);  slice_13210 = view_811 = None
        slice_scatter_2400: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13209, add_402, 2, 0, 16);  slice_13209 = add_402 = None
        slice_scatter_2401: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2398, slice_scatter_2400, 1, 3200, 3216);  slice_scatter_2398 = slice_scatter_2400 = None
        slice_13214: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2401, 1, 3200, 3216)
        slice_13215: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13214, 2, 0, 16)
        slice_scatter_2403: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13214, slice_13215, 2, 0, 16);  slice_13214 = slice_13215 = None
        slice_scatter_2404: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2401, slice_scatter_2403, 1, 3200, 3216);  slice_scatter_2401 = slice_scatter_2403 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13235: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_13201, 2, 16, 32);  slice_13201 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_404: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_13235, memory_format = torch.contiguous_format);  slice_13235 = None
        view_812: "f32[32, 11]" = torch.ops.aten.view.default(clone_404, [32, 11]);  clone_404 = None
        mm_401: "f32[32, 8]" = torch.ops.aten.mm.default(view_812, slice_37)
        view_813: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_401, [2, 16, 8]);  mm_401 = None
        slice_13242: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2404, 1, 3200, 3216)
        slice_13243: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13242, 2, 0, 16)
        add_403: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13243, view_813);  slice_13243 = view_813 = None
        slice_scatter_2406: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13242, add_403, 2, 0, 16);  slice_13242 = add_403 = None
        slice_scatter_2407: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2404, slice_scatter_2406, 1, 3200, 3216);  slice_scatter_2404 = slice_scatter_2406 = None
        slice_13247: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2407, 1, 3200, 3216)
        slice_13248: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13247, 2, 0, 16)
        slice_scatter_2409: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13247, slice_13248, 2, 0, 16);  slice_13247 = slice_13248 = None
        slice_scatter_2410: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2407, slice_scatter_2409, 1, 3200, 3216);  slice_scatter_2407 = slice_scatter_2409 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13267: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3216, 3232)
        slice_13268: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_13267, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_405: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_13268, memory_format = torch.contiguous_format);  slice_13268 = None
        view_814: "f32[32, 16]" = torch.ops.aten.view.default(clone_405, [32, 16]);  clone_405 = None
        mm_402: "f32[32, 8]" = torch.ops.aten.mm.default(view_814, slice_7)
        view_815: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_402, [2, 16, 8]);  mm_402 = None
        slice_13275: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2410, 1, 3216, 3232)
        slice_13276: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13275, 2, 0, 16)
        add_404: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13276, view_815);  slice_13276 = view_815 = None
        slice_scatter_2412: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13275, add_404, 2, 0, 16);  slice_13275 = add_404 = None
        slice_scatter_2413: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2410, slice_scatter_2412, 1, 3216, 3232);  slice_scatter_2410 = slice_scatter_2412 = None
        slice_13280: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2413, 1, 3216, 3232)
        slice_13281: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13280, 2, 0, 16)
        slice_scatter_2415: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13280, slice_13281, 2, 0, 16);  slice_13280 = slice_13281 = None
        slice_scatter_2416: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2413, slice_scatter_2415, 1, 3216, 3232);  slice_scatter_2413 = slice_scatter_2415 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13301: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_13267, 2, 16, 32);  slice_13267 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_406: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_13301, memory_format = torch.contiguous_format);  slice_13301 = None
        view_816: "f32[32, 11]" = torch.ops.aten.view.default(clone_406, [32, 11]);  clone_406 = None
        mm_403: "f32[32, 8]" = torch.ops.aten.mm.default(view_816, slice_37)
        view_817: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_403, [2, 16, 8]);  mm_403 = None
        slice_13308: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2416, 1, 3216, 3232)
        slice_13309: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13308, 2, 0, 16)
        add_405: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13309, view_817);  slice_13309 = view_817 = None
        slice_scatter_2418: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13308, add_405, 2, 0, 16);  slice_13308 = add_405 = None
        slice_scatter_2419: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2416, slice_scatter_2418, 1, 3216, 3232);  slice_scatter_2416 = slice_scatter_2418 = None
        slice_13313: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2419, 1, 3216, 3232)
        slice_13314: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13313, 2, 0, 16)
        slice_scatter_2421: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13313, slice_13314, 2, 0, 16);  slice_13313 = slice_13314 = None
        slice_scatter_2422: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2419, slice_scatter_2421, 1, 3216, 3232);  slice_scatter_2419 = slice_scatter_2421 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13333: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3232, 3248)
        slice_13334: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_13333, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_407: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_13334, memory_format = torch.contiguous_format);  slice_13334 = None
        view_818: "f32[32, 16]" = torch.ops.aten.view.default(clone_407, [32, 16]);  clone_407 = None
        mm_404: "f32[32, 8]" = torch.ops.aten.mm.default(view_818, slice_7)
        view_819: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_404, [2, 16, 8]);  mm_404 = None
        slice_13341: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2422, 1, 3232, 3248)
        slice_13342: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13341, 2, 0, 16)
        add_406: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13342, view_819);  slice_13342 = view_819 = None
        slice_scatter_2424: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13341, add_406, 2, 0, 16);  slice_13341 = add_406 = None
        slice_scatter_2425: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2422, slice_scatter_2424, 1, 3232, 3248);  slice_scatter_2422 = slice_scatter_2424 = None
        slice_13346: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2425, 1, 3232, 3248)
        slice_13347: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13346, 2, 0, 16)
        slice_scatter_2427: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13346, slice_13347, 2, 0, 16);  slice_13346 = slice_13347 = None
        slice_scatter_2428: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2425, slice_scatter_2427, 1, 3232, 3248);  slice_scatter_2425 = slice_scatter_2427 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13367: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_13333, 2, 16, 32);  slice_13333 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_408: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_13367, memory_format = torch.contiguous_format);  slice_13367 = None
        view_820: "f32[32, 11]" = torch.ops.aten.view.default(clone_408, [32, 11]);  clone_408 = None
        mm_405: "f32[32, 8]" = torch.ops.aten.mm.default(view_820, slice_37)
        view_821: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_405, [2, 16, 8]);  mm_405 = None
        slice_13374: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2428, 1, 3232, 3248)
        slice_13375: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13374, 2, 0, 16)
        add_407: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13375, view_821);  slice_13375 = view_821 = None
        slice_scatter_2430: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13374, add_407, 2, 0, 16);  slice_13374 = add_407 = None
        slice_scatter_2431: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2428, slice_scatter_2430, 1, 3232, 3248);  slice_scatter_2428 = slice_scatter_2430 = None
        slice_13379: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2431, 1, 3232, 3248)
        slice_13380: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13379, 2, 0, 16)
        slice_scatter_2433: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13379, slice_13380, 2, 0, 16);  slice_13379 = slice_13380 = None
        slice_scatter_2434: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2431, slice_scatter_2433, 1, 3232, 3248);  slice_scatter_2431 = slice_scatter_2433 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13399: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3248, 3264)
        slice_13400: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_13399, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_409: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_13400, memory_format = torch.contiguous_format);  slice_13400 = None
        view_822: "f32[32, 16]" = torch.ops.aten.view.default(clone_409, [32, 16]);  clone_409 = None
        mm_406: "f32[32, 8]" = torch.ops.aten.mm.default(view_822, slice_7)
        view_823: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_406, [2, 16, 8]);  mm_406 = None
        slice_13407: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2434, 1, 3248, 3264)
        slice_13408: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13407, 2, 0, 16)
        add_408: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13408, view_823);  slice_13408 = view_823 = None
        slice_scatter_2436: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13407, add_408, 2, 0, 16);  slice_13407 = add_408 = None
        slice_scatter_2437: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2434, slice_scatter_2436, 1, 3248, 3264);  slice_scatter_2434 = slice_scatter_2436 = None
        slice_13412: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2437, 1, 3248, 3264)
        slice_13413: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13412, 2, 0, 16)
        slice_scatter_2439: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13412, slice_13413, 2, 0, 16);  slice_13412 = slice_13413 = None
        slice_scatter_2440: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2437, slice_scatter_2439, 1, 3248, 3264);  slice_scatter_2437 = slice_scatter_2439 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13433: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_13399, 2, 16, 32);  slice_13399 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_410: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_13433, memory_format = torch.contiguous_format);  slice_13433 = None
        view_824: "f32[32, 11]" = torch.ops.aten.view.default(clone_410, [32, 11]);  clone_410 = None
        mm_407: "f32[32, 8]" = torch.ops.aten.mm.default(view_824, slice_37)
        view_825: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_407, [2, 16, 8]);  mm_407 = None
        slice_13440: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2440, 1, 3248, 3264)
        slice_13441: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13440, 2, 0, 16)
        add_409: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13441, view_825);  slice_13441 = view_825 = None
        slice_scatter_2442: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13440, add_409, 2, 0, 16);  slice_13440 = add_409 = None
        slice_scatter_2443: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2440, slice_scatter_2442, 1, 3248, 3264);  slice_scatter_2440 = slice_scatter_2442 = None
        slice_13445: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2443, 1, 3248, 3264)
        slice_13446: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13445, 2, 0, 16)
        slice_scatter_2445: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13445, slice_13446, 2, 0, 16);  slice_13445 = slice_13446 = None
        slice_scatter_2446: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2443, slice_scatter_2445, 1, 3248, 3264);  slice_scatter_2443 = slice_scatter_2445 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13465: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3264, 3280)
        slice_13466: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_13465, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_411: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_13466, memory_format = torch.contiguous_format);  slice_13466 = None
        view_826: "f32[32, 16]" = torch.ops.aten.view.default(clone_411, [32, 16]);  clone_411 = None
        mm_408: "f32[32, 8]" = torch.ops.aten.mm.default(view_826, slice_7)
        view_827: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_408, [2, 16, 8]);  mm_408 = None
        slice_13473: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2446, 1, 3264, 3280)
        slice_13474: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13473, 2, 0, 16)
        add_410: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13474, view_827);  slice_13474 = view_827 = None
        slice_scatter_2448: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13473, add_410, 2, 0, 16);  slice_13473 = add_410 = None
        slice_scatter_2449: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2446, slice_scatter_2448, 1, 3264, 3280);  slice_scatter_2446 = slice_scatter_2448 = None
        slice_13478: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2449, 1, 3264, 3280)
        slice_13479: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13478, 2, 0, 16)
        slice_scatter_2451: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13478, slice_13479, 2, 0, 16);  slice_13478 = slice_13479 = None
        slice_scatter_2452: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2449, slice_scatter_2451, 1, 3264, 3280);  slice_scatter_2449 = slice_scatter_2451 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13499: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_13465, 2, 16, 32);  slice_13465 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_412: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_13499, memory_format = torch.contiguous_format);  slice_13499 = None
        view_828: "f32[32, 11]" = torch.ops.aten.view.default(clone_412, [32, 11]);  clone_412 = None
        mm_409: "f32[32, 8]" = torch.ops.aten.mm.default(view_828, slice_37)
        view_829: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_409, [2, 16, 8]);  mm_409 = None
        slice_13506: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2452, 1, 3264, 3280)
        slice_13507: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13506, 2, 0, 16)
        add_411: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13507, view_829);  slice_13507 = view_829 = None
        slice_scatter_2454: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13506, add_411, 2, 0, 16);  slice_13506 = add_411 = None
        slice_scatter_2455: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2452, slice_scatter_2454, 1, 3264, 3280);  slice_scatter_2452 = slice_scatter_2454 = None
        slice_13511: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2455, 1, 3264, 3280)
        slice_13512: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13511, 2, 0, 16)
        slice_scatter_2457: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13511, slice_13512, 2, 0, 16);  slice_13511 = slice_13512 = None
        slice_scatter_2458: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2455, slice_scatter_2457, 1, 3264, 3280);  slice_scatter_2455 = slice_scatter_2457 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13531: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3280, 3296)
        slice_13532: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_13531, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_413: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_13532, memory_format = torch.contiguous_format);  slice_13532 = None
        view_830: "f32[32, 16]" = torch.ops.aten.view.default(clone_413, [32, 16]);  clone_413 = None
        mm_410: "f32[32, 8]" = torch.ops.aten.mm.default(view_830, slice_7)
        view_831: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_410, [2, 16, 8]);  mm_410 = None
        slice_13539: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2458, 1, 3280, 3296)
        slice_13540: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13539, 2, 0, 16)
        add_412: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13540, view_831);  slice_13540 = view_831 = None
        slice_scatter_2460: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13539, add_412, 2, 0, 16);  slice_13539 = add_412 = None
        slice_scatter_2461: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2458, slice_scatter_2460, 1, 3280, 3296);  slice_scatter_2458 = slice_scatter_2460 = None
        slice_13544: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2461, 1, 3280, 3296)
        slice_13545: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13544, 2, 0, 16)
        slice_scatter_2463: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13544, slice_13545, 2, 0, 16);  slice_13544 = slice_13545 = None
        slice_scatter_2464: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2461, slice_scatter_2463, 1, 3280, 3296);  slice_scatter_2461 = slice_scatter_2463 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13565: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_13531, 2, 16, 32);  slice_13531 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_414: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_13565, memory_format = torch.contiguous_format);  slice_13565 = None
        view_832: "f32[32, 11]" = torch.ops.aten.view.default(clone_414, [32, 11]);  clone_414 = None
        mm_411: "f32[32, 8]" = torch.ops.aten.mm.default(view_832, slice_37)
        view_833: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_411, [2, 16, 8]);  mm_411 = None
        slice_13572: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2464, 1, 3280, 3296)
        slice_13573: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13572, 2, 0, 16)
        add_413: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13573, view_833);  slice_13573 = view_833 = None
        slice_scatter_2466: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13572, add_413, 2, 0, 16);  slice_13572 = add_413 = None
        slice_scatter_2467: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2464, slice_scatter_2466, 1, 3280, 3296);  slice_scatter_2464 = slice_scatter_2466 = None
        slice_13577: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2467, 1, 3280, 3296)
        slice_13578: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13577, 2, 0, 16)
        slice_scatter_2469: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13577, slice_13578, 2, 0, 16);  slice_13577 = slice_13578 = None
        slice_scatter_2470: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2467, slice_scatter_2469, 1, 3280, 3296);  slice_scatter_2467 = slice_scatter_2469 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13597: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3296, 3312)
        slice_13598: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_13597, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_415: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_13598, memory_format = torch.contiguous_format);  slice_13598 = None
        view_834: "f32[32, 16]" = torch.ops.aten.view.default(clone_415, [32, 16]);  clone_415 = None
        mm_412: "f32[32, 8]" = torch.ops.aten.mm.default(view_834, slice_7)
        view_835: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_412, [2, 16, 8]);  mm_412 = None
        slice_13605: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2470, 1, 3296, 3312)
        slice_13606: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13605, 2, 0, 16)
        add_414: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13606, view_835);  slice_13606 = view_835 = None
        slice_scatter_2472: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13605, add_414, 2, 0, 16);  slice_13605 = add_414 = None
        slice_scatter_2473: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2470, slice_scatter_2472, 1, 3296, 3312);  slice_scatter_2470 = slice_scatter_2472 = None
        slice_13610: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2473, 1, 3296, 3312)
        slice_13611: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13610, 2, 0, 16)
        slice_scatter_2475: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13610, slice_13611, 2, 0, 16);  slice_13610 = slice_13611 = None
        slice_scatter_2476: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2473, slice_scatter_2475, 1, 3296, 3312);  slice_scatter_2473 = slice_scatter_2475 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13631: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_13597, 2, 16, 32);  slice_13597 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_416: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_13631, memory_format = torch.contiguous_format);  slice_13631 = None
        view_836: "f32[32, 11]" = torch.ops.aten.view.default(clone_416, [32, 11]);  clone_416 = None
        mm_413: "f32[32, 8]" = torch.ops.aten.mm.default(view_836, slice_37)
        view_837: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_413, [2, 16, 8]);  mm_413 = None
        slice_13638: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2476, 1, 3296, 3312)
        slice_13639: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13638, 2, 0, 16)
        add_415: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13639, view_837);  slice_13639 = view_837 = None
        slice_scatter_2478: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13638, add_415, 2, 0, 16);  slice_13638 = add_415 = None
        slice_scatter_2479: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2476, slice_scatter_2478, 1, 3296, 3312);  slice_scatter_2476 = slice_scatter_2478 = None
        slice_13643: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2479, 1, 3296, 3312)
        slice_13644: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13643, 2, 0, 16)
        slice_scatter_2481: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13643, slice_13644, 2, 0, 16);  slice_13643 = slice_13644 = None
        slice_scatter_2482: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2479, slice_scatter_2481, 1, 3296, 3312);  slice_scatter_2479 = slice_scatter_2481 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13663: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3312, 3328)
        slice_13664: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_13663, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_417: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_13664, memory_format = torch.contiguous_format);  slice_13664 = None
        view_838: "f32[32, 16]" = torch.ops.aten.view.default(clone_417, [32, 16]);  clone_417 = None
        mm_414: "f32[32, 8]" = torch.ops.aten.mm.default(view_838, slice_7)
        view_839: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_414, [2, 16, 8]);  mm_414 = None
        slice_13671: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2482, 1, 3312, 3328)
        slice_13672: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13671, 2, 0, 16)
        add_416: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13672, view_839);  slice_13672 = view_839 = None
        slice_scatter_2484: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13671, add_416, 2, 0, 16);  slice_13671 = add_416 = None
        slice_scatter_2485: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2482, slice_scatter_2484, 1, 3312, 3328);  slice_scatter_2482 = slice_scatter_2484 = None
        slice_13676: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2485, 1, 3312, 3328)
        slice_13677: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13676, 2, 0, 16)
        slice_scatter_2487: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13676, slice_13677, 2, 0, 16);  slice_13676 = slice_13677 = None
        slice_scatter_2488: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2485, slice_scatter_2487, 1, 3312, 3328);  slice_scatter_2485 = slice_scatter_2487 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13697: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_13663, 2, 16, 32);  slice_13663 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_418: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_13697, memory_format = torch.contiguous_format);  slice_13697 = None
        view_840: "f32[32, 11]" = torch.ops.aten.view.default(clone_418, [32, 11]);  clone_418 = None
        mm_415: "f32[32, 8]" = torch.ops.aten.mm.default(view_840, slice_37)
        view_841: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_415, [2, 16, 8]);  mm_415 = None
        slice_13704: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2488, 1, 3312, 3328)
        slice_13705: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13704, 2, 0, 16)
        add_417: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13705, view_841);  slice_13705 = view_841 = None
        slice_scatter_2490: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13704, add_417, 2, 0, 16);  slice_13704 = add_417 = None
        slice_scatter_2491: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2488, slice_scatter_2490, 1, 3312, 3328);  slice_scatter_2488 = slice_scatter_2490 = None
        slice_13709: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2491, 1, 3312, 3328)
        slice_13710: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13709, 2, 0, 16)
        slice_scatter_2493: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13709, slice_13710, 2, 0, 16);  slice_13709 = slice_13710 = None
        slice_scatter_2494: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2491, slice_scatter_2493, 1, 3312, 3328);  slice_scatter_2491 = slice_scatter_2493 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13729: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3328, 3344)
        slice_13730: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_13729, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_419: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_13730, memory_format = torch.contiguous_format);  slice_13730 = None
        view_842: "f32[32, 16]" = torch.ops.aten.view.default(clone_419, [32, 16]);  clone_419 = None
        mm_416: "f32[32, 8]" = torch.ops.aten.mm.default(view_842, slice_7)
        view_843: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_416, [2, 16, 8]);  mm_416 = None
        slice_13737: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2494, 1, 3328, 3344)
        slice_13738: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13737, 2, 0, 16)
        add_418: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13738, view_843);  slice_13738 = view_843 = None
        slice_scatter_2496: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13737, add_418, 2, 0, 16);  slice_13737 = add_418 = None
        slice_scatter_2497: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2494, slice_scatter_2496, 1, 3328, 3344);  slice_scatter_2494 = slice_scatter_2496 = None
        slice_13742: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2497, 1, 3328, 3344)
        slice_13743: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13742, 2, 0, 16)
        slice_scatter_2499: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13742, slice_13743, 2, 0, 16);  slice_13742 = slice_13743 = None
        slice_scatter_2500: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2497, slice_scatter_2499, 1, 3328, 3344);  slice_scatter_2497 = slice_scatter_2499 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13763: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_13729, 2, 16, 32);  slice_13729 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_420: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_13763, memory_format = torch.contiguous_format);  slice_13763 = None
        view_844: "f32[32, 11]" = torch.ops.aten.view.default(clone_420, [32, 11]);  clone_420 = None
        mm_417: "f32[32, 8]" = torch.ops.aten.mm.default(view_844, slice_37)
        view_845: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_417, [2, 16, 8]);  mm_417 = None
        slice_13770: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2500, 1, 3328, 3344)
        slice_13771: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13770, 2, 0, 16)
        add_419: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13771, view_845);  slice_13771 = view_845 = None
        slice_scatter_2502: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13770, add_419, 2, 0, 16);  slice_13770 = add_419 = None
        slice_scatter_2503: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2500, slice_scatter_2502, 1, 3328, 3344);  slice_scatter_2500 = slice_scatter_2502 = None
        slice_13775: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2503, 1, 3328, 3344)
        slice_13776: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13775, 2, 0, 16)
        slice_scatter_2505: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13775, slice_13776, 2, 0, 16);  slice_13775 = slice_13776 = None
        slice_scatter_2506: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2503, slice_scatter_2505, 1, 3328, 3344);  slice_scatter_2503 = slice_scatter_2505 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13795: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3344, 3360)
        slice_13796: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_13795, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_421: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_13796, memory_format = torch.contiguous_format);  slice_13796 = None
        view_846: "f32[32, 16]" = torch.ops.aten.view.default(clone_421, [32, 16]);  clone_421 = None
        mm_418: "f32[32, 8]" = torch.ops.aten.mm.default(view_846, slice_7)
        view_847: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_418, [2, 16, 8]);  mm_418 = None
        slice_13803: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2506, 1, 3344, 3360)
        slice_13804: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13803, 2, 0, 16)
        add_420: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13804, view_847);  slice_13804 = view_847 = None
        slice_scatter_2508: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13803, add_420, 2, 0, 16);  slice_13803 = add_420 = None
        slice_scatter_2509: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2506, slice_scatter_2508, 1, 3344, 3360);  slice_scatter_2506 = slice_scatter_2508 = None
        slice_13808: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2509, 1, 3344, 3360)
        slice_13809: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13808, 2, 0, 16)
        slice_scatter_2511: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13808, slice_13809, 2, 0, 16);  slice_13808 = slice_13809 = None
        slice_scatter_2512: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2509, slice_scatter_2511, 1, 3344, 3360);  slice_scatter_2509 = slice_scatter_2511 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13829: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_13795, 2, 16, 32);  slice_13795 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_422: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_13829, memory_format = torch.contiguous_format);  slice_13829 = None
        view_848: "f32[32, 11]" = torch.ops.aten.view.default(clone_422, [32, 11]);  clone_422 = None
        mm_419: "f32[32, 8]" = torch.ops.aten.mm.default(view_848, slice_37)
        view_849: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_419, [2, 16, 8]);  mm_419 = None
        slice_13836: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2512, 1, 3344, 3360)
        slice_13837: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13836, 2, 0, 16)
        add_421: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13837, view_849);  slice_13837 = view_849 = None
        slice_scatter_2514: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13836, add_421, 2, 0, 16);  slice_13836 = add_421 = None
        slice_scatter_2515: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2512, slice_scatter_2514, 1, 3344, 3360);  slice_scatter_2512 = slice_scatter_2514 = None
        slice_13841: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2515, 1, 3344, 3360)
        slice_13842: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13841, 2, 0, 16)
        slice_scatter_2517: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13841, slice_13842, 2, 0, 16);  slice_13841 = slice_13842 = None
        slice_scatter_2518: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2515, slice_scatter_2517, 1, 3344, 3360);  slice_scatter_2515 = slice_scatter_2517 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13861: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3360, 3376)
        slice_13862: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_13861, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_423: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_13862, memory_format = torch.contiguous_format);  slice_13862 = None
        view_850: "f32[32, 16]" = torch.ops.aten.view.default(clone_423, [32, 16]);  clone_423 = None
        mm_420: "f32[32, 8]" = torch.ops.aten.mm.default(view_850, slice_7)
        view_851: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_420, [2, 16, 8]);  mm_420 = None
        slice_13869: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2518, 1, 3360, 3376)
        slice_13870: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13869, 2, 0, 16)
        add_422: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13870, view_851);  slice_13870 = view_851 = None
        slice_scatter_2520: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13869, add_422, 2, 0, 16);  slice_13869 = add_422 = None
        slice_scatter_2521: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2518, slice_scatter_2520, 1, 3360, 3376);  slice_scatter_2518 = slice_scatter_2520 = None
        slice_13874: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2521, 1, 3360, 3376)
        slice_13875: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13874, 2, 0, 16)
        slice_scatter_2523: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13874, slice_13875, 2, 0, 16);  slice_13874 = slice_13875 = None
        slice_scatter_2524: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2521, slice_scatter_2523, 1, 3360, 3376);  slice_scatter_2521 = slice_scatter_2523 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13895: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_13861, 2, 16, 32);  slice_13861 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_424: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_13895, memory_format = torch.contiguous_format);  slice_13895 = None
        view_852: "f32[32, 11]" = torch.ops.aten.view.default(clone_424, [32, 11]);  clone_424 = None
        mm_421: "f32[32, 8]" = torch.ops.aten.mm.default(view_852, slice_37)
        view_853: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_421, [2, 16, 8]);  mm_421 = None
        slice_13902: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2524, 1, 3360, 3376)
        slice_13903: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13902, 2, 0, 16)
        add_423: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13903, view_853);  slice_13903 = view_853 = None
        slice_scatter_2526: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13902, add_423, 2, 0, 16);  slice_13902 = add_423 = None
        slice_scatter_2527: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2524, slice_scatter_2526, 1, 3360, 3376);  slice_scatter_2524 = slice_scatter_2526 = None
        slice_13907: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2527, 1, 3360, 3376)
        slice_13908: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13907, 2, 0, 16)
        slice_scatter_2529: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13907, slice_13908, 2, 0, 16);  slice_13907 = slice_13908 = None
        slice_scatter_2530: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2527, slice_scatter_2529, 1, 3360, 3376);  slice_scatter_2527 = slice_scatter_2529 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13927: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3376, 3392)
        slice_13928: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_13927, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_425: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_13928, memory_format = torch.contiguous_format);  slice_13928 = None
        view_854: "f32[32, 16]" = torch.ops.aten.view.default(clone_425, [32, 16]);  clone_425 = None
        mm_422: "f32[32, 8]" = torch.ops.aten.mm.default(view_854, slice_7)
        view_855: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_422, [2, 16, 8]);  mm_422 = None
        slice_13935: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2530, 1, 3376, 3392)
        slice_13936: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13935, 2, 0, 16)
        add_424: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13936, view_855);  slice_13936 = view_855 = None
        slice_scatter_2532: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13935, add_424, 2, 0, 16);  slice_13935 = add_424 = None
        slice_scatter_2533: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2530, slice_scatter_2532, 1, 3376, 3392);  slice_scatter_2530 = slice_scatter_2532 = None
        slice_13940: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2533, 1, 3376, 3392)
        slice_13941: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13940, 2, 0, 16)
        slice_scatter_2535: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13940, slice_13941, 2, 0, 16);  slice_13940 = slice_13941 = None
        slice_scatter_2536: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2533, slice_scatter_2535, 1, 3376, 3392);  slice_scatter_2533 = slice_scatter_2535 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13961: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_13927, 2, 16, 32);  slice_13927 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_426: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_13961, memory_format = torch.contiguous_format);  slice_13961 = None
        view_856: "f32[32, 11]" = torch.ops.aten.view.default(clone_426, [32, 11]);  clone_426 = None
        mm_423: "f32[32, 8]" = torch.ops.aten.mm.default(view_856, slice_37)
        view_857: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_423, [2, 16, 8]);  mm_423 = None
        slice_13968: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2536, 1, 3376, 3392)
        slice_13969: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13968, 2, 0, 16)
        add_425: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_13969, view_857);  slice_13969 = view_857 = None
        slice_scatter_2538: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13968, add_425, 2, 0, 16);  slice_13968 = add_425 = None
        slice_scatter_2539: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2536, slice_scatter_2538, 1, 3376, 3392);  slice_scatter_2536 = slice_scatter_2538 = None
        slice_13973: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2539, 1, 3376, 3392)
        slice_13974: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_13973, 2, 0, 16)
        slice_scatter_2541: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_13973, slice_13974, 2, 0, 16);  slice_13973 = slice_13974 = None
        slice_scatter_2542: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2539, slice_scatter_2541, 1, 3376, 3392);  slice_scatter_2539 = slice_scatter_2541 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_13993: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3392, 3408)
        slice_13994: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_13993, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_427: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_13994, memory_format = torch.contiguous_format);  slice_13994 = None
        view_858: "f32[32, 16]" = torch.ops.aten.view.default(clone_427, [32, 16]);  clone_427 = None
        mm_424: "f32[32, 8]" = torch.ops.aten.mm.default(view_858, slice_7)
        view_859: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_424, [2, 16, 8]);  mm_424 = None
        slice_14001: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2542, 1, 3392, 3408)
        slice_14002: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14001, 2, 0, 16)
        add_426: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14002, view_859);  slice_14002 = view_859 = None
        slice_scatter_2544: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14001, add_426, 2, 0, 16);  slice_14001 = add_426 = None
        slice_scatter_2545: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2542, slice_scatter_2544, 1, 3392, 3408);  slice_scatter_2542 = slice_scatter_2544 = None
        slice_14006: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2545, 1, 3392, 3408)
        slice_14007: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14006, 2, 0, 16)
        slice_scatter_2547: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14006, slice_14007, 2, 0, 16);  slice_14006 = slice_14007 = None
        slice_scatter_2548: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2545, slice_scatter_2547, 1, 3392, 3408);  slice_scatter_2545 = slice_scatter_2547 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14027: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_13993, 2, 16, 32);  slice_13993 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_428: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_14027, memory_format = torch.contiguous_format);  slice_14027 = None
        view_860: "f32[32, 11]" = torch.ops.aten.view.default(clone_428, [32, 11]);  clone_428 = None
        mm_425: "f32[32, 8]" = torch.ops.aten.mm.default(view_860, slice_37)
        view_861: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_425, [2, 16, 8]);  mm_425 = None
        slice_14034: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2548, 1, 3392, 3408)
        slice_14035: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14034, 2, 0, 16)
        add_427: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14035, view_861);  slice_14035 = view_861 = None
        slice_scatter_2550: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14034, add_427, 2, 0, 16);  slice_14034 = add_427 = None
        slice_scatter_2551: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2548, slice_scatter_2550, 1, 3392, 3408);  slice_scatter_2548 = slice_scatter_2550 = None
        slice_14039: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2551, 1, 3392, 3408)
        slice_14040: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14039, 2, 0, 16)
        slice_scatter_2553: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14039, slice_14040, 2, 0, 16);  slice_14039 = slice_14040 = None
        slice_scatter_2554: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2551, slice_scatter_2553, 1, 3392, 3408);  slice_scatter_2551 = slice_scatter_2553 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14059: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3408, 3424)
        slice_14060: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_14059, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_429: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_14060, memory_format = torch.contiguous_format);  slice_14060 = None
        view_862: "f32[32, 16]" = torch.ops.aten.view.default(clone_429, [32, 16]);  clone_429 = None
        mm_426: "f32[32, 8]" = torch.ops.aten.mm.default(view_862, slice_7)
        view_863: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_426, [2, 16, 8]);  mm_426 = None
        slice_14067: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2554, 1, 3408, 3424)
        slice_14068: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14067, 2, 0, 16)
        add_428: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14068, view_863);  slice_14068 = view_863 = None
        slice_scatter_2556: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14067, add_428, 2, 0, 16);  slice_14067 = add_428 = None
        slice_scatter_2557: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2554, slice_scatter_2556, 1, 3408, 3424);  slice_scatter_2554 = slice_scatter_2556 = None
        slice_14072: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2557, 1, 3408, 3424)
        slice_14073: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14072, 2, 0, 16)
        slice_scatter_2559: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14072, slice_14073, 2, 0, 16);  slice_14072 = slice_14073 = None
        slice_scatter_2560: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2557, slice_scatter_2559, 1, 3408, 3424);  slice_scatter_2557 = slice_scatter_2559 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14093: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_14059, 2, 16, 32);  slice_14059 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_430: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_14093, memory_format = torch.contiguous_format);  slice_14093 = None
        view_864: "f32[32, 11]" = torch.ops.aten.view.default(clone_430, [32, 11]);  clone_430 = None
        mm_427: "f32[32, 8]" = torch.ops.aten.mm.default(view_864, slice_37)
        view_865: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_427, [2, 16, 8]);  mm_427 = None
        slice_14100: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2560, 1, 3408, 3424)
        slice_14101: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14100, 2, 0, 16)
        add_429: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14101, view_865);  slice_14101 = view_865 = None
        slice_scatter_2562: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14100, add_429, 2, 0, 16);  slice_14100 = add_429 = None
        slice_scatter_2563: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2560, slice_scatter_2562, 1, 3408, 3424);  slice_scatter_2560 = slice_scatter_2562 = None
        slice_14105: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2563, 1, 3408, 3424)
        slice_14106: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14105, 2, 0, 16)
        slice_scatter_2565: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14105, slice_14106, 2, 0, 16);  slice_14105 = slice_14106 = None
        slice_scatter_2566: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2563, slice_scatter_2565, 1, 3408, 3424);  slice_scatter_2563 = slice_scatter_2565 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14125: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3424, 3440)
        slice_14126: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_14125, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_431: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_14126, memory_format = torch.contiguous_format);  slice_14126 = None
        view_866: "f32[32, 16]" = torch.ops.aten.view.default(clone_431, [32, 16]);  clone_431 = None
        mm_428: "f32[32, 8]" = torch.ops.aten.mm.default(view_866, slice_7)
        view_867: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_428, [2, 16, 8]);  mm_428 = None
        slice_14133: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2566, 1, 3424, 3440)
        slice_14134: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14133, 2, 0, 16)
        add_430: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14134, view_867);  slice_14134 = view_867 = None
        slice_scatter_2568: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14133, add_430, 2, 0, 16);  slice_14133 = add_430 = None
        slice_scatter_2569: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2566, slice_scatter_2568, 1, 3424, 3440);  slice_scatter_2566 = slice_scatter_2568 = None
        slice_14138: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2569, 1, 3424, 3440)
        slice_14139: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14138, 2, 0, 16)
        slice_scatter_2571: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14138, slice_14139, 2, 0, 16);  slice_14138 = slice_14139 = None
        slice_scatter_2572: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2569, slice_scatter_2571, 1, 3424, 3440);  slice_scatter_2569 = slice_scatter_2571 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14159: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_14125, 2, 16, 32);  slice_14125 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_432: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_14159, memory_format = torch.contiguous_format);  slice_14159 = None
        view_868: "f32[32, 11]" = torch.ops.aten.view.default(clone_432, [32, 11]);  clone_432 = None
        mm_429: "f32[32, 8]" = torch.ops.aten.mm.default(view_868, slice_37)
        view_869: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_429, [2, 16, 8]);  mm_429 = None
        slice_14166: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2572, 1, 3424, 3440)
        slice_14167: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14166, 2, 0, 16)
        add_431: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14167, view_869);  slice_14167 = view_869 = None
        slice_scatter_2574: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14166, add_431, 2, 0, 16);  slice_14166 = add_431 = None
        slice_scatter_2575: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2572, slice_scatter_2574, 1, 3424, 3440);  slice_scatter_2572 = slice_scatter_2574 = None
        slice_14171: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2575, 1, 3424, 3440)
        slice_14172: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14171, 2, 0, 16)
        slice_scatter_2577: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14171, slice_14172, 2, 0, 16);  slice_14171 = slice_14172 = None
        slice_scatter_2578: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2575, slice_scatter_2577, 1, 3424, 3440);  slice_scatter_2575 = slice_scatter_2577 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14191: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3440, 3456)
        slice_14192: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_14191, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_433: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_14192, memory_format = torch.contiguous_format);  slice_14192 = None
        view_870: "f32[32, 16]" = torch.ops.aten.view.default(clone_433, [32, 16]);  clone_433 = None
        mm_430: "f32[32, 8]" = torch.ops.aten.mm.default(view_870, slice_7)
        view_871: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_430, [2, 16, 8]);  mm_430 = None
        slice_14199: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2578, 1, 3440, 3456)
        slice_14200: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14199, 2, 0, 16)
        add_432: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14200, view_871);  slice_14200 = view_871 = None
        slice_scatter_2580: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14199, add_432, 2, 0, 16);  slice_14199 = add_432 = None
        slice_scatter_2581: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2578, slice_scatter_2580, 1, 3440, 3456);  slice_scatter_2578 = slice_scatter_2580 = None
        slice_14204: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2581, 1, 3440, 3456)
        slice_14205: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14204, 2, 0, 16)
        slice_scatter_2583: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14204, slice_14205, 2, 0, 16);  slice_14204 = slice_14205 = None
        slice_scatter_2584: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2581, slice_scatter_2583, 1, 3440, 3456);  slice_scatter_2581 = slice_scatter_2583 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14225: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_14191, 2, 16, 32);  slice_14191 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_434: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_14225, memory_format = torch.contiguous_format);  slice_14225 = None
        view_872: "f32[32, 11]" = torch.ops.aten.view.default(clone_434, [32, 11]);  clone_434 = None
        mm_431: "f32[32, 8]" = torch.ops.aten.mm.default(view_872, slice_37)
        view_873: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_431, [2, 16, 8]);  mm_431 = None
        slice_14232: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2584, 1, 3440, 3456)
        slice_14233: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14232, 2, 0, 16)
        add_433: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14233, view_873);  slice_14233 = view_873 = None
        slice_scatter_2586: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14232, add_433, 2, 0, 16);  slice_14232 = add_433 = None
        slice_scatter_2587: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2584, slice_scatter_2586, 1, 3440, 3456);  slice_scatter_2584 = slice_scatter_2586 = None
        slice_14237: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2587, 1, 3440, 3456)
        slice_14238: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14237, 2, 0, 16)
        slice_scatter_2589: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14237, slice_14238, 2, 0, 16);  slice_14237 = slice_14238 = None
        slice_scatter_2590: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2587, slice_scatter_2589, 1, 3440, 3456);  slice_scatter_2587 = slice_scatter_2589 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14257: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3456, 3472)
        slice_14258: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_14257, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_435: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_14258, memory_format = torch.contiguous_format);  slice_14258 = None
        view_874: "f32[32, 16]" = torch.ops.aten.view.default(clone_435, [32, 16]);  clone_435 = None
        mm_432: "f32[32, 8]" = torch.ops.aten.mm.default(view_874, slice_7)
        view_875: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_432, [2, 16, 8]);  mm_432 = None
        slice_14265: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2590, 1, 3456, 3472)
        slice_14266: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14265, 2, 0, 16)
        add_434: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14266, view_875);  slice_14266 = view_875 = None
        slice_scatter_2592: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14265, add_434, 2, 0, 16);  slice_14265 = add_434 = None
        slice_scatter_2593: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2590, slice_scatter_2592, 1, 3456, 3472);  slice_scatter_2590 = slice_scatter_2592 = None
        slice_14270: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2593, 1, 3456, 3472)
        slice_14271: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14270, 2, 0, 16)
        slice_scatter_2595: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14270, slice_14271, 2, 0, 16);  slice_14270 = slice_14271 = None
        slice_scatter_2596: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2593, slice_scatter_2595, 1, 3456, 3472);  slice_scatter_2593 = slice_scatter_2595 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14291: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_14257, 2, 16, 32);  slice_14257 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_436: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_14291, memory_format = torch.contiguous_format);  slice_14291 = None
        view_876: "f32[32, 11]" = torch.ops.aten.view.default(clone_436, [32, 11]);  clone_436 = None
        mm_433: "f32[32, 8]" = torch.ops.aten.mm.default(view_876, slice_37)
        view_877: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_433, [2, 16, 8]);  mm_433 = None
        slice_14298: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2596, 1, 3456, 3472)
        slice_14299: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14298, 2, 0, 16)
        add_435: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14299, view_877);  slice_14299 = view_877 = None
        slice_scatter_2598: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14298, add_435, 2, 0, 16);  slice_14298 = add_435 = None
        slice_scatter_2599: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2596, slice_scatter_2598, 1, 3456, 3472);  slice_scatter_2596 = slice_scatter_2598 = None
        slice_14303: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2599, 1, 3456, 3472)
        slice_14304: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14303, 2, 0, 16)
        slice_scatter_2601: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14303, slice_14304, 2, 0, 16);  slice_14303 = slice_14304 = None
        slice_scatter_2602: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2599, slice_scatter_2601, 1, 3456, 3472);  slice_scatter_2599 = slice_scatter_2601 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14323: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3472, 3488)
        slice_14324: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_14323, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_437: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_14324, memory_format = torch.contiguous_format);  slice_14324 = None
        view_878: "f32[32, 16]" = torch.ops.aten.view.default(clone_437, [32, 16]);  clone_437 = None
        mm_434: "f32[32, 8]" = torch.ops.aten.mm.default(view_878, slice_7)
        view_879: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_434, [2, 16, 8]);  mm_434 = None
        slice_14331: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2602, 1, 3472, 3488)
        slice_14332: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14331, 2, 0, 16)
        add_436: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14332, view_879);  slice_14332 = view_879 = None
        slice_scatter_2604: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14331, add_436, 2, 0, 16);  slice_14331 = add_436 = None
        slice_scatter_2605: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2602, slice_scatter_2604, 1, 3472, 3488);  slice_scatter_2602 = slice_scatter_2604 = None
        slice_14336: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2605, 1, 3472, 3488)
        slice_14337: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14336, 2, 0, 16)
        slice_scatter_2607: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14336, slice_14337, 2, 0, 16);  slice_14336 = slice_14337 = None
        slice_scatter_2608: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2605, slice_scatter_2607, 1, 3472, 3488);  slice_scatter_2605 = slice_scatter_2607 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14357: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_14323, 2, 16, 32);  slice_14323 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_438: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_14357, memory_format = torch.contiguous_format);  slice_14357 = None
        view_880: "f32[32, 11]" = torch.ops.aten.view.default(clone_438, [32, 11]);  clone_438 = None
        mm_435: "f32[32, 8]" = torch.ops.aten.mm.default(view_880, slice_37)
        view_881: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_435, [2, 16, 8]);  mm_435 = None
        slice_14364: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2608, 1, 3472, 3488)
        slice_14365: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14364, 2, 0, 16)
        add_437: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14365, view_881);  slice_14365 = view_881 = None
        slice_scatter_2610: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14364, add_437, 2, 0, 16);  slice_14364 = add_437 = None
        slice_scatter_2611: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2608, slice_scatter_2610, 1, 3472, 3488);  slice_scatter_2608 = slice_scatter_2610 = None
        slice_14369: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2611, 1, 3472, 3488)
        slice_14370: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14369, 2, 0, 16)
        slice_scatter_2613: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14369, slice_14370, 2, 0, 16);  slice_14369 = slice_14370 = None
        slice_scatter_2614: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2611, slice_scatter_2613, 1, 3472, 3488);  slice_scatter_2611 = slice_scatter_2613 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14389: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3488, 3504)
        slice_14390: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_14389, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_439: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_14390, memory_format = torch.contiguous_format);  slice_14390 = None
        view_882: "f32[32, 16]" = torch.ops.aten.view.default(clone_439, [32, 16]);  clone_439 = None
        mm_436: "f32[32, 8]" = torch.ops.aten.mm.default(view_882, slice_7)
        view_883: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_436, [2, 16, 8]);  mm_436 = None
        slice_14397: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2614, 1, 3488, 3504)
        slice_14398: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14397, 2, 0, 16)
        add_438: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14398, view_883);  slice_14398 = view_883 = None
        slice_scatter_2616: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14397, add_438, 2, 0, 16);  slice_14397 = add_438 = None
        slice_scatter_2617: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2614, slice_scatter_2616, 1, 3488, 3504);  slice_scatter_2614 = slice_scatter_2616 = None
        slice_14402: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2617, 1, 3488, 3504)
        slice_14403: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14402, 2, 0, 16)
        slice_scatter_2619: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14402, slice_14403, 2, 0, 16);  slice_14402 = slice_14403 = None
        slice_scatter_2620: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2617, slice_scatter_2619, 1, 3488, 3504);  slice_scatter_2617 = slice_scatter_2619 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14423: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_14389, 2, 16, 32);  slice_14389 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_440: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_14423, memory_format = torch.contiguous_format);  slice_14423 = None
        view_884: "f32[32, 11]" = torch.ops.aten.view.default(clone_440, [32, 11]);  clone_440 = None
        mm_437: "f32[32, 8]" = torch.ops.aten.mm.default(view_884, slice_37)
        view_885: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_437, [2, 16, 8]);  mm_437 = None
        slice_14430: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2620, 1, 3488, 3504)
        slice_14431: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14430, 2, 0, 16)
        add_439: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14431, view_885);  slice_14431 = view_885 = None
        slice_scatter_2622: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14430, add_439, 2, 0, 16);  slice_14430 = add_439 = None
        slice_scatter_2623: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2620, slice_scatter_2622, 1, 3488, 3504);  slice_scatter_2620 = slice_scatter_2622 = None
        slice_14435: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2623, 1, 3488, 3504)
        slice_14436: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14435, 2, 0, 16)
        slice_scatter_2625: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14435, slice_14436, 2, 0, 16);  slice_14435 = slice_14436 = None
        slice_scatter_2626: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2623, slice_scatter_2625, 1, 3488, 3504);  slice_scatter_2623 = slice_scatter_2625 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14455: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3504, 3520)
        slice_14456: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_14455, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_441: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_14456, memory_format = torch.contiguous_format);  slice_14456 = None
        view_886: "f32[32, 16]" = torch.ops.aten.view.default(clone_441, [32, 16]);  clone_441 = None
        mm_438: "f32[32, 8]" = torch.ops.aten.mm.default(view_886, slice_7)
        view_887: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_438, [2, 16, 8]);  mm_438 = None
        slice_14463: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2626, 1, 3504, 3520)
        slice_14464: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14463, 2, 0, 16)
        add_440: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14464, view_887);  slice_14464 = view_887 = None
        slice_scatter_2628: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14463, add_440, 2, 0, 16);  slice_14463 = add_440 = None
        slice_scatter_2629: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2626, slice_scatter_2628, 1, 3504, 3520);  slice_scatter_2626 = slice_scatter_2628 = None
        slice_14468: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2629, 1, 3504, 3520)
        slice_14469: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14468, 2, 0, 16)
        slice_scatter_2631: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14468, slice_14469, 2, 0, 16);  slice_14468 = slice_14469 = None
        slice_scatter_2632: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2629, slice_scatter_2631, 1, 3504, 3520);  slice_scatter_2629 = slice_scatter_2631 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14489: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_14455, 2, 16, 32);  slice_14455 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_442: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_14489, memory_format = torch.contiguous_format);  slice_14489 = None
        view_888: "f32[32, 11]" = torch.ops.aten.view.default(clone_442, [32, 11]);  clone_442 = None
        mm_439: "f32[32, 8]" = torch.ops.aten.mm.default(view_888, slice_37)
        view_889: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_439, [2, 16, 8]);  mm_439 = None
        slice_14496: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2632, 1, 3504, 3520)
        slice_14497: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14496, 2, 0, 16)
        add_441: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14497, view_889);  slice_14497 = view_889 = None
        slice_scatter_2634: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14496, add_441, 2, 0, 16);  slice_14496 = add_441 = None
        slice_scatter_2635: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2632, slice_scatter_2634, 1, 3504, 3520);  slice_scatter_2632 = slice_scatter_2634 = None
        slice_14501: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2635, 1, 3504, 3520)
        slice_14502: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14501, 2, 0, 16)
        slice_scatter_2637: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14501, slice_14502, 2, 0, 16);  slice_14501 = slice_14502 = None
        slice_scatter_2638: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2635, slice_scatter_2637, 1, 3504, 3520);  slice_scatter_2635 = slice_scatter_2637 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14521: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3520, 3536)
        slice_14522: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_14521, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_443: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_14522, memory_format = torch.contiguous_format);  slice_14522 = None
        view_890: "f32[32, 16]" = torch.ops.aten.view.default(clone_443, [32, 16]);  clone_443 = None
        mm_440: "f32[32, 8]" = torch.ops.aten.mm.default(view_890, slice_7)
        view_891: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_440, [2, 16, 8]);  mm_440 = None
        slice_14529: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2638, 1, 3520, 3536)
        slice_14530: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14529, 2, 0, 16)
        add_442: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14530, view_891);  slice_14530 = view_891 = None
        slice_scatter_2640: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14529, add_442, 2, 0, 16);  slice_14529 = add_442 = None
        slice_scatter_2641: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2638, slice_scatter_2640, 1, 3520, 3536);  slice_scatter_2638 = slice_scatter_2640 = None
        slice_14534: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2641, 1, 3520, 3536)
        slice_14535: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14534, 2, 0, 16)
        slice_scatter_2643: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14534, slice_14535, 2, 0, 16);  slice_14534 = slice_14535 = None
        slice_scatter_2644: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2641, slice_scatter_2643, 1, 3520, 3536);  slice_scatter_2641 = slice_scatter_2643 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14555: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_14521, 2, 16, 32);  slice_14521 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_444: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_14555, memory_format = torch.contiguous_format);  slice_14555 = None
        view_892: "f32[32, 11]" = torch.ops.aten.view.default(clone_444, [32, 11]);  clone_444 = None
        mm_441: "f32[32, 8]" = torch.ops.aten.mm.default(view_892, slice_37)
        view_893: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_441, [2, 16, 8]);  mm_441 = None
        slice_14562: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2644, 1, 3520, 3536)
        slice_14563: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14562, 2, 0, 16)
        add_443: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14563, view_893);  slice_14563 = view_893 = None
        slice_scatter_2646: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14562, add_443, 2, 0, 16);  slice_14562 = add_443 = None
        slice_scatter_2647: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2644, slice_scatter_2646, 1, 3520, 3536);  slice_scatter_2644 = slice_scatter_2646 = None
        slice_14567: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2647, 1, 3520, 3536)
        slice_14568: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14567, 2, 0, 16)
        slice_scatter_2649: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14567, slice_14568, 2, 0, 16);  slice_14567 = slice_14568 = None
        slice_scatter_2650: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2647, slice_scatter_2649, 1, 3520, 3536);  slice_scatter_2647 = slice_scatter_2649 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14587: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3536, 3552)
        slice_14588: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_14587, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_445: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_14588, memory_format = torch.contiguous_format);  slice_14588 = None
        view_894: "f32[32, 16]" = torch.ops.aten.view.default(clone_445, [32, 16]);  clone_445 = None
        mm_442: "f32[32, 8]" = torch.ops.aten.mm.default(view_894, slice_7)
        view_895: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_442, [2, 16, 8]);  mm_442 = None
        slice_14595: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2650, 1, 3536, 3552)
        slice_14596: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14595, 2, 0, 16)
        add_444: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14596, view_895);  slice_14596 = view_895 = None
        slice_scatter_2652: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14595, add_444, 2, 0, 16);  slice_14595 = add_444 = None
        slice_scatter_2653: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2650, slice_scatter_2652, 1, 3536, 3552);  slice_scatter_2650 = slice_scatter_2652 = None
        slice_14600: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2653, 1, 3536, 3552)
        slice_14601: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14600, 2, 0, 16)
        slice_scatter_2655: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14600, slice_14601, 2, 0, 16);  slice_14600 = slice_14601 = None
        slice_scatter_2656: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2653, slice_scatter_2655, 1, 3536, 3552);  slice_scatter_2653 = slice_scatter_2655 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14621: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_14587, 2, 16, 32);  slice_14587 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_446: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_14621, memory_format = torch.contiguous_format);  slice_14621 = None
        view_896: "f32[32, 11]" = torch.ops.aten.view.default(clone_446, [32, 11]);  clone_446 = None
        mm_443: "f32[32, 8]" = torch.ops.aten.mm.default(view_896, slice_37)
        view_897: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_443, [2, 16, 8]);  mm_443 = None
        slice_14628: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2656, 1, 3536, 3552)
        slice_14629: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14628, 2, 0, 16)
        add_445: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14629, view_897);  slice_14629 = view_897 = None
        slice_scatter_2658: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14628, add_445, 2, 0, 16);  slice_14628 = add_445 = None
        slice_scatter_2659: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2656, slice_scatter_2658, 1, 3536, 3552);  slice_scatter_2656 = slice_scatter_2658 = None
        slice_14633: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2659, 1, 3536, 3552)
        slice_14634: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14633, 2, 0, 16)
        slice_scatter_2661: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14633, slice_14634, 2, 0, 16);  slice_14633 = slice_14634 = None
        slice_scatter_2662: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2659, slice_scatter_2661, 1, 3536, 3552);  slice_scatter_2659 = slice_scatter_2661 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14653: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3552, 3568)
        slice_14654: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_14653, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_447: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_14654, memory_format = torch.contiguous_format);  slice_14654 = None
        view_898: "f32[32, 16]" = torch.ops.aten.view.default(clone_447, [32, 16]);  clone_447 = None
        mm_444: "f32[32, 8]" = torch.ops.aten.mm.default(view_898, slice_7)
        view_899: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_444, [2, 16, 8]);  mm_444 = None
        slice_14661: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2662, 1, 3552, 3568)
        slice_14662: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14661, 2, 0, 16)
        add_446: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14662, view_899);  slice_14662 = view_899 = None
        slice_scatter_2664: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14661, add_446, 2, 0, 16);  slice_14661 = add_446 = None
        slice_scatter_2665: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2662, slice_scatter_2664, 1, 3552, 3568);  slice_scatter_2662 = slice_scatter_2664 = None
        slice_14666: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2665, 1, 3552, 3568)
        slice_14667: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14666, 2, 0, 16)
        slice_scatter_2667: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14666, slice_14667, 2, 0, 16);  slice_14666 = slice_14667 = None
        slice_scatter_2668: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2665, slice_scatter_2667, 1, 3552, 3568);  slice_scatter_2665 = slice_scatter_2667 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14687: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_14653, 2, 16, 32);  slice_14653 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_448: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_14687, memory_format = torch.contiguous_format);  slice_14687 = None
        view_900: "f32[32, 11]" = torch.ops.aten.view.default(clone_448, [32, 11]);  clone_448 = None
        mm_445: "f32[32, 8]" = torch.ops.aten.mm.default(view_900, slice_37)
        view_901: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_445, [2, 16, 8]);  mm_445 = None
        slice_14694: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2668, 1, 3552, 3568)
        slice_14695: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14694, 2, 0, 16)
        add_447: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14695, view_901);  slice_14695 = view_901 = None
        slice_scatter_2670: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14694, add_447, 2, 0, 16);  slice_14694 = add_447 = None
        slice_scatter_2671: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2668, slice_scatter_2670, 1, 3552, 3568);  slice_scatter_2668 = slice_scatter_2670 = None
        slice_14699: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2671, 1, 3552, 3568)
        slice_14700: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14699, 2, 0, 16)
        slice_scatter_2673: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14699, slice_14700, 2, 0, 16);  slice_14699 = slice_14700 = None
        slice_scatter_2674: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2671, slice_scatter_2673, 1, 3552, 3568);  slice_scatter_2671 = slice_scatter_2673 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14719: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3568, 3584)
        slice_14720: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_14719, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_449: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_14720, memory_format = torch.contiguous_format);  slice_14720 = None
        view_902: "f32[32, 16]" = torch.ops.aten.view.default(clone_449, [32, 16]);  clone_449 = None
        mm_446: "f32[32, 8]" = torch.ops.aten.mm.default(view_902, slice_7)
        view_903: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_446, [2, 16, 8]);  mm_446 = None
        slice_14727: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2674, 1, 3568, 3584)
        slice_14728: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14727, 2, 0, 16)
        add_448: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14728, view_903);  slice_14728 = view_903 = None
        slice_scatter_2676: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14727, add_448, 2, 0, 16);  slice_14727 = add_448 = None
        slice_scatter_2677: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2674, slice_scatter_2676, 1, 3568, 3584);  slice_scatter_2674 = slice_scatter_2676 = None
        slice_14732: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2677, 1, 3568, 3584)
        slice_14733: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14732, 2, 0, 16)
        slice_scatter_2679: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14732, slice_14733, 2, 0, 16);  slice_14732 = slice_14733 = None
        slice_scatter_2680: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2677, slice_scatter_2679, 1, 3568, 3584);  slice_scatter_2677 = slice_scatter_2679 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14753: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_14719, 2, 16, 32);  slice_14719 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_450: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_14753, memory_format = torch.contiguous_format);  slice_14753 = None
        view_904: "f32[32, 11]" = torch.ops.aten.view.default(clone_450, [32, 11]);  clone_450 = None
        mm_447: "f32[32, 8]" = torch.ops.aten.mm.default(view_904, slice_37)
        view_905: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_447, [2, 16, 8]);  mm_447 = None
        slice_14760: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2680, 1, 3568, 3584)
        slice_14761: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14760, 2, 0, 16)
        add_449: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14761, view_905);  slice_14761 = view_905 = None
        slice_scatter_2682: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14760, add_449, 2, 0, 16);  slice_14760 = add_449 = None
        slice_scatter_2683: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2680, slice_scatter_2682, 1, 3568, 3584);  slice_scatter_2680 = slice_scatter_2682 = None
        slice_14765: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2683, 1, 3568, 3584)
        slice_14766: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14765, 2, 0, 16)
        slice_scatter_2685: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14765, slice_14766, 2, 0, 16);  slice_14765 = slice_14766 = None
        slice_scatter_2686: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2683, slice_scatter_2685, 1, 3568, 3584);  slice_scatter_2683 = slice_scatter_2685 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14785: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3584, 3600)
        slice_14786: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_14785, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_451: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_14786, memory_format = torch.contiguous_format);  slice_14786 = None
        view_906: "f32[32, 16]" = torch.ops.aten.view.default(clone_451, [32, 16]);  clone_451 = None
        mm_448: "f32[32, 8]" = torch.ops.aten.mm.default(view_906, slice_7)
        view_907: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_448, [2, 16, 8]);  mm_448 = None
        slice_14793: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2686, 1, 3584, 3600)
        slice_14794: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14793, 2, 0, 16)
        add_450: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14794, view_907);  slice_14794 = view_907 = None
        slice_scatter_2688: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14793, add_450, 2, 0, 16);  slice_14793 = add_450 = None
        slice_scatter_2689: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2686, slice_scatter_2688, 1, 3584, 3600);  slice_scatter_2686 = slice_scatter_2688 = None
        slice_14798: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2689, 1, 3584, 3600)
        slice_14799: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14798, 2, 0, 16)
        slice_scatter_2691: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14798, slice_14799, 2, 0, 16);  slice_14798 = slice_14799 = None
        slice_scatter_2692: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2689, slice_scatter_2691, 1, 3584, 3600);  slice_scatter_2689 = slice_scatter_2691 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14819: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_14785, 2, 16, 32);  slice_14785 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_452: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_14819, memory_format = torch.contiguous_format);  slice_14819 = None
        view_908: "f32[32, 11]" = torch.ops.aten.view.default(clone_452, [32, 11]);  clone_452 = None
        mm_449: "f32[32, 8]" = torch.ops.aten.mm.default(view_908, slice_37)
        view_909: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_449, [2, 16, 8]);  mm_449 = None
        slice_14826: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2692, 1, 3584, 3600)
        slice_14827: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14826, 2, 0, 16)
        add_451: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14827, view_909);  slice_14827 = view_909 = None
        slice_scatter_2694: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14826, add_451, 2, 0, 16);  slice_14826 = add_451 = None
        slice_scatter_2695: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2692, slice_scatter_2694, 1, 3584, 3600);  slice_scatter_2692 = slice_scatter_2694 = None
        slice_14831: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2695, 1, 3584, 3600)
        slice_14832: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14831, 2, 0, 16)
        slice_scatter_2697: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14831, slice_14832, 2, 0, 16);  slice_14831 = slice_14832 = None
        slice_scatter_2698: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2695, slice_scatter_2697, 1, 3584, 3600);  slice_scatter_2695 = slice_scatter_2697 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14851: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3600, 3616)
        slice_14852: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_14851, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_453: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_14852, memory_format = torch.contiguous_format);  slice_14852 = None
        view_910: "f32[32, 16]" = torch.ops.aten.view.default(clone_453, [32, 16]);  clone_453 = None
        mm_450: "f32[32, 8]" = torch.ops.aten.mm.default(view_910, slice_7)
        view_911: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_450, [2, 16, 8]);  mm_450 = None
        slice_14859: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2698, 1, 3600, 3616)
        slice_14860: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14859, 2, 0, 16)
        add_452: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14860, view_911);  slice_14860 = view_911 = None
        slice_scatter_2700: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14859, add_452, 2, 0, 16);  slice_14859 = add_452 = None
        slice_scatter_2701: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2698, slice_scatter_2700, 1, 3600, 3616);  slice_scatter_2698 = slice_scatter_2700 = None
        slice_14864: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2701, 1, 3600, 3616)
        slice_14865: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14864, 2, 0, 16)
        slice_scatter_2703: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14864, slice_14865, 2, 0, 16);  slice_14864 = slice_14865 = None
        slice_scatter_2704: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2701, slice_scatter_2703, 1, 3600, 3616);  slice_scatter_2701 = slice_scatter_2703 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14885: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_14851, 2, 16, 32);  slice_14851 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_454: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_14885, memory_format = torch.contiguous_format);  slice_14885 = None
        view_912: "f32[32, 11]" = torch.ops.aten.view.default(clone_454, [32, 11]);  clone_454 = None
        mm_451: "f32[32, 8]" = torch.ops.aten.mm.default(view_912, slice_37)
        view_913: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_451, [2, 16, 8]);  mm_451 = None
        slice_14892: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2704, 1, 3600, 3616)
        slice_14893: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14892, 2, 0, 16)
        add_453: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14893, view_913);  slice_14893 = view_913 = None
        slice_scatter_2706: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14892, add_453, 2, 0, 16);  slice_14892 = add_453 = None
        slice_scatter_2707: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2704, slice_scatter_2706, 1, 3600, 3616);  slice_scatter_2704 = slice_scatter_2706 = None
        slice_14897: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2707, 1, 3600, 3616)
        slice_14898: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14897, 2, 0, 16)
        slice_scatter_2709: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14897, slice_14898, 2, 0, 16);  slice_14897 = slice_14898 = None
        slice_scatter_2710: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2707, slice_scatter_2709, 1, 3600, 3616);  slice_scatter_2707 = slice_scatter_2709 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14917: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3616, 3632)
        slice_14918: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_14917, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_455: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_14918, memory_format = torch.contiguous_format);  slice_14918 = None
        view_914: "f32[32, 16]" = torch.ops.aten.view.default(clone_455, [32, 16]);  clone_455 = None
        mm_452: "f32[32, 8]" = torch.ops.aten.mm.default(view_914, slice_7)
        view_915: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_452, [2, 16, 8]);  mm_452 = None
        slice_14925: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2710, 1, 3616, 3632)
        slice_14926: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14925, 2, 0, 16)
        add_454: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14926, view_915);  slice_14926 = view_915 = None
        slice_scatter_2712: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14925, add_454, 2, 0, 16);  slice_14925 = add_454 = None
        slice_scatter_2713: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2710, slice_scatter_2712, 1, 3616, 3632);  slice_scatter_2710 = slice_scatter_2712 = None
        slice_14930: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2713, 1, 3616, 3632)
        slice_14931: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14930, 2, 0, 16)
        slice_scatter_2715: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14930, slice_14931, 2, 0, 16);  slice_14930 = slice_14931 = None
        slice_scatter_2716: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2713, slice_scatter_2715, 1, 3616, 3632);  slice_scatter_2713 = slice_scatter_2715 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14951: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_14917, 2, 16, 32);  slice_14917 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_456: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_14951, memory_format = torch.contiguous_format);  slice_14951 = None
        view_916: "f32[32, 11]" = torch.ops.aten.view.default(clone_456, [32, 11]);  clone_456 = None
        mm_453: "f32[32, 8]" = torch.ops.aten.mm.default(view_916, slice_37)
        view_917: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_453, [2, 16, 8]);  mm_453 = None
        slice_14958: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2716, 1, 3616, 3632)
        slice_14959: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14958, 2, 0, 16)
        add_455: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14959, view_917);  slice_14959 = view_917 = None
        slice_scatter_2718: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14958, add_455, 2, 0, 16);  slice_14958 = add_455 = None
        slice_scatter_2719: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2716, slice_scatter_2718, 1, 3616, 3632);  slice_scatter_2716 = slice_scatter_2718 = None
        slice_14963: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2719, 1, 3616, 3632)
        slice_14964: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14963, 2, 0, 16)
        slice_scatter_2721: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14963, slice_14964, 2, 0, 16);  slice_14963 = slice_14964 = None
        slice_scatter_2722: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2719, slice_scatter_2721, 1, 3616, 3632);  slice_scatter_2719 = slice_scatter_2721 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_14983: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3632, 3648)
        slice_14984: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_14983, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_457: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_14984, memory_format = torch.contiguous_format);  slice_14984 = None
        view_918: "f32[32, 16]" = torch.ops.aten.view.default(clone_457, [32, 16]);  clone_457 = None
        mm_454: "f32[32, 8]" = torch.ops.aten.mm.default(view_918, slice_7)
        view_919: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_454, [2, 16, 8]);  mm_454 = None
        slice_14991: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2722, 1, 3632, 3648)
        slice_14992: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14991, 2, 0, 16)
        add_456: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_14992, view_919);  slice_14992 = view_919 = None
        slice_scatter_2724: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14991, add_456, 2, 0, 16);  slice_14991 = add_456 = None
        slice_scatter_2725: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2722, slice_scatter_2724, 1, 3632, 3648);  slice_scatter_2722 = slice_scatter_2724 = None
        slice_14996: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2725, 1, 3632, 3648)
        slice_14997: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_14996, 2, 0, 16)
        slice_scatter_2727: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_14996, slice_14997, 2, 0, 16);  slice_14996 = slice_14997 = None
        slice_scatter_2728: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2725, slice_scatter_2727, 1, 3632, 3648);  slice_scatter_2725 = slice_scatter_2727 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15017: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_14983, 2, 16, 32);  slice_14983 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_458: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_15017, memory_format = torch.contiguous_format);  slice_15017 = None
        view_920: "f32[32, 11]" = torch.ops.aten.view.default(clone_458, [32, 11]);  clone_458 = None
        mm_455: "f32[32, 8]" = torch.ops.aten.mm.default(view_920, slice_37)
        view_921: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_455, [2, 16, 8]);  mm_455 = None
        slice_15024: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2728, 1, 3632, 3648)
        slice_15025: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15024, 2, 0, 16)
        add_457: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15025, view_921);  slice_15025 = view_921 = None
        slice_scatter_2730: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15024, add_457, 2, 0, 16);  slice_15024 = add_457 = None
        slice_scatter_2731: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2728, slice_scatter_2730, 1, 3632, 3648);  slice_scatter_2728 = slice_scatter_2730 = None
        slice_15029: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2731, 1, 3632, 3648)
        slice_15030: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15029, 2, 0, 16)
        slice_scatter_2733: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15029, slice_15030, 2, 0, 16);  slice_15029 = slice_15030 = None
        slice_scatter_2734: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2731, slice_scatter_2733, 1, 3632, 3648);  slice_scatter_2731 = slice_scatter_2733 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15049: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3648, 3664)
        slice_15050: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_15049, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_459: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_15050, memory_format = torch.contiguous_format);  slice_15050 = None
        view_922: "f32[32, 16]" = torch.ops.aten.view.default(clone_459, [32, 16]);  clone_459 = None
        mm_456: "f32[32, 8]" = torch.ops.aten.mm.default(view_922, slice_7)
        view_923: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_456, [2, 16, 8]);  mm_456 = None
        slice_15057: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2734, 1, 3648, 3664)
        slice_15058: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15057, 2, 0, 16)
        add_458: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15058, view_923);  slice_15058 = view_923 = None
        slice_scatter_2736: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15057, add_458, 2, 0, 16);  slice_15057 = add_458 = None
        slice_scatter_2737: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2734, slice_scatter_2736, 1, 3648, 3664);  slice_scatter_2734 = slice_scatter_2736 = None
        slice_15062: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2737, 1, 3648, 3664)
        slice_15063: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15062, 2, 0, 16)
        slice_scatter_2739: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15062, slice_15063, 2, 0, 16);  slice_15062 = slice_15063 = None
        slice_scatter_2740: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2737, slice_scatter_2739, 1, 3648, 3664);  slice_scatter_2737 = slice_scatter_2739 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15083: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_15049, 2, 16, 32);  slice_15049 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_460: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_15083, memory_format = torch.contiguous_format);  slice_15083 = None
        view_924: "f32[32, 11]" = torch.ops.aten.view.default(clone_460, [32, 11]);  clone_460 = None
        mm_457: "f32[32, 8]" = torch.ops.aten.mm.default(view_924, slice_37)
        view_925: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_457, [2, 16, 8]);  mm_457 = None
        slice_15090: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2740, 1, 3648, 3664)
        slice_15091: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15090, 2, 0, 16)
        add_459: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15091, view_925);  slice_15091 = view_925 = None
        slice_scatter_2742: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15090, add_459, 2, 0, 16);  slice_15090 = add_459 = None
        slice_scatter_2743: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2740, slice_scatter_2742, 1, 3648, 3664);  slice_scatter_2740 = slice_scatter_2742 = None
        slice_15095: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2743, 1, 3648, 3664)
        slice_15096: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15095, 2, 0, 16)
        slice_scatter_2745: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15095, slice_15096, 2, 0, 16);  slice_15095 = slice_15096 = None
        slice_scatter_2746: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2743, slice_scatter_2745, 1, 3648, 3664);  slice_scatter_2743 = slice_scatter_2745 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15115: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3664, 3680)
        slice_15116: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_15115, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_461: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_15116, memory_format = torch.contiguous_format);  slice_15116 = None
        view_926: "f32[32, 16]" = torch.ops.aten.view.default(clone_461, [32, 16]);  clone_461 = None
        mm_458: "f32[32, 8]" = torch.ops.aten.mm.default(view_926, slice_7)
        view_927: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_458, [2, 16, 8]);  mm_458 = None
        slice_15123: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2746, 1, 3664, 3680)
        slice_15124: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15123, 2, 0, 16)
        add_460: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15124, view_927);  slice_15124 = view_927 = None
        slice_scatter_2748: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15123, add_460, 2, 0, 16);  slice_15123 = add_460 = None
        slice_scatter_2749: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2746, slice_scatter_2748, 1, 3664, 3680);  slice_scatter_2746 = slice_scatter_2748 = None
        slice_15128: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2749, 1, 3664, 3680)
        slice_15129: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15128, 2, 0, 16)
        slice_scatter_2751: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15128, slice_15129, 2, 0, 16);  slice_15128 = slice_15129 = None
        slice_scatter_2752: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2749, slice_scatter_2751, 1, 3664, 3680);  slice_scatter_2749 = slice_scatter_2751 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15149: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_15115, 2, 16, 32);  slice_15115 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_462: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_15149, memory_format = torch.contiguous_format);  slice_15149 = None
        view_928: "f32[32, 11]" = torch.ops.aten.view.default(clone_462, [32, 11]);  clone_462 = None
        mm_459: "f32[32, 8]" = torch.ops.aten.mm.default(view_928, slice_37)
        view_929: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_459, [2, 16, 8]);  mm_459 = None
        slice_15156: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2752, 1, 3664, 3680)
        slice_15157: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15156, 2, 0, 16)
        add_461: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15157, view_929);  slice_15157 = view_929 = None
        slice_scatter_2754: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15156, add_461, 2, 0, 16);  slice_15156 = add_461 = None
        slice_scatter_2755: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2752, slice_scatter_2754, 1, 3664, 3680);  slice_scatter_2752 = slice_scatter_2754 = None
        slice_15161: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2755, 1, 3664, 3680)
        slice_15162: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15161, 2, 0, 16)
        slice_scatter_2757: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15161, slice_15162, 2, 0, 16);  slice_15161 = slice_15162 = None
        slice_scatter_2758: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2755, slice_scatter_2757, 1, 3664, 3680);  slice_scatter_2755 = slice_scatter_2757 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15181: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3680, 3696)
        slice_15182: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_15181, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_463: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_15182, memory_format = torch.contiguous_format);  slice_15182 = None
        view_930: "f32[32, 16]" = torch.ops.aten.view.default(clone_463, [32, 16]);  clone_463 = None
        mm_460: "f32[32, 8]" = torch.ops.aten.mm.default(view_930, slice_7)
        view_931: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_460, [2, 16, 8]);  mm_460 = None
        slice_15189: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2758, 1, 3680, 3696)
        slice_15190: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15189, 2, 0, 16)
        add_462: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15190, view_931);  slice_15190 = view_931 = None
        slice_scatter_2760: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15189, add_462, 2, 0, 16);  slice_15189 = add_462 = None
        slice_scatter_2761: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2758, slice_scatter_2760, 1, 3680, 3696);  slice_scatter_2758 = slice_scatter_2760 = None
        slice_15194: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2761, 1, 3680, 3696)
        slice_15195: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15194, 2, 0, 16)
        slice_scatter_2763: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15194, slice_15195, 2, 0, 16);  slice_15194 = slice_15195 = None
        slice_scatter_2764: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2761, slice_scatter_2763, 1, 3680, 3696);  slice_scatter_2761 = slice_scatter_2763 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15215: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_15181, 2, 16, 32);  slice_15181 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_464: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_15215, memory_format = torch.contiguous_format);  slice_15215 = None
        view_932: "f32[32, 11]" = torch.ops.aten.view.default(clone_464, [32, 11]);  clone_464 = None
        mm_461: "f32[32, 8]" = torch.ops.aten.mm.default(view_932, slice_37)
        view_933: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_461, [2, 16, 8]);  mm_461 = None
        slice_15222: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2764, 1, 3680, 3696)
        slice_15223: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15222, 2, 0, 16)
        add_463: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15223, view_933);  slice_15223 = view_933 = None
        slice_scatter_2766: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15222, add_463, 2, 0, 16);  slice_15222 = add_463 = None
        slice_scatter_2767: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2764, slice_scatter_2766, 1, 3680, 3696);  slice_scatter_2764 = slice_scatter_2766 = None
        slice_15227: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2767, 1, 3680, 3696)
        slice_15228: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15227, 2, 0, 16)
        slice_scatter_2769: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15227, slice_15228, 2, 0, 16);  slice_15227 = slice_15228 = None
        slice_scatter_2770: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2767, slice_scatter_2769, 1, 3680, 3696);  slice_scatter_2767 = slice_scatter_2769 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15247: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3696, 3712)
        slice_15248: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_15247, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_465: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_15248, memory_format = torch.contiguous_format);  slice_15248 = None
        view_934: "f32[32, 16]" = torch.ops.aten.view.default(clone_465, [32, 16]);  clone_465 = None
        mm_462: "f32[32, 8]" = torch.ops.aten.mm.default(view_934, slice_7)
        view_935: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_462, [2, 16, 8]);  mm_462 = None
        slice_15255: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2770, 1, 3696, 3712)
        slice_15256: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15255, 2, 0, 16)
        add_464: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15256, view_935);  slice_15256 = view_935 = None
        slice_scatter_2772: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15255, add_464, 2, 0, 16);  slice_15255 = add_464 = None
        slice_scatter_2773: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2770, slice_scatter_2772, 1, 3696, 3712);  slice_scatter_2770 = slice_scatter_2772 = None
        slice_15260: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2773, 1, 3696, 3712)
        slice_15261: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15260, 2, 0, 16)
        slice_scatter_2775: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15260, slice_15261, 2, 0, 16);  slice_15260 = slice_15261 = None
        slice_scatter_2776: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2773, slice_scatter_2775, 1, 3696, 3712);  slice_scatter_2773 = slice_scatter_2775 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15281: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_15247, 2, 16, 32);  slice_15247 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_466: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_15281, memory_format = torch.contiguous_format);  slice_15281 = None
        view_936: "f32[32, 11]" = torch.ops.aten.view.default(clone_466, [32, 11]);  clone_466 = None
        mm_463: "f32[32, 8]" = torch.ops.aten.mm.default(view_936, slice_37)
        view_937: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_463, [2, 16, 8]);  mm_463 = None
        slice_15288: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2776, 1, 3696, 3712)
        slice_15289: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15288, 2, 0, 16)
        add_465: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15289, view_937);  slice_15289 = view_937 = None
        slice_scatter_2778: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15288, add_465, 2, 0, 16);  slice_15288 = add_465 = None
        slice_scatter_2779: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2776, slice_scatter_2778, 1, 3696, 3712);  slice_scatter_2776 = slice_scatter_2778 = None
        slice_15293: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2779, 1, 3696, 3712)
        slice_15294: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15293, 2, 0, 16)
        slice_scatter_2781: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15293, slice_15294, 2, 0, 16);  slice_15293 = slice_15294 = None
        slice_scatter_2782: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2779, slice_scatter_2781, 1, 3696, 3712);  slice_scatter_2779 = slice_scatter_2781 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15313: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3712, 3728)
        slice_15314: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_15313, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_467: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_15314, memory_format = torch.contiguous_format);  slice_15314 = None
        view_938: "f32[32, 16]" = torch.ops.aten.view.default(clone_467, [32, 16]);  clone_467 = None
        mm_464: "f32[32, 8]" = torch.ops.aten.mm.default(view_938, slice_7)
        view_939: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_464, [2, 16, 8]);  mm_464 = None
        slice_15321: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2782, 1, 3712, 3728)
        slice_15322: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15321, 2, 0, 16)
        add_466: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15322, view_939);  slice_15322 = view_939 = None
        slice_scatter_2784: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15321, add_466, 2, 0, 16);  slice_15321 = add_466 = None
        slice_scatter_2785: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2782, slice_scatter_2784, 1, 3712, 3728);  slice_scatter_2782 = slice_scatter_2784 = None
        slice_15326: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2785, 1, 3712, 3728)
        slice_15327: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15326, 2, 0, 16)
        slice_scatter_2787: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15326, slice_15327, 2, 0, 16);  slice_15326 = slice_15327 = None
        slice_scatter_2788: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2785, slice_scatter_2787, 1, 3712, 3728);  slice_scatter_2785 = slice_scatter_2787 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15347: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_15313, 2, 16, 32);  slice_15313 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_468: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_15347, memory_format = torch.contiguous_format);  slice_15347 = None
        view_940: "f32[32, 11]" = torch.ops.aten.view.default(clone_468, [32, 11]);  clone_468 = None
        mm_465: "f32[32, 8]" = torch.ops.aten.mm.default(view_940, slice_37)
        view_941: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_465, [2, 16, 8]);  mm_465 = None
        slice_15354: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2788, 1, 3712, 3728)
        slice_15355: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15354, 2, 0, 16)
        add_467: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15355, view_941);  slice_15355 = view_941 = None
        slice_scatter_2790: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15354, add_467, 2, 0, 16);  slice_15354 = add_467 = None
        slice_scatter_2791: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2788, slice_scatter_2790, 1, 3712, 3728);  slice_scatter_2788 = slice_scatter_2790 = None
        slice_15359: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2791, 1, 3712, 3728)
        slice_15360: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15359, 2, 0, 16)
        slice_scatter_2793: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15359, slice_15360, 2, 0, 16);  slice_15359 = slice_15360 = None
        slice_scatter_2794: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2791, slice_scatter_2793, 1, 3712, 3728);  slice_scatter_2791 = slice_scatter_2793 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15379: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3728, 3744)
        slice_15380: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_15379, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_469: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_15380, memory_format = torch.contiguous_format);  slice_15380 = None
        view_942: "f32[32, 16]" = torch.ops.aten.view.default(clone_469, [32, 16]);  clone_469 = None
        mm_466: "f32[32, 8]" = torch.ops.aten.mm.default(view_942, slice_7)
        view_943: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_466, [2, 16, 8]);  mm_466 = None
        slice_15387: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2794, 1, 3728, 3744)
        slice_15388: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15387, 2, 0, 16)
        add_468: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15388, view_943);  slice_15388 = view_943 = None
        slice_scatter_2796: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15387, add_468, 2, 0, 16);  slice_15387 = add_468 = None
        slice_scatter_2797: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2794, slice_scatter_2796, 1, 3728, 3744);  slice_scatter_2794 = slice_scatter_2796 = None
        slice_15392: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2797, 1, 3728, 3744)
        slice_15393: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15392, 2, 0, 16)
        slice_scatter_2799: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15392, slice_15393, 2, 0, 16);  slice_15392 = slice_15393 = None
        slice_scatter_2800: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2797, slice_scatter_2799, 1, 3728, 3744);  slice_scatter_2797 = slice_scatter_2799 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15413: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_15379, 2, 16, 32);  slice_15379 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_470: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_15413, memory_format = torch.contiguous_format);  slice_15413 = None
        view_944: "f32[32, 11]" = torch.ops.aten.view.default(clone_470, [32, 11]);  clone_470 = None
        mm_467: "f32[32, 8]" = torch.ops.aten.mm.default(view_944, slice_37)
        view_945: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_467, [2, 16, 8]);  mm_467 = None
        slice_15420: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2800, 1, 3728, 3744)
        slice_15421: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15420, 2, 0, 16)
        add_469: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15421, view_945);  slice_15421 = view_945 = None
        slice_scatter_2802: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15420, add_469, 2, 0, 16);  slice_15420 = add_469 = None
        slice_scatter_2803: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2800, slice_scatter_2802, 1, 3728, 3744);  slice_scatter_2800 = slice_scatter_2802 = None
        slice_15425: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2803, 1, 3728, 3744)
        slice_15426: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15425, 2, 0, 16)
        slice_scatter_2805: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15425, slice_15426, 2, 0, 16);  slice_15425 = slice_15426 = None
        slice_scatter_2806: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2803, slice_scatter_2805, 1, 3728, 3744);  slice_scatter_2803 = slice_scatter_2805 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15445: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3744, 3760)
        slice_15446: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_15445, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_471: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_15446, memory_format = torch.contiguous_format);  slice_15446 = None
        view_946: "f32[32, 16]" = torch.ops.aten.view.default(clone_471, [32, 16]);  clone_471 = None
        mm_468: "f32[32, 8]" = torch.ops.aten.mm.default(view_946, slice_7)
        view_947: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_468, [2, 16, 8]);  mm_468 = None
        slice_15453: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2806, 1, 3744, 3760)
        slice_15454: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15453, 2, 0, 16)
        add_470: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15454, view_947);  slice_15454 = view_947 = None
        slice_scatter_2808: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15453, add_470, 2, 0, 16);  slice_15453 = add_470 = None
        slice_scatter_2809: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2806, slice_scatter_2808, 1, 3744, 3760);  slice_scatter_2806 = slice_scatter_2808 = None
        slice_15458: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2809, 1, 3744, 3760)
        slice_15459: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15458, 2, 0, 16)
        slice_scatter_2811: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15458, slice_15459, 2, 0, 16);  slice_15458 = slice_15459 = None
        slice_scatter_2812: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2809, slice_scatter_2811, 1, 3744, 3760);  slice_scatter_2809 = slice_scatter_2811 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15479: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_15445, 2, 16, 32);  slice_15445 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_472: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_15479, memory_format = torch.contiguous_format);  slice_15479 = None
        view_948: "f32[32, 11]" = torch.ops.aten.view.default(clone_472, [32, 11]);  clone_472 = None
        mm_469: "f32[32, 8]" = torch.ops.aten.mm.default(view_948, slice_37)
        view_949: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_469, [2, 16, 8]);  mm_469 = None
        slice_15486: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2812, 1, 3744, 3760)
        slice_15487: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15486, 2, 0, 16)
        add_471: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15487, view_949);  slice_15487 = view_949 = None
        slice_scatter_2814: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15486, add_471, 2, 0, 16);  slice_15486 = add_471 = None
        slice_scatter_2815: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2812, slice_scatter_2814, 1, 3744, 3760);  slice_scatter_2812 = slice_scatter_2814 = None
        slice_15491: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2815, 1, 3744, 3760)
        slice_15492: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15491, 2, 0, 16)
        slice_scatter_2817: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15491, slice_15492, 2, 0, 16);  slice_15491 = slice_15492 = None
        slice_scatter_2818: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2815, slice_scatter_2817, 1, 3744, 3760);  slice_scatter_2815 = slice_scatter_2817 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15511: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3760, 3776)
        slice_15512: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_15511, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_473: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_15512, memory_format = torch.contiguous_format);  slice_15512 = None
        view_950: "f32[32, 16]" = torch.ops.aten.view.default(clone_473, [32, 16]);  clone_473 = None
        mm_470: "f32[32, 8]" = torch.ops.aten.mm.default(view_950, slice_7)
        view_951: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_470, [2, 16, 8]);  mm_470 = None
        slice_15519: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2818, 1, 3760, 3776)
        slice_15520: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15519, 2, 0, 16)
        add_472: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15520, view_951);  slice_15520 = view_951 = None
        slice_scatter_2820: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15519, add_472, 2, 0, 16);  slice_15519 = add_472 = None
        slice_scatter_2821: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2818, slice_scatter_2820, 1, 3760, 3776);  slice_scatter_2818 = slice_scatter_2820 = None
        slice_15524: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2821, 1, 3760, 3776)
        slice_15525: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15524, 2, 0, 16)
        slice_scatter_2823: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15524, slice_15525, 2, 0, 16);  slice_15524 = slice_15525 = None
        slice_scatter_2824: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2821, slice_scatter_2823, 1, 3760, 3776);  slice_scatter_2821 = slice_scatter_2823 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15545: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_15511, 2, 16, 32);  slice_15511 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_474: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_15545, memory_format = torch.contiguous_format);  slice_15545 = None
        view_952: "f32[32, 11]" = torch.ops.aten.view.default(clone_474, [32, 11]);  clone_474 = None
        mm_471: "f32[32, 8]" = torch.ops.aten.mm.default(view_952, slice_37)
        view_953: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_471, [2, 16, 8]);  mm_471 = None
        slice_15552: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2824, 1, 3760, 3776)
        slice_15553: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15552, 2, 0, 16)
        add_473: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15553, view_953);  slice_15553 = view_953 = None
        slice_scatter_2826: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15552, add_473, 2, 0, 16);  slice_15552 = add_473 = None
        slice_scatter_2827: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2824, slice_scatter_2826, 1, 3760, 3776);  slice_scatter_2824 = slice_scatter_2826 = None
        slice_15557: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2827, 1, 3760, 3776)
        slice_15558: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15557, 2, 0, 16)
        slice_scatter_2829: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15557, slice_15558, 2, 0, 16);  slice_15557 = slice_15558 = None
        slice_scatter_2830: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2827, slice_scatter_2829, 1, 3760, 3776);  slice_scatter_2827 = slice_scatter_2829 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15577: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3776, 3792)
        slice_15578: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_15577, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_475: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_15578, memory_format = torch.contiguous_format);  slice_15578 = None
        view_954: "f32[32, 16]" = torch.ops.aten.view.default(clone_475, [32, 16]);  clone_475 = None
        mm_472: "f32[32, 8]" = torch.ops.aten.mm.default(view_954, slice_7)
        view_955: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_472, [2, 16, 8]);  mm_472 = None
        slice_15585: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2830, 1, 3776, 3792)
        slice_15586: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15585, 2, 0, 16)
        add_474: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15586, view_955);  slice_15586 = view_955 = None
        slice_scatter_2832: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15585, add_474, 2, 0, 16);  slice_15585 = add_474 = None
        slice_scatter_2833: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2830, slice_scatter_2832, 1, 3776, 3792);  slice_scatter_2830 = slice_scatter_2832 = None
        slice_15590: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2833, 1, 3776, 3792)
        slice_15591: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15590, 2, 0, 16)
        slice_scatter_2835: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15590, slice_15591, 2, 0, 16);  slice_15590 = slice_15591 = None
        slice_scatter_2836: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2833, slice_scatter_2835, 1, 3776, 3792);  slice_scatter_2833 = slice_scatter_2835 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15611: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_15577, 2, 16, 32);  slice_15577 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_476: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_15611, memory_format = torch.contiguous_format);  slice_15611 = None
        view_956: "f32[32, 11]" = torch.ops.aten.view.default(clone_476, [32, 11]);  clone_476 = None
        mm_473: "f32[32, 8]" = torch.ops.aten.mm.default(view_956, slice_37)
        view_957: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_473, [2, 16, 8]);  mm_473 = None
        slice_15618: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2836, 1, 3776, 3792)
        slice_15619: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15618, 2, 0, 16)
        add_475: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15619, view_957);  slice_15619 = view_957 = None
        slice_scatter_2838: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15618, add_475, 2, 0, 16);  slice_15618 = add_475 = None
        slice_scatter_2839: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2836, slice_scatter_2838, 1, 3776, 3792);  slice_scatter_2836 = slice_scatter_2838 = None
        slice_15623: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2839, 1, 3776, 3792)
        slice_15624: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15623, 2, 0, 16)
        slice_scatter_2841: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15623, slice_15624, 2, 0, 16);  slice_15623 = slice_15624 = None
        slice_scatter_2842: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2839, slice_scatter_2841, 1, 3776, 3792);  slice_scatter_2839 = slice_scatter_2841 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15643: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3792, 3808)
        slice_15644: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_15643, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_477: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_15644, memory_format = torch.contiguous_format);  slice_15644 = None
        view_958: "f32[32, 16]" = torch.ops.aten.view.default(clone_477, [32, 16]);  clone_477 = None
        mm_474: "f32[32, 8]" = torch.ops.aten.mm.default(view_958, slice_7)
        view_959: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_474, [2, 16, 8]);  mm_474 = None
        slice_15651: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2842, 1, 3792, 3808)
        slice_15652: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15651, 2, 0, 16)
        add_476: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15652, view_959);  slice_15652 = view_959 = None
        slice_scatter_2844: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15651, add_476, 2, 0, 16);  slice_15651 = add_476 = None
        slice_scatter_2845: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2842, slice_scatter_2844, 1, 3792, 3808);  slice_scatter_2842 = slice_scatter_2844 = None
        slice_15656: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2845, 1, 3792, 3808)
        slice_15657: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15656, 2, 0, 16)
        slice_scatter_2847: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15656, slice_15657, 2, 0, 16);  slice_15656 = slice_15657 = None
        slice_scatter_2848: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2845, slice_scatter_2847, 1, 3792, 3808);  slice_scatter_2845 = slice_scatter_2847 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15677: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_15643, 2, 16, 32);  slice_15643 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_478: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_15677, memory_format = torch.contiguous_format);  slice_15677 = None
        view_960: "f32[32, 11]" = torch.ops.aten.view.default(clone_478, [32, 11]);  clone_478 = None
        mm_475: "f32[32, 8]" = torch.ops.aten.mm.default(view_960, slice_37)
        view_961: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_475, [2, 16, 8]);  mm_475 = None
        slice_15684: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2848, 1, 3792, 3808)
        slice_15685: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15684, 2, 0, 16)
        add_477: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15685, view_961);  slice_15685 = view_961 = None
        slice_scatter_2850: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15684, add_477, 2, 0, 16);  slice_15684 = add_477 = None
        slice_scatter_2851: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2848, slice_scatter_2850, 1, 3792, 3808);  slice_scatter_2848 = slice_scatter_2850 = None
        slice_15689: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2851, 1, 3792, 3808)
        slice_15690: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15689, 2, 0, 16)
        slice_scatter_2853: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15689, slice_15690, 2, 0, 16);  slice_15689 = slice_15690 = None
        slice_scatter_2854: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2851, slice_scatter_2853, 1, 3792, 3808);  slice_scatter_2851 = slice_scatter_2853 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15709: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3808, 3824)
        slice_15710: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_15709, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_479: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_15710, memory_format = torch.contiguous_format);  slice_15710 = None
        view_962: "f32[32, 16]" = torch.ops.aten.view.default(clone_479, [32, 16]);  clone_479 = None
        mm_476: "f32[32, 8]" = torch.ops.aten.mm.default(view_962, slice_7)
        view_963: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_476, [2, 16, 8]);  mm_476 = None
        slice_15717: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2854, 1, 3808, 3824)
        slice_15718: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15717, 2, 0, 16)
        add_478: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15718, view_963);  slice_15718 = view_963 = None
        slice_scatter_2856: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15717, add_478, 2, 0, 16);  slice_15717 = add_478 = None
        slice_scatter_2857: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2854, slice_scatter_2856, 1, 3808, 3824);  slice_scatter_2854 = slice_scatter_2856 = None
        slice_15722: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2857, 1, 3808, 3824)
        slice_15723: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15722, 2, 0, 16)
        slice_scatter_2859: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15722, slice_15723, 2, 0, 16);  slice_15722 = slice_15723 = None
        slice_scatter_2860: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2857, slice_scatter_2859, 1, 3808, 3824);  slice_scatter_2857 = slice_scatter_2859 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15743: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_15709, 2, 16, 32);  slice_15709 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_480: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_15743, memory_format = torch.contiguous_format);  slice_15743 = None
        view_964: "f32[32, 11]" = torch.ops.aten.view.default(clone_480, [32, 11]);  clone_480 = None
        mm_477: "f32[32, 8]" = torch.ops.aten.mm.default(view_964, slice_37)
        view_965: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_477, [2, 16, 8]);  mm_477 = None
        slice_15750: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2860, 1, 3808, 3824)
        slice_15751: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15750, 2, 0, 16)
        add_479: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15751, view_965);  slice_15751 = view_965 = None
        slice_scatter_2862: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15750, add_479, 2, 0, 16);  slice_15750 = add_479 = None
        slice_scatter_2863: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2860, slice_scatter_2862, 1, 3808, 3824);  slice_scatter_2860 = slice_scatter_2862 = None
        slice_15755: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2863, 1, 3808, 3824)
        slice_15756: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15755, 2, 0, 16)
        slice_scatter_2865: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15755, slice_15756, 2, 0, 16);  slice_15755 = slice_15756 = None
        slice_scatter_2866: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2863, slice_scatter_2865, 1, 3808, 3824);  slice_scatter_2863 = slice_scatter_2865 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15775: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3824, 3840)
        slice_15776: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_15775, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_481: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_15776, memory_format = torch.contiguous_format);  slice_15776 = None
        view_966: "f32[32, 16]" = torch.ops.aten.view.default(clone_481, [32, 16]);  clone_481 = None
        mm_478: "f32[32, 8]" = torch.ops.aten.mm.default(view_966, slice_7)
        view_967: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_478, [2, 16, 8]);  mm_478 = None
        slice_15783: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2866, 1, 3824, 3840)
        slice_15784: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15783, 2, 0, 16)
        add_480: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15784, view_967);  slice_15784 = view_967 = None
        slice_scatter_2868: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15783, add_480, 2, 0, 16);  slice_15783 = add_480 = None
        slice_scatter_2869: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2866, slice_scatter_2868, 1, 3824, 3840);  slice_scatter_2866 = slice_scatter_2868 = None
        slice_15788: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2869, 1, 3824, 3840)
        slice_15789: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15788, 2, 0, 16)
        slice_scatter_2871: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15788, slice_15789, 2, 0, 16);  slice_15788 = slice_15789 = None
        slice_scatter_2872: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2869, slice_scatter_2871, 1, 3824, 3840);  slice_scatter_2869 = slice_scatter_2871 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15809: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_15775, 2, 16, 32);  slice_15775 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_482: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_15809, memory_format = torch.contiguous_format);  slice_15809 = None
        view_968: "f32[32, 11]" = torch.ops.aten.view.default(clone_482, [32, 11]);  clone_482 = None
        mm_479: "f32[32, 8]" = torch.ops.aten.mm.default(view_968, slice_37)
        view_969: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_479, [2, 16, 8]);  mm_479 = None
        slice_15816: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2872, 1, 3824, 3840)
        slice_15817: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15816, 2, 0, 16)
        add_481: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15817, view_969);  slice_15817 = view_969 = None
        slice_scatter_2874: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15816, add_481, 2, 0, 16);  slice_15816 = add_481 = None
        slice_scatter_2875: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2872, slice_scatter_2874, 1, 3824, 3840);  slice_scatter_2872 = slice_scatter_2874 = None
        slice_15821: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2875, 1, 3824, 3840)
        slice_15822: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15821, 2, 0, 16)
        slice_scatter_2877: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15821, slice_15822, 2, 0, 16);  slice_15821 = slice_15822 = None
        slice_scatter_2878: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2875, slice_scatter_2877, 1, 3824, 3840);  slice_scatter_2875 = slice_scatter_2877 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15841: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3840, 3856)
        slice_15842: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_15841, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_483: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_15842, memory_format = torch.contiguous_format);  slice_15842 = None
        view_970: "f32[32, 16]" = torch.ops.aten.view.default(clone_483, [32, 16]);  clone_483 = None
        mm_480: "f32[32, 8]" = torch.ops.aten.mm.default(view_970, slice_7)
        view_971: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_480, [2, 16, 8]);  mm_480 = None
        slice_15849: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2878, 1, 3840, 3856)
        slice_15850: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15849, 2, 0, 16)
        add_482: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15850, view_971);  slice_15850 = view_971 = None
        slice_scatter_2880: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15849, add_482, 2, 0, 16);  slice_15849 = add_482 = None
        slice_scatter_2881: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2878, slice_scatter_2880, 1, 3840, 3856);  slice_scatter_2878 = slice_scatter_2880 = None
        slice_15854: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2881, 1, 3840, 3856)
        slice_15855: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15854, 2, 0, 16)
        slice_scatter_2883: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15854, slice_15855, 2, 0, 16);  slice_15854 = slice_15855 = None
        slice_scatter_2884: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2881, slice_scatter_2883, 1, 3840, 3856);  slice_scatter_2881 = slice_scatter_2883 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15875: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_15841, 2, 16, 32);  slice_15841 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_484: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_15875, memory_format = torch.contiguous_format);  slice_15875 = None
        view_972: "f32[32, 11]" = torch.ops.aten.view.default(clone_484, [32, 11]);  clone_484 = None
        mm_481: "f32[32, 8]" = torch.ops.aten.mm.default(view_972, slice_37)
        view_973: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_481, [2, 16, 8]);  mm_481 = None
        slice_15882: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2884, 1, 3840, 3856)
        slice_15883: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15882, 2, 0, 16)
        add_483: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15883, view_973);  slice_15883 = view_973 = None
        slice_scatter_2886: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15882, add_483, 2, 0, 16);  slice_15882 = add_483 = None
        slice_scatter_2887: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2884, slice_scatter_2886, 1, 3840, 3856);  slice_scatter_2884 = slice_scatter_2886 = None
        slice_15887: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2887, 1, 3840, 3856)
        slice_15888: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15887, 2, 0, 16)
        slice_scatter_2889: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15887, slice_15888, 2, 0, 16);  slice_15887 = slice_15888 = None
        slice_scatter_2890: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2887, slice_scatter_2889, 1, 3840, 3856);  slice_scatter_2887 = slice_scatter_2889 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15907: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3856, 3872)
        slice_15908: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_15907, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_485: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_15908, memory_format = torch.contiguous_format);  slice_15908 = None
        view_974: "f32[32, 16]" = torch.ops.aten.view.default(clone_485, [32, 16]);  clone_485 = None
        mm_482: "f32[32, 8]" = torch.ops.aten.mm.default(view_974, slice_7)
        view_975: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_482, [2, 16, 8]);  mm_482 = None
        slice_15915: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2890, 1, 3856, 3872)
        slice_15916: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15915, 2, 0, 16)
        add_484: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15916, view_975);  slice_15916 = view_975 = None
        slice_scatter_2892: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15915, add_484, 2, 0, 16);  slice_15915 = add_484 = None
        slice_scatter_2893: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2890, slice_scatter_2892, 1, 3856, 3872);  slice_scatter_2890 = slice_scatter_2892 = None
        slice_15920: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2893, 1, 3856, 3872)
        slice_15921: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15920, 2, 0, 16)
        slice_scatter_2895: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15920, slice_15921, 2, 0, 16);  slice_15920 = slice_15921 = None
        slice_scatter_2896: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2893, slice_scatter_2895, 1, 3856, 3872);  slice_scatter_2893 = slice_scatter_2895 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15941: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_15907, 2, 16, 32);  slice_15907 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_486: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_15941, memory_format = torch.contiguous_format);  slice_15941 = None
        view_976: "f32[32, 11]" = torch.ops.aten.view.default(clone_486, [32, 11]);  clone_486 = None
        mm_483: "f32[32, 8]" = torch.ops.aten.mm.default(view_976, slice_37)
        view_977: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_483, [2, 16, 8]);  mm_483 = None
        slice_15948: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2896, 1, 3856, 3872)
        slice_15949: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15948, 2, 0, 16)
        add_485: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15949, view_977);  slice_15949 = view_977 = None
        slice_scatter_2898: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15948, add_485, 2, 0, 16);  slice_15948 = add_485 = None
        slice_scatter_2899: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2896, slice_scatter_2898, 1, 3856, 3872);  slice_scatter_2896 = slice_scatter_2898 = None
        slice_15953: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2899, 1, 3856, 3872)
        slice_15954: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15953, 2, 0, 16)
        slice_scatter_2901: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15953, slice_15954, 2, 0, 16);  slice_15953 = slice_15954 = None
        slice_scatter_2902: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2899, slice_scatter_2901, 1, 3856, 3872);  slice_scatter_2899 = slice_scatter_2901 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_15973: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3872, 3888)
        slice_15974: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_15973, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_487: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_15974, memory_format = torch.contiguous_format);  slice_15974 = None
        view_978: "f32[32, 16]" = torch.ops.aten.view.default(clone_487, [32, 16]);  clone_487 = None
        mm_484: "f32[32, 8]" = torch.ops.aten.mm.default(view_978, slice_7)
        view_979: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_484, [2, 16, 8]);  mm_484 = None
        slice_15981: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2902, 1, 3872, 3888)
        slice_15982: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15981, 2, 0, 16)
        add_486: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_15982, view_979);  slice_15982 = view_979 = None
        slice_scatter_2904: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15981, add_486, 2, 0, 16);  slice_15981 = add_486 = None
        slice_scatter_2905: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2902, slice_scatter_2904, 1, 3872, 3888);  slice_scatter_2902 = slice_scatter_2904 = None
        slice_15986: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2905, 1, 3872, 3888)
        slice_15987: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_15986, 2, 0, 16)
        slice_scatter_2907: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_15986, slice_15987, 2, 0, 16);  slice_15986 = slice_15987 = None
        slice_scatter_2908: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2905, slice_scatter_2907, 1, 3872, 3888);  slice_scatter_2905 = slice_scatter_2907 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16007: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_15973, 2, 16, 32);  slice_15973 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_488: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_16007, memory_format = torch.contiguous_format);  slice_16007 = None
        view_980: "f32[32, 11]" = torch.ops.aten.view.default(clone_488, [32, 11]);  clone_488 = None
        mm_485: "f32[32, 8]" = torch.ops.aten.mm.default(view_980, slice_37)
        view_981: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_485, [2, 16, 8]);  mm_485 = None
        slice_16014: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2908, 1, 3872, 3888)
        slice_16015: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16014, 2, 0, 16)
        add_487: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16015, view_981);  slice_16015 = view_981 = None
        slice_scatter_2910: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16014, add_487, 2, 0, 16);  slice_16014 = add_487 = None
        slice_scatter_2911: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2908, slice_scatter_2910, 1, 3872, 3888);  slice_scatter_2908 = slice_scatter_2910 = None
        slice_16019: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2911, 1, 3872, 3888)
        slice_16020: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16019, 2, 0, 16)
        slice_scatter_2913: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16019, slice_16020, 2, 0, 16);  slice_16019 = slice_16020 = None
        slice_scatter_2914: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2911, slice_scatter_2913, 1, 3872, 3888);  slice_scatter_2911 = slice_scatter_2913 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16039: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3888, 3904)
        slice_16040: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_16039, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_489: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_16040, memory_format = torch.contiguous_format);  slice_16040 = None
        view_982: "f32[32, 16]" = torch.ops.aten.view.default(clone_489, [32, 16]);  clone_489 = None
        mm_486: "f32[32, 8]" = torch.ops.aten.mm.default(view_982, slice_7)
        view_983: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_486, [2, 16, 8]);  mm_486 = None
        slice_16047: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2914, 1, 3888, 3904)
        slice_16048: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16047, 2, 0, 16)
        add_488: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16048, view_983);  slice_16048 = view_983 = None
        slice_scatter_2916: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16047, add_488, 2, 0, 16);  slice_16047 = add_488 = None
        slice_scatter_2917: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2914, slice_scatter_2916, 1, 3888, 3904);  slice_scatter_2914 = slice_scatter_2916 = None
        slice_16052: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2917, 1, 3888, 3904)
        slice_16053: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16052, 2, 0, 16)
        slice_scatter_2919: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16052, slice_16053, 2, 0, 16);  slice_16052 = slice_16053 = None
        slice_scatter_2920: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2917, slice_scatter_2919, 1, 3888, 3904);  slice_scatter_2917 = slice_scatter_2919 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16073: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_16039, 2, 16, 32);  slice_16039 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_490: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_16073, memory_format = torch.contiguous_format);  slice_16073 = None
        view_984: "f32[32, 11]" = torch.ops.aten.view.default(clone_490, [32, 11]);  clone_490 = None
        mm_487: "f32[32, 8]" = torch.ops.aten.mm.default(view_984, slice_37)
        view_985: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_487, [2, 16, 8]);  mm_487 = None
        slice_16080: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2920, 1, 3888, 3904)
        slice_16081: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16080, 2, 0, 16)
        add_489: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16081, view_985);  slice_16081 = view_985 = None
        slice_scatter_2922: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16080, add_489, 2, 0, 16);  slice_16080 = add_489 = None
        slice_scatter_2923: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2920, slice_scatter_2922, 1, 3888, 3904);  slice_scatter_2920 = slice_scatter_2922 = None
        slice_16085: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2923, 1, 3888, 3904)
        slice_16086: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16085, 2, 0, 16)
        slice_scatter_2925: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16085, slice_16086, 2, 0, 16);  slice_16085 = slice_16086 = None
        slice_scatter_2926: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2923, slice_scatter_2925, 1, 3888, 3904);  slice_scatter_2923 = slice_scatter_2925 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16105: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3904, 3920)
        slice_16106: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_16105, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_491: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_16106, memory_format = torch.contiguous_format);  slice_16106 = None
        view_986: "f32[32, 16]" = torch.ops.aten.view.default(clone_491, [32, 16]);  clone_491 = None
        mm_488: "f32[32, 8]" = torch.ops.aten.mm.default(view_986, slice_7)
        view_987: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_488, [2, 16, 8]);  mm_488 = None
        slice_16113: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2926, 1, 3904, 3920)
        slice_16114: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16113, 2, 0, 16)
        add_490: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16114, view_987);  slice_16114 = view_987 = None
        slice_scatter_2928: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16113, add_490, 2, 0, 16);  slice_16113 = add_490 = None
        slice_scatter_2929: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2926, slice_scatter_2928, 1, 3904, 3920);  slice_scatter_2926 = slice_scatter_2928 = None
        slice_16118: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2929, 1, 3904, 3920)
        slice_16119: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16118, 2, 0, 16)
        slice_scatter_2931: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16118, slice_16119, 2, 0, 16);  slice_16118 = slice_16119 = None
        slice_scatter_2932: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2929, slice_scatter_2931, 1, 3904, 3920);  slice_scatter_2929 = slice_scatter_2931 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16139: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_16105, 2, 16, 32);  slice_16105 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_492: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_16139, memory_format = torch.contiguous_format);  slice_16139 = None
        view_988: "f32[32, 11]" = torch.ops.aten.view.default(clone_492, [32, 11]);  clone_492 = None
        mm_489: "f32[32, 8]" = torch.ops.aten.mm.default(view_988, slice_37)
        view_989: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_489, [2, 16, 8]);  mm_489 = None
        slice_16146: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2932, 1, 3904, 3920)
        slice_16147: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16146, 2, 0, 16)
        add_491: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16147, view_989);  slice_16147 = view_989 = None
        slice_scatter_2934: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16146, add_491, 2, 0, 16);  slice_16146 = add_491 = None
        slice_scatter_2935: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2932, slice_scatter_2934, 1, 3904, 3920);  slice_scatter_2932 = slice_scatter_2934 = None
        slice_16151: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2935, 1, 3904, 3920)
        slice_16152: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16151, 2, 0, 16)
        slice_scatter_2937: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16151, slice_16152, 2, 0, 16);  slice_16151 = slice_16152 = None
        slice_scatter_2938: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2935, slice_scatter_2937, 1, 3904, 3920);  slice_scatter_2935 = slice_scatter_2937 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16171: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3920, 3936)
        slice_16172: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_16171, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_493: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_16172, memory_format = torch.contiguous_format);  slice_16172 = None
        view_990: "f32[32, 16]" = torch.ops.aten.view.default(clone_493, [32, 16]);  clone_493 = None
        mm_490: "f32[32, 8]" = torch.ops.aten.mm.default(view_990, slice_7)
        view_991: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_490, [2, 16, 8]);  mm_490 = None
        slice_16179: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2938, 1, 3920, 3936)
        slice_16180: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16179, 2, 0, 16)
        add_492: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16180, view_991);  slice_16180 = view_991 = None
        slice_scatter_2940: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16179, add_492, 2, 0, 16);  slice_16179 = add_492 = None
        slice_scatter_2941: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2938, slice_scatter_2940, 1, 3920, 3936);  slice_scatter_2938 = slice_scatter_2940 = None
        slice_16184: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2941, 1, 3920, 3936)
        slice_16185: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16184, 2, 0, 16)
        slice_scatter_2943: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16184, slice_16185, 2, 0, 16);  slice_16184 = slice_16185 = None
        slice_scatter_2944: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2941, slice_scatter_2943, 1, 3920, 3936);  slice_scatter_2941 = slice_scatter_2943 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16205: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_16171, 2, 16, 32);  slice_16171 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_494: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_16205, memory_format = torch.contiguous_format);  slice_16205 = None
        view_992: "f32[32, 11]" = torch.ops.aten.view.default(clone_494, [32, 11]);  clone_494 = None
        mm_491: "f32[32, 8]" = torch.ops.aten.mm.default(view_992, slice_37)
        view_993: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_491, [2, 16, 8]);  mm_491 = None
        slice_16212: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2944, 1, 3920, 3936)
        slice_16213: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16212, 2, 0, 16)
        add_493: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16213, view_993);  slice_16213 = view_993 = None
        slice_scatter_2946: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16212, add_493, 2, 0, 16);  slice_16212 = add_493 = None
        slice_scatter_2947: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2944, slice_scatter_2946, 1, 3920, 3936);  slice_scatter_2944 = slice_scatter_2946 = None
        slice_16217: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2947, 1, 3920, 3936)
        slice_16218: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16217, 2, 0, 16)
        slice_scatter_2949: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16217, slice_16218, 2, 0, 16);  slice_16217 = slice_16218 = None
        slice_scatter_2950: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2947, slice_scatter_2949, 1, 3920, 3936);  slice_scatter_2947 = slice_scatter_2949 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16237: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3936, 3952)
        slice_16238: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_16237, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_495: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_16238, memory_format = torch.contiguous_format);  slice_16238 = None
        view_994: "f32[32, 16]" = torch.ops.aten.view.default(clone_495, [32, 16]);  clone_495 = None
        mm_492: "f32[32, 8]" = torch.ops.aten.mm.default(view_994, slice_7)
        view_995: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_492, [2, 16, 8]);  mm_492 = None
        slice_16245: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2950, 1, 3936, 3952)
        slice_16246: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16245, 2, 0, 16)
        add_494: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16246, view_995);  slice_16246 = view_995 = None
        slice_scatter_2952: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16245, add_494, 2, 0, 16);  slice_16245 = add_494 = None
        slice_scatter_2953: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2950, slice_scatter_2952, 1, 3936, 3952);  slice_scatter_2950 = slice_scatter_2952 = None
        slice_16250: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2953, 1, 3936, 3952)
        slice_16251: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16250, 2, 0, 16)
        slice_scatter_2955: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16250, slice_16251, 2, 0, 16);  slice_16250 = slice_16251 = None
        slice_scatter_2956: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2953, slice_scatter_2955, 1, 3936, 3952);  slice_scatter_2953 = slice_scatter_2955 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16271: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_16237, 2, 16, 32);  slice_16237 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_496: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_16271, memory_format = torch.contiguous_format);  slice_16271 = None
        view_996: "f32[32, 11]" = torch.ops.aten.view.default(clone_496, [32, 11]);  clone_496 = None
        mm_493: "f32[32, 8]" = torch.ops.aten.mm.default(view_996, slice_37)
        view_997: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_493, [2, 16, 8]);  mm_493 = None
        slice_16278: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2956, 1, 3936, 3952)
        slice_16279: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16278, 2, 0, 16)
        add_495: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16279, view_997);  slice_16279 = view_997 = None
        slice_scatter_2958: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16278, add_495, 2, 0, 16);  slice_16278 = add_495 = None
        slice_scatter_2959: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2956, slice_scatter_2958, 1, 3936, 3952);  slice_scatter_2956 = slice_scatter_2958 = None
        slice_16283: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2959, 1, 3936, 3952)
        slice_16284: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16283, 2, 0, 16)
        slice_scatter_2961: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16283, slice_16284, 2, 0, 16);  slice_16283 = slice_16284 = None
        slice_scatter_2962: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2959, slice_scatter_2961, 1, 3936, 3952);  slice_scatter_2959 = slice_scatter_2961 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16303: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3952, 3968)
        slice_16304: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_16303, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_497: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_16304, memory_format = torch.contiguous_format);  slice_16304 = None
        view_998: "f32[32, 16]" = torch.ops.aten.view.default(clone_497, [32, 16]);  clone_497 = None
        mm_494: "f32[32, 8]" = torch.ops.aten.mm.default(view_998, slice_7)
        view_999: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_494, [2, 16, 8]);  mm_494 = None
        slice_16311: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2962, 1, 3952, 3968)
        slice_16312: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16311, 2, 0, 16)
        add_496: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16312, view_999);  slice_16312 = view_999 = None
        slice_scatter_2964: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16311, add_496, 2, 0, 16);  slice_16311 = add_496 = None
        slice_scatter_2965: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2962, slice_scatter_2964, 1, 3952, 3968);  slice_scatter_2962 = slice_scatter_2964 = None
        slice_16316: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2965, 1, 3952, 3968)
        slice_16317: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16316, 2, 0, 16)
        slice_scatter_2967: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16316, slice_16317, 2, 0, 16);  slice_16316 = slice_16317 = None
        slice_scatter_2968: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2965, slice_scatter_2967, 1, 3952, 3968);  slice_scatter_2965 = slice_scatter_2967 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16337: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_16303, 2, 16, 32);  slice_16303 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_498: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_16337, memory_format = torch.contiguous_format);  slice_16337 = None
        view_1000: "f32[32, 11]" = torch.ops.aten.view.default(clone_498, [32, 11]);  clone_498 = None
        mm_495: "f32[32, 8]" = torch.ops.aten.mm.default(view_1000, slice_37)
        view_1001: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_495, [2, 16, 8]);  mm_495 = None
        slice_16344: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2968, 1, 3952, 3968)
        slice_16345: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16344, 2, 0, 16)
        add_497: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16345, view_1001);  slice_16345 = view_1001 = None
        slice_scatter_2970: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16344, add_497, 2, 0, 16);  slice_16344 = add_497 = None
        slice_scatter_2971: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2968, slice_scatter_2970, 1, 3952, 3968);  slice_scatter_2968 = slice_scatter_2970 = None
        slice_16349: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2971, 1, 3952, 3968)
        slice_16350: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16349, 2, 0, 16)
        slice_scatter_2973: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16349, slice_16350, 2, 0, 16);  slice_16349 = slice_16350 = None
        slice_scatter_2974: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2971, slice_scatter_2973, 1, 3952, 3968);  slice_scatter_2971 = slice_scatter_2973 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16369: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3968, 3984)
        slice_16370: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_16369, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_499: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_16370, memory_format = torch.contiguous_format);  slice_16370 = None
        view_1002: "f32[32, 16]" = torch.ops.aten.view.default(clone_499, [32, 16]);  clone_499 = None
        mm_496: "f32[32, 8]" = torch.ops.aten.mm.default(view_1002, slice_7)
        view_1003: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_496, [2, 16, 8]);  mm_496 = None
        slice_16377: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2974, 1, 3968, 3984)
        slice_16378: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16377, 2, 0, 16)
        add_498: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16378, view_1003);  slice_16378 = view_1003 = None
        slice_scatter_2976: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16377, add_498, 2, 0, 16);  slice_16377 = add_498 = None
        slice_scatter_2977: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2974, slice_scatter_2976, 1, 3968, 3984);  slice_scatter_2974 = slice_scatter_2976 = None
        slice_16382: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2977, 1, 3968, 3984)
        slice_16383: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16382, 2, 0, 16)
        slice_scatter_2979: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16382, slice_16383, 2, 0, 16);  slice_16382 = slice_16383 = None
        slice_scatter_2980: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2977, slice_scatter_2979, 1, 3968, 3984);  slice_scatter_2977 = slice_scatter_2979 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16403: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_16369, 2, 16, 32);  slice_16369 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_500: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_16403, memory_format = torch.contiguous_format);  slice_16403 = None
        view_1004: "f32[32, 11]" = torch.ops.aten.view.default(clone_500, [32, 11]);  clone_500 = None
        mm_497: "f32[32, 8]" = torch.ops.aten.mm.default(view_1004, slice_37)
        view_1005: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_497, [2, 16, 8]);  mm_497 = None
        slice_16410: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2980, 1, 3968, 3984)
        slice_16411: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16410, 2, 0, 16)
        add_499: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16411, view_1005);  slice_16411 = view_1005 = None
        slice_scatter_2982: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16410, add_499, 2, 0, 16);  slice_16410 = add_499 = None
        slice_scatter_2983: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2980, slice_scatter_2982, 1, 3968, 3984);  slice_scatter_2980 = slice_scatter_2982 = None
        slice_16415: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2983, 1, 3968, 3984)
        slice_16416: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16415, 2, 0, 16)
        slice_scatter_2985: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16415, slice_16416, 2, 0, 16);  slice_16415 = slice_16416 = None
        slice_scatter_2986: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2983, slice_scatter_2985, 1, 3968, 3984);  slice_scatter_2983 = slice_scatter_2985 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16435: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 3984, 4000)
        slice_16436: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_16435, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_501: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_16436, memory_format = torch.contiguous_format);  slice_16436 = None
        view_1006: "f32[32, 16]" = torch.ops.aten.view.default(clone_501, [32, 16]);  clone_501 = None
        mm_498: "f32[32, 8]" = torch.ops.aten.mm.default(view_1006, slice_7)
        view_1007: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_498, [2, 16, 8]);  mm_498 = None
        slice_16443: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2986, 1, 3984, 4000)
        slice_16444: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16443, 2, 0, 16)
        add_500: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16444, view_1007);  slice_16444 = view_1007 = None
        slice_scatter_2988: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16443, add_500, 2, 0, 16);  slice_16443 = add_500 = None
        slice_scatter_2989: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2986, slice_scatter_2988, 1, 3984, 4000);  slice_scatter_2986 = slice_scatter_2988 = None
        slice_16448: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2989, 1, 3984, 4000)
        slice_16449: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16448, 2, 0, 16)
        slice_scatter_2991: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16448, slice_16449, 2, 0, 16);  slice_16448 = slice_16449 = None
        slice_scatter_2992: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2989, slice_scatter_2991, 1, 3984, 4000);  slice_scatter_2989 = slice_scatter_2991 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16469: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_16435, 2, 16, 32);  slice_16435 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_502: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_16469, memory_format = torch.contiguous_format);  slice_16469 = None
        view_1008: "f32[32, 11]" = torch.ops.aten.view.default(clone_502, [32, 11]);  clone_502 = None
        mm_499: "f32[32, 8]" = torch.ops.aten.mm.default(view_1008, slice_37)
        view_1009: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_499, [2, 16, 8]);  mm_499 = None
        slice_16476: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2992, 1, 3984, 4000)
        slice_16477: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16476, 2, 0, 16)
        add_501: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16477, view_1009);  slice_16477 = view_1009 = None
        slice_scatter_2994: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16476, add_501, 2, 0, 16);  slice_16476 = add_501 = None
        slice_scatter_2995: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2992, slice_scatter_2994, 1, 3984, 4000);  slice_scatter_2992 = slice_scatter_2994 = None
        slice_16481: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2995, 1, 3984, 4000)
        slice_16482: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16481, 2, 0, 16)
        slice_scatter_2997: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16481, slice_16482, 2, 0, 16);  slice_16481 = slice_16482 = None
        slice_scatter_2998: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2995, slice_scatter_2997, 1, 3984, 4000);  slice_scatter_2995 = slice_scatter_2997 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16501: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4000, 4016)
        slice_16502: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_16501, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_503: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_16502, memory_format = torch.contiguous_format);  slice_16502 = None
        view_1010: "f32[32, 16]" = torch.ops.aten.view.default(clone_503, [32, 16]);  clone_503 = None
        mm_500: "f32[32, 8]" = torch.ops.aten.mm.default(view_1010, slice_7)
        view_1011: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_500, [2, 16, 8]);  mm_500 = None
        slice_16509: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_2998, 1, 4000, 4016)
        slice_16510: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16509, 2, 0, 16)
        add_502: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16510, view_1011);  slice_16510 = view_1011 = None
        slice_scatter_3000: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16509, add_502, 2, 0, 16);  slice_16509 = add_502 = None
        slice_scatter_3001: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_2998, slice_scatter_3000, 1, 4000, 4016);  slice_scatter_2998 = slice_scatter_3000 = None
        slice_16514: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3001, 1, 4000, 4016)
        slice_16515: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16514, 2, 0, 16)
        slice_scatter_3003: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16514, slice_16515, 2, 0, 16);  slice_16514 = slice_16515 = None
        slice_scatter_3004: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3001, slice_scatter_3003, 1, 4000, 4016);  slice_scatter_3001 = slice_scatter_3003 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16535: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_16501, 2, 16, 32);  slice_16501 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_504: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_16535, memory_format = torch.contiguous_format);  slice_16535 = None
        view_1012: "f32[32, 11]" = torch.ops.aten.view.default(clone_504, [32, 11]);  clone_504 = None
        mm_501: "f32[32, 8]" = torch.ops.aten.mm.default(view_1012, slice_37)
        view_1013: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_501, [2, 16, 8]);  mm_501 = None
        slice_16542: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3004, 1, 4000, 4016)
        slice_16543: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16542, 2, 0, 16)
        add_503: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16543, view_1013);  slice_16543 = view_1013 = None
        slice_scatter_3006: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16542, add_503, 2, 0, 16);  slice_16542 = add_503 = None
        slice_scatter_3007: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3004, slice_scatter_3006, 1, 4000, 4016);  slice_scatter_3004 = slice_scatter_3006 = None
        slice_16547: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3007, 1, 4000, 4016)
        slice_16548: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16547, 2, 0, 16)
        slice_scatter_3009: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16547, slice_16548, 2, 0, 16);  slice_16547 = slice_16548 = None
        slice_scatter_3010: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3007, slice_scatter_3009, 1, 4000, 4016);  slice_scatter_3007 = slice_scatter_3009 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16567: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4016, 4032)
        slice_16568: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_16567, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_505: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_16568, memory_format = torch.contiguous_format);  slice_16568 = None
        view_1014: "f32[32, 16]" = torch.ops.aten.view.default(clone_505, [32, 16]);  clone_505 = None
        mm_502: "f32[32, 8]" = torch.ops.aten.mm.default(view_1014, slice_7)
        view_1015: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_502, [2, 16, 8]);  mm_502 = None
        slice_16575: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3010, 1, 4016, 4032)
        slice_16576: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16575, 2, 0, 16)
        add_504: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16576, view_1015);  slice_16576 = view_1015 = None
        slice_scatter_3012: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16575, add_504, 2, 0, 16);  slice_16575 = add_504 = None
        slice_scatter_3013: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3010, slice_scatter_3012, 1, 4016, 4032);  slice_scatter_3010 = slice_scatter_3012 = None
        slice_16580: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3013, 1, 4016, 4032)
        slice_16581: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16580, 2, 0, 16)
        slice_scatter_3015: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16580, slice_16581, 2, 0, 16);  slice_16580 = slice_16581 = None
        slice_scatter_3016: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3013, slice_scatter_3015, 1, 4016, 4032);  slice_scatter_3013 = slice_scatter_3015 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16601: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_16567, 2, 16, 32);  slice_16567 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_506: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_16601, memory_format = torch.contiguous_format);  slice_16601 = None
        view_1016: "f32[32, 11]" = torch.ops.aten.view.default(clone_506, [32, 11]);  clone_506 = None
        mm_503: "f32[32, 8]" = torch.ops.aten.mm.default(view_1016, slice_37)
        view_1017: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_503, [2, 16, 8]);  mm_503 = None
        slice_16608: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3016, 1, 4016, 4032)
        slice_16609: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16608, 2, 0, 16)
        add_505: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16609, view_1017);  slice_16609 = view_1017 = None
        slice_scatter_3018: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16608, add_505, 2, 0, 16);  slice_16608 = add_505 = None
        slice_scatter_3019: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3016, slice_scatter_3018, 1, 4016, 4032);  slice_scatter_3016 = slice_scatter_3018 = None
        slice_16613: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3019, 1, 4016, 4032)
        slice_16614: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16613, 2, 0, 16)
        slice_scatter_3021: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16613, slice_16614, 2, 0, 16);  slice_16613 = slice_16614 = None
        slice_scatter_3022: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3019, slice_scatter_3021, 1, 4016, 4032);  slice_scatter_3019 = slice_scatter_3021 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16633: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4032, 4048)
        slice_16634: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_16633, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_507: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_16634, memory_format = torch.contiguous_format);  slice_16634 = None
        view_1018: "f32[32, 16]" = torch.ops.aten.view.default(clone_507, [32, 16]);  clone_507 = None
        mm_504: "f32[32, 8]" = torch.ops.aten.mm.default(view_1018, slice_7)
        view_1019: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_504, [2, 16, 8]);  mm_504 = None
        slice_16641: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3022, 1, 4032, 4048)
        slice_16642: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16641, 2, 0, 16)
        add_506: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16642, view_1019);  slice_16642 = view_1019 = None
        slice_scatter_3024: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16641, add_506, 2, 0, 16);  slice_16641 = add_506 = None
        slice_scatter_3025: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3022, slice_scatter_3024, 1, 4032, 4048);  slice_scatter_3022 = slice_scatter_3024 = None
        slice_16646: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3025, 1, 4032, 4048)
        slice_16647: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16646, 2, 0, 16)
        slice_scatter_3027: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16646, slice_16647, 2, 0, 16);  slice_16646 = slice_16647 = None
        slice_scatter_3028: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3025, slice_scatter_3027, 1, 4032, 4048);  slice_scatter_3025 = slice_scatter_3027 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16667: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_16633, 2, 16, 32);  slice_16633 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_508: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_16667, memory_format = torch.contiguous_format);  slice_16667 = None
        view_1020: "f32[32, 11]" = torch.ops.aten.view.default(clone_508, [32, 11]);  clone_508 = None
        mm_505: "f32[32, 8]" = torch.ops.aten.mm.default(view_1020, slice_37)
        view_1021: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_505, [2, 16, 8]);  mm_505 = None
        slice_16674: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3028, 1, 4032, 4048)
        slice_16675: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16674, 2, 0, 16)
        add_507: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16675, view_1021);  slice_16675 = view_1021 = None
        slice_scatter_3030: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16674, add_507, 2, 0, 16);  slice_16674 = add_507 = None
        slice_scatter_3031: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3028, slice_scatter_3030, 1, 4032, 4048);  slice_scatter_3028 = slice_scatter_3030 = None
        slice_16679: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3031, 1, 4032, 4048)
        slice_16680: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16679, 2, 0, 16)
        slice_scatter_3033: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16679, slice_16680, 2, 0, 16);  slice_16679 = slice_16680 = None
        slice_scatter_3034: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3031, slice_scatter_3033, 1, 4032, 4048);  slice_scatter_3031 = slice_scatter_3033 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16699: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4048, 4064)
        slice_16700: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_16699, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_509: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_16700, memory_format = torch.contiguous_format);  slice_16700 = None
        view_1022: "f32[32, 16]" = torch.ops.aten.view.default(clone_509, [32, 16]);  clone_509 = None
        mm_506: "f32[32, 8]" = torch.ops.aten.mm.default(view_1022, slice_7)
        view_1023: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_506, [2, 16, 8]);  mm_506 = None
        slice_16707: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3034, 1, 4048, 4064)
        slice_16708: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16707, 2, 0, 16)
        add_508: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16708, view_1023);  slice_16708 = view_1023 = None
        slice_scatter_3036: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16707, add_508, 2, 0, 16);  slice_16707 = add_508 = None
        slice_scatter_3037: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3034, slice_scatter_3036, 1, 4048, 4064);  slice_scatter_3034 = slice_scatter_3036 = None
        slice_16712: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3037, 1, 4048, 4064)
        slice_16713: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16712, 2, 0, 16)
        slice_scatter_3039: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16712, slice_16713, 2, 0, 16);  slice_16712 = slice_16713 = None
        slice_scatter_3040: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3037, slice_scatter_3039, 1, 4048, 4064);  slice_scatter_3037 = slice_scatter_3039 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16733: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_16699, 2, 16, 32);  slice_16699 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_510: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_16733, memory_format = torch.contiguous_format);  slice_16733 = None
        view_1024: "f32[32, 11]" = torch.ops.aten.view.default(clone_510, [32, 11]);  clone_510 = None
        mm_507: "f32[32, 8]" = torch.ops.aten.mm.default(view_1024, slice_37)
        view_1025: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_507, [2, 16, 8]);  mm_507 = None
        slice_16740: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3040, 1, 4048, 4064)
        slice_16741: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16740, 2, 0, 16)
        add_509: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16741, view_1025);  slice_16741 = view_1025 = None
        slice_scatter_3042: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16740, add_509, 2, 0, 16);  slice_16740 = add_509 = None
        slice_scatter_3043: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3040, slice_scatter_3042, 1, 4048, 4064);  slice_scatter_3040 = slice_scatter_3042 = None
        slice_16745: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3043, 1, 4048, 4064)
        slice_16746: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16745, 2, 0, 16)
        slice_scatter_3045: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16745, slice_16746, 2, 0, 16);  slice_16745 = slice_16746 = None
        slice_scatter_3046: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3043, slice_scatter_3045, 1, 4048, 4064);  slice_scatter_3043 = slice_scatter_3045 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16765: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4064, 4080)
        slice_16766: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_16765, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_511: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_16766, memory_format = torch.contiguous_format);  slice_16766 = None
        view_1026: "f32[32, 16]" = torch.ops.aten.view.default(clone_511, [32, 16]);  clone_511 = None
        mm_508: "f32[32, 8]" = torch.ops.aten.mm.default(view_1026, slice_7)
        view_1027: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_508, [2, 16, 8]);  mm_508 = None
        slice_16773: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3046, 1, 4064, 4080)
        slice_16774: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16773, 2, 0, 16)
        add_510: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16774, view_1027);  slice_16774 = view_1027 = None
        slice_scatter_3048: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16773, add_510, 2, 0, 16);  slice_16773 = add_510 = None
        slice_scatter_3049: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3046, slice_scatter_3048, 1, 4064, 4080);  slice_scatter_3046 = slice_scatter_3048 = None
        slice_16778: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3049, 1, 4064, 4080)
        slice_16779: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16778, 2, 0, 16)
        slice_scatter_3051: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16778, slice_16779, 2, 0, 16);  slice_16778 = slice_16779 = None
        slice_scatter_3052: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3049, slice_scatter_3051, 1, 4064, 4080);  slice_scatter_3049 = slice_scatter_3051 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16799: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_16765, 2, 16, 32);  slice_16765 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_512: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_16799, memory_format = torch.contiguous_format);  slice_16799 = None
        view_1028: "f32[32, 11]" = torch.ops.aten.view.default(clone_512, [32, 11]);  clone_512 = None
        mm_509: "f32[32, 8]" = torch.ops.aten.mm.default(view_1028, slice_37)
        view_1029: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_509, [2, 16, 8]);  mm_509 = None
        slice_16806: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3052, 1, 4064, 4080)
        slice_16807: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16806, 2, 0, 16)
        add_511: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16807, view_1029);  slice_16807 = view_1029 = None
        slice_scatter_3054: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16806, add_511, 2, 0, 16);  slice_16806 = add_511 = None
        slice_scatter_3055: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3052, slice_scatter_3054, 1, 4064, 4080);  slice_scatter_3052 = slice_scatter_3054 = None
        slice_16811: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3055, 1, 4064, 4080)
        slice_16812: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16811, 2, 0, 16)
        slice_scatter_3057: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16811, slice_16812, 2, 0, 16);  slice_16811 = slice_16812 = None
        slice_scatter_3058: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3055, slice_scatter_3057, 1, 4064, 4080);  slice_scatter_3055 = slice_scatter_3057 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16831: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4080, 4096)
        slice_16832: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_16831, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_513: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_16832, memory_format = torch.contiguous_format);  slice_16832 = None
        view_1030: "f32[32, 16]" = torch.ops.aten.view.default(clone_513, [32, 16]);  clone_513 = None
        mm_510: "f32[32, 8]" = torch.ops.aten.mm.default(view_1030, slice_7)
        view_1031: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_510, [2, 16, 8]);  mm_510 = None
        slice_16839: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3058, 1, 4080, 4096)
        slice_16840: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16839, 2, 0, 16)
        add_512: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16840, view_1031);  slice_16840 = view_1031 = None
        slice_scatter_3060: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16839, add_512, 2, 0, 16);  slice_16839 = add_512 = None
        slice_scatter_3061: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3058, slice_scatter_3060, 1, 4080, 4096);  slice_scatter_3058 = slice_scatter_3060 = None
        slice_16844: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3061, 1, 4080, 4096)
        slice_16845: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16844, 2, 0, 16)
        slice_scatter_3063: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16844, slice_16845, 2, 0, 16);  slice_16844 = slice_16845 = None
        slice_scatter_3064: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3061, slice_scatter_3063, 1, 4080, 4096);  slice_scatter_3061 = slice_scatter_3063 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16865: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_16831, 2, 16, 32);  slice_16831 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_514: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_16865, memory_format = torch.contiguous_format);  slice_16865 = None
        view_1032: "f32[32, 11]" = torch.ops.aten.view.default(clone_514, [32, 11]);  clone_514 = None
        mm_511: "f32[32, 8]" = torch.ops.aten.mm.default(view_1032, slice_37)
        view_1033: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_511, [2, 16, 8]);  mm_511 = None
        slice_16872: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3064, 1, 4080, 4096)
        slice_16873: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16872, 2, 0, 16)
        add_513: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16873, view_1033);  slice_16873 = view_1033 = None
        slice_scatter_3066: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16872, add_513, 2, 0, 16);  slice_16872 = add_513 = None
        slice_scatter_3067: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3064, slice_scatter_3066, 1, 4080, 4096);  slice_scatter_3064 = slice_scatter_3066 = None
        slice_16877: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3067, 1, 4080, 4096)
        slice_16878: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16877, 2, 0, 16)
        slice_scatter_3069: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16877, slice_16878, 2, 0, 16);  slice_16877 = slice_16878 = None
        slice_scatter_3070: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3067, slice_scatter_3069, 1, 4080, 4096);  slice_scatter_3067 = slice_scatter_3069 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16897: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4096, 4112)
        slice_16898: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_16897, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_515: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_16898, memory_format = torch.contiguous_format);  slice_16898 = None
        view_1034: "f32[32, 16]" = torch.ops.aten.view.default(clone_515, [32, 16]);  clone_515 = None
        mm_512: "f32[32, 8]" = torch.ops.aten.mm.default(view_1034, slice_7)
        view_1035: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_512, [2, 16, 8]);  mm_512 = None
        slice_16905: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3070, 1, 4096, 4112)
        slice_16906: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16905, 2, 0, 16)
        add_514: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16906, view_1035);  slice_16906 = view_1035 = None
        slice_scatter_3072: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16905, add_514, 2, 0, 16);  slice_16905 = add_514 = None
        slice_scatter_3073: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3070, slice_scatter_3072, 1, 4096, 4112);  slice_scatter_3070 = slice_scatter_3072 = None
        slice_16910: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3073, 1, 4096, 4112)
        slice_16911: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16910, 2, 0, 16)
        slice_scatter_3075: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16910, slice_16911, 2, 0, 16);  slice_16910 = slice_16911 = None
        slice_scatter_3076: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3073, slice_scatter_3075, 1, 4096, 4112);  slice_scatter_3073 = slice_scatter_3075 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16931: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_16897, 2, 16, 32);  slice_16897 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_516: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_16931, memory_format = torch.contiguous_format);  slice_16931 = None
        view_1036: "f32[32, 11]" = torch.ops.aten.view.default(clone_516, [32, 11]);  clone_516 = None
        mm_513: "f32[32, 8]" = torch.ops.aten.mm.default(view_1036, slice_37)
        view_1037: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_513, [2, 16, 8]);  mm_513 = None
        slice_16938: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3076, 1, 4096, 4112)
        slice_16939: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16938, 2, 0, 16)
        add_515: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16939, view_1037);  slice_16939 = view_1037 = None
        slice_scatter_3078: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16938, add_515, 2, 0, 16);  slice_16938 = add_515 = None
        slice_scatter_3079: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3076, slice_scatter_3078, 1, 4096, 4112);  slice_scatter_3076 = slice_scatter_3078 = None
        slice_16943: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3079, 1, 4096, 4112)
        slice_16944: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16943, 2, 0, 16)
        slice_scatter_3081: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16943, slice_16944, 2, 0, 16);  slice_16943 = slice_16944 = None
        slice_scatter_3082: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3079, slice_scatter_3081, 1, 4096, 4112);  slice_scatter_3079 = slice_scatter_3081 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16963: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4112, 4128)
        slice_16964: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_16963, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_517: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_16964, memory_format = torch.contiguous_format);  slice_16964 = None
        view_1038: "f32[32, 16]" = torch.ops.aten.view.default(clone_517, [32, 16]);  clone_517 = None
        mm_514: "f32[32, 8]" = torch.ops.aten.mm.default(view_1038, slice_7)
        view_1039: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_514, [2, 16, 8]);  mm_514 = None
        slice_16971: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3082, 1, 4112, 4128)
        slice_16972: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16971, 2, 0, 16)
        add_516: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_16972, view_1039);  slice_16972 = view_1039 = None
        slice_scatter_3084: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16971, add_516, 2, 0, 16);  slice_16971 = add_516 = None
        slice_scatter_3085: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3082, slice_scatter_3084, 1, 4112, 4128);  slice_scatter_3082 = slice_scatter_3084 = None
        slice_16976: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3085, 1, 4112, 4128)
        slice_16977: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_16976, 2, 0, 16)
        slice_scatter_3087: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_16976, slice_16977, 2, 0, 16);  slice_16976 = slice_16977 = None
        slice_scatter_3088: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3085, slice_scatter_3087, 1, 4112, 4128);  slice_scatter_3085 = slice_scatter_3087 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_16997: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_16963, 2, 16, 32);  slice_16963 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_518: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_16997, memory_format = torch.contiguous_format);  slice_16997 = None
        view_1040: "f32[32, 11]" = torch.ops.aten.view.default(clone_518, [32, 11]);  clone_518 = None
        mm_515: "f32[32, 8]" = torch.ops.aten.mm.default(view_1040, slice_37)
        view_1041: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_515, [2, 16, 8]);  mm_515 = None
        slice_17004: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3088, 1, 4112, 4128)
        slice_17005: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17004, 2, 0, 16)
        add_517: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17005, view_1041);  slice_17005 = view_1041 = None
        slice_scatter_3090: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17004, add_517, 2, 0, 16);  slice_17004 = add_517 = None
        slice_scatter_3091: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3088, slice_scatter_3090, 1, 4112, 4128);  slice_scatter_3088 = slice_scatter_3090 = None
        slice_17009: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3091, 1, 4112, 4128)
        slice_17010: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17009, 2, 0, 16)
        slice_scatter_3093: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17009, slice_17010, 2, 0, 16);  slice_17009 = slice_17010 = None
        slice_scatter_3094: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3091, slice_scatter_3093, 1, 4112, 4128);  slice_scatter_3091 = slice_scatter_3093 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17029: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4128, 4144)
        slice_17030: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_17029, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_519: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_17030, memory_format = torch.contiguous_format);  slice_17030 = None
        view_1042: "f32[32, 16]" = torch.ops.aten.view.default(clone_519, [32, 16]);  clone_519 = None
        mm_516: "f32[32, 8]" = torch.ops.aten.mm.default(view_1042, slice_7)
        view_1043: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_516, [2, 16, 8]);  mm_516 = None
        slice_17037: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3094, 1, 4128, 4144)
        slice_17038: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17037, 2, 0, 16)
        add_518: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17038, view_1043);  slice_17038 = view_1043 = None
        slice_scatter_3096: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17037, add_518, 2, 0, 16);  slice_17037 = add_518 = None
        slice_scatter_3097: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3094, slice_scatter_3096, 1, 4128, 4144);  slice_scatter_3094 = slice_scatter_3096 = None
        slice_17042: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3097, 1, 4128, 4144)
        slice_17043: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17042, 2, 0, 16)
        slice_scatter_3099: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17042, slice_17043, 2, 0, 16);  slice_17042 = slice_17043 = None
        slice_scatter_3100: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3097, slice_scatter_3099, 1, 4128, 4144);  slice_scatter_3097 = slice_scatter_3099 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17063: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_17029, 2, 16, 32);  slice_17029 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_520: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_17063, memory_format = torch.contiguous_format);  slice_17063 = None
        view_1044: "f32[32, 11]" = torch.ops.aten.view.default(clone_520, [32, 11]);  clone_520 = None
        mm_517: "f32[32, 8]" = torch.ops.aten.mm.default(view_1044, slice_37)
        view_1045: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_517, [2, 16, 8]);  mm_517 = None
        slice_17070: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3100, 1, 4128, 4144)
        slice_17071: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17070, 2, 0, 16)
        add_519: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17071, view_1045);  slice_17071 = view_1045 = None
        slice_scatter_3102: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17070, add_519, 2, 0, 16);  slice_17070 = add_519 = None
        slice_scatter_3103: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3100, slice_scatter_3102, 1, 4128, 4144);  slice_scatter_3100 = slice_scatter_3102 = None
        slice_17075: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3103, 1, 4128, 4144)
        slice_17076: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17075, 2, 0, 16)
        slice_scatter_3105: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17075, slice_17076, 2, 0, 16);  slice_17075 = slice_17076 = None
        slice_scatter_3106: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3103, slice_scatter_3105, 1, 4128, 4144);  slice_scatter_3103 = slice_scatter_3105 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17095: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4144, 4160)
        slice_17096: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_17095, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_521: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_17096, memory_format = torch.contiguous_format);  slice_17096 = None
        view_1046: "f32[32, 16]" = torch.ops.aten.view.default(clone_521, [32, 16]);  clone_521 = None
        mm_518: "f32[32, 8]" = torch.ops.aten.mm.default(view_1046, slice_7)
        view_1047: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_518, [2, 16, 8]);  mm_518 = None
        slice_17103: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3106, 1, 4144, 4160)
        slice_17104: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17103, 2, 0, 16)
        add_520: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17104, view_1047);  slice_17104 = view_1047 = None
        slice_scatter_3108: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17103, add_520, 2, 0, 16);  slice_17103 = add_520 = None
        slice_scatter_3109: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3106, slice_scatter_3108, 1, 4144, 4160);  slice_scatter_3106 = slice_scatter_3108 = None
        slice_17108: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3109, 1, 4144, 4160)
        slice_17109: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17108, 2, 0, 16)
        slice_scatter_3111: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17108, slice_17109, 2, 0, 16);  slice_17108 = slice_17109 = None
        slice_scatter_3112: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3109, slice_scatter_3111, 1, 4144, 4160);  slice_scatter_3109 = slice_scatter_3111 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17129: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_17095, 2, 16, 32);  slice_17095 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_522: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_17129, memory_format = torch.contiguous_format);  slice_17129 = None
        view_1048: "f32[32, 11]" = torch.ops.aten.view.default(clone_522, [32, 11]);  clone_522 = None
        mm_519: "f32[32, 8]" = torch.ops.aten.mm.default(view_1048, slice_37)
        view_1049: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_519, [2, 16, 8]);  mm_519 = None
        slice_17136: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3112, 1, 4144, 4160)
        slice_17137: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17136, 2, 0, 16)
        add_521: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17137, view_1049);  slice_17137 = view_1049 = None
        slice_scatter_3114: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17136, add_521, 2, 0, 16);  slice_17136 = add_521 = None
        slice_scatter_3115: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3112, slice_scatter_3114, 1, 4144, 4160);  slice_scatter_3112 = slice_scatter_3114 = None
        slice_17141: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3115, 1, 4144, 4160)
        slice_17142: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17141, 2, 0, 16)
        slice_scatter_3117: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17141, slice_17142, 2, 0, 16);  slice_17141 = slice_17142 = None
        slice_scatter_3118: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3115, slice_scatter_3117, 1, 4144, 4160);  slice_scatter_3115 = slice_scatter_3117 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17161: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4160, 4176)
        slice_17162: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_17161, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_523: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_17162, memory_format = torch.contiguous_format);  slice_17162 = None
        view_1050: "f32[32, 16]" = torch.ops.aten.view.default(clone_523, [32, 16]);  clone_523 = None
        mm_520: "f32[32, 8]" = torch.ops.aten.mm.default(view_1050, slice_7)
        view_1051: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_520, [2, 16, 8]);  mm_520 = None
        slice_17169: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3118, 1, 4160, 4176)
        slice_17170: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17169, 2, 0, 16)
        add_522: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17170, view_1051);  slice_17170 = view_1051 = None
        slice_scatter_3120: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17169, add_522, 2, 0, 16);  slice_17169 = add_522 = None
        slice_scatter_3121: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3118, slice_scatter_3120, 1, 4160, 4176);  slice_scatter_3118 = slice_scatter_3120 = None
        slice_17174: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3121, 1, 4160, 4176)
        slice_17175: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17174, 2, 0, 16)
        slice_scatter_3123: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17174, slice_17175, 2, 0, 16);  slice_17174 = slice_17175 = None
        slice_scatter_3124: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3121, slice_scatter_3123, 1, 4160, 4176);  slice_scatter_3121 = slice_scatter_3123 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17195: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_17161, 2, 16, 32);  slice_17161 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_524: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_17195, memory_format = torch.contiguous_format);  slice_17195 = None
        view_1052: "f32[32, 11]" = torch.ops.aten.view.default(clone_524, [32, 11]);  clone_524 = None
        mm_521: "f32[32, 8]" = torch.ops.aten.mm.default(view_1052, slice_37)
        view_1053: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_521, [2, 16, 8]);  mm_521 = None
        slice_17202: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3124, 1, 4160, 4176)
        slice_17203: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17202, 2, 0, 16)
        add_523: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17203, view_1053);  slice_17203 = view_1053 = None
        slice_scatter_3126: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17202, add_523, 2, 0, 16);  slice_17202 = add_523 = None
        slice_scatter_3127: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3124, slice_scatter_3126, 1, 4160, 4176);  slice_scatter_3124 = slice_scatter_3126 = None
        slice_17207: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3127, 1, 4160, 4176)
        slice_17208: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17207, 2, 0, 16)
        slice_scatter_3129: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17207, slice_17208, 2, 0, 16);  slice_17207 = slice_17208 = None
        slice_scatter_3130: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3127, slice_scatter_3129, 1, 4160, 4176);  slice_scatter_3127 = slice_scatter_3129 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17227: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4176, 4192)
        slice_17228: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_17227, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_525: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_17228, memory_format = torch.contiguous_format);  slice_17228 = None
        view_1054: "f32[32, 16]" = torch.ops.aten.view.default(clone_525, [32, 16]);  clone_525 = None
        mm_522: "f32[32, 8]" = torch.ops.aten.mm.default(view_1054, slice_7)
        view_1055: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_522, [2, 16, 8]);  mm_522 = None
        slice_17235: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3130, 1, 4176, 4192)
        slice_17236: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17235, 2, 0, 16)
        add_524: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17236, view_1055);  slice_17236 = view_1055 = None
        slice_scatter_3132: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17235, add_524, 2, 0, 16);  slice_17235 = add_524 = None
        slice_scatter_3133: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3130, slice_scatter_3132, 1, 4176, 4192);  slice_scatter_3130 = slice_scatter_3132 = None
        slice_17240: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3133, 1, 4176, 4192)
        slice_17241: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17240, 2, 0, 16)
        slice_scatter_3135: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17240, slice_17241, 2, 0, 16);  slice_17240 = slice_17241 = None
        slice_scatter_3136: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3133, slice_scatter_3135, 1, 4176, 4192);  slice_scatter_3133 = slice_scatter_3135 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17261: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_17227, 2, 16, 32);  slice_17227 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_526: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_17261, memory_format = torch.contiguous_format);  slice_17261 = None
        view_1056: "f32[32, 11]" = torch.ops.aten.view.default(clone_526, [32, 11]);  clone_526 = None
        mm_523: "f32[32, 8]" = torch.ops.aten.mm.default(view_1056, slice_37)
        view_1057: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_523, [2, 16, 8]);  mm_523 = None
        slice_17268: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3136, 1, 4176, 4192)
        slice_17269: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17268, 2, 0, 16)
        add_525: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17269, view_1057);  slice_17269 = view_1057 = None
        slice_scatter_3138: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17268, add_525, 2, 0, 16);  slice_17268 = add_525 = None
        slice_scatter_3139: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3136, slice_scatter_3138, 1, 4176, 4192);  slice_scatter_3136 = slice_scatter_3138 = None
        slice_17273: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3139, 1, 4176, 4192)
        slice_17274: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17273, 2, 0, 16)
        slice_scatter_3141: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17273, slice_17274, 2, 0, 16);  slice_17273 = slice_17274 = None
        slice_scatter_3142: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3139, slice_scatter_3141, 1, 4176, 4192);  slice_scatter_3139 = slice_scatter_3141 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17293: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4192, 4208)
        slice_17294: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_17293, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_527: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_17294, memory_format = torch.contiguous_format);  slice_17294 = None
        view_1058: "f32[32, 16]" = torch.ops.aten.view.default(clone_527, [32, 16]);  clone_527 = None
        mm_524: "f32[32, 8]" = torch.ops.aten.mm.default(view_1058, slice_7)
        view_1059: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_524, [2, 16, 8]);  mm_524 = None
        slice_17301: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3142, 1, 4192, 4208)
        slice_17302: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17301, 2, 0, 16)
        add_526: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17302, view_1059);  slice_17302 = view_1059 = None
        slice_scatter_3144: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17301, add_526, 2, 0, 16);  slice_17301 = add_526 = None
        slice_scatter_3145: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3142, slice_scatter_3144, 1, 4192, 4208);  slice_scatter_3142 = slice_scatter_3144 = None
        slice_17306: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3145, 1, 4192, 4208)
        slice_17307: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17306, 2, 0, 16)
        slice_scatter_3147: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17306, slice_17307, 2, 0, 16);  slice_17306 = slice_17307 = None
        slice_scatter_3148: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3145, slice_scatter_3147, 1, 4192, 4208);  slice_scatter_3145 = slice_scatter_3147 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17327: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_17293, 2, 16, 32);  slice_17293 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_528: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_17327, memory_format = torch.contiguous_format);  slice_17327 = None
        view_1060: "f32[32, 11]" = torch.ops.aten.view.default(clone_528, [32, 11]);  clone_528 = None
        mm_525: "f32[32, 8]" = torch.ops.aten.mm.default(view_1060, slice_37)
        view_1061: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_525, [2, 16, 8]);  mm_525 = None
        slice_17334: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3148, 1, 4192, 4208)
        slice_17335: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17334, 2, 0, 16)
        add_527: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17335, view_1061);  slice_17335 = view_1061 = None
        slice_scatter_3150: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17334, add_527, 2, 0, 16);  slice_17334 = add_527 = None
        slice_scatter_3151: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3148, slice_scatter_3150, 1, 4192, 4208);  slice_scatter_3148 = slice_scatter_3150 = None
        slice_17339: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3151, 1, 4192, 4208)
        slice_17340: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17339, 2, 0, 16)
        slice_scatter_3153: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17339, slice_17340, 2, 0, 16);  slice_17339 = slice_17340 = None
        slice_scatter_3154: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3151, slice_scatter_3153, 1, 4192, 4208);  slice_scatter_3151 = slice_scatter_3153 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17359: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4208, 4224)
        slice_17360: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_17359, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_529: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_17360, memory_format = torch.contiguous_format);  slice_17360 = None
        view_1062: "f32[32, 16]" = torch.ops.aten.view.default(clone_529, [32, 16]);  clone_529 = None
        mm_526: "f32[32, 8]" = torch.ops.aten.mm.default(view_1062, slice_7)
        view_1063: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_526, [2, 16, 8]);  mm_526 = None
        slice_17367: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3154, 1, 4208, 4224)
        slice_17368: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17367, 2, 0, 16)
        add_528: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17368, view_1063);  slice_17368 = view_1063 = None
        slice_scatter_3156: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17367, add_528, 2, 0, 16);  slice_17367 = add_528 = None
        slice_scatter_3157: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3154, slice_scatter_3156, 1, 4208, 4224);  slice_scatter_3154 = slice_scatter_3156 = None
        slice_17372: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3157, 1, 4208, 4224)
        slice_17373: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17372, 2, 0, 16)
        slice_scatter_3159: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17372, slice_17373, 2, 0, 16);  slice_17372 = slice_17373 = None
        slice_scatter_3160: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3157, slice_scatter_3159, 1, 4208, 4224);  slice_scatter_3157 = slice_scatter_3159 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17393: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_17359, 2, 16, 32);  slice_17359 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_530: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_17393, memory_format = torch.contiguous_format);  slice_17393 = None
        view_1064: "f32[32, 11]" = torch.ops.aten.view.default(clone_530, [32, 11]);  clone_530 = None
        mm_527: "f32[32, 8]" = torch.ops.aten.mm.default(view_1064, slice_37)
        view_1065: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_527, [2, 16, 8]);  mm_527 = None
        slice_17400: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3160, 1, 4208, 4224)
        slice_17401: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17400, 2, 0, 16)
        add_529: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17401, view_1065);  slice_17401 = view_1065 = None
        slice_scatter_3162: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17400, add_529, 2, 0, 16);  slice_17400 = add_529 = None
        slice_scatter_3163: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3160, slice_scatter_3162, 1, 4208, 4224);  slice_scatter_3160 = slice_scatter_3162 = None
        slice_17405: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3163, 1, 4208, 4224)
        slice_17406: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17405, 2, 0, 16)
        slice_scatter_3165: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17405, slice_17406, 2, 0, 16);  slice_17405 = slice_17406 = None
        slice_scatter_3166: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3163, slice_scatter_3165, 1, 4208, 4224);  slice_scatter_3163 = slice_scatter_3165 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17425: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4224, 4240)
        slice_17426: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_17425, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_531: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_17426, memory_format = torch.contiguous_format);  slice_17426 = None
        view_1066: "f32[32, 16]" = torch.ops.aten.view.default(clone_531, [32, 16]);  clone_531 = None
        mm_528: "f32[32, 8]" = torch.ops.aten.mm.default(view_1066, slice_7)
        view_1067: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_528, [2, 16, 8]);  mm_528 = None
        slice_17433: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3166, 1, 4224, 4240)
        slice_17434: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17433, 2, 0, 16)
        add_530: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17434, view_1067);  slice_17434 = view_1067 = None
        slice_scatter_3168: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17433, add_530, 2, 0, 16);  slice_17433 = add_530 = None
        slice_scatter_3169: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3166, slice_scatter_3168, 1, 4224, 4240);  slice_scatter_3166 = slice_scatter_3168 = None
        slice_17438: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3169, 1, 4224, 4240)
        slice_17439: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17438, 2, 0, 16)
        slice_scatter_3171: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17438, slice_17439, 2, 0, 16);  slice_17438 = slice_17439 = None
        slice_scatter_3172: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3169, slice_scatter_3171, 1, 4224, 4240);  slice_scatter_3169 = slice_scatter_3171 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17459: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_17425, 2, 16, 32);  slice_17425 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_532: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_17459, memory_format = torch.contiguous_format);  slice_17459 = None
        view_1068: "f32[32, 11]" = torch.ops.aten.view.default(clone_532, [32, 11]);  clone_532 = None
        mm_529: "f32[32, 8]" = torch.ops.aten.mm.default(view_1068, slice_37)
        view_1069: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_529, [2, 16, 8]);  mm_529 = None
        slice_17466: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3172, 1, 4224, 4240)
        slice_17467: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17466, 2, 0, 16)
        add_531: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17467, view_1069);  slice_17467 = view_1069 = None
        slice_scatter_3174: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17466, add_531, 2, 0, 16);  slice_17466 = add_531 = None
        slice_scatter_3175: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3172, slice_scatter_3174, 1, 4224, 4240);  slice_scatter_3172 = slice_scatter_3174 = None
        slice_17471: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3175, 1, 4224, 4240)
        slice_17472: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17471, 2, 0, 16)
        slice_scatter_3177: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17471, slice_17472, 2, 0, 16);  slice_17471 = slice_17472 = None
        slice_scatter_3178: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3175, slice_scatter_3177, 1, 4224, 4240);  slice_scatter_3175 = slice_scatter_3177 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17491: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4240, 4256)
        slice_17492: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_17491, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_533: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_17492, memory_format = torch.contiguous_format);  slice_17492 = None
        view_1070: "f32[32, 16]" = torch.ops.aten.view.default(clone_533, [32, 16]);  clone_533 = None
        mm_530: "f32[32, 8]" = torch.ops.aten.mm.default(view_1070, slice_7)
        view_1071: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_530, [2, 16, 8]);  mm_530 = None
        slice_17499: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3178, 1, 4240, 4256)
        slice_17500: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17499, 2, 0, 16)
        add_532: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17500, view_1071);  slice_17500 = view_1071 = None
        slice_scatter_3180: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17499, add_532, 2, 0, 16);  slice_17499 = add_532 = None
        slice_scatter_3181: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3178, slice_scatter_3180, 1, 4240, 4256);  slice_scatter_3178 = slice_scatter_3180 = None
        slice_17504: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3181, 1, 4240, 4256)
        slice_17505: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17504, 2, 0, 16)
        slice_scatter_3183: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17504, slice_17505, 2, 0, 16);  slice_17504 = slice_17505 = None
        slice_scatter_3184: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3181, slice_scatter_3183, 1, 4240, 4256);  slice_scatter_3181 = slice_scatter_3183 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17525: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_17491, 2, 16, 32);  slice_17491 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_534: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_17525, memory_format = torch.contiguous_format);  slice_17525 = None
        view_1072: "f32[32, 11]" = torch.ops.aten.view.default(clone_534, [32, 11]);  clone_534 = None
        mm_531: "f32[32, 8]" = torch.ops.aten.mm.default(view_1072, slice_37)
        view_1073: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_531, [2, 16, 8]);  mm_531 = None
        slice_17532: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3184, 1, 4240, 4256)
        slice_17533: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17532, 2, 0, 16)
        add_533: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17533, view_1073);  slice_17533 = view_1073 = None
        slice_scatter_3186: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17532, add_533, 2, 0, 16);  slice_17532 = add_533 = None
        slice_scatter_3187: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3184, slice_scatter_3186, 1, 4240, 4256);  slice_scatter_3184 = slice_scatter_3186 = None
        slice_17537: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3187, 1, 4240, 4256)
        slice_17538: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17537, 2, 0, 16)
        slice_scatter_3189: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17537, slice_17538, 2, 0, 16);  slice_17537 = slice_17538 = None
        slice_scatter_3190: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3187, slice_scatter_3189, 1, 4240, 4256);  slice_scatter_3187 = slice_scatter_3189 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17557: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4256, 4272)
        slice_17558: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_17557, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_535: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_17558, memory_format = torch.contiguous_format);  slice_17558 = None
        view_1074: "f32[32, 16]" = torch.ops.aten.view.default(clone_535, [32, 16]);  clone_535 = None
        mm_532: "f32[32, 8]" = torch.ops.aten.mm.default(view_1074, slice_7)
        view_1075: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_532, [2, 16, 8]);  mm_532 = None
        slice_17565: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3190, 1, 4256, 4272)
        slice_17566: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17565, 2, 0, 16)
        add_534: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17566, view_1075);  slice_17566 = view_1075 = None
        slice_scatter_3192: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17565, add_534, 2, 0, 16);  slice_17565 = add_534 = None
        slice_scatter_3193: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3190, slice_scatter_3192, 1, 4256, 4272);  slice_scatter_3190 = slice_scatter_3192 = None
        slice_17570: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3193, 1, 4256, 4272)
        slice_17571: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17570, 2, 0, 16)
        slice_scatter_3195: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17570, slice_17571, 2, 0, 16);  slice_17570 = slice_17571 = None
        slice_scatter_3196: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3193, slice_scatter_3195, 1, 4256, 4272);  slice_scatter_3193 = slice_scatter_3195 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17591: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_17557, 2, 16, 32);  slice_17557 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_536: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_17591, memory_format = torch.contiguous_format);  slice_17591 = None
        view_1076: "f32[32, 11]" = torch.ops.aten.view.default(clone_536, [32, 11]);  clone_536 = None
        mm_533: "f32[32, 8]" = torch.ops.aten.mm.default(view_1076, slice_37)
        view_1077: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_533, [2, 16, 8]);  mm_533 = None
        slice_17598: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3196, 1, 4256, 4272)
        slice_17599: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17598, 2, 0, 16)
        add_535: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17599, view_1077);  slice_17599 = view_1077 = None
        slice_scatter_3198: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17598, add_535, 2, 0, 16);  slice_17598 = add_535 = None
        slice_scatter_3199: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3196, slice_scatter_3198, 1, 4256, 4272);  slice_scatter_3196 = slice_scatter_3198 = None
        slice_17603: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3199, 1, 4256, 4272)
        slice_17604: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17603, 2, 0, 16)
        slice_scatter_3201: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17603, slice_17604, 2, 0, 16);  slice_17603 = slice_17604 = None
        slice_scatter_3202: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3199, slice_scatter_3201, 1, 4256, 4272);  slice_scatter_3199 = slice_scatter_3201 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17623: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4272, 4288)
        slice_17624: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_17623, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_537: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_17624, memory_format = torch.contiguous_format);  slice_17624 = None
        view_1078: "f32[32, 16]" = torch.ops.aten.view.default(clone_537, [32, 16]);  clone_537 = None
        mm_534: "f32[32, 8]" = torch.ops.aten.mm.default(view_1078, slice_7)
        view_1079: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_534, [2, 16, 8]);  mm_534 = None
        slice_17631: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3202, 1, 4272, 4288)
        slice_17632: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17631, 2, 0, 16)
        add_536: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17632, view_1079);  slice_17632 = view_1079 = None
        slice_scatter_3204: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17631, add_536, 2, 0, 16);  slice_17631 = add_536 = None
        slice_scatter_3205: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3202, slice_scatter_3204, 1, 4272, 4288);  slice_scatter_3202 = slice_scatter_3204 = None
        slice_17636: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3205, 1, 4272, 4288)
        slice_17637: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17636, 2, 0, 16)
        slice_scatter_3207: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17636, slice_17637, 2, 0, 16);  slice_17636 = slice_17637 = None
        slice_scatter_3208: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3205, slice_scatter_3207, 1, 4272, 4288);  slice_scatter_3205 = slice_scatter_3207 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17657: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_17623, 2, 16, 32);  slice_17623 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_538: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_17657, memory_format = torch.contiguous_format);  slice_17657 = None
        view_1080: "f32[32, 11]" = torch.ops.aten.view.default(clone_538, [32, 11]);  clone_538 = None
        mm_535: "f32[32, 8]" = torch.ops.aten.mm.default(view_1080, slice_37)
        view_1081: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_535, [2, 16, 8]);  mm_535 = None
        slice_17664: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3208, 1, 4272, 4288)
        slice_17665: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17664, 2, 0, 16)
        add_537: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17665, view_1081);  slice_17665 = view_1081 = None
        slice_scatter_3210: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17664, add_537, 2, 0, 16);  slice_17664 = add_537 = None
        slice_scatter_3211: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3208, slice_scatter_3210, 1, 4272, 4288);  slice_scatter_3208 = slice_scatter_3210 = None
        slice_17669: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3211, 1, 4272, 4288)
        slice_17670: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17669, 2, 0, 16)
        slice_scatter_3213: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17669, slice_17670, 2, 0, 16);  slice_17669 = slice_17670 = None
        slice_scatter_3214: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3211, slice_scatter_3213, 1, 4272, 4288);  slice_scatter_3211 = slice_scatter_3213 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17689: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4288, 4304)
        slice_17690: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_17689, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_539: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_17690, memory_format = torch.contiguous_format);  slice_17690 = None
        view_1082: "f32[32, 16]" = torch.ops.aten.view.default(clone_539, [32, 16]);  clone_539 = None
        mm_536: "f32[32, 8]" = torch.ops.aten.mm.default(view_1082, slice_7)
        view_1083: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_536, [2, 16, 8]);  mm_536 = None
        slice_17697: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3214, 1, 4288, 4304)
        slice_17698: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17697, 2, 0, 16)
        add_538: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17698, view_1083);  slice_17698 = view_1083 = None
        slice_scatter_3216: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17697, add_538, 2, 0, 16);  slice_17697 = add_538 = None
        slice_scatter_3217: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3214, slice_scatter_3216, 1, 4288, 4304);  slice_scatter_3214 = slice_scatter_3216 = None
        slice_17702: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3217, 1, 4288, 4304)
        slice_17703: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17702, 2, 0, 16)
        slice_scatter_3219: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17702, slice_17703, 2, 0, 16);  slice_17702 = slice_17703 = None
        slice_scatter_3220: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3217, slice_scatter_3219, 1, 4288, 4304);  slice_scatter_3217 = slice_scatter_3219 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17723: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_17689, 2, 16, 32);  slice_17689 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_540: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_17723, memory_format = torch.contiguous_format);  slice_17723 = None
        view_1084: "f32[32, 11]" = torch.ops.aten.view.default(clone_540, [32, 11]);  clone_540 = None
        mm_537: "f32[32, 8]" = torch.ops.aten.mm.default(view_1084, slice_37)
        view_1085: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_537, [2, 16, 8]);  mm_537 = None
        slice_17730: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3220, 1, 4288, 4304)
        slice_17731: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17730, 2, 0, 16)
        add_539: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17731, view_1085);  slice_17731 = view_1085 = None
        slice_scatter_3222: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17730, add_539, 2, 0, 16);  slice_17730 = add_539 = None
        slice_scatter_3223: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3220, slice_scatter_3222, 1, 4288, 4304);  slice_scatter_3220 = slice_scatter_3222 = None
        slice_17735: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3223, 1, 4288, 4304)
        slice_17736: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17735, 2, 0, 16)
        slice_scatter_3225: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17735, slice_17736, 2, 0, 16);  slice_17735 = slice_17736 = None
        slice_scatter_3226: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3223, slice_scatter_3225, 1, 4288, 4304);  slice_scatter_3223 = slice_scatter_3225 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17755: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4304, 4320)
        slice_17756: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_17755, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_541: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_17756, memory_format = torch.contiguous_format);  slice_17756 = None
        view_1086: "f32[32, 16]" = torch.ops.aten.view.default(clone_541, [32, 16]);  clone_541 = None
        mm_538: "f32[32, 8]" = torch.ops.aten.mm.default(view_1086, slice_7)
        view_1087: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_538, [2, 16, 8]);  mm_538 = None
        slice_17763: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3226, 1, 4304, 4320)
        slice_17764: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17763, 2, 0, 16)
        add_540: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17764, view_1087);  slice_17764 = view_1087 = None
        slice_scatter_3228: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17763, add_540, 2, 0, 16);  slice_17763 = add_540 = None
        slice_scatter_3229: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3226, slice_scatter_3228, 1, 4304, 4320);  slice_scatter_3226 = slice_scatter_3228 = None
        slice_17768: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3229, 1, 4304, 4320)
        slice_17769: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17768, 2, 0, 16)
        slice_scatter_3231: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17768, slice_17769, 2, 0, 16);  slice_17768 = slice_17769 = None
        slice_scatter_3232: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3229, slice_scatter_3231, 1, 4304, 4320);  slice_scatter_3229 = slice_scatter_3231 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17789: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_17755, 2, 16, 32);  slice_17755 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_542: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_17789, memory_format = torch.contiguous_format);  slice_17789 = None
        view_1088: "f32[32, 11]" = torch.ops.aten.view.default(clone_542, [32, 11]);  clone_542 = None
        mm_539: "f32[32, 8]" = torch.ops.aten.mm.default(view_1088, slice_37)
        view_1089: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_539, [2, 16, 8]);  mm_539 = None
        slice_17796: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3232, 1, 4304, 4320)
        slice_17797: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17796, 2, 0, 16)
        add_541: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17797, view_1089);  slice_17797 = view_1089 = None
        slice_scatter_3234: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17796, add_541, 2, 0, 16);  slice_17796 = add_541 = None
        slice_scatter_3235: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3232, slice_scatter_3234, 1, 4304, 4320);  slice_scatter_3232 = slice_scatter_3234 = None
        slice_17801: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3235, 1, 4304, 4320)
        slice_17802: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17801, 2, 0, 16)
        slice_scatter_3237: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17801, slice_17802, 2, 0, 16);  slice_17801 = slice_17802 = None
        slice_scatter_3238: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3235, slice_scatter_3237, 1, 4304, 4320);  slice_scatter_3235 = slice_scatter_3237 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17821: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4320, 4336)
        slice_17822: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_17821, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_543: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_17822, memory_format = torch.contiguous_format);  slice_17822 = None
        view_1090: "f32[32, 16]" = torch.ops.aten.view.default(clone_543, [32, 16]);  clone_543 = None
        mm_540: "f32[32, 8]" = torch.ops.aten.mm.default(view_1090, slice_7)
        view_1091: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_540, [2, 16, 8]);  mm_540 = None
        slice_17829: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3238, 1, 4320, 4336)
        slice_17830: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17829, 2, 0, 16)
        add_542: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17830, view_1091);  slice_17830 = view_1091 = None
        slice_scatter_3240: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17829, add_542, 2, 0, 16);  slice_17829 = add_542 = None
        slice_scatter_3241: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3238, slice_scatter_3240, 1, 4320, 4336);  slice_scatter_3238 = slice_scatter_3240 = None
        slice_17834: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3241, 1, 4320, 4336)
        slice_17835: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17834, 2, 0, 16)
        slice_scatter_3243: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17834, slice_17835, 2, 0, 16);  slice_17834 = slice_17835 = None
        slice_scatter_3244: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3241, slice_scatter_3243, 1, 4320, 4336);  slice_scatter_3241 = slice_scatter_3243 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17855: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_17821, 2, 16, 32);  slice_17821 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_544: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_17855, memory_format = torch.contiguous_format);  slice_17855 = None
        view_1092: "f32[32, 11]" = torch.ops.aten.view.default(clone_544, [32, 11]);  clone_544 = None
        mm_541: "f32[32, 8]" = torch.ops.aten.mm.default(view_1092, slice_37)
        view_1093: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_541, [2, 16, 8]);  mm_541 = None
        slice_17862: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3244, 1, 4320, 4336)
        slice_17863: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17862, 2, 0, 16)
        add_543: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17863, view_1093);  slice_17863 = view_1093 = None
        slice_scatter_3246: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17862, add_543, 2, 0, 16);  slice_17862 = add_543 = None
        slice_scatter_3247: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3244, slice_scatter_3246, 1, 4320, 4336);  slice_scatter_3244 = slice_scatter_3246 = None
        slice_17867: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3247, 1, 4320, 4336)
        slice_17868: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17867, 2, 0, 16)
        slice_scatter_3249: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17867, slice_17868, 2, 0, 16);  slice_17867 = slice_17868 = None
        slice_scatter_3250: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3247, slice_scatter_3249, 1, 4320, 4336);  slice_scatter_3247 = slice_scatter_3249 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17887: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4336, 4352)
        slice_17888: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_17887, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_545: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_17888, memory_format = torch.contiguous_format);  slice_17888 = None
        view_1094: "f32[32, 16]" = torch.ops.aten.view.default(clone_545, [32, 16]);  clone_545 = None
        mm_542: "f32[32, 8]" = torch.ops.aten.mm.default(view_1094, slice_7)
        view_1095: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_542, [2, 16, 8]);  mm_542 = None
        slice_17895: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3250, 1, 4336, 4352)
        slice_17896: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17895, 2, 0, 16)
        add_544: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17896, view_1095);  slice_17896 = view_1095 = None
        slice_scatter_3252: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17895, add_544, 2, 0, 16);  slice_17895 = add_544 = None
        slice_scatter_3253: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3250, slice_scatter_3252, 1, 4336, 4352);  slice_scatter_3250 = slice_scatter_3252 = None
        slice_17900: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3253, 1, 4336, 4352)
        slice_17901: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17900, 2, 0, 16)
        slice_scatter_3255: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17900, slice_17901, 2, 0, 16);  slice_17900 = slice_17901 = None
        slice_scatter_3256: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3253, slice_scatter_3255, 1, 4336, 4352);  slice_scatter_3253 = slice_scatter_3255 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17921: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_17887, 2, 16, 32);  slice_17887 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_546: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_17921, memory_format = torch.contiguous_format);  slice_17921 = None
        view_1096: "f32[32, 11]" = torch.ops.aten.view.default(clone_546, [32, 11]);  clone_546 = None
        mm_543: "f32[32, 8]" = torch.ops.aten.mm.default(view_1096, slice_37)
        view_1097: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_543, [2, 16, 8]);  mm_543 = None
        slice_17928: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3256, 1, 4336, 4352)
        slice_17929: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17928, 2, 0, 16)
        add_545: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17929, view_1097);  slice_17929 = view_1097 = None
        slice_scatter_3258: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17928, add_545, 2, 0, 16);  slice_17928 = add_545 = None
        slice_scatter_3259: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3256, slice_scatter_3258, 1, 4336, 4352);  slice_scatter_3256 = slice_scatter_3258 = None
        slice_17933: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3259, 1, 4336, 4352)
        slice_17934: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17933, 2, 0, 16)
        slice_scatter_3261: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17933, slice_17934, 2, 0, 16);  slice_17933 = slice_17934 = None
        slice_scatter_3262: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3259, slice_scatter_3261, 1, 4336, 4352);  slice_scatter_3259 = slice_scatter_3261 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17953: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4352, 4368)
        slice_17954: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_17953, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_547: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_17954, memory_format = torch.contiguous_format);  slice_17954 = None
        view_1098: "f32[32, 16]" = torch.ops.aten.view.default(clone_547, [32, 16]);  clone_547 = None
        mm_544: "f32[32, 8]" = torch.ops.aten.mm.default(view_1098, slice_7)
        view_1099: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_544, [2, 16, 8]);  mm_544 = None
        slice_17961: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3262, 1, 4352, 4368)
        slice_17962: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17961, 2, 0, 16)
        add_546: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17962, view_1099);  slice_17962 = view_1099 = None
        slice_scatter_3264: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17961, add_546, 2, 0, 16);  slice_17961 = add_546 = None
        slice_scatter_3265: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3262, slice_scatter_3264, 1, 4352, 4368);  slice_scatter_3262 = slice_scatter_3264 = None
        slice_17966: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3265, 1, 4352, 4368)
        slice_17967: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17966, 2, 0, 16)
        slice_scatter_3267: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17966, slice_17967, 2, 0, 16);  slice_17966 = slice_17967 = None
        slice_scatter_3268: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3265, slice_scatter_3267, 1, 4352, 4368);  slice_scatter_3265 = slice_scatter_3267 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_17987: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_17953, 2, 16, 32);  slice_17953 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_548: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_17987, memory_format = torch.contiguous_format);  slice_17987 = None
        view_1100: "f32[32, 11]" = torch.ops.aten.view.default(clone_548, [32, 11]);  clone_548 = None
        mm_545: "f32[32, 8]" = torch.ops.aten.mm.default(view_1100, slice_37)
        view_1101: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_545, [2, 16, 8]);  mm_545 = None
        slice_17994: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3268, 1, 4352, 4368)
        slice_17995: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17994, 2, 0, 16)
        add_547: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_17995, view_1101);  slice_17995 = view_1101 = None
        slice_scatter_3270: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17994, add_547, 2, 0, 16);  slice_17994 = add_547 = None
        slice_scatter_3271: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3268, slice_scatter_3270, 1, 4352, 4368);  slice_scatter_3268 = slice_scatter_3270 = None
        slice_17999: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3271, 1, 4352, 4368)
        slice_18000: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_17999, 2, 0, 16)
        slice_scatter_3273: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_17999, slice_18000, 2, 0, 16);  slice_17999 = slice_18000 = None
        slice_scatter_3274: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3271, slice_scatter_3273, 1, 4352, 4368);  slice_scatter_3271 = slice_scatter_3273 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18019: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4368, 4384)
        slice_18020: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_18019, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_549: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_18020, memory_format = torch.contiguous_format);  slice_18020 = None
        view_1102: "f32[32, 16]" = torch.ops.aten.view.default(clone_549, [32, 16]);  clone_549 = None
        mm_546: "f32[32, 8]" = torch.ops.aten.mm.default(view_1102, slice_7)
        view_1103: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_546, [2, 16, 8]);  mm_546 = None
        slice_18027: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3274, 1, 4368, 4384)
        slice_18028: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18027, 2, 0, 16)
        add_548: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18028, view_1103);  slice_18028 = view_1103 = None
        slice_scatter_3276: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18027, add_548, 2, 0, 16);  slice_18027 = add_548 = None
        slice_scatter_3277: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3274, slice_scatter_3276, 1, 4368, 4384);  slice_scatter_3274 = slice_scatter_3276 = None
        slice_18032: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3277, 1, 4368, 4384)
        slice_18033: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18032, 2, 0, 16)
        slice_scatter_3279: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18032, slice_18033, 2, 0, 16);  slice_18032 = slice_18033 = None
        slice_scatter_3280: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3277, slice_scatter_3279, 1, 4368, 4384);  slice_scatter_3277 = slice_scatter_3279 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18053: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_18019, 2, 16, 32);  slice_18019 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_550: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_18053, memory_format = torch.contiguous_format);  slice_18053 = None
        view_1104: "f32[32, 11]" = torch.ops.aten.view.default(clone_550, [32, 11]);  clone_550 = None
        mm_547: "f32[32, 8]" = torch.ops.aten.mm.default(view_1104, slice_37)
        view_1105: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_547, [2, 16, 8]);  mm_547 = None
        slice_18060: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3280, 1, 4368, 4384)
        slice_18061: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18060, 2, 0, 16)
        add_549: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18061, view_1105);  slice_18061 = view_1105 = None
        slice_scatter_3282: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18060, add_549, 2, 0, 16);  slice_18060 = add_549 = None
        slice_scatter_3283: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3280, slice_scatter_3282, 1, 4368, 4384);  slice_scatter_3280 = slice_scatter_3282 = None
        slice_18065: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3283, 1, 4368, 4384)
        slice_18066: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18065, 2, 0, 16)
        slice_scatter_3285: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18065, slice_18066, 2, 0, 16);  slice_18065 = slice_18066 = None
        slice_scatter_3286: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3283, slice_scatter_3285, 1, 4368, 4384);  slice_scatter_3283 = slice_scatter_3285 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18085: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4384, 4400)
        slice_18086: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_18085, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_551: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_18086, memory_format = torch.contiguous_format);  slice_18086 = None
        view_1106: "f32[32, 16]" = torch.ops.aten.view.default(clone_551, [32, 16]);  clone_551 = None
        mm_548: "f32[32, 8]" = torch.ops.aten.mm.default(view_1106, slice_7)
        view_1107: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_548, [2, 16, 8]);  mm_548 = None
        slice_18093: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3286, 1, 4384, 4400)
        slice_18094: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18093, 2, 0, 16)
        add_550: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18094, view_1107);  slice_18094 = view_1107 = None
        slice_scatter_3288: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18093, add_550, 2, 0, 16);  slice_18093 = add_550 = None
        slice_scatter_3289: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3286, slice_scatter_3288, 1, 4384, 4400);  slice_scatter_3286 = slice_scatter_3288 = None
        slice_18098: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3289, 1, 4384, 4400)
        slice_18099: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18098, 2, 0, 16)
        slice_scatter_3291: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18098, slice_18099, 2, 0, 16);  slice_18098 = slice_18099 = None
        slice_scatter_3292: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3289, slice_scatter_3291, 1, 4384, 4400);  slice_scatter_3289 = slice_scatter_3291 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18119: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_18085, 2, 16, 32);  slice_18085 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_552: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_18119, memory_format = torch.contiguous_format);  slice_18119 = None
        view_1108: "f32[32, 11]" = torch.ops.aten.view.default(clone_552, [32, 11]);  clone_552 = None
        mm_549: "f32[32, 8]" = torch.ops.aten.mm.default(view_1108, slice_37)
        view_1109: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_549, [2, 16, 8]);  mm_549 = None
        slice_18126: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3292, 1, 4384, 4400)
        slice_18127: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18126, 2, 0, 16)
        add_551: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18127, view_1109);  slice_18127 = view_1109 = None
        slice_scatter_3294: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18126, add_551, 2, 0, 16);  slice_18126 = add_551 = None
        slice_scatter_3295: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3292, slice_scatter_3294, 1, 4384, 4400);  slice_scatter_3292 = slice_scatter_3294 = None
        slice_18131: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3295, 1, 4384, 4400)
        slice_18132: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18131, 2, 0, 16)
        slice_scatter_3297: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18131, slice_18132, 2, 0, 16);  slice_18131 = slice_18132 = None
        slice_scatter_3298: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3295, slice_scatter_3297, 1, 4384, 4400);  slice_scatter_3295 = slice_scatter_3297 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18151: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4400, 4416)
        slice_18152: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_18151, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_553: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_18152, memory_format = torch.contiguous_format);  slice_18152 = None
        view_1110: "f32[32, 16]" = torch.ops.aten.view.default(clone_553, [32, 16]);  clone_553 = None
        mm_550: "f32[32, 8]" = torch.ops.aten.mm.default(view_1110, slice_7)
        view_1111: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_550, [2, 16, 8]);  mm_550 = None
        slice_18159: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3298, 1, 4400, 4416)
        slice_18160: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18159, 2, 0, 16)
        add_552: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18160, view_1111);  slice_18160 = view_1111 = None
        slice_scatter_3300: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18159, add_552, 2, 0, 16);  slice_18159 = add_552 = None
        slice_scatter_3301: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3298, slice_scatter_3300, 1, 4400, 4416);  slice_scatter_3298 = slice_scatter_3300 = None
        slice_18164: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3301, 1, 4400, 4416)
        slice_18165: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18164, 2, 0, 16)
        slice_scatter_3303: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18164, slice_18165, 2, 0, 16);  slice_18164 = slice_18165 = None
        slice_scatter_3304: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3301, slice_scatter_3303, 1, 4400, 4416);  slice_scatter_3301 = slice_scatter_3303 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18185: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_18151, 2, 16, 32);  slice_18151 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_554: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_18185, memory_format = torch.contiguous_format);  slice_18185 = None
        view_1112: "f32[32, 11]" = torch.ops.aten.view.default(clone_554, [32, 11]);  clone_554 = None
        mm_551: "f32[32, 8]" = torch.ops.aten.mm.default(view_1112, slice_37)
        view_1113: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_551, [2, 16, 8]);  mm_551 = None
        slice_18192: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3304, 1, 4400, 4416)
        slice_18193: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18192, 2, 0, 16)
        add_553: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18193, view_1113);  slice_18193 = view_1113 = None
        slice_scatter_3306: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18192, add_553, 2, 0, 16);  slice_18192 = add_553 = None
        slice_scatter_3307: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3304, slice_scatter_3306, 1, 4400, 4416);  slice_scatter_3304 = slice_scatter_3306 = None
        slice_18197: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3307, 1, 4400, 4416)
        slice_18198: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18197, 2, 0, 16)
        slice_scatter_3309: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18197, slice_18198, 2, 0, 16);  slice_18197 = slice_18198 = None
        slice_scatter_3310: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3307, slice_scatter_3309, 1, 4400, 4416);  slice_scatter_3307 = slice_scatter_3309 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18217: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4416, 4432)
        slice_18218: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_18217, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_555: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_18218, memory_format = torch.contiguous_format);  slice_18218 = None
        view_1114: "f32[32, 16]" = torch.ops.aten.view.default(clone_555, [32, 16]);  clone_555 = None
        mm_552: "f32[32, 8]" = torch.ops.aten.mm.default(view_1114, slice_7)
        view_1115: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_552, [2, 16, 8]);  mm_552 = None
        slice_18225: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3310, 1, 4416, 4432)
        slice_18226: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18225, 2, 0, 16)
        add_554: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18226, view_1115);  slice_18226 = view_1115 = None
        slice_scatter_3312: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18225, add_554, 2, 0, 16);  slice_18225 = add_554 = None
        slice_scatter_3313: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3310, slice_scatter_3312, 1, 4416, 4432);  slice_scatter_3310 = slice_scatter_3312 = None
        slice_18230: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3313, 1, 4416, 4432)
        slice_18231: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18230, 2, 0, 16)
        slice_scatter_3315: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18230, slice_18231, 2, 0, 16);  slice_18230 = slice_18231 = None
        slice_scatter_3316: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3313, slice_scatter_3315, 1, 4416, 4432);  slice_scatter_3313 = slice_scatter_3315 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18251: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_18217, 2, 16, 32);  slice_18217 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_556: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_18251, memory_format = torch.contiguous_format);  slice_18251 = None
        view_1116: "f32[32, 11]" = torch.ops.aten.view.default(clone_556, [32, 11]);  clone_556 = None
        mm_553: "f32[32, 8]" = torch.ops.aten.mm.default(view_1116, slice_37)
        view_1117: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_553, [2, 16, 8]);  mm_553 = None
        slice_18258: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3316, 1, 4416, 4432)
        slice_18259: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18258, 2, 0, 16)
        add_555: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18259, view_1117);  slice_18259 = view_1117 = None
        slice_scatter_3318: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18258, add_555, 2, 0, 16);  slice_18258 = add_555 = None
        slice_scatter_3319: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3316, slice_scatter_3318, 1, 4416, 4432);  slice_scatter_3316 = slice_scatter_3318 = None
        slice_18263: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3319, 1, 4416, 4432)
        slice_18264: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18263, 2, 0, 16)
        slice_scatter_3321: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18263, slice_18264, 2, 0, 16);  slice_18263 = slice_18264 = None
        slice_scatter_3322: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3319, slice_scatter_3321, 1, 4416, 4432);  slice_scatter_3319 = slice_scatter_3321 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18283: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4432, 4448)
        slice_18284: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_18283, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_557: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_18284, memory_format = torch.contiguous_format);  slice_18284 = None
        view_1118: "f32[32, 16]" = torch.ops.aten.view.default(clone_557, [32, 16]);  clone_557 = None
        mm_554: "f32[32, 8]" = torch.ops.aten.mm.default(view_1118, slice_7)
        view_1119: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_554, [2, 16, 8]);  mm_554 = None
        slice_18291: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3322, 1, 4432, 4448)
        slice_18292: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18291, 2, 0, 16)
        add_556: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18292, view_1119);  slice_18292 = view_1119 = None
        slice_scatter_3324: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18291, add_556, 2, 0, 16);  slice_18291 = add_556 = None
        slice_scatter_3325: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3322, slice_scatter_3324, 1, 4432, 4448);  slice_scatter_3322 = slice_scatter_3324 = None
        slice_18296: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3325, 1, 4432, 4448)
        slice_18297: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18296, 2, 0, 16)
        slice_scatter_3327: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18296, slice_18297, 2, 0, 16);  slice_18296 = slice_18297 = None
        slice_scatter_3328: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3325, slice_scatter_3327, 1, 4432, 4448);  slice_scatter_3325 = slice_scatter_3327 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18317: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_18283, 2, 16, 32);  slice_18283 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_558: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_18317, memory_format = torch.contiguous_format);  slice_18317 = None
        view_1120: "f32[32, 11]" = torch.ops.aten.view.default(clone_558, [32, 11]);  clone_558 = None
        mm_555: "f32[32, 8]" = torch.ops.aten.mm.default(view_1120, slice_37)
        view_1121: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_555, [2, 16, 8]);  mm_555 = None
        slice_18324: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3328, 1, 4432, 4448)
        slice_18325: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18324, 2, 0, 16)
        add_557: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18325, view_1121);  slice_18325 = view_1121 = None
        slice_scatter_3330: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18324, add_557, 2, 0, 16);  slice_18324 = add_557 = None
        slice_scatter_3331: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3328, slice_scatter_3330, 1, 4432, 4448);  slice_scatter_3328 = slice_scatter_3330 = None
        slice_18329: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3331, 1, 4432, 4448)
        slice_18330: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18329, 2, 0, 16)
        slice_scatter_3333: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18329, slice_18330, 2, 0, 16);  slice_18329 = slice_18330 = None
        slice_scatter_3334: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3331, slice_scatter_3333, 1, 4432, 4448);  slice_scatter_3331 = slice_scatter_3333 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18349: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4448, 4464)
        slice_18350: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_18349, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_559: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_18350, memory_format = torch.contiguous_format);  slice_18350 = None
        view_1122: "f32[32, 16]" = torch.ops.aten.view.default(clone_559, [32, 16]);  clone_559 = None
        mm_556: "f32[32, 8]" = torch.ops.aten.mm.default(view_1122, slice_7)
        view_1123: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_556, [2, 16, 8]);  mm_556 = None
        slice_18357: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3334, 1, 4448, 4464)
        slice_18358: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18357, 2, 0, 16)
        add_558: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18358, view_1123);  slice_18358 = view_1123 = None
        slice_scatter_3336: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18357, add_558, 2, 0, 16);  slice_18357 = add_558 = None
        slice_scatter_3337: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3334, slice_scatter_3336, 1, 4448, 4464);  slice_scatter_3334 = slice_scatter_3336 = None
        slice_18362: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3337, 1, 4448, 4464)
        slice_18363: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18362, 2, 0, 16)
        slice_scatter_3339: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18362, slice_18363, 2, 0, 16);  slice_18362 = slice_18363 = None
        slice_scatter_3340: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3337, slice_scatter_3339, 1, 4448, 4464);  slice_scatter_3337 = slice_scatter_3339 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18383: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_18349, 2, 16, 32);  slice_18349 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_560: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_18383, memory_format = torch.contiguous_format);  slice_18383 = None
        view_1124: "f32[32, 11]" = torch.ops.aten.view.default(clone_560, [32, 11]);  clone_560 = None
        mm_557: "f32[32, 8]" = torch.ops.aten.mm.default(view_1124, slice_37)
        view_1125: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_557, [2, 16, 8]);  mm_557 = None
        slice_18390: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3340, 1, 4448, 4464)
        slice_18391: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18390, 2, 0, 16)
        add_559: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18391, view_1125);  slice_18391 = view_1125 = None
        slice_scatter_3342: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18390, add_559, 2, 0, 16);  slice_18390 = add_559 = None
        slice_scatter_3343: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3340, slice_scatter_3342, 1, 4448, 4464);  slice_scatter_3340 = slice_scatter_3342 = None
        slice_18395: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3343, 1, 4448, 4464)
        slice_18396: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18395, 2, 0, 16)
        slice_scatter_3345: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18395, slice_18396, 2, 0, 16);  slice_18395 = slice_18396 = None
        slice_scatter_3346: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3343, slice_scatter_3345, 1, 4448, 4464);  slice_scatter_3343 = slice_scatter_3345 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18415: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4464, 4480)
        slice_18416: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_18415, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_561: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_18416, memory_format = torch.contiguous_format);  slice_18416 = None
        view_1126: "f32[32, 16]" = torch.ops.aten.view.default(clone_561, [32, 16]);  clone_561 = None
        mm_558: "f32[32, 8]" = torch.ops.aten.mm.default(view_1126, slice_7)
        view_1127: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_558, [2, 16, 8]);  mm_558 = None
        slice_18423: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3346, 1, 4464, 4480)
        slice_18424: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18423, 2, 0, 16)
        add_560: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18424, view_1127);  slice_18424 = view_1127 = None
        slice_scatter_3348: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18423, add_560, 2, 0, 16);  slice_18423 = add_560 = None
        slice_scatter_3349: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3346, slice_scatter_3348, 1, 4464, 4480);  slice_scatter_3346 = slice_scatter_3348 = None
        slice_18428: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3349, 1, 4464, 4480)
        slice_18429: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18428, 2, 0, 16)
        slice_scatter_3351: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18428, slice_18429, 2, 0, 16);  slice_18428 = slice_18429 = None
        slice_scatter_3352: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3349, slice_scatter_3351, 1, 4464, 4480);  slice_scatter_3349 = slice_scatter_3351 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18449: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_18415, 2, 16, 32);  slice_18415 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_562: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_18449, memory_format = torch.contiguous_format);  slice_18449 = None
        view_1128: "f32[32, 11]" = torch.ops.aten.view.default(clone_562, [32, 11]);  clone_562 = None
        mm_559: "f32[32, 8]" = torch.ops.aten.mm.default(view_1128, slice_37)
        view_1129: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_559, [2, 16, 8]);  mm_559 = None
        slice_18456: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3352, 1, 4464, 4480)
        slice_18457: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18456, 2, 0, 16)
        add_561: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18457, view_1129);  slice_18457 = view_1129 = None
        slice_scatter_3354: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18456, add_561, 2, 0, 16);  slice_18456 = add_561 = None
        slice_scatter_3355: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3352, slice_scatter_3354, 1, 4464, 4480);  slice_scatter_3352 = slice_scatter_3354 = None
        slice_18461: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3355, 1, 4464, 4480)
        slice_18462: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18461, 2, 0, 16)
        slice_scatter_3357: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18461, slice_18462, 2, 0, 16);  slice_18461 = slice_18462 = None
        slice_scatter_3358: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3355, slice_scatter_3357, 1, 4464, 4480);  slice_scatter_3355 = slice_scatter_3357 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18481: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4480, 4496)
        slice_18482: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_18481, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_563: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_18482, memory_format = torch.contiguous_format);  slice_18482 = None
        view_1130: "f32[32, 16]" = torch.ops.aten.view.default(clone_563, [32, 16]);  clone_563 = None
        mm_560: "f32[32, 8]" = torch.ops.aten.mm.default(view_1130, slice_7)
        view_1131: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_560, [2, 16, 8]);  mm_560 = None
        slice_18489: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3358, 1, 4480, 4496)
        slice_18490: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18489, 2, 0, 16)
        add_562: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18490, view_1131);  slice_18490 = view_1131 = None
        slice_scatter_3360: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18489, add_562, 2, 0, 16);  slice_18489 = add_562 = None
        slice_scatter_3361: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3358, slice_scatter_3360, 1, 4480, 4496);  slice_scatter_3358 = slice_scatter_3360 = None
        slice_18494: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3361, 1, 4480, 4496)
        slice_18495: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18494, 2, 0, 16)
        slice_scatter_3363: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18494, slice_18495, 2, 0, 16);  slice_18494 = slice_18495 = None
        slice_scatter_3364: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3361, slice_scatter_3363, 1, 4480, 4496);  slice_scatter_3361 = slice_scatter_3363 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18515: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_18481, 2, 16, 32);  slice_18481 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_564: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_18515, memory_format = torch.contiguous_format);  slice_18515 = None
        view_1132: "f32[32, 11]" = torch.ops.aten.view.default(clone_564, [32, 11]);  clone_564 = None
        mm_561: "f32[32, 8]" = torch.ops.aten.mm.default(view_1132, slice_37)
        view_1133: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_561, [2, 16, 8]);  mm_561 = None
        slice_18522: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3364, 1, 4480, 4496)
        slice_18523: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18522, 2, 0, 16)
        add_563: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18523, view_1133);  slice_18523 = view_1133 = None
        slice_scatter_3366: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18522, add_563, 2, 0, 16);  slice_18522 = add_563 = None
        slice_scatter_3367: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3364, slice_scatter_3366, 1, 4480, 4496);  slice_scatter_3364 = slice_scatter_3366 = None
        slice_18527: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3367, 1, 4480, 4496)
        slice_18528: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18527, 2, 0, 16)
        slice_scatter_3369: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18527, slice_18528, 2, 0, 16);  slice_18527 = slice_18528 = None
        slice_scatter_3370: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3367, slice_scatter_3369, 1, 4480, 4496);  slice_scatter_3367 = slice_scatter_3369 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18547: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4496, 4512)
        slice_18548: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_18547, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_565: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_18548, memory_format = torch.contiguous_format);  slice_18548 = None
        view_1134: "f32[32, 16]" = torch.ops.aten.view.default(clone_565, [32, 16]);  clone_565 = None
        mm_562: "f32[32, 8]" = torch.ops.aten.mm.default(view_1134, slice_7)
        view_1135: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_562, [2, 16, 8]);  mm_562 = None
        slice_18555: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3370, 1, 4496, 4512)
        slice_18556: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18555, 2, 0, 16)
        add_564: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18556, view_1135);  slice_18556 = view_1135 = None
        slice_scatter_3372: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18555, add_564, 2, 0, 16);  slice_18555 = add_564 = None
        slice_scatter_3373: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3370, slice_scatter_3372, 1, 4496, 4512);  slice_scatter_3370 = slice_scatter_3372 = None
        slice_18560: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3373, 1, 4496, 4512)
        slice_18561: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18560, 2, 0, 16)
        slice_scatter_3375: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18560, slice_18561, 2, 0, 16);  slice_18560 = slice_18561 = None
        slice_scatter_3376: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3373, slice_scatter_3375, 1, 4496, 4512);  slice_scatter_3373 = slice_scatter_3375 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18581: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_18547, 2, 16, 32);  slice_18547 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_566: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_18581, memory_format = torch.contiguous_format);  slice_18581 = None
        view_1136: "f32[32, 11]" = torch.ops.aten.view.default(clone_566, [32, 11]);  clone_566 = None
        mm_563: "f32[32, 8]" = torch.ops.aten.mm.default(view_1136, slice_37)
        view_1137: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_563, [2, 16, 8]);  mm_563 = None
        slice_18588: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3376, 1, 4496, 4512)
        slice_18589: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18588, 2, 0, 16)
        add_565: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18589, view_1137);  slice_18589 = view_1137 = None
        slice_scatter_3378: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18588, add_565, 2, 0, 16);  slice_18588 = add_565 = None
        slice_scatter_3379: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3376, slice_scatter_3378, 1, 4496, 4512);  slice_scatter_3376 = slice_scatter_3378 = None
        slice_18593: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3379, 1, 4496, 4512)
        slice_18594: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18593, 2, 0, 16)
        slice_scatter_3381: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18593, slice_18594, 2, 0, 16);  slice_18593 = slice_18594 = None
        slice_scatter_3382: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3379, slice_scatter_3381, 1, 4496, 4512);  slice_scatter_3379 = slice_scatter_3381 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18613: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4512, 4528)
        slice_18614: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_18613, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_567: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_18614, memory_format = torch.contiguous_format);  slice_18614 = None
        view_1138: "f32[32, 16]" = torch.ops.aten.view.default(clone_567, [32, 16]);  clone_567 = None
        mm_564: "f32[32, 8]" = torch.ops.aten.mm.default(view_1138, slice_7)
        view_1139: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_564, [2, 16, 8]);  mm_564 = None
        slice_18621: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3382, 1, 4512, 4528)
        slice_18622: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18621, 2, 0, 16)
        add_566: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18622, view_1139);  slice_18622 = view_1139 = None
        slice_scatter_3384: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18621, add_566, 2, 0, 16);  slice_18621 = add_566 = None
        slice_scatter_3385: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3382, slice_scatter_3384, 1, 4512, 4528);  slice_scatter_3382 = slice_scatter_3384 = None
        slice_18626: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3385, 1, 4512, 4528)
        slice_18627: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18626, 2, 0, 16)
        slice_scatter_3387: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18626, slice_18627, 2, 0, 16);  slice_18626 = slice_18627 = None
        slice_scatter_3388: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3385, slice_scatter_3387, 1, 4512, 4528);  slice_scatter_3385 = slice_scatter_3387 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18647: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_18613, 2, 16, 32);  slice_18613 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_568: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_18647, memory_format = torch.contiguous_format);  slice_18647 = None
        view_1140: "f32[32, 11]" = torch.ops.aten.view.default(clone_568, [32, 11]);  clone_568 = None
        mm_565: "f32[32, 8]" = torch.ops.aten.mm.default(view_1140, slice_37)
        view_1141: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_565, [2, 16, 8]);  mm_565 = None
        slice_18654: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3388, 1, 4512, 4528)
        slice_18655: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18654, 2, 0, 16)
        add_567: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18655, view_1141);  slice_18655 = view_1141 = None
        slice_scatter_3390: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18654, add_567, 2, 0, 16);  slice_18654 = add_567 = None
        slice_scatter_3391: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3388, slice_scatter_3390, 1, 4512, 4528);  slice_scatter_3388 = slice_scatter_3390 = None
        slice_18659: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3391, 1, 4512, 4528)
        slice_18660: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18659, 2, 0, 16)
        slice_scatter_3393: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18659, slice_18660, 2, 0, 16);  slice_18659 = slice_18660 = None
        slice_scatter_3394: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3391, slice_scatter_3393, 1, 4512, 4528);  slice_scatter_3391 = slice_scatter_3393 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18679: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4528, 4544)
        slice_18680: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_18679, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_569: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_18680, memory_format = torch.contiguous_format);  slice_18680 = None
        view_1142: "f32[32, 16]" = torch.ops.aten.view.default(clone_569, [32, 16]);  clone_569 = None
        mm_566: "f32[32, 8]" = torch.ops.aten.mm.default(view_1142, slice_7)
        view_1143: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_566, [2, 16, 8]);  mm_566 = None
        slice_18687: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3394, 1, 4528, 4544)
        slice_18688: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18687, 2, 0, 16)
        add_568: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18688, view_1143);  slice_18688 = view_1143 = None
        slice_scatter_3396: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18687, add_568, 2, 0, 16);  slice_18687 = add_568 = None
        slice_scatter_3397: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3394, slice_scatter_3396, 1, 4528, 4544);  slice_scatter_3394 = slice_scatter_3396 = None
        slice_18692: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3397, 1, 4528, 4544)
        slice_18693: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18692, 2, 0, 16)
        slice_scatter_3399: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18692, slice_18693, 2, 0, 16);  slice_18692 = slice_18693 = None
        slice_scatter_3400: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3397, slice_scatter_3399, 1, 4528, 4544);  slice_scatter_3397 = slice_scatter_3399 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18713: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_18679, 2, 16, 32);  slice_18679 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_570: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_18713, memory_format = torch.contiguous_format);  slice_18713 = None
        view_1144: "f32[32, 11]" = torch.ops.aten.view.default(clone_570, [32, 11]);  clone_570 = None
        mm_567: "f32[32, 8]" = torch.ops.aten.mm.default(view_1144, slice_37)
        view_1145: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_567, [2, 16, 8]);  mm_567 = None
        slice_18720: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3400, 1, 4528, 4544)
        slice_18721: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18720, 2, 0, 16)
        add_569: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18721, view_1145);  slice_18721 = view_1145 = None
        slice_scatter_3402: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18720, add_569, 2, 0, 16);  slice_18720 = add_569 = None
        slice_scatter_3403: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3400, slice_scatter_3402, 1, 4528, 4544);  slice_scatter_3400 = slice_scatter_3402 = None
        slice_18725: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3403, 1, 4528, 4544)
        slice_18726: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18725, 2, 0, 16)
        slice_scatter_3405: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18725, slice_18726, 2, 0, 16);  slice_18725 = slice_18726 = None
        slice_scatter_3406: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3403, slice_scatter_3405, 1, 4528, 4544);  slice_scatter_3403 = slice_scatter_3405 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18745: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4544, 4560)
        slice_18746: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_18745, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_571: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_18746, memory_format = torch.contiguous_format);  slice_18746 = None
        view_1146: "f32[32, 16]" = torch.ops.aten.view.default(clone_571, [32, 16]);  clone_571 = None
        mm_568: "f32[32, 8]" = torch.ops.aten.mm.default(view_1146, slice_7)
        view_1147: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_568, [2, 16, 8]);  mm_568 = None
        slice_18753: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3406, 1, 4544, 4560)
        slice_18754: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18753, 2, 0, 16)
        add_570: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18754, view_1147);  slice_18754 = view_1147 = None
        slice_scatter_3408: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18753, add_570, 2, 0, 16);  slice_18753 = add_570 = None
        slice_scatter_3409: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3406, slice_scatter_3408, 1, 4544, 4560);  slice_scatter_3406 = slice_scatter_3408 = None
        slice_18758: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3409, 1, 4544, 4560)
        slice_18759: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18758, 2, 0, 16)
        slice_scatter_3411: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18758, slice_18759, 2, 0, 16);  slice_18758 = slice_18759 = None
        slice_scatter_3412: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3409, slice_scatter_3411, 1, 4544, 4560);  slice_scatter_3409 = slice_scatter_3411 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18779: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_18745, 2, 16, 32);  slice_18745 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_572: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_18779, memory_format = torch.contiguous_format);  slice_18779 = None
        view_1148: "f32[32, 11]" = torch.ops.aten.view.default(clone_572, [32, 11]);  clone_572 = None
        mm_569: "f32[32, 8]" = torch.ops.aten.mm.default(view_1148, slice_37)
        view_1149: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_569, [2, 16, 8]);  mm_569 = None
        slice_18786: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3412, 1, 4544, 4560)
        slice_18787: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18786, 2, 0, 16)
        add_571: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18787, view_1149);  slice_18787 = view_1149 = None
        slice_scatter_3414: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18786, add_571, 2, 0, 16);  slice_18786 = add_571 = None
        slice_scatter_3415: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3412, slice_scatter_3414, 1, 4544, 4560);  slice_scatter_3412 = slice_scatter_3414 = None
        slice_18791: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3415, 1, 4544, 4560)
        slice_18792: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18791, 2, 0, 16)
        slice_scatter_3417: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18791, slice_18792, 2, 0, 16);  slice_18791 = slice_18792 = None
        slice_scatter_3418: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3415, slice_scatter_3417, 1, 4544, 4560);  slice_scatter_3415 = slice_scatter_3417 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18811: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4560, 4576)
        slice_18812: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_18811, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_573: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_18812, memory_format = torch.contiguous_format);  slice_18812 = None
        view_1150: "f32[32, 16]" = torch.ops.aten.view.default(clone_573, [32, 16]);  clone_573 = None
        mm_570: "f32[32, 8]" = torch.ops.aten.mm.default(view_1150, slice_7)
        view_1151: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_570, [2, 16, 8]);  mm_570 = None
        slice_18819: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3418, 1, 4560, 4576)
        slice_18820: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18819, 2, 0, 16)
        add_572: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18820, view_1151);  slice_18820 = view_1151 = None
        slice_scatter_3420: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18819, add_572, 2, 0, 16);  slice_18819 = add_572 = None
        slice_scatter_3421: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3418, slice_scatter_3420, 1, 4560, 4576);  slice_scatter_3418 = slice_scatter_3420 = None
        slice_18824: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3421, 1, 4560, 4576)
        slice_18825: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18824, 2, 0, 16)
        slice_scatter_3423: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18824, slice_18825, 2, 0, 16);  slice_18824 = slice_18825 = None
        slice_scatter_3424: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3421, slice_scatter_3423, 1, 4560, 4576);  slice_scatter_3421 = slice_scatter_3423 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18845: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_18811, 2, 16, 32);  slice_18811 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_574: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_18845, memory_format = torch.contiguous_format);  slice_18845 = None
        view_1152: "f32[32, 11]" = torch.ops.aten.view.default(clone_574, [32, 11]);  clone_574 = None
        mm_571: "f32[32, 8]" = torch.ops.aten.mm.default(view_1152, slice_37)
        view_1153: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_571, [2, 16, 8]);  mm_571 = None
        slice_18852: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3424, 1, 4560, 4576)
        slice_18853: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18852, 2, 0, 16)
        add_573: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18853, view_1153);  slice_18853 = view_1153 = None
        slice_scatter_3426: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18852, add_573, 2, 0, 16);  slice_18852 = add_573 = None
        slice_scatter_3427: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3424, slice_scatter_3426, 1, 4560, 4576);  slice_scatter_3424 = slice_scatter_3426 = None
        slice_18857: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3427, 1, 4560, 4576)
        slice_18858: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18857, 2, 0, 16)
        slice_scatter_3429: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18857, slice_18858, 2, 0, 16);  slice_18857 = slice_18858 = None
        slice_scatter_3430: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3427, slice_scatter_3429, 1, 4560, 4576);  slice_scatter_3427 = slice_scatter_3429 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18877: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4576, 4592)
        slice_18878: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_18877, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_575: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_18878, memory_format = torch.contiguous_format);  slice_18878 = None
        view_1154: "f32[32, 16]" = torch.ops.aten.view.default(clone_575, [32, 16]);  clone_575 = None
        mm_572: "f32[32, 8]" = torch.ops.aten.mm.default(view_1154, slice_7)
        view_1155: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_572, [2, 16, 8]);  mm_572 = None
        slice_18885: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3430, 1, 4576, 4592)
        slice_18886: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18885, 2, 0, 16)
        add_574: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18886, view_1155);  slice_18886 = view_1155 = None
        slice_scatter_3432: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18885, add_574, 2, 0, 16);  slice_18885 = add_574 = None
        slice_scatter_3433: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3430, slice_scatter_3432, 1, 4576, 4592);  slice_scatter_3430 = slice_scatter_3432 = None
        slice_18890: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3433, 1, 4576, 4592)
        slice_18891: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18890, 2, 0, 16)
        slice_scatter_3435: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18890, slice_18891, 2, 0, 16);  slice_18890 = slice_18891 = None
        slice_scatter_3436: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3433, slice_scatter_3435, 1, 4576, 4592);  slice_scatter_3433 = slice_scatter_3435 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18911: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_18877, 2, 16, 32);  slice_18877 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_576: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_18911, memory_format = torch.contiguous_format);  slice_18911 = None
        view_1156: "f32[32, 11]" = torch.ops.aten.view.default(clone_576, [32, 11]);  clone_576 = None
        mm_573: "f32[32, 8]" = torch.ops.aten.mm.default(view_1156, slice_37)
        view_1157: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_573, [2, 16, 8]);  mm_573 = None
        slice_18918: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3436, 1, 4576, 4592)
        slice_18919: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18918, 2, 0, 16)
        add_575: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18919, view_1157);  slice_18919 = view_1157 = None
        slice_scatter_3438: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18918, add_575, 2, 0, 16);  slice_18918 = add_575 = None
        slice_scatter_3439: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3436, slice_scatter_3438, 1, 4576, 4592);  slice_scatter_3436 = slice_scatter_3438 = None
        slice_18923: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3439, 1, 4576, 4592)
        slice_18924: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18923, 2, 0, 16)
        slice_scatter_3441: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18923, slice_18924, 2, 0, 16);  slice_18923 = slice_18924 = None
        slice_scatter_3442: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3439, slice_scatter_3441, 1, 4576, 4592);  slice_scatter_3439 = slice_scatter_3441 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18943: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4592, 4608)
        slice_18944: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_18943, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_577: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_18944, memory_format = torch.contiguous_format);  slice_18944 = None
        view_1158: "f32[32, 16]" = torch.ops.aten.view.default(clone_577, [32, 16]);  clone_577 = None
        mm_574: "f32[32, 8]" = torch.ops.aten.mm.default(view_1158, slice_7)
        view_1159: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_574, [2, 16, 8]);  mm_574 = None
        slice_18951: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3442, 1, 4592, 4608)
        slice_18952: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18951, 2, 0, 16)
        add_576: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18952, view_1159);  slice_18952 = view_1159 = None
        slice_scatter_3444: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18951, add_576, 2, 0, 16);  slice_18951 = add_576 = None
        slice_scatter_3445: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3442, slice_scatter_3444, 1, 4592, 4608);  slice_scatter_3442 = slice_scatter_3444 = None
        slice_18956: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3445, 1, 4592, 4608)
        slice_18957: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18956, 2, 0, 16)
        slice_scatter_3447: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18956, slice_18957, 2, 0, 16);  slice_18956 = slice_18957 = None
        slice_scatter_3448: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3445, slice_scatter_3447, 1, 4592, 4608);  slice_scatter_3445 = slice_scatter_3447 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_18977: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_18943, 2, 16, 32);  slice_18943 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_578: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_18977, memory_format = torch.contiguous_format);  slice_18977 = None
        view_1160: "f32[32, 11]" = torch.ops.aten.view.default(clone_578, [32, 11]);  clone_578 = None
        mm_575: "f32[32, 8]" = torch.ops.aten.mm.default(view_1160, slice_37)
        view_1161: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_575, [2, 16, 8]);  mm_575 = None
        slice_18984: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3448, 1, 4592, 4608)
        slice_18985: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18984, 2, 0, 16)
        add_577: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_18985, view_1161);  slice_18985 = view_1161 = None
        slice_scatter_3450: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18984, add_577, 2, 0, 16);  slice_18984 = add_577 = None
        slice_scatter_3451: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3448, slice_scatter_3450, 1, 4592, 4608);  slice_scatter_3448 = slice_scatter_3450 = None
        slice_18989: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3451, 1, 4592, 4608)
        slice_18990: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_18989, 2, 0, 16)
        slice_scatter_3453: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_18989, slice_18990, 2, 0, 16);  slice_18989 = slice_18990 = None
        slice_scatter_3454: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3451, slice_scatter_3453, 1, 4592, 4608);  slice_scatter_3451 = slice_scatter_3453 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19009: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4608, 4624)
        slice_19010: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_19009, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_579: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_19010, memory_format = torch.contiguous_format);  slice_19010 = None
        view_1162: "f32[32, 16]" = torch.ops.aten.view.default(clone_579, [32, 16]);  clone_579 = None
        mm_576: "f32[32, 8]" = torch.ops.aten.mm.default(view_1162, slice_7)
        view_1163: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_576, [2, 16, 8]);  mm_576 = None
        slice_19017: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3454, 1, 4608, 4624)
        slice_19018: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19017, 2, 0, 16)
        add_578: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19018, view_1163);  slice_19018 = view_1163 = None
        slice_scatter_3456: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19017, add_578, 2, 0, 16);  slice_19017 = add_578 = None
        slice_scatter_3457: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3454, slice_scatter_3456, 1, 4608, 4624);  slice_scatter_3454 = slice_scatter_3456 = None
        slice_19022: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3457, 1, 4608, 4624)
        slice_19023: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19022, 2, 0, 16)
        slice_scatter_3459: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19022, slice_19023, 2, 0, 16);  slice_19022 = slice_19023 = None
        slice_scatter_3460: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3457, slice_scatter_3459, 1, 4608, 4624);  slice_scatter_3457 = slice_scatter_3459 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19043: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_19009, 2, 16, 32);  slice_19009 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_580: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_19043, memory_format = torch.contiguous_format);  slice_19043 = None
        view_1164: "f32[32, 11]" = torch.ops.aten.view.default(clone_580, [32, 11]);  clone_580 = None
        mm_577: "f32[32, 8]" = torch.ops.aten.mm.default(view_1164, slice_37)
        view_1165: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_577, [2, 16, 8]);  mm_577 = None
        slice_19050: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3460, 1, 4608, 4624)
        slice_19051: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19050, 2, 0, 16)
        add_579: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19051, view_1165);  slice_19051 = view_1165 = None
        slice_scatter_3462: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19050, add_579, 2, 0, 16);  slice_19050 = add_579 = None
        slice_scatter_3463: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3460, slice_scatter_3462, 1, 4608, 4624);  slice_scatter_3460 = slice_scatter_3462 = None
        slice_19055: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3463, 1, 4608, 4624)
        slice_19056: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19055, 2, 0, 16)
        slice_scatter_3465: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19055, slice_19056, 2, 0, 16);  slice_19055 = slice_19056 = None
        slice_scatter_3466: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3463, slice_scatter_3465, 1, 4608, 4624);  slice_scatter_3463 = slice_scatter_3465 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19075: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4624, 4640)
        slice_19076: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_19075, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_581: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_19076, memory_format = torch.contiguous_format);  slice_19076 = None
        view_1166: "f32[32, 16]" = torch.ops.aten.view.default(clone_581, [32, 16]);  clone_581 = None
        mm_578: "f32[32, 8]" = torch.ops.aten.mm.default(view_1166, slice_7)
        view_1167: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_578, [2, 16, 8]);  mm_578 = None
        slice_19083: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3466, 1, 4624, 4640)
        slice_19084: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19083, 2, 0, 16)
        add_580: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19084, view_1167);  slice_19084 = view_1167 = None
        slice_scatter_3468: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19083, add_580, 2, 0, 16);  slice_19083 = add_580 = None
        slice_scatter_3469: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3466, slice_scatter_3468, 1, 4624, 4640);  slice_scatter_3466 = slice_scatter_3468 = None
        slice_19088: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3469, 1, 4624, 4640)
        slice_19089: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19088, 2, 0, 16)
        slice_scatter_3471: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19088, slice_19089, 2, 0, 16);  slice_19088 = slice_19089 = None
        slice_scatter_3472: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3469, slice_scatter_3471, 1, 4624, 4640);  slice_scatter_3469 = slice_scatter_3471 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19109: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_19075, 2, 16, 32);  slice_19075 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_582: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_19109, memory_format = torch.contiguous_format);  slice_19109 = None
        view_1168: "f32[32, 11]" = torch.ops.aten.view.default(clone_582, [32, 11]);  clone_582 = None
        mm_579: "f32[32, 8]" = torch.ops.aten.mm.default(view_1168, slice_37)
        view_1169: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_579, [2, 16, 8]);  mm_579 = None
        slice_19116: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3472, 1, 4624, 4640)
        slice_19117: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19116, 2, 0, 16)
        add_581: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19117, view_1169);  slice_19117 = view_1169 = None
        slice_scatter_3474: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19116, add_581, 2, 0, 16);  slice_19116 = add_581 = None
        slice_scatter_3475: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3472, slice_scatter_3474, 1, 4624, 4640);  slice_scatter_3472 = slice_scatter_3474 = None
        slice_19121: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3475, 1, 4624, 4640)
        slice_19122: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19121, 2, 0, 16)
        slice_scatter_3477: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19121, slice_19122, 2, 0, 16);  slice_19121 = slice_19122 = None
        slice_scatter_3478: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3475, slice_scatter_3477, 1, 4624, 4640);  slice_scatter_3475 = slice_scatter_3477 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19141: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4640, 4656)
        slice_19142: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_19141, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_583: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_19142, memory_format = torch.contiguous_format);  slice_19142 = None
        view_1170: "f32[32, 16]" = torch.ops.aten.view.default(clone_583, [32, 16]);  clone_583 = None
        mm_580: "f32[32, 8]" = torch.ops.aten.mm.default(view_1170, slice_7)
        view_1171: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_580, [2, 16, 8]);  mm_580 = None
        slice_19149: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3478, 1, 4640, 4656)
        slice_19150: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19149, 2, 0, 16)
        add_582: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19150, view_1171);  slice_19150 = view_1171 = None
        slice_scatter_3480: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19149, add_582, 2, 0, 16);  slice_19149 = add_582 = None
        slice_scatter_3481: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3478, slice_scatter_3480, 1, 4640, 4656);  slice_scatter_3478 = slice_scatter_3480 = None
        slice_19154: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3481, 1, 4640, 4656)
        slice_19155: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19154, 2, 0, 16)
        slice_scatter_3483: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19154, slice_19155, 2, 0, 16);  slice_19154 = slice_19155 = None
        slice_scatter_3484: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3481, slice_scatter_3483, 1, 4640, 4656);  slice_scatter_3481 = slice_scatter_3483 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19175: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_19141, 2, 16, 32);  slice_19141 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_584: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_19175, memory_format = torch.contiguous_format);  slice_19175 = None
        view_1172: "f32[32, 11]" = torch.ops.aten.view.default(clone_584, [32, 11]);  clone_584 = None
        mm_581: "f32[32, 8]" = torch.ops.aten.mm.default(view_1172, slice_37)
        view_1173: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_581, [2, 16, 8]);  mm_581 = None
        slice_19182: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3484, 1, 4640, 4656)
        slice_19183: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19182, 2, 0, 16)
        add_583: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19183, view_1173);  slice_19183 = view_1173 = None
        slice_scatter_3486: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19182, add_583, 2, 0, 16);  slice_19182 = add_583 = None
        slice_scatter_3487: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3484, slice_scatter_3486, 1, 4640, 4656);  slice_scatter_3484 = slice_scatter_3486 = None
        slice_19187: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3487, 1, 4640, 4656)
        slice_19188: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19187, 2, 0, 16)
        slice_scatter_3489: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19187, slice_19188, 2, 0, 16);  slice_19187 = slice_19188 = None
        slice_scatter_3490: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3487, slice_scatter_3489, 1, 4640, 4656);  slice_scatter_3487 = slice_scatter_3489 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19207: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4656, 4672)
        slice_19208: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_19207, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_585: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_19208, memory_format = torch.contiguous_format);  slice_19208 = None
        view_1174: "f32[32, 16]" = torch.ops.aten.view.default(clone_585, [32, 16]);  clone_585 = None
        mm_582: "f32[32, 8]" = torch.ops.aten.mm.default(view_1174, slice_7)
        view_1175: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_582, [2, 16, 8]);  mm_582 = None
        slice_19215: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3490, 1, 4656, 4672)
        slice_19216: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19215, 2, 0, 16)
        add_584: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19216, view_1175);  slice_19216 = view_1175 = None
        slice_scatter_3492: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19215, add_584, 2, 0, 16);  slice_19215 = add_584 = None
        slice_scatter_3493: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3490, slice_scatter_3492, 1, 4656, 4672);  slice_scatter_3490 = slice_scatter_3492 = None
        slice_19220: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3493, 1, 4656, 4672)
        slice_19221: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19220, 2, 0, 16)
        slice_scatter_3495: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19220, slice_19221, 2, 0, 16);  slice_19220 = slice_19221 = None
        slice_scatter_3496: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3493, slice_scatter_3495, 1, 4656, 4672);  slice_scatter_3493 = slice_scatter_3495 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19241: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_19207, 2, 16, 32);  slice_19207 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_586: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_19241, memory_format = torch.contiguous_format);  slice_19241 = None
        view_1176: "f32[32, 11]" = torch.ops.aten.view.default(clone_586, [32, 11]);  clone_586 = None
        mm_583: "f32[32, 8]" = torch.ops.aten.mm.default(view_1176, slice_37)
        view_1177: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_583, [2, 16, 8]);  mm_583 = None
        slice_19248: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3496, 1, 4656, 4672)
        slice_19249: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19248, 2, 0, 16)
        add_585: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19249, view_1177);  slice_19249 = view_1177 = None
        slice_scatter_3498: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19248, add_585, 2, 0, 16);  slice_19248 = add_585 = None
        slice_scatter_3499: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3496, slice_scatter_3498, 1, 4656, 4672);  slice_scatter_3496 = slice_scatter_3498 = None
        slice_19253: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3499, 1, 4656, 4672)
        slice_19254: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19253, 2, 0, 16)
        slice_scatter_3501: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19253, slice_19254, 2, 0, 16);  slice_19253 = slice_19254 = None
        slice_scatter_3502: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3499, slice_scatter_3501, 1, 4656, 4672);  slice_scatter_3499 = slice_scatter_3501 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19273: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4672, 4688)
        slice_19274: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_19273, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_587: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_19274, memory_format = torch.contiguous_format);  slice_19274 = None
        view_1178: "f32[32, 16]" = torch.ops.aten.view.default(clone_587, [32, 16]);  clone_587 = None
        mm_584: "f32[32, 8]" = torch.ops.aten.mm.default(view_1178, slice_7)
        view_1179: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_584, [2, 16, 8]);  mm_584 = None
        slice_19281: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3502, 1, 4672, 4688)
        slice_19282: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19281, 2, 0, 16)
        add_586: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19282, view_1179);  slice_19282 = view_1179 = None
        slice_scatter_3504: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19281, add_586, 2, 0, 16);  slice_19281 = add_586 = None
        slice_scatter_3505: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3502, slice_scatter_3504, 1, 4672, 4688);  slice_scatter_3502 = slice_scatter_3504 = None
        slice_19286: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3505, 1, 4672, 4688)
        slice_19287: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19286, 2, 0, 16)
        slice_scatter_3507: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19286, slice_19287, 2, 0, 16);  slice_19286 = slice_19287 = None
        slice_scatter_3508: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3505, slice_scatter_3507, 1, 4672, 4688);  slice_scatter_3505 = slice_scatter_3507 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19307: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_19273, 2, 16, 32);  slice_19273 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_588: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_19307, memory_format = torch.contiguous_format);  slice_19307 = None
        view_1180: "f32[32, 11]" = torch.ops.aten.view.default(clone_588, [32, 11]);  clone_588 = None
        mm_585: "f32[32, 8]" = torch.ops.aten.mm.default(view_1180, slice_37)
        view_1181: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_585, [2, 16, 8]);  mm_585 = None
        slice_19314: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3508, 1, 4672, 4688)
        slice_19315: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19314, 2, 0, 16)
        add_587: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19315, view_1181);  slice_19315 = view_1181 = None
        slice_scatter_3510: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19314, add_587, 2, 0, 16);  slice_19314 = add_587 = None
        slice_scatter_3511: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3508, slice_scatter_3510, 1, 4672, 4688);  slice_scatter_3508 = slice_scatter_3510 = None
        slice_19319: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3511, 1, 4672, 4688)
        slice_19320: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19319, 2, 0, 16)
        slice_scatter_3513: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19319, slice_19320, 2, 0, 16);  slice_19319 = slice_19320 = None
        slice_scatter_3514: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3511, slice_scatter_3513, 1, 4672, 4688);  slice_scatter_3511 = slice_scatter_3513 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19339: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4688, 4704)
        slice_19340: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_19339, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_589: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_19340, memory_format = torch.contiguous_format);  slice_19340 = None
        view_1182: "f32[32, 16]" = torch.ops.aten.view.default(clone_589, [32, 16]);  clone_589 = None
        mm_586: "f32[32, 8]" = torch.ops.aten.mm.default(view_1182, slice_7)
        view_1183: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_586, [2, 16, 8]);  mm_586 = None
        slice_19347: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3514, 1, 4688, 4704)
        slice_19348: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19347, 2, 0, 16)
        add_588: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19348, view_1183);  slice_19348 = view_1183 = None
        slice_scatter_3516: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19347, add_588, 2, 0, 16);  slice_19347 = add_588 = None
        slice_scatter_3517: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3514, slice_scatter_3516, 1, 4688, 4704);  slice_scatter_3514 = slice_scatter_3516 = None
        slice_19352: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3517, 1, 4688, 4704)
        slice_19353: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19352, 2, 0, 16)
        slice_scatter_3519: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19352, slice_19353, 2, 0, 16);  slice_19352 = slice_19353 = None
        slice_scatter_3520: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3517, slice_scatter_3519, 1, 4688, 4704);  slice_scatter_3517 = slice_scatter_3519 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19373: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_19339, 2, 16, 32);  slice_19339 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_590: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_19373, memory_format = torch.contiguous_format);  slice_19373 = None
        view_1184: "f32[32, 11]" = torch.ops.aten.view.default(clone_590, [32, 11]);  clone_590 = None
        mm_587: "f32[32, 8]" = torch.ops.aten.mm.default(view_1184, slice_37)
        view_1185: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_587, [2, 16, 8]);  mm_587 = None
        slice_19380: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3520, 1, 4688, 4704)
        slice_19381: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19380, 2, 0, 16)
        add_589: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19381, view_1185);  slice_19381 = view_1185 = None
        slice_scatter_3522: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19380, add_589, 2, 0, 16);  slice_19380 = add_589 = None
        slice_scatter_3523: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3520, slice_scatter_3522, 1, 4688, 4704);  slice_scatter_3520 = slice_scatter_3522 = None
        slice_19385: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3523, 1, 4688, 4704)
        slice_19386: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19385, 2, 0, 16)
        slice_scatter_3525: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19385, slice_19386, 2, 0, 16);  slice_19385 = slice_19386 = None
        slice_scatter_3526: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3523, slice_scatter_3525, 1, 4688, 4704);  slice_scatter_3523 = slice_scatter_3525 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19405: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4704, 4720)
        slice_19406: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_19405, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_591: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_19406, memory_format = torch.contiguous_format);  slice_19406 = None
        view_1186: "f32[32, 16]" = torch.ops.aten.view.default(clone_591, [32, 16]);  clone_591 = None
        mm_588: "f32[32, 8]" = torch.ops.aten.mm.default(view_1186, slice_7)
        view_1187: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_588, [2, 16, 8]);  mm_588 = None
        slice_19413: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3526, 1, 4704, 4720)
        slice_19414: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19413, 2, 0, 16)
        add_590: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19414, view_1187);  slice_19414 = view_1187 = None
        slice_scatter_3528: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19413, add_590, 2, 0, 16);  slice_19413 = add_590 = None
        slice_scatter_3529: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3526, slice_scatter_3528, 1, 4704, 4720);  slice_scatter_3526 = slice_scatter_3528 = None
        slice_19418: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3529, 1, 4704, 4720)
        slice_19419: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19418, 2, 0, 16)
        slice_scatter_3531: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19418, slice_19419, 2, 0, 16);  slice_19418 = slice_19419 = None
        slice_scatter_3532: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3529, slice_scatter_3531, 1, 4704, 4720);  slice_scatter_3529 = slice_scatter_3531 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19439: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_19405, 2, 16, 32);  slice_19405 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_592: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_19439, memory_format = torch.contiguous_format);  slice_19439 = None
        view_1188: "f32[32, 11]" = torch.ops.aten.view.default(clone_592, [32, 11]);  clone_592 = None
        mm_589: "f32[32, 8]" = torch.ops.aten.mm.default(view_1188, slice_37)
        view_1189: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_589, [2, 16, 8]);  mm_589 = None
        slice_19446: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3532, 1, 4704, 4720)
        slice_19447: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19446, 2, 0, 16)
        add_591: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19447, view_1189);  slice_19447 = view_1189 = None
        slice_scatter_3534: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19446, add_591, 2, 0, 16);  slice_19446 = add_591 = None
        slice_scatter_3535: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3532, slice_scatter_3534, 1, 4704, 4720);  slice_scatter_3532 = slice_scatter_3534 = None
        slice_19451: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3535, 1, 4704, 4720)
        slice_19452: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19451, 2, 0, 16)
        slice_scatter_3537: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19451, slice_19452, 2, 0, 16);  slice_19451 = slice_19452 = None
        slice_scatter_3538: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3535, slice_scatter_3537, 1, 4704, 4720);  slice_scatter_3535 = slice_scatter_3537 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19471: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4720, 4736)
        slice_19472: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_19471, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_593: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_19472, memory_format = torch.contiguous_format);  slice_19472 = None
        view_1190: "f32[32, 16]" = torch.ops.aten.view.default(clone_593, [32, 16]);  clone_593 = None
        mm_590: "f32[32, 8]" = torch.ops.aten.mm.default(view_1190, slice_7)
        view_1191: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_590, [2, 16, 8]);  mm_590 = None
        slice_19479: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3538, 1, 4720, 4736)
        slice_19480: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19479, 2, 0, 16)
        add_592: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19480, view_1191);  slice_19480 = view_1191 = None
        slice_scatter_3540: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19479, add_592, 2, 0, 16);  slice_19479 = add_592 = None
        slice_scatter_3541: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3538, slice_scatter_3540, 1, 4720, 4736);  slice_scatter_3538 = slice_scatter_3540 = None
        slice_19484: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3541, 1, 4720, 4736)
        slice_19485: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19484, 2, 0, 16)
        slice_scatter_3543: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19484, slice_19485, 2, 0, 16);  slice_19484 = slice_19485 = None
        slice_scatter_3544: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3541, slice_scatter_3543, 1, 4720, 4736);  slice_scatter_3541 = slice_scatter_3543 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19505: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_19471, 2, 16, 32);  slice_19471 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_594: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_19505, memory_format = torch.contiguous_format);  slice_19505 = None
        view_1192: "f32[32, 11]" = torch.ops.aten.view.default(clone_594, [32, 11]);  clone_594 = None
        mm_591: "f32[32, 8]" = torch.ops.aten.mm.default(view_1192, slice_37)
        view_1193: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_591, [2, 16, 8]);  mm_591 = None
        slice_19512: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3544, 1, 4720, 4736)
        slice_19513: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19512, 2, 0, 16)
        add_593: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19513, view_1193);  slice_19513 = view_1193 = None
        slice_scatter_3546: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19512, add_593, 2, 0, 16);  slice_19512 = add_593 = None
        slice_scatter_3547: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3544, slice_scatter_3546, 1, 4720, 4736);  slice_scatter_3544 = slice_scatter_3546 = None
        slice_19517: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3547, 1, 4720, 4736)
        slice_19518: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19517, 2, 0, 16)
        slice_scatter_3549: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19517, slice_19518, 2, 0, 16);  slice_19517 = slice_19518 = None
        slice_scatter_3550: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3547, slice_scatter_3549, 1, 4720, 4736);  slice_scatter_3547 = slice_scatter_3549 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19537: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4736, 4752)
        slice_19538: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_19537, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_595: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_19538, memory_format = torch.contiguous_format);  slice_19538 = None
        view_1194: "f32[32, 16]" = torch.ops.aten.view.default(clone_595, [32, 16]);  clone_595 = None
        mm_592: "f32[32, 8]" = torch.ops.aten.mm.default(view_1194, slice_7)
        view_1195: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_592, [2, 16, 8]);  mm_592 = None
        slice_19545: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3550, 1, 4736, 4752)
        slice_19546: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19545, 2, 0, 16)
        add_594: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19546, view_1195);  slice_19546 = view_1195 = None
        slice_scatter_3552: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19545, add_594, 2, 0, 16);  slice_19545 = add_594 = None
        slice_scatter_3553: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3550, slice_scatter_3552, 1, 4736, 4752);  slice_scatter_3550 = slice_scatter_3552 = None
        slice_19550: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3553, 1, 4736, 4752)
        slice_19551: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19550, 2, 0, 16)
        slice_scatter_3555: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19550, slice_19551, 2, 0, 16);  slice_19550 = slice_19551 = None
        slice_scatter_3556: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3553, slice_scatter_3555, 1, 4736, 4752);  slice_scatter_3553 = slice_scatter_3555 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19571: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_19537, 2, 16, 32);  slice_19537 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_596: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_19571, memory_format = torch.contiguous_format);  slice_19571 = None
        view_1196: "f32[32, 11]" = torch.ops.aten.view.default(clone_596, [32, 11]);  clone_596 = None
        mm_593: "f32[32, 8]" = torch.ops.aten.mm.default(view_1196, slice_37)
        view_1197: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_593, [2, 16, 8]);  mm_593 = None
        slice_19578: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3556, 1, 4736, 4752)
        slice_19579: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19578, 2, 0, 16)
        add_595: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19579, view_1197);  slice_19579 = view_1197 = None
        slice_scatter_3558: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19578, add_595, 2, 0, 16);  slice_19578 = add_595 = None
        slice_scatter_3559: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3556, slice_scatter_3558, 1, 4736, 4752);  slice_scatter_3556 = slice_scatter_3558 = None
        slice_19583: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3559, 1, 4736, 4752)
        slice_19584: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19583, 2, 0, 16)
        slice_scatter_3561: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19583, slice_19584, 2, 0, 16);  slice_19583 = slice_19584 = None
        slice_scatter_3562: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3559, slice_scatter_3561, 1, 4736, 4752);  slice_scatter_3559 = slice_scatter_3561 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19603: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4752, 4768)
        slice_19604: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_19603, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_597: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_19604, memory_format = torch.contiguous_format);  slice_19604 = None
        view_1198: "f32[32, 16]" = torch.ops.aten.view.default(clone_597, [32, 16]);  clone_597 = None
        mm_594: "f32[32, 8]" = torch.ops.aten.mm.default(view_1198, slice_7)
        view_1199: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_594, [2, 16, 8]);  mm_594 = None
        slice_19611: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3562, 1, 4752, 4768)
        slice_19612: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19611, 2, 0, 16)
        add_596: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19612, view_1199);  slice_19612 = view_1199 = None
        slice_scatter_3564: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19611, add_596, 2, 0, 16);  slice_19611 = add_596 = None
        slice_scatter_3565: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3562, slice_scatter_3564, 1, 4752, 4768);  slice_scatter_3562 = slice_scatter_3564 = None
        slice_19616: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3565, 1, 4752, 4768)
        slice_19617: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19616, 2, 0, 16)
        slice_scatter_3567: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19616, slice_19617, 2, 0, 16);  slice_19616 = slice_19617 = None
        slice_scatter_3568: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3565, slice_scatter_3567, 1, 4752, 4768);  slice_scatter_3565 = slice_scatter_3567 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19637: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_19603, 2, 16, 32);  slice_19603 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_598: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_19637, memory_format = torch.contiguous_format);  slice_19637 = None
        view_1200: "f32[32, 11]" = torch.ops.aten.view.default(clone_598, [32, 11]);  clone_598 = None
        mm_595: "f32[32, 8]" = torch.ops.aten.mm.default(view_1200, slice_37)
        view_1201: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_595, [2, 16, 8]);  mm_595 = None
        slice_19644: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3568, 1, 4752, 4768)
        slice_19645: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19644, 2, 0, 16)
        add_597: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19645, view_1201);  slice_19645 = view_1201 = None
        slice_scatter_3570: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19644, add_597, 2, 0, 16);  slice_19644 = add_597 = None
        slice_scatter_3571: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3568, slice_scatter_3570, 1, 4752, 4768);  slice_scatter_3568 = slice_scatter_3570 = None
        slice_19649: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3571, 1, 4752, 4768)
        slice_19650: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19649, 2, 0, 16)
        slice_scatter_3573: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19649, slice_19650, 2, 0, 16);  slice_19649 = slice_19650 = None
        slice_scatter_3574: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3571, slice_scatter_3573, 1, 4752, 4768);  slice_scatter_3571 = slice_scatter_3573 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19669: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4768, 4784)
        slice_19670: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_19669, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_599: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_19670, memory_format = torch.contiguous_format);  slice_19670 = None
        view_1202: "f32[32, 16]" = torch.ops.aten.view.default(clone_599, [32, 16]);  clone_599 = None
        mm_596: "f32[32, 8]" = torch.ops.aten.mm.default(view_1202, slice_7)
        view_1203: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_596, [2, 16, 8]);  mm_596 = None
        slice_19677: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3574, 1, 4768, 4784)
        slice_19678: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19677, 2, 0, 16)
        add_598: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19678, view_1203);  slice_19678 = view_1203 = None
        slice_scatter_3576: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19677, add_598, 2, 0, 16);  slice_19677 = add_598 = None
        slice_scatter_3577: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3574, slice_scatter_3576, 1, 4768, 4784);  slice_scatter_3574 = slice_scatter_3576 = None
        slice_19682: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3577, 1, 4768, 4784)
        slice_19683: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19682, 2, 0, 16)
        slice_scatter_3579: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19682, slice_19683, 2, 0, 16);  slice_19682 = slice_19683 = None
        slice_scatter_3580: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3577, slice_scatter_3579, 1, 4768, 4784);  slice_scatter_3577 = slice_scatter_3579 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19703: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_19669, 2, 16, 32);  slice_19669 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_600: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_19703, memory_format = torch.contiguous_format);  slice_19703 = None
        view_1204: "f32[32, 11]" = torch.ops.aten.view.default(clone_600, [32, 11]);  clone_600 = None
        mm_597: "f32[32, 8]" = torch.ops.aten.mm.default(view_1204, slice_37)
        view_1205: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_597, [2, 16, 8]);  mm_597 = None
        slice_19710: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3580, 1, 4768, 4784)
        slice_19711: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19710, 2, 0, 16)
        add_599: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19711, view_1205);  slice_19711 = view_1205 = None
        slice_scatter_3582: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19710, add_599, 2, 0, 16);  slice_19710 = add_599 = None
        slice_scatter_3583: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3580, slice_scatter_3582, 1, 4768, 4784);  slice_scatter_3580 = slice_scatter_3582 = None
        slice_19715: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3583, 1, 4768, 4784)
        slice_19716: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19715, 2, 0, 16)
        slice_scatter_3585: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19715, slice_19716, 2, 0, 16);  slice_19715 = slice_19716 = None
        slice_scatter_3586: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3583, slice_scatter_3585, 1, 4768, 4784);  slice_scatter_3583 = slice_scatter_3585 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19735: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4784, 4800)
        slice_19736: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_19735, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_601: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_19736, memory_format = torch.contiguous_format);  slice_19736 = None
        view_1206: "f32[32, 16]" = torch.ops.aten.view.default(clone_601, [32, 16]);  clone_601 = None
        mm_598: "f32[32, 8]" = torch.ops.aten.mm.default(view_1206, slice_7)
        view_1207: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_598, [2, 16, 8]);  mm_598 = None
        slice_19743: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3586, 1, 4784, 4800)
        slice_19744: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19743, 2, 0, 16)
        add_600: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19744, view_1207);  slice_19744 = view_1207 = None
        slice_scatter_3588: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19743, add_600, 2, 0, 16);  slice_19743 = add_600 = None
        slice_scatter_3589: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3586, slice_scatter_3588, 1, 4784, 4800);  slice_scatter_3586 = slice_scatter_3588 = None
        slice_19748: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3589, 1, 4784, 4800)
        slice_19749: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19748, 2, 0, 16)
        slice_scatter_3591: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19748, slice_19749, 2, 0, 16);  slice_19748 = slice_19749 = None
        slice_scatter_3592: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3589, slice_scatter_3591, 1, 4784, 4800);  slice_scatter_3589 = slice_scatter_3591 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19769: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_19735, 2, 16, 32);  slice_19735 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_602: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_19769, memory_format = torch.contiguous_format);  slice_19769 = None
        view_1208: "f32[32, 11]" = torch.ops.aten.view.default(clone_602, [32, 11]);  clone_602 = None
        mm_599: "f32[32, 8]" = torch.ops.aten.mm.default(view_1208, slice_37)
        view_1209: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_599, [2, 16, 8]);  mm_599 = None
        slice_19776: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3592, 1, 4784, 4800)
        slice_19777: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19776, 2, 0, 16)
        add_601: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19777, view_1209);  slice_19777 = view_1209 = None
        slice_scatter_3594: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19776, add_601, 2, 0, 16);  slice_19776 = add_601 = None
        slice_scatter_3595: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3592, slice_scatter_3594, 1, 4784, 4800);  slice_scatter_3592 = slice_scatter_3594 = None
        slice_19781: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3595, 1, 4784, 4800)
        slice_19782: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19781, 2, 0, 16)
        slice_scatter_3597: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19781, slice_19782, 2, 0, 16);  slice_19781 = slice_19782 = None
        slice_scatter_3598: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3595, slice_scatter_3597, 1, 4784, 4800);  slice_scatter_3595 = slice_scatter_3597 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19801: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4800, 4816)
        slice_19802: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_19801, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_603: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_19802, memory_format = torch.contiguous_format);  slice_19802 = None
        view_1210: "f32[32, 16]" = torch.ops.aten.view.default(clone_603, [32, 16]);  clone_603 = None
        mm_600: "f32[32, 8]" = torch.ops.aten.mm.default(view_1210, slice_7)
        view_1211: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_600, [2, 16, 8]);  mm_600 = None
        slice_19809: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3598, 1, 4800, 4816)
        slice_19810: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19809, 2, 0, 16)
        add_602: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19810, view_1211);  slice_19810 = view_1211 = None
        slice_scatter_3600: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19809, add_602, 2, 0, 16);  slice_19809 = add_602 = None
        slice_scatter_3601: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3598, slice_scatter_3600, 1, 4800, 4816);  slice_scatter_3598 = slice_scatter_3600 = None
        slice_19814: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3601, 1, 4800, 4816)
        slice_19815: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19814, 2, 0, 16)
        slice_scatter_3603: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19814, slice_19815, 2, 0, 16);  slice_19814 = slice_19815 = None
        slice_scatter_3604: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3601, slice_scatter_3603, 1, 4800, 4816);  slice_scatter_3601 = slice_scatter_3603 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19835: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_19801, 2, 16, 32);  slice_19801 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_604: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_19835, memory_format = torch.contiguous_format);  slice_19835 = None
        view_1212: "f32[32, 11]" = torch.ops.aten.view.default(clone_604, [32, 11]);  clone_604 = None
        mm_601: "f32[32, 8]" = torch.ops.aten.mm.default(view_1212, slice_37)
        view_1213: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_601, [2, 16, 8]);  mm_601 = None
        slice_19842: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3604, 1, 4800, 4816)
        slice_19843: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19842, 2, 0, 16)
        add_603: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19843, view_1213);  slice_19843 = view_1213 = None
        slice_scatter_3606: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19842, add_603, 2, 0, 16);  slice_19842 = add_603 = None
        slice_scatter_3607: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3604, slice_scatter_3606, 1, 4800, 4816);  slice_scatter_3604 = slice_scatter_3606 = None
        slice_19847: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3607, 1, 4800, 4816)
        slice_19848: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19847, 2, 0, 16)
        slice_scatter_3609: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19847, slice_19848, 2, 0, 16);  slice_19847 = slice_19848 = None
        slice_scatter_3610: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3607, slice_scatter_3609, 1, 4800, 4816);  slice_scatter_3607 = slice_scatter_3609 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19867: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4816, 4832)
        slice_19868: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_19867, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_605: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_19868, memory_format = torch.contiguous_format);  slice_19868 = None
        view_1214: "f32[32, 16]" = torch.ops.aten.view.default(clone_605, [32, 16]);  clone_605 = None
        mm_602: "f32[32, 8]" = torch.ops.aten.mm.default(view_1214, slice_7)
        view_1215: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_602, [2, 16, 8]);  mm_602 = None
        slice_19875: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3610, 1, 4816, 4832)
        slice_19876: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19875, 2, 0, 16)
        add_604: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19876, view_1215);  slice_19876 = view_1215 = None
        slice_scatter_3612: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19875, add_604, 2, 0, 16);  slice_19875 = add_604 = None
        slice_scatter_3613: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3610, slice_scatter_3612, 1, 4816, 4832);  slice_scatter_3610 = slice_scatter_3612 = None
        slice_19880: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3613, 1, 4816, 4832)
        slice_19881: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19880, 2, 0, 16)
        slice_scatter_3615: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19880, slice_19881, 2, 0, 16);  slice_19880 = slice_19881 = None
        slice_scatter_3616: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3613, slice_scatter_3615, 1, 4816, 4832);  slice_scatter_3613 = slice_scatter_3615 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19901: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_19867, 2, 16, 32);  slice_19867 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_606: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_19901, memory_format = torch.contiguous_format);  slice_19901 = None
        view_1216: "f32[32, 11]" = torch.ops.aten.view.default(clone_606, [32, 11]);  clone_606 = None
        mm_603: "f32[32, 8]" = torch.ops.aten.mm.default(view_1216, slice_37)
        view_1217: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_603, [2, 16, 8]);  mm_603 = None
        slice_19908: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3616, 1, 4816, 4832)
        slice_19909: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19908, 2, 0, 16)
        add_605: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19909, view_1217);  slice_19909 = view_1217 = None
        slice_scatter_3618: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19908, add_605, 2, 0, 16);  slice_19908 = add_605 = None
        slice_scatter_3619: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3616, slice_scatter_3618, 1, 4816, 4832);  slice_scatter_3616 = slice_scatter_3618 = None
        slice_19913: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3619, 1, 4816, 4832)
        slice_19914: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19913, 2, 0, 16)
        slice_scatter_3621: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19913, slice_19914, 2, 0, 16);  slice_19913 = slice_19914 = None
        slice_scatter_3622: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3619, slice_scatter_3621, 1, 4816, 4832);  slice_scatter_3619 = slice_scatter_3621 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19933: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4832, 4848)
        slice_19934: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_19933, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_607: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_19934, memory_format = torch.contiguous_format);  slice_19934 = None
        view_1218: "f32[32, 16]" = torch.ops.aten.view.default(clone_607, [32, 16]);  clone_607 = None
        mm_604: "f32[32, 8]" = torch.ops.aten.mm.default(view_1218, slice_7)
        view_1219: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_604, [2, 16, 8]);  mm_604 = None
        slice_19941: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3622, 1, 4832, 4848)
        slice_19942: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19941, 2, 0, 16)
        add_606: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19942, view_1219);  slice_19942 = view_1219 = None
        slice_scatter_3624: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19941, add_606, 2, 0, 16);  slice_19941 = add_606 = None
        slice_scatter_3625: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3622, slice_scatter_3624, 1, 4832, 4848);  slice_scatter_3622 = slice_scatter_3624 = None
        slice_19946: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3625, 1, 4832, 4848)
        slice_19947: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19946, 2, 0, 16)
        slice_scatter_3627: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19946, slice_19947, 2, 0, 16);  slice_19946 = slice_19947 = None
        slice_scatter_3628: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3625, slice_scatter_3627, 1, 4832, 4848);  slice_scatter_3625 = slice_scatter_3627 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19967: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_19933, 2, 16, 32);  slice_19933 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_608: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_19967, memory_format = torch.contiguous_format);  slice_19967 = None
        view_1220: "f32[32, 11]" = torch.ops.aten.view.default(clone_608, [32, 11]);  clone_608 = None
        mm_605: "f32[32, 8]" = torch.ops.aten.mm.default(view_1220, slice_37)
        view_1221: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_605, [2, 16, 8]);  mm_605 = None
        slice_19974: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3628, 1, 4832, 4848)
        slice_19975: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19974, 2, 0, 16)
        add_607: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_19975, view_1221);  slice_19975 = view_1221 = None
        slice_scatter_3630: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19974, add_607, 2, 0, 16);  slice_19974 = add_607 = None
        slice_scatter_3631: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3628, slice_scatter_3630, 1, 4832, 4848);  slice_scatter_3628 = slice_scatter_3630 = None
        slice_19979: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3631, 1, 4832, 4848)
        slice_19980: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_19979, 2, 0, 16)
        slice_scatter_3633: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_19979, slice_19980, 2, 0, 16);  slice_19979 = slice_19980 = None
        slice_scatter_3634: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3631, slice_scatter_3633, 1, 4832, 4848);  slice_scatter_3631 = slice_scatter_3633 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_19999: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4848, 4864)
        slice_20000: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_19999, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_609: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_20000, memory_format = torch.contiguous_format);  slice_20000 = None
        view_1222: "f32[32, 16]" = torch.ops.aten.view.default(clone_609, [32, 16]);  clone_609 = None
        mm_606: "f32[32, 8]" = torch.ops.aten.mm.default(view_1222, slice_7)
        view_1223: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_606, [2, 16, 8]);  mm_606 = None
        slice_20007: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3634, 1, 4848, 4864)
        slice_20008: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20007, 2, 0, 16)
        add_608: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20008, view_1223);  slice_20008 = view_1223 = None
        slice_scatter_3636: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20007, add_608, 2, 0, 16);  slice_20007 = add_608 = None
        slice_scatter_3637: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3634, slice_scatter_3636, 1, 4848, 4864);  slice_scatter_3634 = slice_scatter_3636 = None
        slice_20012: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3637, 1, 4848, 4864)
        slice_20013: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20012, 2, 0, 16)
        slice_scatter_3639: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20012, slice_20013, 2, 0, 16);  slice_20012 = slice_20013 = None
        slice_scatter_3640: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3637, slice_scatter_3639, 1, 4848, 4864);  slice_scatter_3637 = slice_scatter_3639 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20033: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_19999, 2, 16, 32);  slice_19999 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_610: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_20033, memory_format = torch.contiguous_format);  slice_20033 = None
        view_1224: "f32[32, 11]" = torch.ops.aten.view.default(clone_610, [32, 11]);  clone_610 = None
        mm_607: "f32[32, 8]" = torch.ops.aten.mm.default(view_1224, slice_37)
        view_1225: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_607, [2, 16, 8]);  mm_607 = None
        slice_20040: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3640, 1, 4848, 4864)
        slice_20041: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20040, 2, 0, 16)
        add_609: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20041, view_1225);  slice_20041 = view_1225 = None
        slice_scatter_3642: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20040, add_609, 2, 0, 16);  slice_20040 = add_609 = None
        slice_scatter_3643: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3640, slice_scatter_3642, 1, 4848, 4864);  slice_scatter_3640 = slice_scatter_3642 = None
        slice_20045: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3643, 1, 4848, 4864)
        slice_20046: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20045, 2, 0, 16)
        slice_scatter_3645: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20045, slice_20046, 2, 0, 16);  slice_20045 = slice_20046 = None
        slice_scatter_3646: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3643, slice_scatter_3645, 1, 4848, 4864);  slice_scatter_3643 = slice_scatter_3645 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20065: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4864, 4880)
        slice_20066: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_20065, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_611: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_20066, memory_format = torch.contiguous_format);  slice_20066 = None
        view_1226: "f32[32, 16]" = torch.ops.aten.view.default(clone_611, [32, 16]);  clone_611 = None
        mm_608: "f32[32, 8]" = torch.ops.aten.mm.default(view_1226, slice_7)
        view_1227: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_608, [2, 16, 8]);  mm_608 = None
        slice_20073: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3646, 1, 4864, 4880)
        slice_20074: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20073, 2, 0, 16)
        add_610: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20074, view_1227);  slice_20074 = view_1227 = None
        slice_scatter_3648: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20073, add_610, 2, 0, 16);  slice_20073 = add_610 = None
        slice_scatter_3649: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3646, slice_scatter_3648, 1, 4864, 4880);  slice_scatter_3646 = slice_scatter_3648 = None
        slice_20078: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3649, 1, 4864, 4880)
        slice_20079: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20078, 2, 0, 16)
        slice_scatter_3651: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20078, slice_20079, 2, 0, 16);  slice_20078 = slice_20079 = None
        slice_scatter_3652: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3649, slice_scatter_3651, 1, 4864, 4880);  slice_scatter_3649 = slice_scatter_3651 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20099: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_20065, 2, 16, 32);  slice_20065 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_612: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_20099, memory_format = torch.contiguous_format);  slice_20099 = None
        view_1228: "f32[32, 11]" = torch.ops.aten.view.default(clone_612, [32, 11]);  clone_612 = None
        mm_609: "f32[32, 8]" = torch.ops.aten.mm.default(view_1228, slice_37)
        view_1229: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_609, [2, 16, 8]);  mm_609 = None
        slice_20106: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3652, 1, 4864, 4880)
        slice_20107: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20106, 2, 0, 16)
        add_611: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20107, view_1229);  slice_20107 = view_1229 = None
        slice_scatter_3654: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20106, add_611, 2, 0, 16);  slice_20106 = add_611 = None
        slice_scatter_3655: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3652, slice_scatter_3654, 1, 4864, 4880);  slice_scatter_3652 = slice_scatter_3654 = None
        slice_20111: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3655, 1, 4864, 4880)
        slice_20112: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20111, 2, 0, 16)
        slice_scatter_3657: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20111, slice_20112, 2, 0, 16);  slice_20111 = slice_20112 = None
        slice_scatter_3658: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3655, slice_scatter_3657, 1, 4864, 4880);  slice_scatter_3655 = slice_scatter_3657 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20131: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4880, 4896)
        slice_20132: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_20131, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_613: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_20132, memory_format = torch.contiguous_format);  slice_20132 = None
        view_1230: "f32[32, 16]" = torch.ops.aten.view.default(clone_613, [32, 16]);  clone_613 = None
        mm_610: "f32[32, 8]" = torch.ops.aten.mm.default(view_1230, slice_7)
        view_1231: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_610, [2, 16, 8]);  mm_610 = None
        slice_20139: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3658, 1, 4880, 4896)
        slice_20140: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20139, 2, 0, 16)
        add_612: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20140, view_1231);  slice_20140 = view_1231 = None
        slice_scatter_3660: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20139, add_612, 2, 0, 16);  slice_20139 = add_612 = None
        slice_scatter_3661: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3658, slice_scatter_3660, 1, 4880, 4896);  slice_scatter_3658 = slice_scatter_3660 = None
        slice_20144: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3661, 1, 4880, 4896)
        slice_20145: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20144, 2, 0, 16)
        slice_scatter_3663: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20144, slice_20145, 2, 0, 16);  slice_20144 = slice_20145 = None
        slice_scatter_3664: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3661, slice_scatter_3663, 1, 4880, 4896);  slice_scatter_3661 = slice_scatter_3663 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20165: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_20131, 2, 16, 32);  slice_20131 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_614: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_20165, memory_format = torch.contiguous_format);  slice_20165 = None
        view_1232: "f32[32, 11]" = torch.ops.aten.view.default(clone_614, [32, 11]);  clone_614 = None
        mm_611: "f32[32, 8]" = torch.ops.aten.mm.default(view_1232, slice_37)
        view_1233: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_611, [2, 16, 8]);  mm_611 = None
        slice_20172: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3664, 1, 4880, 4896)
        slice_20173: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20172, 2, 0, 16)
        add_613: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20173, view_1233);  slice_20173 = view_1233 = None
        slice_scatter_3666: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20172, add_613, 2, 0, 16);  slice_20172 = add_613 = None
        slice_scatter_3667: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3664, slice_scatter_3666, 1, 4880, 4896);  slice_scatter_3664 = slice_scatter_3666 = None
        slice_20177: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3667, 1, 4880, 4896)
        slice_20178: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20177, 2, 0, 16)
        slice_scatter_3669: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20177, slice_20178, 2, 0, 16);  slice_20177 = slice_20178 = None
        slice_scatter_3670: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3667, slice_scatter_3669, 1, 4880, 4896);  slice_scatter_3667 = slice_scatter_3669 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20197: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4896, 4912)
        slice_20198: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_20197, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_615: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_20198, memory_format = torch.contiguous_format);  slice_20198 = None
        view_1234: "f32[32, 16]" = torch.ops.aten.view.default(clone_615, [32, 16]);  clone_615 = None
        mm_612: "f32[32, 8]" = torch.ops.aten.mm.default(view_1234, slice_7)
        view_1235: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_612, [2, 16, 8]);  mm_612 = None
        slice_20205: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3670, 1, 4896, 4912)
        slice_20206: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20205, 2, 0, 16)
        add_614: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20206, view_1235);  slice_20206 = view_1235 = None
        slice_scatter_3672: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20205, add_614, 2, 0, 16);  slice_20205 = add_614 = None
        slice_scatter_3673: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3670, slice_scatter_3672, 1, 4896, 4912);  slice_scatter_3670 = slice_scatter_3672 = None
        slice_20210: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3673, 1, 4896, 4912)
        slice_20211: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20210, 2, 0, 16)
        slice_scatter_3675: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20210, slice_20211, 2, 0, 16);  slice_20210 = slice_20211 = None
        slice_scatter_3676: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3673, slice_scatter_3675, 1, 4896, 4912);  slice_scatter_3673 = slice_scatter_3675 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20231: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_20197, 2, 16, 32);  slice_20197 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_616: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_20231, memory_format = torch.contiguous_format);  slice_20231 = None
        view_1236: "f32[32, 11]" = torch.ops.aten.view.default(clone_616, [32, 11]);  clone_616 = None
        mm_613: "f32[32, 8]" = torch.ops.aten.mm.default(view_1236, slice_37)
        view_1237: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_613, [2, 16, 8]);  mm_613 = None
        slice_20238: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3676, 1, 4896, 4912)
        slice_20239: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20238, 2, 0, 16)
        add_615: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20239, view_1237);  slice_20239 = view_1237 = None
        slice_scatter_3678: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20238, add_615, 2, 0, 16);  slice_20238 = add_615 = None
        slice_scatter_3679: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3676, slice_scatter_3678, 1, 4896, 4912);  slice_scatter_3676 = slice_scatter_3678 = None
        slice_20243: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3679, 1, 4896, 4912)
        slice_20244: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20243, 2, 0, 16)
        slice_scatter_3681: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20243, slice_20244, 2, 0, 16);  slice_20243 = slice_20244 = None
        slice_scatter_3682: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3679, slice_scatter_3681, 1, 4896, 4912);  slice_scatter_3679 = slice_scatter_3681 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20263: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4912, 4928)
        slice_20264: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_20263, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_617: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_20264, memory_format = torch.contiguous_format);  slice_20264 = None
        view_1238: "f32[32, 16]" = torch.ops.aten.view.default(clone_617, [32, 16]);  clone_617 = None
        mm_614: "f32[32, 8]" = torch.ops.aten.mm.default(view_1238, slice_7)
        view_1239: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_614, [2, 16, 8]);  mm_614 = None
        slice_20271: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3682, 1, 4912, 4928)
        slice_20272: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20271, 2, 0, 16)
        add_616: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20272, view_1239);  slice_20272 = view_1239 = None
        slice_scatter_3684: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20271, add_616, 2, 0, 16);  slice_20271 = add_616 = None
        slice_scatter_3685: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3682, slice_scatter_3684, 1, 4912, 4928);  slice_scatter_3682 = slice_scatter_3684 = None
        slice_20276: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3685, 1, 4912, 4928)
        slice_20277: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20276, 2, 0, 16)
        slice_scatter_3687: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20276, slice_20277, 2, 0, 16);  slice_20276 = slice_20277 = None
        slice_scatter_3688: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3685, slice_scatter_3687, 1, 4912, 4928);  slice_scatter_3685 = slice_scatter_3687 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20297: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_20263, 2, 16, 32);  slice_20263 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_618: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_20297, memory_format = torch.contiguous_format);  slice_20297 = None
        view_1240: "f32[32, 11]" = torch.ops.aten.view.default(clone_618, [32, 11]);  clone_618 = None
        mm_615: "f32[32, 8]" = torch.ops.aten.mm.default(view_1240, slice_37)
        view_1241: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_615, [2, 16, 8]);  mm_615 = None
        slice_20304: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3688, 1, 4912, 4928)
        slice_20305: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20304, 2, 0, 16)
        add_617: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20305, view_1241);  slice_20305 = view_1241 = None
        slice_scatter_3690: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20304, add_617, 2, 0, 16);  slice_20304 = add_617 = None
        slice_scatter_3691: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3688, slice_scatter_3690, 1, 4912, 4928);  slice_scatter_3688 = slice_scatter_3690 = None
        slice_20309: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3691, 1, 4912, 4928)
        slice_20310: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20309, 2, 0, 16)
        slice_scatter_3693: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20309, slice_20310, 2, 0, 16);  slice_20309 = slice_20310 = None
        slice_scatter_3694: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3691, slice_scatter_3693, 1, 4912, 4928);  slice_scatter_3691 = slice_scatter_3693 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20329: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4928, 4944)
        slice_20330: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_20329, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_619: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_20330, memory_format = torch.contiguous_format);  slice_20330 = None
        view_1242: "f32[32, 16]" = torch.ops.aten.view.default(clone_619, [32, 16]);  clone_619 = None
        mm_616: "f32[32, 8]" = torch.ops.aten.mm.default(view_1242, slice_7)
        view_1243: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_616, [2, 16, 8]);  mm_616 = None
        slice_20337: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3694, 1, 4928, 4944)
        slice_20338: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20337, 2, 0, 16)
        add_618: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20338, view_1243);  slice_20338 = view_1243 = None
        slice_scatter_3696: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20337, add_618, 2, 0, 16);  slice_20337 = add_618 = None
        slice_scatter_3697: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3694, slice_scatter_3696, 1, 4928, 4944);  slice_scatter_3694 = slice_scatter_3696 = None
        slice_20342: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3697, 1, 4928, 4944)
        slice_20343: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20342, 2, 0, 16)
        slice_scatter_3699: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20342, slice_20343, 2, 0, 16);  slice_20342 = slice_20343 = None
        slice_scatter_3700: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3697, slice_scatter_3699, 1, 4928, 4944);  slice_scatter_3697 = slice_scatter_3699 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20363: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_20329, 2, 16, 32);  slice_20329 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_620: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_20363, memory_format = torch.contiguous_format);  slice_20363 = None
        view_1244: "f32[32, 11]" = torch.ops.aten.view.default(clone_620, [32, 11]);  clone_620 = None
        mm_617: "f32[32, 8]" = torch.ops.aten.mm.default(view_1244, slice_37)
        view_1245: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_617, [2, 16, 8]);  mm_617 = None
        slice_20370: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3700, 1, 4928, 4944)
        slice_20371: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20370, 2, 0, 16)
        add_619: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20371, view_1245);  slice_20371 = view_1245 = None
        slice_scatter_3702: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20370, add_619, 2, 0, 16);  slice_20370 = add_619 = None
        slice_scatter_3703: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3700, slice_scatter_3702, 1, 4928, 4944);  slice_scatter_3700 = slice_scatter_3702 = None
        slice_20375: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3703, 1, 4928, 4944)
        slice_20376: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20375, 2, 0, 16)
        slice_scatter_3705: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20375, slice_20376, 2, 0, 16);  slice_20375 = slice_20376 = None
        slice_scatter_3706: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3703, slice_scatter_3705, 1, 4928, 4944);  slice_scatter_3703 = slice_scatter_3705 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20395: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4944, 4960)
        slice_20396: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_20395, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_621: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_20396, memory_format = torch.contiguous_format);  slice_20396 = None
        view_1246: "f32[32, 16]" = torch.ops.aten.view.default(clone_621, [32, 16]);  clone_621 = None
        mm_618: "f32[32, 8]" = torch.ops.aten.mm.default(view_1246, slice_7)
        view_1247: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_618, [2, 16, 8]);  mm_618 = None
        slice_20403: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3706, 1, 4944, 4960)
        slice_20404: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20403, 2, 0, 16)
        add_620: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20404, view_1247);  slice_20404 = view_1247 = None
        slice_scatter_3708: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20403, add_620, 2, 0, 16);  slice_20403 = add_620 = None
        slice_scatter_3709: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3706, slice_scatter_3708, 1, 4944, 4960);  slice_scatter_3706 = slice_scatter_3708 = None
        slice_20408: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3709, 1, 4944, 4960)
        slice_20409: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20408, 2, 0, 16)
        slice_scatter_3711: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20408, slice_20409, 2, 0, 16);  slice_20408 = slice_20409 = None
        slice_scatter_3712: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3709, slice_scatter_3711, 1, 4944, 4960);  slice_scatter_3709 = slice_scatter_3711 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20429: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_20395, 2, 16, 32);  slice_20395 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_622: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_20429, memory_format = torch.contiguous_format);  slice_20429 = None
        view_1248: "f32[32, 11]" = torch.ops.aten.view.default(clone_622, [32, 11]);  clone_622 = None
        mm_619: "f32[32, 8]" = torch.ops.aten.mm.default(view_1248, slice_37)
        view_1249: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_619, [2, 16, 8]);  mm_619 = None
        slice_20436: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3712, 1, 4944, 4960)
        slice_20437: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20436, 2, 0, 16)
        add_621: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20437, view_1249);  slice_20437 = view_1249 = None
        slice_scatter_3714: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20436, add_621, 2, 0, 16);  slice_20436 = add_621 = None
        slice_scatter_3715: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3712, slice_scatter_3714, 1, 4944, 4960);  slice_scatter_3712 = slice_scatter_3714 = None
        slice_20441: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3715, 1, 4944, 4960)
        slice_20442: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20441, 2, 0, 16)
        slice_scatter_3717: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20441, slice_20442, 2, 0, 16);  slice_20441 = slice_20442 = None
        slice_scatter_3718: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3715, slice_scatter_3717, 1, 4944, 4960);  slice_scatter_3715 = slice_scatter_3717 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20461: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4960, 4976)
        slice_20462: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_20461, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_623: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_20462, memory_format = torch.contiguous_format);  slice_20462 = None
        view_1250: "f32[32, 16]" = torch.ops.aten.view.default(clone_623, [32, 16]);  clone_623 = None
        mm_620: "f32[32, 8]" = torch.ops.aten.mm.default(view_1250, slice_7)
        view_1251: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_620, [2, 16, 8]);  mm_620 = None
        slice_20469: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3718, 1, 4960, 4976)
        slice_20470: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20469, 2, 0, 16)
        add_622: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20470, view_1251);  slice_20470 = view_1251 = None
        slice_scatter_3720: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20469, add_622, 2, 0, 16);  slice_20469 = add_622 = None
        slice_scatter_3721: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3718, slice_scatter_3720, 1, 4960, 4976);  slice_scatter_3718 = slice_scatter_3720 = None
        slice_20474: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3721, 1, 4960, 4976)
        slice_20475: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20474, 2, 0, 16)
        slice_scatter_3723: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20474, slice_20475, 2, 0, 16);  slice_20474 = slice_20475 = None
        slice_scatter_3724: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3721, slice_scatter_3723, 1, 4960, 4976);  slice_scatter_3721 = slice_scatter_3723 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20495: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_20461, 2, 16, 32);  slice_20461 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_624: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_20495, memory_format = torch.contiguous_format);  slice_20495 = None
        view_1252: "f32[32, 11]" = torch.ops.aten.view.default(clone_624, [32, 11]);  clone_624 = None
        mm_621: "f32[32, 8]" = torch.ops.aten.mm.default(view_1252, slice_37)
        view_1253: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_621, [2, 16, 8]);  mm_621 = None
        slice_20502: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3724, 1, 4960, 4976)
        slice_20503: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20502, 2, 0, 16)
        add_623: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20503, view_1253);  slice_20503 = view_1253 = None
        slice_scatter_3726: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20502, add_623, 2, 0, 16);  slice_20502 = add_623 = None
        slice_scatter_3727: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3724, slice_scatter_3726, 1, 4960, 4976);  slice_scatter_3724 = slice_scatter_3726 = None
        slice_20507: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3727, 1, 4960, 4976)
        slice_20508: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20507, 2, 0, 16)
        slice_scatter_3729: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20507, slice_20508, 2, 0, 16);  slice_20507 = slice_20508 = None
        slice_scatter_3730: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3727, slice_scatter_3729, 1, 4960, 4976);  slice_scatter_3727 = slice_scatter_3729 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20527: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4976, 4992)
        slice_20528: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_20527, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_625: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_20528, memory_format = torch.contiguous_format);  slice_20528 = None
        view_1254: "f32[32, 16]" = torch.ops.aten.view.default(clone_625, [32, 16]);  clone_625 = None
        mm_622: "f32[32, 8]" = torch.ops.aten.mm.default(view_1254, slice_7)
        view_1255: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_622, [2, 16, 8]);  mm_622 = None
        slice_20535: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3730, 1, 4976, 4992)
        slice_20536: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20535, 2, 0, 16)
        add_624: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20536, view_1255);  slice_20536 = view_1255 = None
        slice_scatter_3732: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20535, add_624, 2, 0, 16);  slice_20535 = add_624 = None
        slice_scatter_3733: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3730, slice_scatter_3732, 1, 4976, 4992);  slice_scatter_3730 = slice_scatter_3732 = None
        slice_20540: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3733, 1, 4976, 4992)
        slice_20541: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20540, 2, 0, 16)
        slice_scatter_3735: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20540, slice_20541, 2, 0, 16);  slice_20540 = slice_20541 = None
        slice_scatter_3736: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3733, slice_scatter_3735, 1, 4976, 4992);  slice_scatter_3733 = slice_scatter_3735 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20561: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_20527, 2, 16, 32);  slice_20527 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_626: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_20561, memory_format = torch.contiguous_format);  slice_20561 = None
        view_1256: "f32[32, 11]" = torch.ops.aten.view.default(clone_626, [32, 11]);  clone_626 = None
        mm_623: "f32[32, 8]" = torch.ops.aten.mm.default(view_1256, slice_37)
        view_1257: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_623, [2, 16, 8]);  mm_623 = None
        slice_20568: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3736, 1, 4976, 4992)
        slice_20569: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20568, 2, 0, 16)
        add_625: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20569, view_1257);  slice_20569 = view_1257 = None
        slice_scatter_3738: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20568, add_625, 2, 0, 16);  slice_20568 = add_625 = None
        slice_scatter_3739: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3736, slice_scatter_3738, 1, 4976, 4992);  slice_scatter_3736 = slice_scatter_3738 = None
        slice_20573: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3739, 1, 4976, 4992)
        slice_20574: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20573, 2, 0, 16)
        slice_scatter_3741: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20573, slice_20574, 2, 0, 16);  slice_20573 = slice_20574 = None
        slice_scatter_3742: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3739, slice_scatter_3741, 1, 4976, 4992);  slice_scatter_3739 = slice_scatter_3741 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20593: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 4992, 5008)
        slice_20594: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_20593, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_627: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_20594, memory_format = torch.contiguous_format);  slice_20594 = None
        view_1258: "f32[32, 16]" = torch.ops.aten.view.default(clone_627, [32, 16]);  clone_627 = None
        mm_624: "f32[32, 8]" = torch.ops.aten.mm.default(view_1258, slice_7)
        view_1259: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_624, [2, 16, 8]);  mm_624 = None
        slice_20601: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3742, 1, 4992, 5008)
        slice_20602: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20601, 2, 0, 16)
        add_626: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20602, view_1259);  slice_20602 = view_1259 = None
        slice_scatter_3744: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20601, add_626, 2, 0, 16);  slice_20601 = add_626 = None
        slice_scatter_3745: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3742, slice_scatter_3744, 1, 4992, 5008);  slice_scatter_3742 = slice_scatter_3744 = None
        slice_20606: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3745, 1, 4992, 5008)
        slice_20607: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20606, 2, 0, 16)
        slice_scatter_3747: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20606, slice_20607, 2, 0, 16);  slice_20606 = slice_20607 = None
        slice_scatter_3748: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3745, slice_scatter_3747, 1, 4992, 5008);  slice_scatter_3745 = slice_scatter_3747 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20627: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_20593, 2, 16, 32);  slice_20593 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_628: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_20627, memory_format = torch.contiguous_format);  slice_20627 = None
        view_1260: "f32[32, 11]" = torch.ops.aten.view.default(clone_628, [32, 11]);  clone_628 = None
        mm_625: "f32[32, 8]" = torch.ops.aten.mm.default(view_1260, slice_37)
        view_1261: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_625, [2, 16, 8]);  mm_625 = None
        slice_20634: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3748, 1, 4992, 5008)
        slice_20635: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20634, 2, 0, 16)
        add_627: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20635, view_1261);  slice_20635 = view_1261 = None
        slice_scatter_3750: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20634, add_627, 2, 0, 16);  slice_20634 = add_627 = None
        slice_scatter_3751: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3748, slice_scatter_3750, 1, 4992, 5008);  slice_scatter_3748 = slice_scatter_3750 = None
        slice_20639: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3751, 1, 4992, 5008)
        slice_20640: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20639, 2, 0, 16)
        slice_scatter_3753: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20639, slice_20640, 2, 0, 16);  slice_20639 = slice_20640 = None
        slice_scatter_3754: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3751, slice_scatter_3753, 1, 4992, 5008);  slice_scatter_3751 = slice_scatter_3753 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20659: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5008, 5024)
        slice_20660: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_20659, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_629: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_20660, memory_format = torch.contiguous_format);  slice_20660 = None
        view_1262: "f32[32, 16]" = torch.ops.aten.view.default(clone_629, [32, 16]);  clone_629 = None
        mm_626: "f32[32, 8]" = torch.ops.aten.mm.default(view_1262, slice_7)
        view_1263: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_626, [2, 16, 8]);  mm_626 = None
        slice_20667: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3754, 1, 5008, 5024)
        slice_20668: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20667, 2, 0, 16)
        add_628: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20668, view_1263);  slice_20668 = view_1263 = None
        slice_scatter_3756: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20667, add_628, 2, 0, 16);  slice_20667 = add_628 = None
        slice_scatter_3757: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3754, slice_scatter_3756, 1, 5008, 5024);  slice_scatter_3754 = slice_scatter_3756 = None
        slice_20672: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3757, 1, 5008, 5024)
        slice_20673: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20672, 2, 0, 16)
        slice_scatter_3759: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20672, slice_20673, 2, 0, 16);  slice_20672 = slice_20673 = None
        slice_scatter_3760: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3757, slice_scatter_3759, 1, 5008, 5024);  slice_scatter_3757 = slice_scatter_3759 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20693: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_20659, 2, 16, 32);  slice_20659 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_630: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_20693, memory_format = torch.contiguous_format);  slice_20693 = None
        view_1264: "f32[32, 11]" = torch.ops.aten.view.default(clone_630, [32, 11]);  clone_630 = None
        mm_627: "f32[32, 8]" = torch.ops.aten.mm.default(view_1264, slice_37)
        view_1265: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_627, [2, 16, 8]);  mm_627 = None
        slice_20700: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3760, 1, 5008, 5024)
        slice_20701: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20700, 2, 0, 16)
        add_629: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20701, view_1265);  slice_20701 = view_1265 = None
        slice_scatter_3762: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20700, add_629, 2, 0, 16);  slice_20700 = add_629 = None
        slice_scatter_3763: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3760, slice_scatter_3762, 1, 5008, 5024);  slice_scatter_3760 = slice_scatter_3762 = None
        slice_20705: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3763, 1, 5008, 5024)
        slice_20706: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20705, 2, 0, 16)
        slice_scatter_3765: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20705, slice_20706, 2, 0, 16);  slice_20705 = slice_20706 = None
        slice_scatter_3766: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3763, slice_scatter_3765, 1, 5008, 5024);  slice_scatter_3763 = slice_scatter_3765 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20725: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5024, 5040)
        slice_20726: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_20725, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_631: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_20726, memory_format = torch.contiguous_format);  slice_20726 = None
        view_1266: "f32[32, 16]" = torch.ops.aten.view.default(clone_631, [32, 16]);  clone_631 = None
        mm_628: "f32[32, 8]" = torch.ops.aten.mm.default(view_1266, slice_7)
        view_1267: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_628, [2, 16, 8]);  mm_628 = None
        slice_20733: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3766, 1, 5024, 5040)
        slice_20734: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20733, 2, 0, 16)
        add_630: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20734, view_1267);  slice_20734 = view_1267 = None
        slice_scatter_3768: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20733, add_630, 2, 0, 16);  slice_20733 = add_630 = None
        slice_scatter_3769: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3766, slice_scatter_3768, 1, 5024, 5040);  slice_scatter_3766 = slice_scatter_3768 = None
        slice_20738: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3769, 1, 5024, 5040)
        slice_20739: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20738, 2, 0, 16)
        slice_scatter_3771: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20738, slice_20739, 2, 0, 16);  slice_20738 = slice_20739 = None
        slice_scatter_3772: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3769, slice_scatter_3771, 1, 5024, 5040);  slice_scatter_3769 = slice_scatter_3771 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20759: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_20725, 2, 16, 32);  slice_20725 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_632: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_20759, memory_format = torch.contiguous_format);  slice_20759 = None
        view_1268: "f32[32, 11]" = torch.ops.aten.view.default(clone_632, [32, 11]);  clone_632 = None
        mm_629: "f32[32, 8]" = torch.ops.aten.mm.default(view_1268, slice_37)
        view_1269: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_629, [2, 16, 8]);  mm_629 = None
        slice_20766: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3772, 1, 5024, 5040)
        slice_20767: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20766, 2, 0, 16)
        add_631: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20767, view_1269);  slice_20767 = view_1269 = None
        slice_scatter_3774: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20766, add_631, 2, 0, 16);  slice_20766 = add_631 = None
        slice_scatter_3775: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3772, slice_scatter_3774, 1, 5024, 5040);  slice_scatter_3772 = slice_scatter_3774 = None
        slice_20771: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3775, 1, 5024, 5040)
        slice_20772: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20771, 2, 0, 16)
        slice_scatter_3777: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20771, slice_20772, 2, 0, 16);  slice_20771 = slice_20772 = None
        slice_scatter_3778: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3775, slice_scatter_3777, 1, 5024, 5040);  slice_scatter_3775 = slice_scatter_3777 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20791: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5040, 5056)
        slice_20792: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_20791, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_633: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_20792, memory_format = torch.contiguous_format);  slice_20792 = None
        view_1270: "f32[32, 16]" = torch.ops.aten.view.default(clone_633, [32, 16]);  clone_633 = None
        mm_630: "f32[32, 8]" = torch.ops.aten.mm.default(view_1270, slice_7)
        view_1271: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_630, [2, 16, 8]);  mm_630 = None
        slice_20799: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3778, 1, 5040, 5056)
        slice_20800: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20799, 2, 0, 16)
        add_632: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20800, view_1271);  slice_20800 = view_1271 = None
        slice_scatter_3780: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20799, add_632, 2, 0, 16);  slice_20799 = add_632 = None
        slice_scatter_3781: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3778, slice_scatter_3780, 1, 5040, 5056);  slice_scatter_3778 = slice_scatter_3780 = None
        slice_20804: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3781, 1, 5040, 5056)
        slice_20805: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20804, 2, 0, 16)
        slice_scatter_3783: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20804, slice_20805, 2, 0, 16);  slice_20804 = slice_20805 = None
        slice_scatter_3784: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3781, slice_scatter_3783, 1, 5040, 5056);  slice_scatter_3781 = slice_scatter_3783 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20825: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_20791, 2, 16, 32);  slice_20791 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_634: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_20825, memory_format = torch.contiguous_format);  slice_20825 = None
        view_1272: "f32[32, 11]" = torch.ops.aten.view.default(clone_634, [32, 11]);  clone_634 = None
        mm_631: "f32[32, 8]" = torch.ops.aten.mm.default(view_1272, slice_37)
        view_1273: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_631, [2, 16, 8]);  mm_631 = None
        slice_20832: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3784, 1, 5040, 5056)
        slice_20833: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20832, 2, 0, 16)
        add_633: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20833, view_1273);  slice_20833 = view_1273 = None
        slice_scatter_3786: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20832, add_633, 2, 0, 16);  slice_20832 = add_633 = None
        slice_scatter_3787: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3784, slice_scatter_3786, 1, 5040, 5056);  slice_scatter_3784 = slice_scatter_3786 = None
        slice_20837: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3787, 1, 5040, 5056)
        slice_20838: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20837, 2, 0, 16)
        slice_scatter_3789: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20837, slice_20838, 2, 0, 16);  slice_20837 = slice_20838 = None
        slice_scatter_3790: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3787, slice_scatter_3789, 1, 5040, 5056);  slice_scatter_3787 = slice_scatter_3789 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20857: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5056, 5072)
        slice_20858: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_20857, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_635: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_20858, memory_format = torch.contiguous_format);  slice_20858 = None
        view_1274: "f32[32, 16]" = torch.ops.aten.view.default(clone_635, [32, 16]);  clone_635 = None
        mm_632: "f32[32, 8]" = torch.ops.aten.mm.default(view_1274, slice_7)
        view_1275: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_632, [2, 16, 8]);  mm_632 = None
        slice_20865: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3790, 1, 5056, 5072)
        slice_20866: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20865, 2, 0, 16)
        add_634: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20866, view_1275);  slice_20866 = view_1275 = None
        slice_scatter_3792: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20865, add_634, 2, 0, 16);  slice_20865 = add_634 = None
        slice_scatter_3793: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3790, slice_scatter_3792, 1, 5056, 5072);  slice_scatter_3790 = slice_scatter_3792 = None
        slice_20870: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3793, 1, 5056, 5072)
        slice_20871: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20870, 2, 0, 16)
        slice_scatter_3795: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20870, slice_20871, 2, 0, 16);  slice_20870 = slice_20871 = None
        slice_scatter_3796: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3793, slice_scatter_3795, 1, 5056, 5072);  slice_scatter_3793 = slice_scatter_3795 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20891: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_20857, 2, 16, 32);  slice_20857 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_636: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_20891, memory_format = torch.contiguous_format);  slice_20891 = None
        view_1276: "f32[32, 11]" = torch.ops.aten.view.default(clone_636, [32, 11]);  clone_636 = None
        mm_633: "f32[32, 8]" = torch.ops.aten.mm.default(view_1276, slice_37)
        view_1277: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_633, [2, 16, 8]);  mm_633 = None
        slice_20898: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3796, 1, 5056, 5072)
        slice_20899: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20898, 2, 0, 16)
        add_635: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20899, view_1277);  slice_20899 = view_1277 = None
        slice_scatter_3798: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20898, add_635, 2, 0, 16);  slice_20898 = add_635 = None
        slice_scatter_3799: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3796, slice_scatter_3798, 1, 5056, 5072);  slice_scatter_3796 = slice_scatter_3798 = None
        slice_20903: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3799, 1, 5056, 5072)
        slice_20904: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20903, 2, 0, 16)
        slice_scatter_3801: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20903, slice_20904, 2, 0, 16);  slice_20903 = slice_20904 = None
        slice_scatter_3802: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3799, slice_scatter_3801, 1, 5056, 5072);  slice_scatter_3799 = slice_scatter_3801 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20923: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5072, 5088)
        slice_20924: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_20923, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_637: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_20924, memory_format = torch.contiguous_format);  slice_20924 = None
        view_1278: "f32[32, 16]" = torch.ops.aten.view.default(clone_637, [32, 16]);  clone_637 = None
        mm_634: "f32[32, 8]" = torch.ops.aten.mm.default(view_1278, slice_7)
        view_1279: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_634, [2, 16, 8]);  mm_634 = None
        slice_20931: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3802, 1, 5072, 5088)
        slice_20932: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20931, 2, 0, 16)
        add_636: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20932, view_1279);  slice_20932 = view_1279 = None
        slice_scatter_3804: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20931, add_636, 2, 0, 16);  slice_20931 = add_636 = None
        slice_scatter_3805: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3802, slice_scatter_3804, 1, 5072, 5088);  slice_scatter_3802 = slice_scatter_3804 = None
        slice_20936: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3805, 1, 5072, 5088)
        slice_20937: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20936, 2, 0, 16)
        slice_scatter_3807: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20936, slice_20937, 2, 0, 16);  slice_20936 = slice_20937 = None
        slice_scatter_3808: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3805, slice_scatter_3807, 1, 5072, 5088);  slice_scatter_3805 = slice_scatter_3807 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20957: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_20923, 2, 16, 32);  slice_20923 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_638: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_20957, memory_format = torch.contiguous_format);  slice_20957 = None
        view_1280: "f32[32, 11]" = torch.ops.aten.view.default(clone_638, [32, 11]);  clone_638 = None
        mm_635: "f32[32, 8]" = torch.ops.aten.mm.default(view_1280, slice_37)
        view_1281: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_635, [2, 16, 8]);  mm_635 = None
        slice_20964: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3808, 1, 5072, 5088)
        slice_20965: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20964, 2, 0, 16)
        add_637: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20965, view_1281);  slice_20965 = view_1281 = None
        slice_scatter_3810: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20964, add_637, 2, 0, 16);  slice_20964 = add_637 = None
        slice_scatter_3811: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3808, slice_scatter_3810, 1, 5072, 5088);  slice_scatter_3808 = slice_scatter_3810 = None
        slice_20969: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3811, 1, 5072, 5088)
        slice_20970: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20969, 2, 0, 16)
        slice_scatter_3813: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20969, slice_20970, 2, 0, 16);  slice_20969 = slice_20970 = None
        slice_scatter_3814: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3811, slice_scatter_3813, 1, 5072, 5088);  slice_scatter_3811 = slice_scatter_3813 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_20989: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5088, 5104)
        slice_20990: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_20989, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_639: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_20990, memory_format = torch.contiguous_format);  slice_20990 = None
        view_1282: "f32[32, 16]" = torch.ops.aten.view.default(clone_639, [32, 16]);  clone_639 = None
        mm_636: "f32[32, 8]" = torch.ops.aten.mm.default(view_1282, slice_7)
        view_1283: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_636, [2, 16, 8]);  mm_636 = None
        slice_20997: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3814, 1, 5088, 5104)
        slice_20998: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_20997, 2, 0, 16)
        add_638: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_20998, view_1283);  slice_20998 = view_1283 = None
        slice_scatter_3816: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_20997, add_638, 2, 0, 16);  slice_20997 = add_638 = None
        slice_scatter_3817: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3814, slice_scatter_3816, 1, 5088, 5104);  slice_scatter_3814 = slice_scatter_3816 = None
        slice_21002: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3817, 1, 5088, 5104)
        slice_21003: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21002, 2, 0, 16)
        slice_scatter_3819: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21002, slice_21003, 2, 0, 16);  slice_21002 = slice_21003 = None
        slice_scatter_3820: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3817, slice_scatter_3819, 1, 5088, 5104);  slice_scatter_3817 = slice_scatter_3819 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21023: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_20989, 2, 16, 32);  slice_20989 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_640: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_21023, memory_format = torch.contiguous_format);  slice_21023 = None
        view_1284: "f32[32, 11]" = torch.ops.aten.view.default(clone_640, [32, 11]);  clone_640 = None
        mm_637: "f32[32, 8]" = torch.ops.aten.mm.default(view_1284, slice_37)
        view_1285: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_637, [2, 16, 8]);  mm_637 = None
        slice_21030: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3820, 1, 5088, 5104)
        slice_21031: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21030, 2, 0, 16)
        add_639: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21031, view_1285);  slice_21031 = view_1285 = None
        slice_scatter_3822: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21030, add_639, 2, 0, 16);  slice_21030 = add_639 = None
        slice_scatter_3823: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3820, slice_scatter_3822, 1, 5088, 5104);  slice_scatter_3820 = slice_scatter_3822 = None
        slice_21035: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3823, 1, 5088, 5104)
        slice_21036: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21035, 2, 0, 16)
        slice_scatter_3825: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21035, slice_21036, 2, 0, 16);  slice_21035 = slice_21036 = None
        slice_scatter_3826: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3823, slice_scatter_3825, 1, 5088, 5104);  slice_scatter_3823 = slice_scatter_3825 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21055: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5104, 5120)
        slice_21056: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_21055, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_641: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_21056, memory_format = torch.contiguous_format);  slice_21056 = None
        view_1286: "f32[32, 16]" = torch.ops.aten.view.default(clone_641, [32, 16]);  clone_641 = None
        mm_638: "f32[32, 8]" = torch.ops.aten.mm.default(view_1286, slice_7)
        view_1287: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_638, [2, 16, 8]);  mm_638 = None
        slice_21063: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3826, 1, 5104, 5120)
        slice_21064: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21063, 2, 0, 16)
        add_640: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21064, view_1287);  slice_21064 = view_1287 = None
        slice_scatter_3828: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21063, add_640, 2, 0, 16);  slice_21063 = add_640 = None
        slice_scatter_3829: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3826, slice_scatter_3828, 1, 5104, 5120);  slice_scatter_3826 = slice_scatter_3828 = None
        slice_21068: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3829, 1, 5104, 5120)
        slice_21069: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21068, 2, 0, 16)
        slice_scatter_3831: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21068, slice_21069, 2, 0, 16);  slice_21068 = slice_21069 = None
        slice_scatter_3832: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3829, slice_scatter_3831, 1, 5104, 5120);  slice_scatter_3829 = slice_scatter_3831 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21089: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_21055, 2, 16, 32);  slice_21055 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_642: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_21089, memory_format = torch.contiguous_format);  slice_21089 = None
        view_1288: "f32[32, 11]" = torch.ops.aten.view.default(clone_642, [32, 11]);  clone_642 = None
        mm_639: "f32[32, 8]" = torch.ops.aten.mm.default(view_1288, slice_37)
        view_1289: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_639, [2, 16, 8]);  mm_639 = None
        slice_21096: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3832, 1, 5104, 5120)
        slice_21097: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21096, 2, 0, 16)
        add_641: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21097, view_1289);  slice_21097 = view_1289 = None
        slice_scatter_3834: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21096, add_641, 2, 0, 16);  slice_21096 = add_641 = None
        slice_scatter_3835: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3832, slice_scatter_3834, 1, 5104, 5120);  slice_scatter_3832 = slice_scatter_3834 = None
        slice_21101: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3835, 1, 5104, 5120)
        slice_21102: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21101, 2, 0, 16)
        slice_scatter_3837: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21101, slice_21102, 2, 0, 16);  slice_21101 = slice_21102 = None
        slice_scatter_3838: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3835, slice_scatter_3837, 1, 5104, 5120);  slice_scatter_3835 = slice_scatter_3837 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21121: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5120, 5136)
        slice_21122: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_21121, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_643: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_21122, memory_format = torch.contiguous_format);  slice_21122 = None
        view_1290: "f32[32, 16]" = torch.ops.aten.view.default(clone_643, [32, 16]);  clone_643 = None
        mm_640: "f32[32, 8]" = torch.ops.aten.mm.default(view_1290, slice_7)
        view_1291: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_640, [2, 16, 8]);  mm_640 = None
        slice_21129: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3838, 1, 5120, 5136)
        slice_21130: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21129, 2, 0, 16)
        add_642: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21130, view_1291);  slice_21130 = view_1291 = None
        slice_scatter_3840: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21129, add_642, 2, 0, 16);  slice_21129 = add_642 = None
        slice_scatter_3841: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3838, slice_scatter_3840, 1, 5120, 5136);  slice_scatter_3838 = slice_scatter_3840 = None
        slice_21134: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3841, 1, 5120, 5136)
        slice_21135: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21134, 2, 0, 16)
        slice_scatter_3843: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21134, slice_21135, 2, 0, 16);  slice_21134 = slice_21135 = None
        slice_scatter_3844: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3841, slice_scatter_3843, 1, 5120, 5136);  slice_scatter_3841 = slice_scatter_3843 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21155: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_21121, 2, 16, 32);  slice_21121 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_644: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_21155, memory_format = torch.contiguous_format);  slice_21155 = None
        view_1292: "f32[32, 11]" = torch.ops.aten.view.default(clone_644, [32, 11]);  clone_644 = None
        mm_641: "f32[32, 8]" = torch.ops.aten.mm.default(view_1292, slice_37)
        view_1293: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_641, [2, 16, 8]);  mm_641 = None
        slice_21162: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3844, 1, 5120, 5136)
        slice_21163: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21162, 2, 0, 16)
        add_643: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21163, view_1293);  slice_21163 = view_1293 = None
        slice_scatter_3846: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21162, add_643, 2, 0, 16);  slice_21162 = add_643 = None
        slice_scatter_3847: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3844, slice_scatter_3846, 1, 5120, 5136);  slice_scatter_3844 = slice_scatter_3846 = None
        slice_21167: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3847, 1, 5120, 5136)
        slice_21168: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21167, 2, 0, 16)
        slice_scatter_3849: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21167, slice_21168, 2, 0, 16);  slice_21167 = slice_21168 = None
        slice_scatter_3850: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3847, slice_scatter_3849, 1, 5120, 5136);  slice_scatter_3847 = slice_scatter_3849 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21187: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5136, 5152)
        slice_21188: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_21187, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_645: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_21188, memory_format = torch.contiguous_format);  slice_21188 = None
        view_1294: "f32[32, 16]" = torch.ops.aten.view.default(clone_645, [32, 16]);  clone_645 = None
        mm_642: "f32[32, 8]" = torch.ops.aten.mm.default(view_1294, slice_7)
        view_1295: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_642, [2, 16, 8]);  mm_642 = None
        slice_21195: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3850, 1, 5136, 5152)
        slice_21196: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21195, 2, 0, 16)
        add_644: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21196, view_1295);  slice_21196 = view_1295 = None
        slice_scatter_3852: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21195, add_644, 2, 0, 16);  slice_21195 = add_644 = None
        slice_scatter_3853: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3850, slice_scatter_3852, 1, 5136, 5152);  slice_scatter_3850 = slice_scatter_3852 = None
        slice_21200: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3853, 1, 5136, 5152)
        slice_21201: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21200, 2, 0, 16)
        slice_scatter_3855: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21200, slice_21201, 2, 0, 16);  slice_21200 = slice_21201 = None
        slice_scatter_3856: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3853, slice_scatter_3855, 1, 5136, 5152);  slice_scatter_3853 = slice_scatter_3855 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21221: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_21187, 2, 16, 32);  slice_21187 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_646: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_21221, memory_format = torch.contiguous_format);  slice_21221 = None
        view_1296: "f32[32, 11]" = torch.ops.aten.view.default(clone_646, [32, 11]);  clone_646 = None
        mm_643: "f32[32, 8]" = torch.ops.aten.mm.default(view_1296, slice_37)
        view_1297: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_643, [2, 16, 8]);  mm_643 = None
        slice_21228: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3856, 1, 5136, 5152)
        slice_21229: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21228, 2, 0, 16)
        add_645: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21229, view_1297);  slice_21229 = view_1297 = None
        slice_scatter_3858: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21228, add_645, 2, 0, 16);  slice_21228 = add_645 = None
        slice_scatter_3859: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3856, slice_scatter_3858, 1, 5136, 5152);  slice_scatter_3856 = slice_scatter_3858 = None
        slice_21233: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3859, 1, 5136, 5152)
        slice_21234: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21233, 2, 0, 16)
        slice_scatter_3861: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21233, slice_21234, 2, 0, 16);  slice_21233 = slice_21234 = None
        slice_scatter_3862: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3859, slice_scatter_3861, 1, 5136, 5152);  slice_scatter_3859 = slice_scatter_3861 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21253: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5152, 5168)
        slice_21254: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_21253, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_647: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_21254, memory_format = torch.contiguous_format);  slice_21254 = None
        view_1298: "f32[32, 16]" = torch.ops.aten.view.default(clone_647, [32, 16]);  clone_647 = None
        mm_644: "f32[32, 8]" = torch.ops.aten.mm.default(view_1298, slice_7)
        view_1299: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_644, [2, 16, 8]);  mm_644 = None
        slice_21261: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3862, 1, 5152, 5168)
        slice_21262: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21261, 2, 0, 16)
        add_646: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21262, view_1299);  slice_21262 = view_1299 = None
        slice_scatter_3864: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21261, add_646, 2, 0, 16);  slice_21261 = add_646 = None
        slice_scatter_3865: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3862, slice_scatter_3864, 1, 5152, 5168);  slice_scatter_3862 = slice_scatter_3864 = None
        slice_21266: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3865, 1, 5152, 5168)
        slice_21267: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21266, 2, 0, 16)
        slice_scatter_3867: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21266, slice_21267, 2, 0, 16);  slice_21266 = slice_21267 = None
        slice_scatter_3868: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3865, slice_scatter_3867, 1, 5152, 5168);  slice_scatter_3865 = slice_scatter_3867 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21287: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_21253, 2, 16, 32);  slice_21253 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_648: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_21287, memory_format = torch.contiguous_format);  slice_21287 = None
        view_1300: "f32[32, 11]" = torch.ops.aten.view.default(clone_648, [32, 11]);  clone_648 = None
        mm_645: "f32[32, 8]" = torch.ops.aten.mm.default(view_1300, slice_37)
        view_1301: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_645, [2, 16, 8]);  mm_645 = None
        slice_21294: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3868, 1, 5152, 5168)
        slice_21295: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21294, 2, 0, 16)
        add_647: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21295, view_1301);  slice_21295 = view_1301 = None
        slice_scatter_3870: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21294, add_647, 2, 0, 16);  slice_21294 = add_647 = None
        slice_scatter_3871: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3868, slice_scatter_3870, 1, 5152, 5168);  slice_scatter_3868 = slice_scatter_3870 = None
        slice_21299: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3871, 1, 5152, 5168)
        slice_21300: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21299, 2, 0, 16)
        slice_scatter_3873: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21299, slice_21300, 2, 0, 16);  slice_21299 = slice_21300 = None
        slice_scatter_3874: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3871, slice_scatter_3873, 1, 5152, 5168);  slice_scatter_3871 = slice_scatter_3873 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21319: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5168, 5184)
        slice_21320: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_21319, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_649: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_21320, memory_format = torch.contiguous_format);  slice_21320 = None
        view_1302: "f32[32, 16]" = torch.ops.aten.view.default(clone_649, [32, 16]);  clone_649 = None
        mm_646: "f32[32, 8]" = torch.ops.aten.mm.default(view_1302, slice_7)
        view_1303: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_646, [2, 16, 8]);  mm_646 = None
        slice_21327: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3874, 1, 5168, 5184)
        slice_21328: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21327, 2, 0, 16)
        add_648: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21328, view_1303);  slice_21328 = view_1303 = None
        slice_scatter_3876: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21327, add_648, 2, 0, 16);  slice_21327 = add_648 = None
        slice_scatter_3877: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3874, slice_scatter_3876, 1, 5168, 5184);  slice_scatter_3874 = slice_scatter_3876 = None
        slice_21332: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3877, 1, 5168, 5184)
        slice_21333: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21332, 2, 0, 16)
        slice_scatter_3879: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21332, slice_21333, 2, 0, 16);  slice_21332 = slice_21333 = None
        slice_scatter_3880: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3877, slice_scatter_3879, 1, 5168, 5184);  slice_scatter_3877 = slice_scatter_3879 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21353: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_21319, 2, 16, 32);  slice_21319 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_650: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_21353, memory_format = torch.contiguous_format);  slice_21353 = None
        view_1304: "f32[32, 11]" = torch.ops.aten.view.default(clone_650, [32, 11]);  clone_650 = None
        mm_647: "f32[32, 8]" = torch.ops.aten.mm.default(view_1304, slice_37)
        view_1305: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_647, [2, 16, 8]);  mm_647 = None
        slice_21360: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3880, 1, 5168, 5184)
        slice_21361: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21360, 2, 0, 16)
        add_649: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21361, view_1305);  slice_21361 = view_1305 = None
        slice_scatter_3882: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21360, add_649, 2, 0, 16);  slice_21360 = add_649 = None
        slice_scatter_3883: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3880, slice_scatter_3882, 1, 5168, 5184);  slice_scatter_3880 = slice_scatter_3882 = None
        slice_21365: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3883, 1, 5168, 5184)
        slice_21366: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21365, 2, 0, 16)
        slice_scatter_3885: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21365, slice_21366, 2, 0, 16);  slice_21365 = slice_21366 = None
        slice_scatter_3886: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3883, slice_scatter_3885, 1, 5168, 5184);  slice_scatter_3883 = slice_scatter_3885 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21385: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5184, 5200)
        slice_21386: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_21385, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_651: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_21386, memory_format = torch.contiguous_format);  slice_21386 = None
        view_1306: "f32[32, 16]" = torch.ops.aten.view.default(clone_651, [32, 16]);  clone_651 = None
        mm_648: "f32[32, 8]" = torch.ops.aten.mm.default(view_1306, slice_7)
        view_1307: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_648, [2, 16, 8]);  mm_648 = None
        slice_21393: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3886, 1, 5184, 5200)
        slice_21394: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21393, 2, 0, 16)
        add_650: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21394, view_1307);  slice_21394 = view_1307 = None
        slice_scatter_3888: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21393, add_650, 2, 0, 16);  slice_21393 = add_650 = None
        slice_scatter_3889: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3886, slice_scatter_3888, 1, 5184, 5200);  slice_scatter_3886 = slice_scatter_3888 = None
        slice_21398: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3889, 1, 5184, 5200)
        slice_21399: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21398, 2, 0, 16)
        slice_scatter_3891: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21398, slice_21399, 2, 0, 16);  slice_21398 = slice_21399 = None
        slice_scatter_3892: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3889, slice_scatter_3891, 1, 5184, 5200);  slice_scatter_3889 = slice_scatter_3891 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21419: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_21385, 2, 16, 32);  slice_21385 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_652: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_21419, memory_format = torch.contiguous_format);  slice_21419 = None
        view_1308: "f32[32, 11]" = torch.ops.aten.view.default(clone_652, [32, 11]);  clone_652 = None
        mm_649: "f32[32, 8]" = torch.ops.aten.mm.default(view_1308, slice_37)
        view_1309: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_649, [2, 16, 8]);  mm_649 = None
        slice_21426: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3892, 1, 5184, 5200)
        slice_21427: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21426, 2, 0, 16)
        add_651: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21427, view_1309);  slice_21427 = view_1309 = None
        slice_scatter_3894: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21426, add_651, 2, 0, 16);  slice_21426 = add_651 = None
        slice_scatter_3895: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3892, slice_scatter_3894, 1, 5184, 5200);  slice_scatter_3892 = slice_scatter_3894 = None
        slice_21431: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3895, 1, 5184, 5200)
        slice_21432: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21431, 2, 0, 16)
        slice_scatter_3897: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21431, slice_21432, 2, 0, 16);  slice_21431 = slice_21432 = None
        slice_scatter_3898: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3895, slice_scatter_3897, 1, 5184, 5200);  slice_scatter_3895 = slice_scatter_3897 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21451: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5200, 5216)
        slice_21452: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_21451, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_653: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_21452, memory_format = torch.contiguous_format);  slice_21452 = None
        view_1310: "f32[32, 16]" = torch.ops.aten.view.default(clone_653, [32, 16]);  clone_653 = None
        mm_650: "f32[32, 8]" = torch.ops.aten.mm.default(view_1310, slice_7)
        view_1311: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_650, [2, 16, 8]);  mm_650 = None
        slice_21459: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3898, 1, 5200, 5216)
        slice_21460: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21459, 2, 0, 16)
        add_652: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21460, view_1311);  slice_21460 = view_1311 = None
        slice_scatter_3900: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21459, add_652, 2, 0, 16);  slice_21459 = add_652 = None
        slice_scatter_3901: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3898, slice_scatter_3900, 1, 5200, 5216);  slice_scatter_3898 = slice_scatter_3900 = None
        slice_21464: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3901, 1, 5200, 5216)
        slice_21465: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21464, 2, 0, 16)
        slice_scatter_3903: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21464, slice_21465, 2, 0, 16);  slice_21464 = slice_21465 = None
        slice_scatter_3904: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3901, slice_scatter_3903, 1, 5200, 5216);  slice_scatter_3901 = slice_scatter_3903 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21485: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_21451, 2, 16, 32);  slice_21451 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_654: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_21485, memory_format = torch.contiguous_format);  slice_21485 = None
        view_1312: "f32[32, 11]" = torch.ops.aten.view.default(clone_654, [32, 11]);  clone_654 = None
        mm_651: "f32[32, 8]" = torch.ops.aten.mm.default(view_1312, slice_37)
        view_1313: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_651, [2, 16, 8]);  mm_651 = None
        slice_21492: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3904, 1, 5200, 5216)
        slice_21493: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21492, 2, 0, 16)
        add_653: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21493, view_1313);  slice_21493 = view_1313 = None
        slice_scatter_3906: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21492, add_653, 2, 0, 16);  slice_21492 = add_653 = None
        slice_scatter_3907: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3904, slice_scatter_3906, 1, 5200, 5216);  slice_scatter_3904 = slice_scatter_3906 = None
        slice_21497: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3907, 1, 5200, 5216)
        slice_21498: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21497, 2, 0, 16)
        slice_scatter_3909: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21497, slice_21498, 2, 0, 16);  slice_21497 = slice_21498 = None
        slice_scatter_3910: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3907, slice_scatter_3909, 1, 5200, 5216);  slice_scatter_3907 = slice_scatter_3909 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21517: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5216, 5232)
        slice_21518: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_21517, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_655: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_21518, memory_format = torch.contiguous_format);  slice_21518 = None
        view_1314: "f32[32, 16]" = torch.ops.aten.view.default(clone_655, [32, 16]);  clone_655 = None
        mm_652: "f32[32, 8]" = torch.ops.aten.mm.default(view_1314, slice_7)
        view_1315: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_652, [2, 16, 8]);  mm_652 = None
        slice_21525: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3910, 1, 5216, 5232)
        slice_21526: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21525, 2, 0, 16)
        add_654: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21526, view_1315);  slice_21526 = view_1315 = None
        slice_scatter_3912: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21525, add_654, 2, 0, 16);  slice_21525 = add_654 = None
        slice_scatter_3913: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3910, slice_scatter_3912, 1, 5216, 5232);  slice_scatter_3910 = slice_scatter_3912 = None
        slice_21530: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3913, 1, 5216, 5232)
        slice_21531: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21530, 2, 0, 16)
        slice_scatter_3915: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21530, slice_21531, 2, 0, 16);  slice_21530 = slice_21531 = None
        slice_scatter_3916: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3913, slice_scatter_3915, 1, 5216, 5232);  slice_scatter_3913 = slice_scatter_3915 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21551: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_21517, 2, 16, 32);  slice_21517 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_656: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_21551, memory_format = torch.contiguous_format);  slice_21551 = None
        view_1316: "f32[32, 11]" = torch.ops.aten.view.default(clone_656, [32, 11]);  clone_656 = None
        mm_653: "f32[32, 8]" = torch.ops.aten.mm.default(view_1316, slice_37)
        view_1317: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_653, [2, 16, 8]);  mm_653 = None
        slice_21558: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3916, 1, 5216, 5232)
        slice_21559: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21558, 2, 0, 16)
        add_655: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21559, view_1317);  slice_21559 = view_1317 = None
        slice_scatter_3918: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21558, add_655, 2, 0, 16);  slice_21558 = add_655 = None
        slice_scatter_3919: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3916, slice_scatter_3918, 1, 5216, 5232);  slice_scatter_3916 = slice_scatter_3918 = None
        slice_21563: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3919, 1, 5216, 5232)
        slice_21564: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21563, 2, 0, 16)
        slice_scatter_3921: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21563, slice_21564, 2, 0, 16);  slice_21563 = slice_21564 = None
        slice_scatter_3922: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3919, slice_scatter_3921, 1, 5216, 5232);  slice_scatter_3919 = slice_scatter_3921 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21583: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5232, 5248)
        slice_21584: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_21583, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_657: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_21584, memory_format = torch.contiguous_format);  slice_21584 = None
        view_1318: "f32[32, 16]" = torch.ops.aten.view.default(clone_657, [32, 16]);  clone_657 = None
        mm_654: "f32[32, 8]" = torch.ops.aten.mm.default(view_1318, slice_7)
        view_1319: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_654, [2, 16, 8]);  mm_654 = None
        slice_21591: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3922, 1, 5232, 5248)
        slice_21592: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21591, 2, 0, 16)
        add_656: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21592, view_1319);  slice_21592 = view_1319 = None
        slice_scatter_3924: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21591, add_656, 2, 0, 16);  slice_21591 = add_656 = None
        slice_scatter_3925: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3922, slice_scatter_3924, 1, 5232, 5248);  slice_scatter_3922 = slice_scatter_3924 = None
        slice_21596: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3925, 1, 5232, 5248)
        slice_21597: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21596, 2, 0, 16)
        slice_scatter_3927: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21596, slice_21597, 2, 0, 16);  slice_21596 = slice_21597 = None
        slice_scatter_3928: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3925, slice_scatter_3927, 1, 5232, 5248);  slice_scatter_3925 = slice_scatter_3927 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21617: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_21583, 2, 16, 32);  slice_21583 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_658: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_21617, memory_format = torch.contiguous_format);  slice_21617 = None
        view_1320: "f32[32, 11]" = torch.ops.aten.view.default(clone_658, [32, 11]);  clone_658 = None
        mm_655: "f32[32, 8]" = torch.ops.aten.mm.default(view_1320, slice_37)
        view_1321: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_655, [2, 16, 8]);  mm_655 = None
        slice_21624: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3928, 1, 5232, 5248)
        slice_21625: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21624, 2, 0, 16)
        add_657: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21625, view_1321);  slice_21625 = view_1321 = None
        slice_scatter_3930: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21624, add_657, 2, 0, 16);  slice_21624 = add_657 = None
        slice_scatter_3931: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3928, slice_scatter_3930, 1, 5232, 5248);  slice_scatter_3928 = slice_scatter_3930 = None
        slice_21629: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3931, 1, 5232, 5248)
        slice_21630: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21629, 2, 0, 16)
        slice_scatter_3933: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21629, slice_21630, 2, 0, 16);  slice_21629 = slice_21630 = None
        slice_scatter_3934: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3931, slice_scatter_3933, 1, 5232, 5248);  slice_scatter_3931 = slice_scatter_3933 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21649: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5248, 5264)
        slice_21650: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_21649, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_659: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_21650, memory_format = torch.contiguous_format);  slice_21650 = None
        view_1322: "f32[32, 16]" = torch.ops.aten.view.default(clone_659, [32, 16]);  clone_659 = None
        mm_656: "f32[32, 8]" = torch.ops.aten.mm.default(view_1322, slice_7)
        view_1323: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_656, [2, 16, 8]);  mm_656 = None
        slice_21657: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3934, 1, 5248, 5264)
        slice_21658: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21657, 2, 0, 16)
        add_658: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21658, view_1323);  slice_21658 = view_1323 = None
        slice_scatter_3936: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21657, add_658, 2, 0, 16);  slice_21657 = add_658 = None
        slice_scatter_3937: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3934, slice_scatter_3936, 1, 5248, 5264);  slice_scatter_3934 = slice_scatter_3936 = None
        slice_21662: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3937, 1, 5248, 5264)
        slice_21663: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21662, 2, 0, 16)
        slice_scatter_3939: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21662, slice_21663, 2, 0, 16);  slice_21662 = slice_21663 = None
        slice_scatter_3940: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3937, slice_scatter_3939, 1, 5248, 5264);  slice_scatter_3937 = slice_scatter_3939 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21683: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_21649, 2, 16, 32);  slice_21649 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_660: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_21683, memory_format = torch.contiguous_format);  slice_21683 = None
        view_1324: "f32[32, 11]" = torch.ops.aten.view.default(clone_660, [32, 11]);  clone_660 = None
        mm_657: "f32[32, 8]" = torch.ops.aten.mm.default(view_1324, slice_37)
        view_1325: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_657, [2, 16, 8]);  mm_657 = None
        slice_21690: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3940, 1, 5248, 5264)
        slice_21691: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21690, 2, 0, 16)
        add_659: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21691, view_1325);  slice_21691 = view_1325 = None
        slice_scatter_3942: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21690, add_659, 2, 0, 16);  slice_21690 = add_659 = None
        slice_scatter_3943: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3940, slice_scatter_3942, 1, 5248, 5264);  slice_scatter_3940 = slice_scatter_3942 = None
        slice_21695: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3943, 1, 5248, 5264)
        slice_21696: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21695, 2, 0, 16)
        slice_scatter_3945: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21695, slice_21696, 2, 0, 16);  slice_21695 = slice_21696 = None
        slice_scatter_3946: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3943, slice_scatter_3945, 1, 5248, 5264);  slice_scatter_3943 = slice_scatter_3945 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21715: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5264, 5280)
        slice_21716: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_21715, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_661: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_21716, memory_format = torch.contiguous_format);  slice_21716 = None
        view_1326: "f32[32, 16]" = torch.ops.aten.view.default(clone_661, [32, 16]);  clone_661 = None
        mm_658: "f32[32, 8]" = torch.ops.aten.mm.default(view_1326, slice_7)
        view_1327: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_658, [2, 16, 8]);  mm_658 = None
        slice_21723: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3946, 1, 5264, 5280)
        slice_21724: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21723, 2, 0, 16)
        add_660: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21724, view_1327);  slice_21724 = view_1327 = None
        slice_scatter_3948: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21723, add_660, 2, 0, 16);  slice_21723 = add_660 = None
        slice_scatter_3949: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3946, slice_scatter_3948, 1, 5264, 5280);  slice_scatter_3946 = slice_scatter_3948 = None
        slice_21728: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3949, 1, 5264, 5280)
        slice_21729: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21728, 2, 0, 16)
        slice_scatter_3951: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21728, slice_21729, 2, 0, 16);  slice_21728 = slice_21729 = None
        slice_scatter_3952: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3949, slice_scatter_3951, 1, 5264, 5280);  slice_scatter_3949 = slice_scatter_3951 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21749: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_21715, 2, 16, 32);  slice_21715 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_662: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_21749, memory_format = torch.contiguous_format);  slice_21749 = None
        view_1328: "f32[32, 11]" = torch.ops.aten.view.default(clone_662, [32, 11]);  clone_662 = None
        mm_659: "f32[32, 8]" = torch.ops.aten.mm.default(view_1328, slice_37)
        view_1329: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_659, [2, 16, 8]);  mm_659 = None
        slice_21756: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3952, 1, 5264, 5280)
        slice_21757: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21756, 2, 0, 16)
        add_661: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21757, view_1329);  slice_21757 = view_1329 = None
        slice_scatter_3954: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21756, add_661, 2, 0, 16);  slice_21756 = add_661 = None
        slice_scatter_3955: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3952, slice_scatter_3954, 1, 5264, 5280);  slice_scatter_3952 = slice_scatter_3954 = None
        slice_21761: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3955, 1, 5264, 5280)
        slice_21762: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21761, 2, 0, 16)
        slice_scatter_3957: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21761, slice_21762, 2, 0, 16);  slice_21761 = slice_21762 = None
        slice_scatter_3958: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3955, slice_scatter_3957, 1, 5264, 5280);  slice_scatter_3955 = slice_scatter_3957 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21781: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5280, 5296)
        slice_21782: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_21781, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_663: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_21782, memory_format = torch.contiguous_format);  slice_21782 = None
        view_1330: "f32[32, 16]" = torch.ops.aten.view.default(clone_663, [32, 16]);  clone_663 = None
        mm_660: "f32[32, 8]" = torch.ops.aten.mm.default(view_1330, slice_7)
        view_1331: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_660, [2, 16, 8]);  mm_660 = None
        slice_21789: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3958, 1, 5280, 5296)
        slice_21790: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21789, 2, 0, 16)
        add_662: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21790, view_1331);  slice_21790 = view_1331 = None
        slice_scatter_3960: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21789, add_662, 2, 0, 16);  slice_21789 = add_662 = None
        slice_scatter_3961: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3958, slice_scatter_3960, 1, 5280, 5296);  slice_scatter_3958 = slice_scatter_3960 = None
        slice_21794: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3961, 1, 5280, 5296)
        slice_21795: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21794, 2, 0, 16)
        slice_scatter_3963: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21794, slice_21795, 2, 0, 16);  slice_21794 = slice_21795 = None
        slice_scatter_3964: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3961, slice_scatter_3963, 1, 5280, 5296);  slice_scatter_3961 = slice_scatter_3963 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21815: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_21781, 2, 16, 32);  slice_21781 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_664: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_21815, memory_format = torch.contiguous_format);  slice_21815 = None
        view_1332: "f32[32, 11]" = torch.ops.aten.view.default(clone_664, [32, 11]);  clone_664 = None
        mm_661: "f32[32, 8]" = torch.ops.aten.mm.default(view_1332, slice_37)
        view_1333: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_661, [2, 16, 8]);  mm_661 = None
        slice_21822: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3964, 1, 5280, 5296)
        slice_21823: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21822, 2, 0, 16)
        add_663: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21823, view_1333);  slice_21823 = view_1333 = None
        slice_scatter_3966: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21822, add_663, 2, 0, 16);  slice_21822 = add_663 = None
        slice_scatter_3967: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3964, slice_scatter_3966, 1, 5280, 5296);  slice_scatter_3964 = slice_scatter_3966 = None
        slice_21827: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3967, 1, 5280, 5296)
        slice_21828: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21827, 2, 0, 16)
        slice_scatter_3969: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21827, slice_21828, 2, 0, 16);  slice_21827 = slice_21828 = None
        slice_scatter_3970: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3967, slice_scatter_3969, 1, 5280, 5296);  slice_scatter_3967 = slice_scatter_3969 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21847: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5296, 5312)
        slice_21848: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_21847, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_665: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_21848, memory_format = torch.contiguous_format);  slice_21848 = None
        view_1334: "f32[32, 16]" = torch.ops.aten.view.default(clone_665, [32, 16]);  clone_665 = None
        mm_662: "f32[32, 8]" = torch.ops.aten.mm.default(view_1334, slice_7)
        view_1335: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_662, [2, 16, 8]);  mm_662 = None
        slice_21855: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3970, 1, 5296, 5312)
        slice_21856: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21855, 2, 0, 16)
        add_664: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21856, view_1335);  slice_21856 = view_1335 = None
        slice_scatter_3972: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21855, add_664, 2, 0, 16);  slice_21855 = add_664 = None
        slice_scatter_3973: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3970, slice_scatter_3972, 1, 5296, 5312);  slice_scatter_3970 = slice_scatter_3972 = None
        slice_21860: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3973, 1, 5296, 5312)
        slice_21861: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21860, 2, 0, 16)
        slice_scatter_3975: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21860, slice_21861, 2, 0, 16);  slice_21860 = slice_21861 = None
        slice_scatter_3976: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3973, slice_scatter_3975, 1, 5296, 5312);  slice_scatter_3973 = slice_scatter_3975 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21881: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_21847, 2, 16, 32);  slice_21847 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_666: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_21881, memory_format = torch.contiguous_format);  slice_21881 = None
        view_1336: "f32[32, 11]" = torch.ops.aten.view.default(clone_666, [32, 11]);  clone_666 = None
        mm_663: "f32[32, 8]" = torch.ops.aten.mm.default(view_1336, slice_37)
        view_1337: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_663, [2, 16, 8]);  mm_663 = None
        slice_21888: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3976, 1, 5296, 5312)
        slice_21889: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21888, 2, 0, 16)
        add_665: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21889, view_1337);  slice_21889 = view_1337 = None
        slice_scatter_3978: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21888, add_665, 2, 0, 16);  slice_21888 = add_665 = None
        slice_scatter_3979: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3976, slice_scatter_3978, 1, 5296, 5312);  slice_scatter_3976 = slice_scatter_3978 = None
        slice_21893: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3979, 1, 5296, 5312)
        slice_21894: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21893, 2, 0, 16)
        slice_scatter_3981: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21893, slice_21894, 2, 0, 16);  slice_21893 = slice_21894 = None
        slice_scatter_3982: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3979, slice_scatter_3981, 1, 5296, 5312);  slice_scatter_3979 = slice_scatter_3981 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21913: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5312, 5328)
        slice_21914: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_21913, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_667: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_21914, memory_format = torch.contiguous_format);  slice_21914 = None
        view_1338: "f32[32, 16]" = torch.ops.aten.view.default(clone_667, [32, 16]);  clone_667 = None
        mm_664: "f32[32, 8]" = torch.ops.aten.mm.default(view_1338, slice_7)
        view_1339: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_664, [2, 16, 8]);  mm_664 = None
        slice_21921: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3982, 1, 5312, 5328)
        slice_21922: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21921, 2, 0, 16)
        add_666: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21922, view_1339);  slice_21922 = view_1339 = None
        slice_scatter_3984: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21921, add_666, 2, 0, 16);  slice_21921 = add_666 = None
        slice_scatter_3985: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3982, slice_scatter_3984, 1, 5312, 5328);  slice_scatter_3982 = slice_scatter_3984 = None
        slice_21926: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3985, 1, 5312, 5328)
        slice_21927: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21926, 2, 0, 16)
        slice_scatter_3987: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21926, slice_21927, 2, 0, 16);  slice_21926 = slice_21927 = None
        slice_scatter_3988: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3985, slice_scatter_3987, 1, 5312, 5328);  slice_scatter_3985 = slice_scatter_3987 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21947: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_21913, 2, 16, 32);  slice_21913 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_668: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_21947, memory_format = torch.contiguous_format);  slice_21947 = None
        view_1340: "f32[32, 11]" = torch.ops.aten.view.default(clone_668, [32, 11]);  clone_668 = None
        mm_665: "f32[32, 8]" = torch.ops.aten.mm.default(view_1340, slice_37)
        view_1341: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_665, [2, 16, 8]);  mm_665 = None
        slice_21954: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3988, 1, 5312, 5328)
        slice_21955: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21954, 2, 0, 16)
        add_667: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21955, view_1341);  slice_21955 = view_1341 = None
        slice_scatter_3990: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21954, add_667, 2, 0, 16);  slice_21954 = add_667 = None
        slice_scatter_3991: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3988, slice_scatter_3990, 1, 5312, 5328);  slice_scatter_3988 = slice_scatter_3990 = None
        slice_21959: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3991, 1, 5312, 5328)
        slice_21960: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21959, 2, 0, 16)
        slice_scatter_3993: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21959, slice_21960, 2, 0, 16);  slice_21959 = slice_21960 = None
        slice_scatter_3994: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3991, slice_scatter_3993, 1, 5312, 5328);  slice_scatter_3991 = slice_scatter_3993 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_21979: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5328, 5344)
        slice_21980: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_21979, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_669: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_21980, memory_format = torch.contiguous_format);  slice_21980 = None
        view_1342: "f32[32, 16]" = torch.ops.aten.view.default(clone_669, [32, 16]);  clone_669 = None
        mm_666: "f32[32, 8]" = torch.ops.aten.mm.default(view_1342, slice_7)
        view_1343: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_666, [2, 16, 8]);  mm_666 = None
        slice_21987: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3994, 1, 5328, 5344)
        slice_21988: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21987, 2, 0, 16)
        add_668: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_21988, view_1343);  slice_21988 = view_1343 = None
        slice_scatter_3996: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21987, add_668, 2, 0, 16);  slice_21987 = add_668 = None
        slice_scatter_3997: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3994, slice_scatter_3996, 1, 5328, 5344);  slice_scatter_3994 = slice_scatter_3996 = None
        slice_21992: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_3997, 1, 5328, 5344)
        slice_21993: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_21992, 2, 0, 16)
        slice_scatter_3999: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_21992, slice_21993, 2, 0, 16);  slice_21992 = slice_21993 = None
        slice_scatter_4000: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_3997, slice_scatter_3999, 1, 5328, 5344);  slice_scatter_3997 = slice_scatter_3999 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22013: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_21979, 2, 16, 32);  slice_21979 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_670: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_22013, memory_format = torch.contiguous_format);  slice_22013 = None
        view_1344: "f32[32, 11]" = torch.ops.aten.view.default(clone_670, [32, 11]);  clone_670 = None
        mm_667: "f32[32, 8]" = torch.ops.aten.mm.default(view_1344, slice_37)
        view_1345: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_667, [2, 16, 8]);  mm_667 = None
        slice_22020: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4000, 1, 5328, 5344)
        slice_22021: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22020, 2, 0, 16)
        add_669: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22021, view_1345);  slice_22021 = view_1345 = None
        slice_scatter_4002: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22020, add_669, 2, 0, 16);  slice_22020 = add_669 = None
        slice_scatter_4003: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4000, slice_scatter_4002, 1, 5328, 5344);  slice_scatter_4000 = slice_scatter_4002 = None
        slice_22025: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4003, 1, 5328, 5344)
        slice_22026: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22025, 2, 0, 16)
        slice_scatter_4005: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22025, slice_22026, 2, 0, 16);  slice_22025 = slice_22026 = None
        slice_scatter_4006: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4003, slice_scatter_4005, 1, 5328, 5344);  slice_scatter_4003 = slice_scatter_4005 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22045: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5344, 5360)
        slice_22046: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_22045, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_671: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_22046, memory_format = torch.contiguous_format);  slice_22046 = None
        view_1346: "f32[32, 16]" = torch.ops.aten.view.default(clone_671, [32, 16]);  clone_671 = None
        mm_668: "f32[32, 8]" = torch.ops.aten.mm.default(view_1346, slice_7)
        view_1347: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_668, [2, 16, 8]);  mm_668 = None
        slice_22053: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4006, 1, 5344, 5360)
        slice_22054: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22053, 2, 0, 16)
        add_670: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22054, view_1347);  slice_22054 = view_1347 = None
        slice_scatter_4008: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22053, add_670, 2, 0, 16);  slice_22053 = add_670 = None
        slice_scatter_4009: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4006, slice_scatter_4008, 1, 5344, 5360);  slice_scatter_4006 = slice_scatter_4008 = None
        slice_22058: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4009, 1, 5344, 5360)
        slice_22059: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22058, 2, 0, 16)
        slice_scatter_4011: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22058, slice_22059, 2, 0, 16);  slice_22058 = slice_22059 = None
        slice_scatter_4012: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4009, slice_scatter_4011, 1, 5344, 5360);  slice_scatter_4009 = slice_scatter_4011 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22079: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_22045, 2, 16, 32);  slice_22045 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_672: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_22079, memory_format = torch.contiguous_format);  slice_22079 = None
        view_1348: "f32[32, 11]" = torch.ops.aten.view.default(clone_672, [32, 11]);  clone_672 = None
        mm_669: "f32[32, 8]" = torch.ops.aten.mm.default(view_1348, slice_37)
        view_1349: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_669, [2, 16, 8]);  mm_669 = None
        slice_22086: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4012, 1, 5344, 5360)
        slice_22087: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22086, 2, 0, 16)
        add_671: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22087, view_1349);  slice_22087 = view_1349 = None
        slice_scatter_4014: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22086, add_671, 2, 0, 16);  slice_22086 = add_671 = None
        slice_scatter_4015: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4012, slice_scatter_4014, 1, 5344, 5360);  slice_scatter_4012 = slice_scatter_4014 = None
        slice_22091: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4015, 1, 5344, 5360)
        slice_22092: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22091, 2, 0, 16)
        slice_scatter_4017: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22091, slice_22092, 2, 0, 16);  slice_22091 = slice_22092 = None
        slice_scatter_4018: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4015, slice_scatter_4017, 1, 5344, 5360);  slice_scatter_4015 = slice_scatter_4017 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22111: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5360, 5376)
        slice_22112: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_22111, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_673: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_22112, memory_format = torch.contiguous_format);  slice_22112 = None
        view_1350: "f32[32, 16]" = torch.ops.aten.view.default(clone_673, [32, 16]);  clone_673 = None
        mm_670: "f32[32, 8]" = torch.ops.aten.mm.default(view_1350, slice_7)
        view_1351: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_670, [2, 16, 8]);  mm_670 = None
        slice_22119: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4018, 1, 5360, 5376)
        slice_22120: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22119, 2, 0, 16)
        add_672: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22120, view_1351);  slice_22120 = view_1351 = None
        slice_scatter_4020: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22119, add_672, 2, 0, 16);  slice_22119 = add_672 = None
        slice_scatter_4021: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4018, slice_scatter_4020, 1, 5360, 5376);  slice_scatter_4018 = slice_scatter_4020 = None
        slice_22124: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4021, 1, 5360, 5376)
        slice_22125: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22124, 2, 0, 16)
        slice_scatter_4023: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22124, slice_22125, 2, 0, 16);  slice_22124 = slice_22125 = None
        slice_scatter_4024: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4021, slice_scatter_4023, 1, 5360, 5376);  slice_scatter_4021 = slice_scatter_4023 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22145: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_22111, 2, 16, 32);  slice_22111 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_674: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_22145, memory_format = torch.contiguous_format);  slice_22145 = None
        view_1352: "f32[32, 11]" = torch.ops.aten.view.default(clone_674, [32, 11]);  clone_674 = None
        mm_671: "f32[32, 8]" = torch.ops.aten.mm.default(view_1352, slice_37)
        view_1353: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_671, [2, 16, 8]);  mm_671 = None
        slice_22152: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4024, 1, 5360, 5376)
        slice_22153: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22152, 2, 0, 16)
        add_673: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22153, view_1353);  slice_22153 = view_1353 = None
        slice_scatter_4026: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22152, add_673, 2, 0, 16);  slice_22152 = add_673 = None
        slice_scatter_4027: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4024, slice_scatter_4026, 1, 5360, 5376);  slice_scatter_4024 = slice_scatter_4026 = None
        slice_22157: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4027, 1, 5360, 5376)
        slice_22158: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22157, 2, 0, 16)
        slice_scatter_4029: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22157, slice_22158, 2, 0, 16);  slice_22157 = slice_22158 = None
        slice_scatter_4030: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4027, slice_scatter_4029, 1, 5360, 5376);  slice_scatter_4027 = slice_scatter_4029 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22177: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5376, 5392)
        slice_22178: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_22177, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_675: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_22178, memory_format = torch.contiguous_format);  slice_22178 = None
        view_1354: "f32[32, 16]" = torch.ops.aten.view.default(clone_675, [32, 16]);  clone_675 = None
        mm_672: "f32[32, 8]" = torch.ops.aten.mm.default(view_1354, slice_7)
        view_1355: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_672, [2, 16, 8]);  mm_672 = None
        slice_22185: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4030, 1, 5376, 5392)
        slice_22186: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22185, 2, 0, 16)
        add_674: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22186, view_1355);  slice_22186 = view_1355 = None
        slice_scatter_4032: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22185, add_674, 2, 0, 16);  slice_22185 = add_674 = None
        slice_scatter_4033: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4030, slice_scatter_4032, 1, 5376, 5392);  slice_scatter_4030 = slice_scatter_4032 = None
        slice_22190: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4033, 1, 5376, 5392)
        slice_22191: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22190, 2, 0, 16)
        slice_scatter_4035: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22190, slice_22191, 2, 0, 16);  slice_22190 = slice_22191 = None
        slice_scatter_4036: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4033, slice_scatter_4035, 1, 5376, 5392);  slice_scatter_4033 = slice_scatter_4035 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22211: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_22177, 2, 16, 32);  slice_22177 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_676: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_22211, memory_format = torch.contiguous_format);  slice_22211 = None
        view_1356: "f32[32, 11]" = torch.ops.aten.view.default(clone_676, [32, 11]);  clone_676 = None
        mm_673: "f32[32, 8]" = torch.ops.aten.mm.default(view_1356, slice_37)
        view_1357: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_673, [2, 16, 8]);  mm_673 = None
        slice_22218: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4036, 1, 5376, 5392)
        slice_22219: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22218, 2, 0, 16)
        add_675: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22219, view_1357);  slice_22219 = view_1357 = None
        slice_scatter_4038: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22218, add_675, 2, 0, 16);  slice_22218 = add_675 = None
        slice_scatter_4039: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4036, slice_scatter_4038, 1, 5376, 5392);  slice_scatter_4036 = slice_scatter_4038 = None
        slice_22223: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4039, 1, 5376, 5392)
        slice_22224: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22223, 2, 0, 16)
        slice_scatter_4041: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22223, slice_22224, 2, 0, 16);  slice_22223 = slice_22224 = None
        slice_scatter_4042: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4039, slice_scatter_4041, 1, 5376, 5392);  slice_scatter_4039 = slice_scatter_4041 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22243: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5392, 5408)
        slice_22244: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_22243, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_677: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_22244, memory_format = torch.contiguous_format);  slice_22244 = None
        view_1358: "f32[32, 16]" = torch.ops.aten.view.default(clone_677, [32, 16]);  clone_677 = None
        mm_674: "f32[32, 8]" = torch.ops.aten.mm.default(view_1358, slice_7)
        view_1359: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_674, [2, 16, 8]);  mm_674 = None
        slice_22251: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4042, 1, 5392, 5408)
        slice_22252: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22251, 2, 0, 16)
        add_676: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22252, view_1359);  slice_22252 = view_1359 = None
        slice_scatter_4044: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22251, add_676, 2, 0, 16);  slice_22251 = add_676 = None
        slice_scatter_4045: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4042, slice_scatter_4044, 1, 5392, 5408);  slice_scatter_4042 = slice_scatter_4044 = None
        slice_22256: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4045, 1, 5392, 5408)
        slice_22257: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22256, 2, 0, 16)
        slice_scatter_4047: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22256, slice_22257, 2, 0, 16);  slice_22256 = slice_22257 = None
        slice_scatter_4048: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4045, slice_scatter_4047, 1, 5392, 5408);  slice_scatter_4045 = slice_scatter_4047 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22277: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_22243, 2, 16, 32);  slice_22243 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_678: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_22277, memory_format = torch.contiguous_format);  slice_22277 = None
        view_1360: "f32[32, 11]" = torch.ops.aten.view.default(clone_678, [32, 11]);  clone_678 = None
        mm_675: "f32[32, 8]" = torch.ops.aten.mm.default(view_1360, slice_37)
        view_1361: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_675, [2, 16, 8]);  mm_675 = None
        slice_22284: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4048, 1, 5392, 5408)
        slice_22285: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22284, 2, 0, 16)
        add_677: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22285, view_1361);  slice_22285 = view_1361 = None
        slice_scatter_4050: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22284, add_677, 2, 0, 16);  slice_22284 = add_677 = None
        slice_scatter_4051: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4048, slice_scatter_4050, 1, 5392, 5408);  slice_scatter_4048 = slice_scatter_4050 = None
        slice_22289: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4051, 1, 5392, 5408)
        slice_22290: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22289, 2, 0, 16)
        slice_scatter_4053: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22289, slice_22290, 2, 0, 16);  slice_22289 = slice_22290 = None
        slice_scatter_4054: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4051, slice_scatter_4053, 1, 5392, 5408);  slice_scatter_4051 = slice_scatter_4053 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22309: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5408, 5424)
        slice_22310: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_22309, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_679: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_22310, memory_format = torch.contiguous_format);  slice_22310 = None
        view_1362: "f32[32, 16]" = torch.ops.aten.view.default(clone_679, [32, 16]);  clone_679 = None
        mm_676: "f32[32, 8]" = torch.ops.aten.mm.default(view_1362, slice_7)
        view_1363: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_676, [2, 16, 8]);  mm_676 = None
        slice_22317: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4054, 1, 5408, 5424)
        slice_22318: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22317, 2, 0, 16)
        add_678: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22318, view_1363);  slice_22318 = view_1363 = None
        slice_scatter_4056: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22317, add_678, 2, 0, 16);  slice_22317 = add_678 = None
        slice_scatter_4057: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4054, slice_scatter_4056, 1, 5408, 5424);  slice_scatter_4054 = slice_scatter_4056 = None
        slice_22322: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4057, 1, 5408, 5424)
        slice_22323: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22322, 2, 0, 16)
        slice_scatter_4059: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22322, slice_22323, 2, 0, 16);  slice_22322 = slice_22323 = None
        slice_scatter_4060: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4057, slice_scatter_4059, 1, 5408, 5424);  slice_scatter_4057 = slice_scatter_4059 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22343: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_22309, 2, 16, 32);  slice_22309 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_680: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_22343, memory_format = torch.contiguous_format);  slice_22343 = None
        view_1364: "f32[32, 11]" = torch.ops.aten.view.default(clone_680, [32, 11]);  clone_680 = None
        mm_677: "f32[32, 8]" = torch.ops.aten.mm.default(view_1364, slice_37)
        view_1365: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_677, [2, 16, 8]);  mm_677 = None
        slice_22350: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4060, 1, 5408, 5424)
        slice_22351: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22350, 2, 0, 16)
        add_679: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22351, view_1365);  slice_22351 = view_1365 = None
        slice_scatter_4062: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22350, add_679, 2, 0, 16);  slice_22350 = add_679 = None
        slice_scatter_4063: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4060, slice_scatter_4062, 1, 5408, 5424);  slice_scatter_4060 = slice_scatter_4062 = None
        slice_22355: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4063, 1, 5408, 5424)
        slice_22356: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22355, 2, 0, 16)
        slice_scatter_4065: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22355, slice_22356, 2, 0, 16);  slice_22355 = slice_22356 = None
        slice_scatter_4066: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4063, slice_scatter_4065, 1, 5408, 5424);  slice_scatter_4063 = slice_scatter_4065 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22375: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5424, 5440)
        slice_22376: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_22375, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_681: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_22376, memory_format = torch.contiguous_format);  slice_22376 = None
        view_1366: "f32[32, 16]" = torch.ops.aten.view.default(clone_681, [32, 16]);  clone_681 = None
        mm_678: "f32[32, 8]" = torch.ops.aten.mm.default(view_1366, slice_7)
        view_1367: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_678, [2, 16, 8]);  mm_678 = None
        slice_22383: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4066, 1, 5424, 5440)
        slice_22384: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22383, 2, 0, 16)
        add_680: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22384, view_1367);  slice_22384 = view_1367 = None
        slice_scatter_4068: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22383, add_680, 2, 0, 16);  slice_22383 = add_680 = None
        slice_scatter_4069: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4066, slice_scatter_4068, 1, 5424, 5440);  slice_scatter_4066 = slice_scatter_4068 = None
        slice_22388: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4069, 1, 5424, 5440)
        slice_22389: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22388, 2, 0, 16)
        slice_scatter_4071: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22388, slice_22389, 2, 0, 16);  slice_22388 = slice_22389 = None
        slice_scatter_4072: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4069, slice_scatter_4071, 1, 5424, 5440);  slice_scatter_4069 = slice_scatter_4071 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22409: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_22375, 2, 16, 32);  slice_22375 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_682: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_22409, memory_format = torch.contiguous_format);  slice_22409 = None
        view_1368: "f32[32, 11]" = torch.ops.aten.view.default(clone_682, [32, 11]);  clone_682 = None
        mm_679: "f32[32, 8]" = torch.ops.aten.mm.default(view_1368, slice_37)
        view_1369: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_679, [2, 16, 8]);  mm_679 = None
        slice_22416: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4072, 1, 5424, 5440)
        slice_22417: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22416, 2, 0, 16)
        add_681: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22417, view_1369);  slice_22417 = view_1369 = None
        slice_scatter_4074: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22416, add_681, 2, 0, 16);  slice_22416 = add_681 = None
        slice_scatter_4075: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4072, slice_scatter_4074, 1, 5424, 5440);  slice_scatter_4072 = slice_scatter_4074 = None
        slice_22421: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4075, 1, 5424, 5440)
        slice_22422: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22421, 2, 0, 16)
        slice_scatter_4077: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22421, slice_22422, 2, 0, 16);  slice_22421 = slice_22422 = None
        slice_scatter_4078: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4075, slice_scatter_4077, 1, 5424, 5440);  slice_scatter_4075 = slice_scatter_4077 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22441: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5440, 5456)
        slice_22442: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_22441, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_683: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_22442, memory_format = torch.contiguous_format);  slice_22442 = None
        view_1370: "f32[32, 16]" = torch.ops.aten.view.default(clone_683, [32, 16]);  clone_683 = None
        mm_680: "f32[32, 8]" = torch.ops.aten.mm.default(view_1370, slice_7)
        view_1371: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_680, [2, 16, 8]);  mm_680 = None
        slice_22449: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4078, 1, 5440, 5456)
        slice_22450: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22449, 2, 0, 16)
        add_682: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22450, view_1371);  slice_22450 = view_1371 = None
        slice_scatter_4080: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22449, add_682, 2, 0, 16);  slice_22449 = add_682 = None
        slice_scatter_4081: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4078, slice_scatter_4080, 1, 5440, 5456);  slice_scatter_4078 = slice_scatter_4080 = None
        slice_22454: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4081, 1, 5440, 5456)
        slice_22455: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22454, 2, 0, 16)
        slice_scatter_4083: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22454, slice_22455, 2, 0, 16);  slice_22454 = slice_22455 = None
        slice_scatter_4084: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4081, slice_scatter_4083, 1, 5440, 5456);  slice_scatter_4081 = slice_scatter_4083 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22475: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_22441, 2, 16, 32);  slice_22441 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_684: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_22475, memory_format = torch.contiguous_format);  slice_22475 = None
        view_1372: "f32[32, 11]" = torch.ops.aten.view.default(clone_684, [32, 11]);  clone_684 = None
        mm_681: "f32[32, 8]" = torch.ops.aten.mm.default(view_1372, slice_37)
        view_1373: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_681, [2, 16, 8]);  mm_681 = None
        slice_22482: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4084, 1, 5440, 5456)
        slice_22483: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22482, 2, 0, 16)
        add_683: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22483, view_1373);  slice_22483 = view_1373 = None
        slice_scatter_4086: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22482, add_683, 2, 0, 16);  slice_22482 = add_683 = None
        slice_scatter_4087: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4084, slice_scatter_4086, 1, 5440, 5456);  slice_scatter_4084 = slice_scatter_4086 = None
        slice_22487: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4087, 1, 5440, 5456)
        slice_22488: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22487, 2, 0, 16)
        slice_scatter_4089: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22487, slice_22488, 2, 0, 16);  slice_22487 = slice_22488 = None
        slice_scatter_4090: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4087, slice_scatter_4089, 1, 5440, 5456);  slice_scatter_4087 = slice_scatter_4089 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22507: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5456, 5472)
        slice_22508: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_22507, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_685: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_22508, memory_format = torch.contiguous_format);  slice_22508 = None
        view_1374: "f32[32, 16]" = torch.ops.aten.view.default(clone_685, [32, 16]);  clone_685 = None
        mm_682: "f32[32, 8]" = torch.ops.aten.mm.default(view_1374, slice_7)
        view_1375: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_682, [2, 16, 8]);  mm_682 = None
        slice_22515: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4090, 1, 5456, 5472)
        slice_22516: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22515, 2, 0, 16)
        add_684: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22516, view_1375);  slice_22516 = view_1375 = None
        slice_scatter_4092: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22515, add_684, 2, 0, 16);  slice_22515 = add_684 = None
        slice_scatter_4093: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4090, slice_scatter_4092, 1, 5456, 5472);  slice_scatter_4090 = slice_scatter_4092 = None
        slice_22520: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4093, 1, 5456, 5472)
        slice_22521: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22520, 2, 0, 16)
        slice_scatter_4095: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22520, slice_22521, 2, 0, 16);  slice_22520 = slice_22521 = None
        slice_scatter_4096: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4093, slice_scatter_4095, 1, 5456, 5472);  slice_scatter_4093 = slice_scatter_4095 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22541: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_22507, 2, 16, 32);  slice_22507 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_686: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_22541, memory_format = torch.contiguous_format);  slice_22541 = None
        view_1376: "f32[32, 11]" = torch.ops.aten.view.default(clone_686, [32, 11]);  clone_686 = None
        mm_683: "f32[32, 8]" = torch.ops.aten.mm.default(view_1376, slice_37)
        view_1377: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_683, [2, 16, 8]);  mm_683 = None
        slice_22548: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4096, 1, 5456, 5472)
        slice_22549: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22548, 2, 0, 16)
        add_685: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22549, view_1377);  slice_22549 = view_1377 = None
        slice_scatter_4098: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22548, add_685, 2, 0, 16);  slice_22548 = add_685 = None
        slice_scatter_4099: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4096, slice_scatter_4098, 1, 5456, 5472);  slice_scatter_4096 = slice_scatter_4098 = None
        slice_22553: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4099, 1, 5456, 5472)
        slice_22554: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22553, 2, 0, 16)
        slice_scatter_4101: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22553, slice_22554, 2, 0, 16);  slice_22553 = slice_22554 = None
        slice_scatter_4102: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4099, slice_scatter_4101, 1, 5456, 5472);  slice_scatter_4099 = slice_scatter_4101 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22573: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5472, 5488)
        slice_22574: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_22573, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_687: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_22574, memory_format = torch.contiguous_format);  slice_22574 = None
        view_1378: "f32[32, 16]" = torch.ops.aten.view.default(clone_687, [32, 16]);  clone_687 = None
        mm_684: "f32[32, 8]" = torch.ops.aten.mm.default(view_1378, slice_7)
        view_1379: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_684, [2, 16, 8]);  mm_684 = None
        slice_22581: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4102, 1, 5472, 5488)
        slice_22582: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22581, 2, 0, 16)
        add_686: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22582, view_1379);  slice_22582 = view_1379 = None
        slice_scatter_4104: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22581, add_686, 2, 0, 16);  slice_22581 = add_686 = None
        slice_scatter_4105: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4102, slice_scatter_4104, 1, 5472, 5488);  slice_scatter_4102 = slice_scatter_4104 = None
        slice_22586: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4105, 1, 5472, 5488)
        slice_22587: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22586, 2, 0, 16)
        slice_scatter_4107: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22586, slice_22587, 2, 0, 16);  slice_22586 = slice_22587 = None
        slice_scatter_4108: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4105, slice_scatter_4107, 1, 5472, 5488);  slice_scatter_4105 = slice_scatter_4107 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22607: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_22573, 2, 16, 32);  slice_22573 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_688: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_22607, memory_format = torch.contiguous_format);  slice_22607 = None
        view_1380: "f32[32, 11]" = torch.ops.aten.view.default(clone_688, [32, 11]);  clone_688 = None
        mm_685: "f32[32, 8]" = torch.ops.aten.mm.default(view_1380, slice_37)
        view_1381: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_685, [2, 16, 8]);  mm_685 = None
        slice_22614: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4108, 1, 5472, 5488)
        slice_22615: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22614, 2, 0, 16)
        add_687: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22615, view_1381);  slice_22615 = view_1381 = None
        slice_scatter_4110: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22614, add_687, 2, 0, 16);  slice_22614 = add_687 = None
        slice_scatter_4111: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4108, slice_scatter_4110, 1, 5472, 5488);  slice_scatter_4108 = slice_scatter_4110 = None
        slice_22619: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4111, 1, 5472, 5488)
        slice_22620: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22619, 2, 0, 16)
        slice_scatter_4113: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22619, slice_22620, 2, 0, 16);  slice_22619 = slice_22620 = None
        slice_scatter_4114: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4111, slice_scatter_4113, 1, 5472, 5488);  slice_scatter_4111 = slice_scatter_4113 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22639: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5488, 5504)
        slice_22640: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_22639, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_689: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_22640, memory_format = torch.contiguous_format);  slice_22640 = None
        view_1382: "f32[32, 16]" = torch.ops.aten.view.default(clone_689, [32, 16]);  clone_689 = None
        mm_686: "f32[32, 8]" = torch.ops.aten.mm.default(view_1382, slice_7)
        view_1383: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_686, [2, 16, 8]);  mm_686 = None
        slice_22647: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4114, 1, 5488, 5504)
        slice_22648: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22647, 2, 0, 16)
        add_688: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22648, view_1383);  slice_22648 = view_1383 = None
        slice_scatter_4116: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22647, add_688, 2, 0, 16);  slice_22647 = add_688 = None
        slice_scatter_4117: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4114, slice_scatter_4116, 1, 5488, 5504);  slice_scatter_4114 = slice_scatter_4116 = None
        slice_22652: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4117, 1, 5488, 5504)
        slice_22653: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22652, 2, 0, 16)
        slice_scatter_4119: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22652, slice_22653, 2, 0, 16);  slice_22652 = slice_22653 = None
        slice_scatter_4120: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4117, slice_scatter_4119, 1, 5488, 5504);  slice_scatter_4117 = slice_scatter_4119 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22673: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_22639, 2, 16, 32);  slice_22639 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_690: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_22673, memory_format = torch.contiguous_format);  slice_22673 = None
        view_1384: "f32[32, 11]" = torch.ops.aten.view.default(clone_690, [32, 11]);  clone_690 = None
        mm_687: "f32[32, 8]" = torch.ops.aten.mm.default(view_1384, slice_37)
        view_1385: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_687, [2, 16, 8]);  mm_687 = None
        slice_22680: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4120, 1, 5488, 5504)
        slice_22681: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22680, 2, 0, 16)
        add_689: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22681, view_1385);  slice_22681 = view_1385 = None
        slice_scatter_4122: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22680, add_689, 2, 0, 16);  slice_22680 = add_689 = None
        slice_scatter_4123: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4120, slice_scatter_4122, 1, 5488, 5504);  slice_scatter_4120 = slice_scatter_4122 = None
        slice_22685: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4123, 1, 5488, 5504)
        slice_22686: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22685, 2, 0, 16)
        slice_scatter_4125: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22685, slice_22686, 2, 0, 16);  slice_22685 = slice_22686 = None
        slice_scatter_4126: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4123, slice_scatter_4125, 1, 5488, 5504);  slice_scatter_4123 = slice_scatter_4125 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22705: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5504, 5520)
        slice_22706: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_22705, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_691: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_22706, memory_format = torch.contiguous_format);  slice_22706 = None
        view_1386: "f32[32, 16]" = torch.ops.aten.view.default(clone_691, [32, 16]);  clone_691 = None
        mm_688: "f32[32, 8]" = torch.ops.aten.mm.default(view_1386, slice_7)
        view_1387: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_688, [2, 16, 8]);  mm_688 = None
        slice_22713: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4126, 1, 5504, 5520)
        slice_22714: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22713, 2, 0, 16)
        add_690: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22714, view_1387);  slice_22714 = view_1387 = None
        slice_scatter_4128: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22713, add_690, 2, 0, 16);  slice_22713 = add_690 = None
        slice_scatter_4129: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4126, slice_scatter_4128, 1, 5504, 5520);  slice_scatter_4126 = slice_scatter_4128 = None
        slice_22718: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4129, 1, 5504, 5520)
        slice_22719: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22718, 2, 0, 16)
        slice_scatter_4131: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22718, slice_22719, 2, 0, 16);  slice_22718 = slice_22719 = None
        slice_scatter_4132: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4129, slice_scatter_4131, 1, 5504, 5520);  slice_scatter_4129 = slice_scatter_4131 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22739: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_22705, 2, 16, 32);  slice_22705 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_692: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_22739, memory_format = torch.contiguous_format);  slice_22739 = None
        view_1388: "f32[32, 11]" = torch.ops.aten.view.default(clone_692, [32, 11]);  clone_692 = None
        mm_689: "f32[32, 8]" = torch.ops.aten.mm.default(view_1388, slice_37)
        view_1389: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_689, [2, 16, 8]);  mm_689 = None
        slice_22746: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4132, 1, 5504, 5520)
        slice_22747: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22746, 2, 0, 16)
        add_691: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22747, view_1389);  slice_22747 = view_1389 = None
        slice_scatter_4134: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22746, add_691, 2, 0, 16);  slice_22746 = add_691 = None
        slice_scatter_4135: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4132, slice_scatter_4134, 1, 5504, 5520);  slice_scatter_4132 = slice_scatter_4134 = None
        slice_22751: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4135, 1, 5504, 5520)
        slice_22752: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22751, 2, 0, 16)
        slice_scatter_4137: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22751, slice_22752, 2, 0, 16);  slice_22751 = slice_22752 = None
        slice_scatter_4138: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4135, slice_scatter_4137, 1, 5504, 5520);  slice_scatter_4135 = slice_scatter_4137 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22771: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5520, 5536)
        slice_22772: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_22771, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_693: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_22772, memory_format = torch.contiguous_format);  slice_22772 = None
        view_1390: "f32[32, 16]" = torch.ops.aten.view.default(clone_693, [32, 16]);  clone_693 = None
        mm_690: "f32[32, 8]" = torch.ops.aten.mm.default(view_1390, slice_7)
        view_1391: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_690, [2, 16, 8]);  mm_690 = None
        slice_22779: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4138, 1, 5520, 5536)
        slice_22780: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22779, 2, 0, 16)
        add_692: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22780, view_1391);  slice_22780 = view_1391 = None
        slice_scatter_4140: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22779, add_692, 2, 0, 16);  slice_22779 = add_692 = None
        slice_scatter_4141: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4138, slice_scatter_4140, 1, 5520, 5536);  slice_scatter_4138 = slice_scatter_4140 = None
        slice_22784: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4141, 1, 5520, 5536)
        slice_22785: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22784, 2, 0, 16)
        slice_scatter_4143: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22784, slice_22785, 2, 0, 16);  slice_22784 = slice_22785 = None
        slice_scatter_4144: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4141, slice_scatter_4143, 1, 5520, 5536);  slice_scatter_4141 = slice_scatter_4143 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22805: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_22771, 2, 16, 32);  slice_22771 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_694: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_22805, memory_format = torch.contiguous_format);  slice_22805 = None
        view_1392: "f32[32, 11]" = torch.ops.aten.view.default(clone_694, [32, 11]);  clone_694 = None
        mm_691: "f32[32, 8]" = torch.ops.aten.mm.default(view_1392, slice_37)
        view_1393: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_691, [2, 16, 8]);  mm_691 = None
        slice_22812: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4144, 1, 5520, 5536)
        slice_22813: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22812, 2, 0, 16)
        add_693: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22813, view_1393);  slice_22813 = view_1393 = None
        slice_scatter_4146: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22812, add_693, 2, 0, 16);  slice_22812 = add_693 = None
        slice_scatter_4147: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4144, slice_scatter_4146, 1, 5520, 5536);  slice_scatter_4144 = slice_scatter_4146 = None
        slice_22817: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4147, 1, 5520, 5536)
        slice_22818: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22817, 2, 0, 16)
        slice_scatter_4149: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22817, slice_22818, 2, 0, 16);  slice_22817 = slice_22818 = None
        slice_scatter_4150: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4147, slice_scatter_4149, 1, 5520, 5536);  slice_scatter_4147 = slice_scatter_4149 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22837: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5536, 5552)
        slice_22838: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_22837, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_695: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_22838, memory_format = torch.contiguous_format);  slice_22838 = None
        view_1394: "f32[32, 16]" = torch.ops.aten.view.default(clone_695, [32, 16]);  clone_695 = None
        mm_692: "f32[32, 8]" = torch.ops.aten.mm.default(view_1394, slice_7)
        view_1395: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_692, [2, 16, 8]);  mm_692 = None
        slice_22845: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4150, 1, 5536, 5552)
        slice_22846: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22845, 2, 0, 16)
        add_694: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22846, view_1395);  slice_22846 = view_1395 = None
        slice_scatter_4152: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22845, add_694, 2, 0, 16);  slice_22845 = add_694 = None
        slice_scatter_4153: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4150, slice_scatter_4152, 1, 5536, 5552);  slice_scatter_4150 = slice_scatter_4152 = None
        slice_22850: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4153, 1, 5536, 5552)
        slice_22851: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22850, 2, 0, 16)
        slice_scatter_4155: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22850, slice_22851, 2, 0, 16);  slice_22850 = slice_22851 = None
        slice_scatter_4156: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4153, slice_scatter_4155, 1, 5536, 5552);  slice_scatter_4153 = slice_scatter_4155 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22871: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_22837, 2, 16, 32);  slice_22837 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_696: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_22871, memory_format = torch.contiguous_format);  slice_22871 = None
        view_1396: "f32[32, 11]" = torch.ops.aten.view.default(clone_696, [32, 11]);  clone_696 = None
        mm_693: "f32[32, 8]" = torch.ops.aten.mm.default(view_1396, slice_37)
        view_1397: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_693, [2, 16, 8]);  mm_693 = None
        slice_22878: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4156, 1, 5536, 5552)
        slice_22879: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22878, 2, 0, 16)
        add_695: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22879, view_1397);  slice_22879 = view_1397 = None
        slice_scatter_4158: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22878, add_695, 2, 0, 16);  slice_22878 = add_695 = None
        slice_scatter_4159: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4156, slice_scatter_4158, 1, 5536, 5552);  slice_scatter_4156 = slice_scatter_4158 = None
        slice_22883: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4159, 1, 5536, 5552)
        slice_22884: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22883, 2, 0, 16)
        slice_scatter_4161: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22883, slice_22884, 2, 0, 16);  slice_22883 = slice_22884 = None
        slice_scatter_4162: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4159, slice_scatter_4161, 1, 5536, 5552);  slice_scatter_4159 = slice_scatter_4161 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22903: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5552, 5568)
        slice_22904: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_22903, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_697: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_22904, memory_format = torch.contiguous_format);  slice_22904 = None
        view_1398: "f32[32, 16]" = torch.ops.aten.view.default(clone_697, [32, 16]);  clone_697 = None
        mm_694: "f32[32, 8]" = torch.ops.aten.mm.default(view_1398, slice_7)
        view_1399: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_694, [2, 16, 8]);  mm_694 = None
        slice_22911: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4162, 1, 5552, 5568)
        slice_22912: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22911, 2, 0, 16)
        add_696: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22912, view_1399);  slice_22912 = view_1399 = None
        slice_scatter_4164: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22911, add_696, 2, 0, 16);  slice_22911 = add_696 = None
        slice_scatter_4165: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4162, slice_scatter_4164, 1, 5552, 5568);  slice_scatter_4162 = slice_scatter_4164 = None
        slice_22916: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4165, 1, 5552, 5568)
        slice_22917: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22916, 2, 0, 16)
        slice_scatter_4167: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22916, slice_22917, 2, 0, 16);  slice_22916 = slice_22917 = None
        slice_scatter_4168: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4165, slice_scatter_4167, 1, 5552, 5568);  slice_scatter_4165 = slice_scatter_4167 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22937: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_22903, 2, 16, 32);  slice_22903 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_698: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_22937, memory_format = torch.contiguous_format);  slice_22937 = None
        view_1400: "f32[32, 11]" = torch.ops.aten.view.default(clone_698, [32, 11]);  clone_698 = None
        mm_695: "f32[32, 8]" = torch.ops.aten.mm.default(view_1400, slice_37)
        view_1401: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_695, [2, 16, 8]);  mm_695 = None
        slice_22944: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4168, 1, 5552, 5568)
        slice_22945: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22944, 2, 0, 16)
        add_697: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22945, view_1401);  slice_22945 = view_1401 = None
        slice_scatter_4170: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22944, add_697, 2, 0, 16);  slice_22944 = add_697 = None
        slice_scatter_4171: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4168, slice_scatter_4170, 1, 5552, 5568);  slice_scatter_4168 = slice_scatter_4170 = None
        slice_22949: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4171, 1, 5552, 5568)
        slice_22950: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22949, 2, 0, 16)
        slice_scatter_4173: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22949, slice_22950, 2, 0, 16);  slice_22949 = slice_22950 = None
        slice_scatter_4174: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4171, slice_scatter_4173, 1, 5552, 5568);  slice_scatter_4171 = slice_scatter_4173 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_22969: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5568, 5584)
        slice_22970: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_22969, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_699: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_22970, memory_format = torch.contiguous_format);  slice_22970 = None
        view_1402: "f32[32, 16]" = torch.ops.aten.view.default(clone_699, [32, 16]);  clone_699 = None
        mm_696: "f32[32, 8]" = torch.ops.aten.mm.default(view_1402, slice_7)
        view_1403: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_696, [2, 16, 8]);  mm_696 = None
        slice_22977: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4174, 1, 5568, 5584)
        slice_22978: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22977, 2, 0, 16)
        add_698: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_22978, view_1403);  slice_22978 = view_1403 = None
        slice_scatter_4176: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22977, add_698, 2, 0, 16);  slice_22977 = add_698 = None
        slice_scatter_4177: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4174, slice_scatter_4176, 1, 5568, 5584);  slice_scatter_4174 = slice_scatter_4176 = None
        slice_22982: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4177, 1, 5568, 5584)
        slice_22983: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_22982, 2, 0, 16)
        slice_scatter_4179: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_22982, slice_22983, 2, 0, 16);  slice_22982 = slice_22983 = None
        slice_scatter_4180: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4177, slice_scatter_4179, 1, 5568, 5584);  slice_scatter_4177 = slice_scatter_4179 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23003: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_22969, 2, 16, 32);  slice_22969 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_700: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_23003, memory_format = torch.contiguous_format);  slice_23003 = None
        view_1404: "f32[32, 11]" = torch.ops.aten.view.default(clone_700, [32, 11]);  clone_700 = None
        mm_697: "f32[32, 8]" = torch.ops.aten.mm.default(view_1404, slice_37)
        view_1405: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_697, [2, 16, 8]);  mm_697 = None
        slice_23010: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4180, 1, 5568, 5584)
        slice_23011: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23010, 2, 0, 16)
        add_699: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23011, view_1405);  slice_23011 = view_1405 = None
        slice_scatter_4182: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23010, add_699, 2, 0, 16);  slice_23010 = add_699 = None
        slice_scatter_4183: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4180, slice_scatter_4182, 1, 5568, 5584);  slice_scatter_4180 = slice_scatter_4182 = None
        slice_23015: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4183, 1, 5568, 5584)
        slice_23016: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23015, 2, 0, 16)
        slice_scatter_4185: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23015, slice_23016, 2, 0, 16);  slice_23015 = slice_23016 = None
        slice_scatter_4186: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4183, slice_scatter_4185, 1, 5568, 5584);  slice_scatter_4183 = slice_scatter_4185 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23035: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5584, 5600)
        slice_23036: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_23035, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_701: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_23036, memory_format = torch.contiguous_format);  slice_23036 = None
        view_1406: "f32[32, 16]" = torch.ops.aten.view.default(clone_701, [32, 16]);  clone_701 = None
        mm_698: "f32[32, 8]" = torch.ops.aten.mm.default(view_1406, slice_7)
        view_1407: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_698, [2, 16, 8]);  mm_698 = None
        slice_23043: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4186, 1, 5584, 5600)
        slice_23044: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23043, 2, 0, 16)
        add_700: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23044, view_1407);  slice_23044 = view_1407 = None
        slice_scatter_4188: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23043, add_700, 2, 0, 16);  slice_23043 = add_700 = None
        slice_scatter_4189: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4186, slice_scatter_4188, 1, 5584, 5600);  slice_scatter_4186 = slice_scatter_4188 = None
        slice_23048: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4189, 1, 5584, 5600)
        slice_23049: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23048, 2, 0, 16)
        slice_scatter_4191: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23048, slice_23049, 2, 0, 16);  slice_23048 = slice_23049 = None
        slice_scatter_4192: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4189, slice_scatter_4191, 1, 5584, 5600);  slice_scatter_4189 = slice_scatter_4191 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23069: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_23035, 2, 16, 32);  slice_23035 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_702: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_23069, memory_format = torch.contiguous_format);  slice_23069 = None
        view_1408: "f32[32, 11]" = torch.ops.aten.view.default(clone_702, [32, 11]);  clone_702 = None
        mm_699: "f32[32, 8]" = torch.ops.aten.mm.default(view_1408, slice_37)
        view_1409: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_699, [2, 16, 8]);  mm_699 = None
        slice_23076: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4192, 1, 5584, 5600)
        slice_23077: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23076, 2, 0, 16)
        add_701: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23077, view_1409);  slice_23077 = view_1409 = None
        slice_scatter_4194: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23076, add_701, 2, 0, 16);  slice_23076 = add_701 = None
        slice_scatter_4195: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4192, slice_scatter_4194, 1, 5584, 5600);  slice_scatter_4192 = slice_scatter_4194 = None
        slice_23081: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4195, 1, 5584, 5600)
        slice_23082: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23081, 2, 0, 16)
        slice_scatter_4197: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23081, slice_23082, 2, 0, 16);  slice_23081 = slice_23082 = None
        slice_scatter_4198: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4195, slice_scatter_4197, 1, 5584, 5600);  slice_scatter_4195 = slice_scatter_4197 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23101: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5600, 5616)
        slice_23102: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_23101, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_703: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_23102, memory_format = torch.contiguous_format);  slice_23102 = None
        view_1410: "f32[32, 16]" = torch.ops.aten.view.default(clone_703, [32, 16]);  clone_703 = None
        mm_700: "f32[32, 8]" = torch.ops.aten.mm.default(view_1410, slice_7)
        view_1411: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_700, [2, 16, 8]);  mm_700 = None
        slice_23109: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4198, 1, 5600, 5616)
        slice_23110: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23109, 2, 0, 16)
        add_702: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23110, view_1411);  slice_23110 = view_1411 = None
        slice_scatter_4200: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23109, add_702, 2, 0, 16);  slice_23109 = add_702 = None
        slice_scatter_4201: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4198, slice_scatter_4200, 1, 5600, 5616);  slice_scatter_4198 = slice_scatter_4200 = None
        slice_23114: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4201, 1, 5600, 5616)
        slice_23115: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23114, 2, 0, 16)
        slice_scatter_4203: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23114, slice_23115, 2, 0, 16);  slice_23114 = slice_23115 = None
        slice_scatter_4204: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4201, slice_scatter_4203, 1, 5600, 5616);  slice_scatter_4201 = slice_scatter_4203 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23135: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_23101, 2, 16, 32);  slice_23101 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_704: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_23135, memory_format = torch.contiguous_format);  slice_23135 = None
        view_1412: "f32[32, 11]" = torch.ops.aten.view.default(clone_704, [32, 11]);  clone_704 = None
        mm_701: "f32[32, 8]" = torch.ops.aten.mm.default(view_1412, slice_37)
        view_1413: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_701, [2, 16, 8]);  mm_701 = None
        slice_23142: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4204, 1, 5600, 5616)
        slice_23143: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23142, 2, 0, 16)
        add_703: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23143, view_1413);  slice_23143 = view_1413 = None
        slice_scatter_4206: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23142, add_703, 2, 0, 16);  slice_23142 = add_703 = None
        slice_scatter_4207: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4204, slice_scatter_4206, 1, 5600, 5616);  slice_scatter_4204 = slice_scatter_4206 = None
        slice_23147: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4207, 1, 5600, 5616)
        slice_23148: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23147, 2, 0, 16)
        slice_scatter_4209: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23147, slice_23148, 2, 0, 16);  slice_23147 = slice_23148 = None
        slice_scatter_4210: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4207, slice_scatter_4209, 1, 5600, 5616);  slice_scatter_4207 = slice_scatter_4209 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23167: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5616, 5632)
        slice_23168: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_23167, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_705: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_23168, memory_format = torch.contiguous_format);  slice_23168 = None
        view_1414: "f32[32, 16]" = torch.ops.aten.view.default(clone_705, [32, 16]);  clone_705 = None
        mm_702: "f32[32, 8]" = torch.ops.aten.mm.default(view_1414, slice_7)
        view_1415: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_702, [2, 16, 8]);  mm_702 = None
        slice_23175: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4210, 1, 5616, 5632)
        slice_23176: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23175, 2, 0, 16)
        add_704: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23176, view_1415);  slice_23176 = view_1415 = None
        slice_scatter_4212: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23175, add_704, 2, 0, 16);  slice_23175 = add_704 = None
        slice_scatter_4213: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4210, slice_scatter_4212, 1, 5616, 5632);  slice_scatter_4210 = slice_scatter_4212 = None
        slice_23180: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4213, 1, 5616, 5632)
        slice_23181: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23180, 2, 0, 16)
        slice_scatter_4215: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23180, slice_23181, 2, 0, 16);  slice_23180 = slice_23181 = None
        slice_scatter_4216: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4213, slice_scatter_4215, 1, 5616, 5632);  slice_scatter_4213 = slice_scatter_4215 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23201: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_23167, 2, 16, 32);  slice_23167 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_706: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_23201, memory_format = torch.contiguous_format);  slice_23201 = None
        view_1416: "f32[32, 11]" = torch.ops.aten.view.default(clone_706, [32, 11]);  clone_706 = None
        mm_703: "f32[32, 8]" = torch.ops.aten.mm.default(view_1416, slice_37)
        view_1417: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_703, [2, 16, 8]);  mm_703 = None
        slice_23208: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4216, 1, 5616, 5632)
        slice_23209: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23208, 2, 0, 16)
        add_705: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23209, view_1417);  slice_23209 = view_1417 = None
        slice_scatter_4218: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23208, add_705, 2, 0, 16);  slice_23208 = add_705 = None
        slice_scatter_4219: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4216, slice_scatter_4218, 1, 5616, 5632);  slice_scatter_4216 = slice_scatter_4218 = None
        slice_23213: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4219, 1, 5616, 5632)
        slice_23214: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23213, 2, 0, 16)
        slice_scatter_4221: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23213, slice_23214, 2, 0, 16);  slice_23213 = slice_23214 = None
        slice_scatter_4222: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4219, slice_scatter_4221, 1, 5616, 5632);  slice_scatter_4219 = slice_scatter_4221 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23233: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5632, 5648)
        slice_23234: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_23233, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_707: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_23234, memory_format = torch.contiguous_format);  slice_23234 = None
        view_1418: "f32[32, 16]" = torch.ops.aten.view.default(clone_707, [32, 16]);  clone_707 = None
        mm_704: "f32[32, 8]" = torch.ops.aten.mm.default(view_1418, slice_7)
        view_1419: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_704, [2, 16, 8]);  mm_704 = None
        slice_23241: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4222, 1, 5632, 5648)
        slice_23242: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23241, 2, 0, 16)
        add_706: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23242, view_1419);  slice_23242 = view_1419 = None
        slice_scatter_4224: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23241, add_706, 2, 0, 16);  slice_23241 = add_706 = None
        slice_scatter_4225: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4222, slice_scatter_4224, 1, 5632, 5648);  slice_scatter_4222 = slice_scatter_4224 = None
        slice_23246: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4225, 1, 5632, 5648)
        slice_23247: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23246, 2, 0, 16)
        slice_scatter_4227: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23246, slice_23247, 2, 0, 16);  slice_23246 = slice_23247 = None
        slice_scatter_4228: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4225, slice_scatter_4227, 1, 5632, 5648);  slice_scatter_4225 = slice_scatter_4227 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23267: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_23233, 2, 16, 32);  slice_23233 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_708: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_23267, memory_format = torch.contiguous_format);  slice_23267 = None
        view_1420: "f32[32, 11]" = torch.ops.aten.view.default(clone_708, [32, 11]);  clone_708 = None
        mm_705: "f32[32, 8]" = torch.ops.aten.mm.default(view_1420, slice_37)
        view_1421: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_705, [2, 16, 8]);  mm_705 = None
        slice_23274: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4228, 1, 5632, 5648)
        slice_23275: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23274, 2, 0, 16)
        add_707: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23275, view_1421);  slice_23275 = view_1421 = None
        slice_scatter_4230: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23274, add_707, 2, 0, 16);  slice_23274 = add_707 = None
        slice_scatter_4231: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4228, slice_scatter_4230, 1, 5632, 5648);  slice_scatter_4228 = slice_scatter_4230 = None
        slice_23279: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4231, 1, 5632, 5648)
        slice_23280: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23279, 2, 0, 16)
        slice_scatter_4233: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23279, slice_23280, 2, 0, 16);  slice_23279 = slice_23280 = None
        slice_scatter_4234: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4231, slice_scatter_4233, 1, 5632, 5648);  slice_scatter_4231 = slice_scatter_4233 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23299: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5648, 5664)
        slice_23300: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_23299, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_709: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_23300, memory_format = torch.contiguous_format);  slice_23300 = None
        view_1422: "f32[32, 16]" = torch.ops.aten.view.default(clone_709, [32, 16]);  clone_709 = None
        mm_706: "f32[32, 8]" = torch.ops.aten.mm.default(view_1422, slice_7)
        view_1423: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_706, [2, 16, 8]);  mm_706 = None
        slice_23307: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4234, 1, 5648, 5664)
        slice_23308: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23307, 2, 0, 16)
        add_708: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23308, view_1423);  slice_23308 = view_1423 = None
        slice_scatter_4236: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23307, add_708, 2, 0, 16);  slice_23307 = add_708 = None
        slice_scatter_4237: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4234, slice_scatter_4236, 1, 5648, 5664);  slice_scatter_4234 = slice_scatter_4236 = None
        slice_23312: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4237, 1, 5648, 5664)
        slice_23313: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23312, 2, 0, 16)
        slice_scatter_4239: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23312, slice_23313, 2, 0, 16);  slice_23312 = slice_23313 = None
        slice_scatter_4240: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4237, slice_scatter_4239, 1, 5648, 5664);  slice_scatter_4237 = slice_scatter_4239 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23333: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_23299, 2, 16, 32);  slice_23299 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_710: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_23333, memory_format = torch.contiguous_format);  slice_23333 = None
        view_1424: "f32[32, 11]" = torch.ops.aten.view.default(clone_710, [32, 11]);  clone_710 = None
        mm_707: "f32[32, 8]" = torch.ops.aten.mm.default(view_1424, slice_37)
        view_1425: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_707, [2, 16, 8]);  mm_707 = None
        slice_23340: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4240, 1, 5648, 5664)
        slice_23341: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23340, 2, 0, 16)
        add_709: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23341, view_1425);  slice_23341 = view_1425 = None
        slice_scatter_4242: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23340, add_709, 2, 0, 16);  slice_23340 = add_709 = None
        slice_scatter_4243: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4240, slice_scatter_4242, 1, 5648, 5664);  slice_scatter_4240 = slice_scatter_4242 = None
        slice_23345: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4243, 1, 5648, 5664)
        slice_23346: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23345, 2, 0, 16)
        slice_scatter_4245: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23345, slice_23346, 2, 0, 16);  slice_23345 = slice_23346 = None
        slice_scatter_4246: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4243, slice_scatter_4245, 1, 5648, 5664);  slice_scatter_4243 = slice_scatter_4245 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23365: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5664, 5680)
        slice_23366: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_23365, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_711: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_23366, memory_format = torch.contiguous_format);  slice_23366 = None
        view_1426: "f32[32, 16]" = torch.ops.aten.view.default(clone_711, [32, 16]);  clone_711 = None
        mm_708: "f32[32, 8]" = torch.ops.aten.mm.default(view_1426, slice_7)
        view_1427: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_708, [2, 16, 8]);  mm_708 = None
        slice_23373: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4246, 1, 5664, 5680)
        slice_23374: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23373, 2, 0, 16)
        add_710: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23374, view_1427);  slice_23374 = view_1427 = None
        slice_scatter_4248: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23373, add_710, 2, 0, 16);  slice_23373 = add_710 = None
        slice_scatter_4249: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4246, slice_scatter_4248, 1, 5664, 5680);  slice_scatter_4246 = slice_scatter_4248 = None
        slice_23378: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4249, 1, 5664, 5680)
        slice_23379: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23378, 2, 0, 16)
        slice_scatter_4251: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23378, slice_23379, 2, 0, 16);  slice_23378 = slice_23379 = None
        slice_scatter_4252: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4249, slice_scatter_4251, 1, 5664, 5680);  slice_scatter_4249 = slice_scatter_4251 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23399: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_23365, 2, 16, 32);  slice_23365 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_712: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_23399, memory_format = torch.contiguous_format);  slice_23399 = None
        view_1428: "f32[32, 11]" = torch.ops.aten.view.default(clone_712, [32, 11]);  clone_712 = None
        mm_709: "f32[32, 8]" = torch.ops.aten.mm.default(view_1428, slice_37)
        view_1429: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_709, [2, 16, 8]);  mm_709 = None
        slice_23406: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4252, 1, 5664, 5680)
        slice_23407: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23406, 2, 0, 16)
        add_711: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23407, view_1429);  slice_23407 = view_1429 = None
        slice_scatter_4254: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23406, add_711, 2, 0, 16);  slice_23406 = add_711 = None
        slice_scatter_4255: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4252, slice_scatter_4254, 1, 5664, 5680);  slice_scatter_4252 = slice_scatter_4254 = None
        slice_23411: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4255, 1, 5664, 5680)
        slice_23412: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23411, 2, 0, 16)
        slice_scatter_4257: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23411, slice_23412, 2, 0, 16);  slice_23411 = slice_23412 = None
        slice_scatter_4258: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4255, slice_scatter_4257, 1, 5664, 5680);  slice_scatter_4255 = slice_scatter_4257 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23431: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5680, 5696)
        slice_23432: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_23431, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_713: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_23432, memory_format = torch.contiguous_format);  slice_23432 = None
        view_1430: "f32[32, 16]" = torch.ops.aten.view.default(clone_713, [32, 16]);  clone_713 = None
        mm_710: "f32[32, 8]" = torch.ops.aten.mm.default(view_1430, slice_7)
        view_1431: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_710, [2, 16, 8]);  mm_710 = None
        slice_23439: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4258, 1, 5680, 5696)
        slice_23440: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23439, 2, 0, 16)
        add_712: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23440, view_1431);  slice_23440 = view_1431 = None
        slice_scatter_4260: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23439, add_712, 2, 0, 16);  slice_23439 = add_712 = None
        slice_scatter_4261: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4258, slice_scatter_4260, 1, 5680, 5696);  slice_scatter_4258 = slice_scatter_4260 = None
        slice_23444: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4261, 1, 5680, 5696)
        slice_23445: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23444, 2, 0, 16)
        slice_scatter_4263: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23444, slice_23445, 2, 0, 16);  slice_23444 = slice_23445 = None
        slice_scatter_4264: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4261, slice_scatter_4263, 1, 5680, 5696);  slice_scatter_4261 = slice_scatter_4263 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23465: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_23431, 2, 16, 32);  slice_23431 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_714: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_23465, memory_format = torch.contiguous_format);  slice_23465 = None
        view_1432: "f32[32, 11]" = torch.ops.aten.view.default(clone_714, [32, 11]);  clone_714 = None
        mm_711: "f32[32, 8]" = torch.ops.aten.mm.default(view_1432, slice_37)
        view_1433: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_711, [2, 16, 8]);  mm_711 = None
        slice_23472: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4264, 1, 5680, 5696)
        slice_23473: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23472, 2, 0, 16)
        add_713: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23473, view_1433);  slice_23473 = view_1433 = None
        slice_scatter_4266: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23472, add_713, 2, 0, 16);  slice_23472 = add_713 = None
        slice_scatter_4267: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4264, slice_scatter_4266, 1, 5680, 5696);  slice_scatter_4264 = slice_scatter_4266 = None
        slice_23477: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4267, 1, 5680, 5696)
        slice_23478: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23477, 2, 0, 16)
        slice_scatter_4269: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23477, slice_23478, 2, 0, 16);  slice_23477 = slice_23478 = None
        slice_scatter_4270: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4267, slice_scatter_4269, 1, 5680, 5696);  slice_scatter_4267 = slice_scatter_4269 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23497: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5696, 5712)
        slice_23498: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_23497, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_715: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_23498, memory_format = torch.contiguous_format);  slice_23498 = None
        view_1434: "f32[32, 16]" = torch.ops.aten.view.default(clone_715, [32, 16]);  clone_715 = None
        mm_712: "f32[32, 8]" = torch.ops.aten.mm.default(view_1434, slice_7)
        view_1435: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_712, [2, 16, 8]);  mm_712 = None
        slice_23505: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4270, 1, 5696, 5712)
        slice_23506: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23505, 2, 0, 16)
        add_714: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23506, view_1435);  slice_23506 = view_1435 = None
        slice_scatter_4272: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23505, add_714, 2, 0, 16);  slice_23505 = add_714 = None
        slice_scatter_4273: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4270, slice_scatter_4272, 1, 5696, 5712);  slice_scatter_4270 = slice_scatter_4272 = None
        slice_23510: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4273, 1, 5696, 5712)
        slice_23511: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23510, 2, 0, 16)
        slice_scatter_4275: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23510, slice_23511, 2, 0, 16);  slice_23510 = slice_23511 = None
        slice_scatter_4276: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4273, slice_scatter_4275, 1, 5696, 5712);  slice_scatter_4273 = slice_scatter_4275 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23531: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_23497, 2, 16, 32);  slice_23497 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_716: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_23531, memory_format = torch.contiguous_format);  slice_23531 = None
        view_1436: "f32[32, 11]" = torch.ops.aten.view.default(clone_716, [32, 11]);  clone_716 = None
        mm_713: "f32[32, 8]" = torch.ops.aten.mm.default(view_1436, slice_37)
        view_1437: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_713, [2, 16, 8]);  mm_713 = None
        slice_23538: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4276, 1, 5696, 5712)
        slice_23539: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23538, 2, 0, 16)
        add_715: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23539, view_1437);  slice_23539 = view_1437 = None
        slice_scatter_4278: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23538, add_715, 2, 0, 16);  slice_23538 = add_715 = None
        slice_scatter_4279: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4276, slice_scatter_4278, 1, 5696, 5712);  slice_scatter_4276 = slice_scatter_4278 = None
        slice_23543: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4279, 1, 5696, 5712)
        slice_23544: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23543, 2, 0, 16)
        slice_scatter_4281: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23543, slice_23544, 2, 0, 16);  slice_23543 = slice_23544 = None
        slice_scatter_4282: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4279, slice_scatter_4281, 1, 5696, 5712);  slice_scatter_4279 = slice_scatter_4281 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23563: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5712, 5728)
        slice_23564: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_23563, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_717: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_23564, memory_format = torch.contiguous_format);  slice_23564 = None
        view_1438: "f32[32, 16]" = torch.ops.aten.view.default(clone_717, [32, 16]);  clone_717 = None
        mm_714: "f32[32, 8]" = torch.ops.aten.mm.default(view_1438, slice_7)
        view_1439: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_714, [2, 16, 8]);  mm_714 = None
        slice_23571: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4282, 1, 5712, 5728)
        slice_23572: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23571, 2, 0, 16)
        add_716: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23572, view_1439);  slice_23572 = view_1439 = None
        slice_scatter_4284: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23571, add_716, 2, 0, 16);  slice_23571 = add_716 = None
        slice_scatter_4285: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4282, slice_scatter_4284, 1, 5712, 5728);  slice_scatter_4282 = slice_scatter_4284 = None
        slice_23576: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4285, 1, 5712, 5728)
        slice_23577: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23576, 2, 0, 16)
        slice_scatter_4287: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23576, slice_23577, 2, 0, 16);  slice_23576 = slice_23577 = None
        slice_scatter_4288: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4285, slice_scatter_4287, 1, 5712, 5728);  slice_scatter_4285 = slice_scatter_4287 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23597: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_23563, 2, 16, 32);  slice_23563 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_718: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_23597, memory_format = torch.contiguous_format);  slice_23597 = None
        view_1440: "f32[32, 11]" = torch.ops.aten.view.default(clone_718, [32, 11]);  clone_718 = None
        mm_715: "f32[32, 8]" = torch.ops.aten.mm.default(view_1440, slice_37)
        view_1441: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_715, [2, 16, 8]);  mm_715 = None
        slice_23604: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4288, 1, 5712, 5728)
        slice_23605: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23604, 2, 0, 16)
        add_717: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23605, view_1441);  slice_23605 = view_1441 = None
        slice_scatter_4290: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23604, add_717, 2, 0, 16);  slice_23604 = add_717 = None
        slice_scatter_4291: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4288, slice_scatter_4290, 1, 5712, 5728);  slice_scatter_4288 = slice_scatter_4290 = None
        slice_23609: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4291, 1, 5712, 5728)
        slice_23610: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23609, 2, 0, 16)
        slice_scatter_4293: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23609, slice_23610, 2, 0, 16);  slice_23609 = slice_23610 = None
        slice_scatter_4294: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4291, slice_scatter_4293, 1, 5712, 5728);  slice_scatter_4291 = slice_scatter_4293 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23629: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5728, 5744)
        slice_23630: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_23629, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_719: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_23630, memory_format = torch.contiguous_format);  slice_23630 = None
        view_1442: "f32[32, 16]" = torch.ops.aten.view.default(clone_719, [32, 16]);  clone_719 = None
        mm_716: "f32[32, 8]" = torch.ops.aten.mm.default(view_1442, slice_7)
        view_1443: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_716, [2, 16, 8]);  mm_716 = None
        slice_23637: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4294, 1, 5728, 5744)
        slice_23638: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23637, 2, 0, 16)
        add_718: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23638, view_1443);  slice_23638 = view_1443 = None
        slice_scatter_4296: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23637, add_718, 2, 0, 16);  slice_23637 = add_718 = None
        slice_scatter_4297: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4294, slice_scatter_4296, 1, 5728, 5744);  slice_scatter_4294 = slice_scatter_4296 = None
        slice_23642: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4297, 1, 5728, 5744)
        slice_23643: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23642, 2, 0, 16)
        slice_scatter_4299: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23642, slice_23643, 2, 0, 16);  slice_23642 = slice_23643 = None
        slice_scatter_4300: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4297, slice_scatter_4299, 1, 5728, 5744);  slice_scatter_4297 = slice_scatter_4299 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23663: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_23629, 2, 16, 32);  slice_23629 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_720: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_23663, memory_format = torch.contiguous_format);  slice_23663 = None
        view_1444: "f32[32, 11]" = torch.ops.aten.view.default(clone_720, [32, 11]);  clone_720 = None
        mm_717: "f32[32, 8]" = torch.ops.aten.mm.default(view_1444, slice_37)
        view_1445: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_717, [2, 16, 8]);  mm_717 = None
        slice_23670: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4300, 1, 5728, 5744)
        slice_23671: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23670, 2, 0, 16)
        add_719: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23671, view_1445);  slice_23671 = view_1445 = None
        slice_scatter_4302: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23670, add_719, 2, 0, 16);  slice_23670 = add_719 = None
        slice_scatter_4303: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4300, slice_scatter_4302, 1, 5728, 5744);  slice_scatter_4300 = slice_scatter_4302 = None
        slice_23675: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4303, 1, 5728, 5744)
        slice_23676: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23675, 2, 0, 16)
        slice_scatter_4305: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23675, slice_23676, 2, 0, 16);  slice_23675 = slice_23676 = None
        slice_scatter_4306: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4303, slice_scatter_4305, 1, 5728, 5744);  slice_scatter_4303 = slice_scatter_4305 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23695: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5744, 5760)
        slice_23696: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_23695, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_721: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_23696, memory_format = torch.contiguous_format);  slice_23696 = None
        view_1446: "f32[32, 16]" = torch.ops.aten.view.default(clone_721, [32, 16]);  clone_721 = None
        mm_718: "f32[32, 8]" = torch.ops.aten.mm.default(view_1446, slice_7)
        view_1447: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_718, [2, 16, 8]);  mm_718 = None
        slice_23703: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4306, 1, 5744, 5760)
        slice_23704: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23703, 2, 0, 16)
        add_720: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23704, view_1447);  slice_23704 = view_1447 = None
        slice_scatter_4308: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23703, add_720, 2, 0, 16);  slice_23703 = add_720 = None
        slice_scatter_4309: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4306, slice_scatter_4308, 1, 5744, 5760);  slice_scatter_4306 = slice_scatter_4308 = None
        slice_23708: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4309, 1, 5744, 5760)
        slice_23709: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23708, 2, 0, 16)
        slice_scatter_4311: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23708, slice_23709, 2, 0, 16);  slice_23708 = slice_23709 = None
        slice_scatter_4312: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4309, slice_scatter_4311, 1, 5744, 5760);  slice_scatter_4309 = slice_scatter_4311 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23729: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_23695, 2, 16, 32);  slice_23695 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_722: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_23729, memory_format = torch.contiguous_format);  slice_23729 = None
        view_1448: "f32[32, 11]" = torch.ops.aten.view.default(clone_722, [32, 11]);  clone_722 = None
        mm_719: "f32[32, 8]" = torch.ops.aten.mm.default(view_1448, slice_37)
        view_1449: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_719, [2, 16, 8]);  mm_719 = None
        slice_23736: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4312, 1, 5744, 5760)
        slice_23737: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23736, 2, 0, 16)
        add_721: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23737, view_1449);  slice_23737 = view_1449 = None
        slice_scatter_4314: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23736, add_721, 2, 0, 16);  slice_23736 = add_721 = None
        slice_scatter_4315: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4312, slice_scatter_4314, 1, 5744, 5760);  slice_scatter_4312 = slice_scatter_4314 = None
        slice_23741: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4315, 1, 5744, 5760)
        slice_23742: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23741, 2, 0, 16)
        slice_scatter_4317: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23741, slice_23742, 2, 0, 16);  slice_23741 = slice_23742 = None
        slice_scatter_4318: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4315, slice_scatter_4317, 1, 5744, 5760);  slice_scatter_4315 = slice_scatter_4317 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23761: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5760, 5776)
        slice_23762: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_23761, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_723: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_23762, memory_format = torch.contiguous_format);  slice_23762 = None
        view_1450: "f32[32, 16]" = torch.ops.aten.view.default(clone_723, [32, 16]);  clone_723 = None
        mm_720: "f32[32, 8]" = torch.ops.aten.mm.default(view_1450, slice_7)
        view_1451: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_720, [2, 16, 8]);  mm_720 = None
        slice_23769: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4318, 1, 5760, 5776)
        slice_23770: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23769, 2, 0, 16)
        add_722: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23770, view_1451);  slice_23770 = view_1451 = None
        slice_scatter_4320: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23769, add_722, 2, 0, 16);  slice_23769 = add_722 = None
        slice_scatter_4321: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4318, slice_scatter_4320, 1, 5760, 5776);  slice_scatter_4318 = slice_scatter_4320 = None
        slice_23774: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4321, 1, 5760, 5776)
        slice_23775: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23774, 2, 0, 16)
        slice_scatter_4323: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23774, slice_23775, 2, 0, 16);  slice_23774 = slice_23775 = None
        slice_scatter_4324: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4321, slice_scatter_4323, 1, 5760, 5776);  slice_scatter_4321 = slice_scatter_4323 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23795: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_23761, 2, 16, 32);  slice_23761 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_724: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_23795, memory_format = torch.contiguous_format);  slice_23795 = None
        view_1452: "f32[32, 11]" = torch.ops.aten.view.default(clone_724, [32, 11]);  clone_724 = None
        mm_721: "f32[32, 8]" = torch.ops.aten.mm.default(view_1452, slice_37)
        view_1453: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_721, [2, 16, 8]);  mm_721 = None
        slice_23802: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4324, 1, 5760, 5776)
        slice_23803: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23802, 2, 0, 16)
        add_723: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23803, view_1453);  slice_23803 = view_1453 = None
        slice_scatter_4326: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23802, add_723, 2, 0, 16);  slice_23802 = add_723 = None
        slice_scatter_4327: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4324, slice_scatter_4326, 1, 5760, 5776);  slice_scatter_4324 = slice_scatter_4326 = None
        slice_23807: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4327, 1, 5760, 5776)
        slice_23808: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23807, 2, 0, 16)
        slice_scatter_4329: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23807, slice_23808, 2, 0, 16);  slice_23807 = slice_23808 = None
        slice_scatter_4330: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4327, slice_scatter_4329, 1, 5760, 5776);  slice_scatter_4327 = slice_scatter_4329 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23827: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5776, 5792)
        slice_23828: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_23827, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_725: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_23828, memory_format = torch.contiguous_format);  slice_23828 = None
        view_1454: "f32[32, 16]" = torch.ops.aten.view.default(clone_725, [32, 16]);  clone_725 = None
        mm_722: "f32[32, 8]" = torch.ops.aten.mm.default(view_1454, slice_7)
        view_1455: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_722, [2, 16, 8]);  mm_722 = None
        slice_23835: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4330, 1, 5776, 5792)
        slice_23836: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23835, 2, 0, 16)
        add_724: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23836, view_1455);  slice_23836 = view_1455 = None
        slice_scatter_4332: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23835, add_724, 2, 0, 16);  slice_23835 = add_724 = None
        slice_scatter_4333: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4330, slice_scatter_4332, 1, 5776, 5792);  slice_scatter_4330 = slice_scatter_4332 = None
        slice_23840: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4333, 1, 5776, 5792)
        slice_23841: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23840, 2, 0, 16)
        slice_scatter_4335: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23840, slice_23841, 2, 0, 16);  slice_23840 = slice_23841 = None
        slice_scatter_4336: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4333, slice_scatter_4335, 1, 5776, 5792);  slice_scatter_4333 = slice_scatter_4335 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23861: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_23827, 2, 16, 32);  slice_23827 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_726: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_23861, memory_format = torch.contiguous_format);  slice_23861 = None
        view_1456: "f32[32, 11]" = torch.ops.aten.view.default(clone_726, [32, 11]);  clone_726 = None
        mm_723: "f32[32, 8]" = torch.ops.aten.mm.default(view_1456, slice_37)
        view_1457: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_723, [2, 16, 8]);  mm_723 = None
        slice_23868: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4336, 1, 5776, 5792)
        slice_23869: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23868, 2, 0, 16)
        add_725: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23869, view_1457);  slice_23869 = view_1457 = None
        slice_scatter_4338: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23868, add_725, 2, 0, 16);  slice_23868 = add_725 = None
        slice_scatter_4339: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4336, slice_scatter_4338, 1, 5776, 5792);  slice_scatter_4336 = slice_scatter_4338 = None
        slice_23873: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4339, 1, 5776, 5792)
        slice_23874: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23873, 2, 0, 16)
        slice_scatter_4341: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23873, slice_23874, 2, 0, 16);  slice_23873 = slice_23874 = None
        slice_scatter_4342: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4339, slice_scatter_4341, 1, 5776, 5792);  slice_scatter_4339 = slice_scatter_4341 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23893: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5792, 5808)
        slice_23894: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_23893, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_727: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_23894, memory_format = torch.contiguous_format);  slice_23894 = None
        view_1458: "f32[32, 16]" = torch.ops.aten.view.default(clone_727, [32, 16]);  clone_727 = None
        mm_724: "f32[32, 8]" = torch.ops.aten.mm.default(view_1458, slice_7)
        view_1459: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_724, [2, 16, 8]);  mm_724 = None
        slice_23901: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4342, 1, 5792, 5808)
        slice_23902: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23901, 2, 0, 16)
        add_726: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23902, view_1459);  slice_23902 = view_1459 = None
        slice_scatter_4344: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23901, add_726, 2, 0, 16);  slice_23901 = add_726 = None
        slice_scatter_4345: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4342, slice_scatter_4344, 1, 5792, 5808);  slice_scatter_4342 = slice_scatter_4344 = None
        slice_23906: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4345, 1, 5792, 5808)
        slice_23907: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23906, 2, 0, 16)
        slice_scatter_4347: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23906, slice_23907, 2, 0, 16);  slice_23906 = slice_23907 = None
        slice_scatter_4348: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4345, slice_scatter_4347, 1, 5792, 5808);  slice_scatter_4345 = slice_scatter_4347 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23927: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_23893, 2, 16, 32);  slice_23893 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_728: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_23927, memory_format = torch.contiguous_format);  slice_23927 = None
        view_1460: "f32[32, 11]" = torch.ops.aten.view.default(clone_728, [32, 11]);  clone_728 = None
        mm_725: "f32[32, 8]" = torch.ops.aten.mm.default(view_1460, slice_37)
        view_1461: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_725, [2, 16, 8]);  mm_725 = None
        slice_23934: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4348, 1, 5792, 5808)
        slice_23935: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23934, 2, 0, 16)
        add_727: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23935, view_1461);  slice_23935 = view_1461 = None
        slice_scatter_4350: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23934, add_727, 2, 0, 16);  slice_23934 = add_727 = None
        slice_scatter_4351: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4348, slice_scatter_4350, 1, 5792, 5808);  slice_scatter_4348 = slice_scatter_4350 = None
        slice_23939: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4351, 1, 5792, 5808)
        slice_23940: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23939, 2, 0, 16)
        slice_scatter_4353: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23939, slice_23940, 2, 0, 16);  slice_23939 = slice_23940 = None
        slice_scatter_4354: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4351, slice_scatter_4353, 1, 5792, 5808);  slice_scatter_4351 = slice_scatter_4353 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23959: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5808, 5824)
        slice_23960: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_23959, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_729: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_23960, memory_format = torch.contiguous_format);  slice_23960 = None
        view_1462: "f32[32, 16]" = torch.ops.aten.view.default(clone_729, [32, 16]);  clone_729 = None
        mm_726: "f32[32, 8]" = torch.ops.aten.mm.default(view_1462, slice_7)
        view_1463: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_726, [2, 16, 8]);  mm_726 = None
        slice_23967: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4354, 1, 5808, 5824)
        slice_23968: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23967, 2, 0, 16)
        add_728: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_23968, view_1463);  slice_23968 = view_1463 = None
        slice_scatter_4356: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23967, add_728, 2, 0, 16);  slice_23967 = add_728 = None
        slice_scatter_4357: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4354, slice_scatter_4356, 1, 5808, 5824);  slice_scatter_4354 = slice_scatter_4356 = None
        slice_23972: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4357, 1, 5808, 5824)
        slice_23973: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_23972, 2, 0, 16)
        slice_scatter_4359: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_23972, slice_23973, 2, 0, 16);  slice_23972 = slice_23973 = None
        slice_scatter_4360: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4357, slice_scatter_4359, 1, 5808, 5824);  slice_scatter_4357 = slice_scatter_4359 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_23993: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_23959, 2, 16, 32);  slice_23959 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_730: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_23993, memory_format = torch.contiguous_format);  slice_23993 = None
        view_1464: "f32[32, 11]" = torch.ops.aten.view.default(clone_730, [32, 11]);  clone_730 = None
        mm_727: "f32[32, 8]" = torch.ops.aten.mm.default(view_1464, slice_37)
        view_1465: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_727, [2, 16, 8]);  mm_727 = None
        slice_24000: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4360, 1, 5808, 5824)
        slice_24001: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24000, 2, 0, 16)
        add_729: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24001, view_1465);  slice_24001 = view_1465 = None
        slice_scatter_4362: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24000, add_729, 2, 0, 16);  slice_24000 = add_729 = None
        slice_scatter_4363: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4360, slice_scatter_4362, 1, 5808, 5824);  slice_scatter_4360 = slice_scatter_4362 = None
        slice_24005: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4363, 1, 5808, 5824)
        slice_24006: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24005, 2, 0, 16)
        slice_scatter_4365: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24005, slice_24006, 2, 0, 16);  slice_24005 = slice_24006 = None
        slice_scatter_4366: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4363, slice_scatter_4365, 1, 5808, 5824);  slice_scatter_4363 = slice_scatter_4365 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24025: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5824, 5840)
        slice_24026: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_24025, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_731: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_24026, memory_format = torch.contiguous_format);  slice_24026 = None
        view_1466: "f32[32, 16]" = torch.ops.aten.view.default(clone_731, [32, 16]);  clone_731 = None
        mm_728: "f32[32, 8]" = torch.ops.aten.mm.default(view_1466, slice_7)
        view_1467: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_728, [2, 16, 8]);  mm_728 = None
        slice_24033: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4366, 1, 5824, 5840)
        slice_24034: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24033, 2, 0, 16)
        add_730: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24034, view_1467);  slice_24034 = view_1467 = None
        slice_scatter_4368: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24033, add_730, 2, 0, 16);  slice_24033 = add_730 = None
        slice_scatter_4369: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4366, slice_scatter_4368, 1, 5824, 5840);  slice_scatter_4366 = slice_scatter_4368 = None
        slice_24038: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4369, 1, 5824, 5840)
        slice_24039: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24038, 2, 0, 16)
        slice_scatter_4371: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24038, slice_24039, 2, 0, 16);  slice_24038 = slice_24039 = None
        slice_scatter_4372: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4369, slice_scatter_4371, 1, 5824, 5840);  slice_scatter_4369 = slice_scatter_4371 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24059: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_24025, 2, 16, 32);  slice_24025 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_732: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_24059, memory_format = torch.contiguous_format);  slice_24059 = None
        view_1468: "f32[32, 11]" = torch.ops.aten.view.default(clone_732, [32, 11]);  clone_732 = None
        mm_729: "f32[32, 8]" = torch.ops.aten.mm.default(view_1468, slice_37)
        view_1469: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_729, [2, 16, 8]);  mm_729 = None
        slice_24066: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4372, 1, 5824, 5840)
        slice_24067: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24066, 2, 0, 16)
        add_731: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24067, view_1469);  slice_24067 = view_1469 = None
        slice_scatter_4374: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24066, add_731, 2, 0, 16);  slice_24066 = add_731 = None
        slice_scatter_4375: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4372, slice_scatter_4374, 1, 5824, 5840);  slice_scatter_4372 = slice_scatter_4374 = None
        slice_24071: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4375, 1, 5824, 5840)
        slice_24072: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24071, 2, 0, 16)
        slice_scatter_4377: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24071, slice_24072, 2, 0, 16);  slice_24071 = slice_24072 = None
        slice_scatter_4378: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4375, slice_scatter_4377, 1, 5824, 5840);  slice_scatter_4375 = slice_scatter_4377 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24091: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5840, 5856)
        slice_24092: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_24091, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_733: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_24092, memory_format = torch.contiguous_format);  slice_24092 = None
        view_1470: "f32[32, 16]" = torch.ops.aten.view.default(clone_733, [32, 16]);  clone_733 = None
        mm_730: "f32[32, 8]" = torch.ops.aten.mm.default(view_1470, slice_7)
        view_1471: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_730, [2, 16, 8]);  mm_730 = None
        slice_24099: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4378, 1, 5840, 5856)
        slice_24100: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24099, 2, 0, 16)
        add_732: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24100, view_1471);  slice_24100 = view_1471 = None
        slice_scatter_4380: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24099, add_732, 2, 0, 16);  slice_24099 = add_732 = None
        slice_scatter_4381: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4378, slice_scatter_4380, 1, 5840, 5856);  slice_scatter_4378 = slice_scatter_4380 = None
        slice_24104: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4381, 1, 5840, 5856)
        slice_24105: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24104, 2, 0, 16)
        slice_scatter_4383: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24104, slice_24105, 2, 0, 16);  slice_24104 = slice_24105 = None
        slice_scatter_4384: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4381, slice_scatter_4383, 1, 5840, 5856);  slice_scatter_4381 = slice_scatter_4383 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24125: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_24091, 2, 16, 32);  slice_24091 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_734: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_24125, memory_format = torch.contiguous_format);  slice_24125 = None
        view_1472: "f32[32, 11]" = torch.ops.aten.view.default(clone_734, [32, 11]);  clone_734 = None
        mm_731: "f32[32, 8]" = torch.ops.aten.mm.default(view_1472, slice_37)
        view_1473: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_731, [2, 16, 8]);  mm_731 = None
        slice_24132: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4384, 1, 5840, 5856)
        slice_24133: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24132, 2, 0, 16)
        add_733: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24133, view_1473);  slice_24133 = view_1473 = None
        slice_scatter_4386: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24132, add_733, 2, 0, 16);  slice_24132 = add_733 = None
        slice_scatter_4387: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4384, slice_scatter_4386, 1, 5840, 5856);  slice_scatter_4384 = slice_scatter_4386 = None
        slice_24137: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4387, 1, 5840, 5856)
        slice_24138: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24137, 2, 0, 16)
        slice_scatter_4389: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24137, slice_24138, 2, 0, 16);  slice_24137 = slice_24138 = None
        slice_scatter_4390: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4387, slice_scatter_4389, 1, 5840, 5856);  slice_scatter_4387 = slice_scatter_4389 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24157: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5856, 5872)
        slice_24158: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_24157, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_735: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_24158, memory_format = torch.contiguous_format);  slice_24158 = None
        view_1474: "f32[32, 16]" = torch.ops.aten.view.default(clone_735, [32, 16]);  clone_735 = None
        mm_732: "f32[32, 8]" = torch.ops.aten.mm.default(view_1474, slice_7)
        view_1475: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_732, [2, 16, 8]);  mm_732 = None
        slice_24165: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4390, 1, 5856, 5872)
        slice_24166: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24165, 2, 0, 16)
        add_734: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24166, view_1475);  slice_24166 = view_1475 = None
        slice_scatter_4392: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24165, add_734, 2, 0, 16);  slice_24165 = add_734 = None
        slice_scatter_4393: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4390, slice_scatter_4392, 1, 5856, 5872);  slice_scatter_4390 = slice_scatter_4392 = None
        slice_24170: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4393, 1, 5856, 5872)
        slice_24171: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24170, 2, 0, 16)
        slice_scatter_4395: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24170, slice_24171, 2, 0, 16);  slice_24170 = slice_24171 = None
        slice_scatter_4396: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4393, slice_scatter_4395, 1, 5856, 5872);  slice_scatter_4393 = slice_scatter_4395 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24191: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_24157, 2, 16, 32);  slice_24157 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_736: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_24191, memory_format = torch.contiguous_format);  slice_24191 = None
        view_1476: "f32[32, 11]" = torch.ops.aten.view.default(clone_736, [32, 11]);  clone_736 = None
        mm_733: "f32[32, 8]" = torch.ops.aten.mm.default(view_1476, slice_37)
        view_1477: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_733, [2, 16, 8]);  mm_733 = None
        slice_24198: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4396, 1, 5856, 5872)
        slice_24199: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24198, 2, 0, 16)
        add_735: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24199, view_1477);  slice_24199 = view_1477 = None
        slice_scatter_4398: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24198, add_735, 2, 0, 16);  slice_24198 = add_735 = None
        slice_scatter_4399: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4396, slice_scatter_4398, 1, 5856, 5872);  slice_scatter_4396 = slice_scatter_4398 = None
        slice_24203: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4399, 1, 5856, 5872)
        slice_24204: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24203, 2, 0, 16)
        slice_scatter_4401: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24203, slice_24204, 2, 0, 16);  slice_24203 = slice_24204 = None
        slice_scatter_4402: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4399, slice_scatter_4401, 1, 5856, 5872);  slice_scatter_4399 = slice_scatter_4401 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24223: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5872, 5888)
        slice_24224: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_24223, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_737: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_24224, memory_format = torch.contiguous_format);  slice_24224 = None
        view_1478: "f32[32, 16]" = torch.ops.aten.view.default(clone_737, [32, 16]);  clone_737 = None
        mm_734: "f32[32, 8]" = torch.ops.aten.mm.default(view_1478, slice_7)
        view_1479: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_734, [2, 16, 8]);  mm_734 = None
        slice_24231: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4402, 1, 5872, 5888)
        slice_24232: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24231, 2, 0, 16)
        add_736: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24232, view_1479);  slice_24232 = view_1479 = None
        slice_scatter_4404: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24231, add_736, 2, 0, 16);  slice_24231 = add_736 = None
        slice_scatter_4405: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4402, slice_scatter_4404, 1, 5872, 5888);  slice_scatter_4402 = slice_scatter_4404 = None
        slice_24236: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4405, 1, 5872, 5888)
        slice_24237: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24236, 2, 0, 16)
        slice_scatter_4407: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24236, slice_24237, 2, 0, 16);  slice_24236 = slice_24237 = None
        slice_scatter_4408: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4405, slice_scatter_4407, 1, 5872, 5888);  slice_scatter_4405 = slice_scatter_4407 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24257: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_24223, 2, 16, 32);  slice_24223 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_738: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_24257, memory_format = torch.contiguous_format);  slice_24257 = None
        view_1480: "f32[32, 11]" = torch.ops.aten.view.default(clone_738, [32, 11]);  clone_738 = None
        mm_735: "f32[32, 8]" = torch.ops.aten.mm.default(view_1480, slice_37)
        view_1481: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_735, [2, 16, 8]);  mm_735 = None
        slice_24264: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4408, 1, 5872, 5888)
        slice_24265: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24264, 2, 0, 16)
        add_737: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24265, view_1481);  slice_24265 = view_1481 = None
        slice_scatter_4410: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24264, add_737, 2, 0, 16);  slice_24264 = add_737 = None
        slice_scatter_4411: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4408, slice_scatter_4410, 1, 5872, 5888);  slice_scatter_4408 = slice_scatter_4410 = None
        slice_24269: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4411, 1, 5872, 5888)
        slice_24270: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24269, 2, 0, 16)
        slice_scatter_4413: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24269, slice_24270, 2, 0, 16);  slice_24269 = slice_24270 = None
        slice_scatter_4414: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4411, slice_scatter_4413, 1, 5872, 5888);  slice_scatter_4411 = slice_scatter_4413 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24289: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5888, 5904)
        slice_24290: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_24289, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_739: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_24290, memory_format = torch.contiguous_format);  slice_24290 = None
        view_1482: "f32[32, 16]" = torch.ops.aten.view.default(clone_739, [32, 16]);  clone_739 = None
        mm_736: "f32[32, 8]" = torch.ops.aten.mm.default(view_1482, slice_7)
        view_1483: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_736, [2, 16, 8]);  mm_736 = None
        slice_24297: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4414, 1, 5888, 5904)
        slice_24298: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24297, 2, 0, 16)
        add_738: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24298, view_1483);  slice_24298 = view_1483 = None
        slice_scatter_4416: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24297, add_738, 2, 0, 16);  slice_24297 = add_738 = None
        slice_scatter_4417: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4414, slice_scatter_4416, 1, 5888, 5904);  slice_scatter_4414 = slice_scatter_4416 = None
        slice_24302: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4417, 1, 5888, 5904)
        slice_24303: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24302, 2, 0, 16)
        slice_scatter_4419: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24302, slice_24303, 2, 0, 16);  slice_24302 = slice_24303 = None
        slice_scatter_4420: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4417, slice_scatter_4419, 1, 5888, 5904);  slice_scatter_4417 = slice_scatter_4419 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24323: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_24289, 2, 16, 32);  slice_24289 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_740: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_24323, memory_format = torch.contiguous_format);  slice_24323 = None
        view_1484: "f32[32, 11]" = torch.ops.aten.view.default(clone_740, [32, 11]);  clone_740 = None
        mm_737: "f32[32, 8]" = torch.ops.aten.mm.default(view_1484, slice_37)
        view_1485: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_737, [2, 16, 8]);  mm_737 = None
        slice_24330: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4420, 1, 5888, 5904)
        slice_24331: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24330, 2, 0, 16)
        add_739: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24331, view_1485);  slice_24331 = view_1485 = None
        slice_scatter_4422: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24330, add_739, 2, 0, 16);  slice_24330 = add_739 = None
        slice_scatter_4423: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4420, slice_scatter_4422, 1, 5888, 5904);  slice_scatter_4420 = slice_scatter_4422 = None
        slice_24335: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4423, 1, 5888, 5904)
        slice_24336: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24335, 2, 0, 16)
        slice_scatter_4425: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24335, slice_24336, 2, 0, 16);  slice_24335 = slice_24336 = None
        slice_scatter_4426: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4423, slice_scatter_4425, 1, 5888, 5904);  slice_scatter_4423 = slice_scatter_4425 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24355: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5904, 5920)
        slice_24356: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_24355, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_741: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_24356, memory_format = torch.contiguous_format);  slice_24356 = None
        view_1486: "f32[32, 16]" = torch.ops.aten.view.default(clone_741, [32, 16]);  clone_741 = None
        mm_738: "f32[32, 8]" = torch.ops.aten.mm.default(view_1486, slice_7)
        view_1487: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_738, [2, 16, 8]);  mm_738 = None
        slice_24363: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4426, 1, 5904, 5920)
        slice_24364: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24363, 2, 0, 16)
        add_740: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24364, view_1487);  slice_24364 = view_1487 = None
        slice_scatter_4428: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24363, add_740, 2, 0, 16);  slice_24363 = add_740 = None
        slice_scatter_4429: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4426, slice_scatter_4428, 1, 5904, 5920);  slice_scatter_4426 = slice_scatter_4428 = None
        slice_24368: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4429, 1, 5904, 5920)
        slice_24369: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24368, 2, 0, 16)
        slice_scatter_4431: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24368, slice_24369, 2, 0, 16);  slice_24368 = slice_24369 = None
        slice_scatter_4432: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4429, slice_scatter_4431, 1, 5904, 5920);  slice_scatter_4429 = slice_scatter_4431 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24389: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_24355, 2, 16, 32);  slice_24355 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_742: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_24389, memory_format = torch.contiguous_format);  slice_24389 = None
        view_1488: "f32[32, 11]" = torch.ops.aten.view.default(clone_742, [32, 11]);  clone_742 = None
        mm_739: "f32[32, 8]" = torch.ops.aten.mm.default(view_1488, slice_37)
        view_1489: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_739, [2, 16, 8]);  mm_739 = None
        slice_24396: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4432, 1, 5904, 5920)
        slice_24397: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24396, 2, 0, 16)
        add_741: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24397, view_1489);  slice_24397 = view_1489 = None
        slice_scatter_4434: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24396, add_741, 2, 0, 16);  slice_24396 = add_741 = None
        slice_scatter_4435: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4432, slice_scatter_4434, 1, 5904, 5920);  slice_scatter_4432 = slice_scatter_4434 = None
        slice_24401: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4435, 1, 5904, 5920)
        slice_24402: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24401, 2, 0, 16)
        slice_scatter_4437: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24401, slice_24402, 2, 0, 16);  slice_24401 = slice_24402 = None
        slice_scatter_4438: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4435, slice_scatter_4437, 1, 5904, 5920);  slice_scatter_4435 = slice_scatter_4437 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24421: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5920, 5936)
        slice_24422: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_24421, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_743: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_24422, memory_format = torch.contiguous_format);  slice_24422 = None
        view_1490: "f32[32, 16]" = torch.ops.aten.view.default(clone_743, [32, 16]);  clone_743 = None
        mm_740: "f32[32, 8]" = torch.ops.aten.mm.default(view_1490, slice_7)
        view_1491: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_740, [2, 16, 8]);  mm_740 = None
        slice_24429: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4438, 1, 5920, 5936)
        slice_24430: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24429, 2, 0, 16)
        add_742: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24430, view_1491);  slice_24430 = view_1491 = None
        slice_scatter_4440: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24429, add_742, 2, 0, 16);  slice_24429 = add_742 = None
        slice_scatter_4441: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4438, slice_scatter_4440, 1, 5920, 5936);  slice_scatter_4438 = slice_scatter_4440 = None
        slice_24434: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4441, 1, 5920, 5936)
        slice_24435: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24434, 2, 0, 16)
        slice_scatter_4443: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24434, slice_24435, 2, 0, 16);  slice_24434 = slice_24435 = None
        slice_scatter_4444: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4441, slice_scatter_4443, 1, 5920, 5936);  slice_scatter_4441 = slice_scatter_4443 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24455: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_24421, 2, 16, 32);  slice_24421 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_744: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_24455, memory_format = torch.contiguous_format);  slice_24455 = None
        view_1492: "f32[32, 11]" = torch.ops.aten.view.default(clone_744, [32, 11]);  clone_744 = None
        mm_741: "f32[32, 8]" = torch.ops.aten.mm.default(view_1492, slice_37)
        view_1493: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_741, [2, 16, 8]);  mm_741 = None
        slice_24462: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4444, 1, 5920, 5936)
        slice_24463: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24462, 2, 0, 16)
        add_743: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24463, view_1493);  slice_24463 = view_1493 = None
        slice_scatter_4446: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24462, add_743, 2, 0, 16);  slice_24462 = add_743 = None
        slice_scatter_4447: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4444, slice_scatter_4446, 1, 5920, 5936);  slice_scatter_4444 = slice_scatter_4446 = None
        slice_24467: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4447, 1, 5920, 5936)
        slice_24468: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24467, 2, 0, 16)
        slice_scatter_4449: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24467, slice_24468, 2, 0, 16);  slice_24467 = slice_24468 = None
        slice_scatter_4450: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4447, slice_scatter_4449, 1, 5920, 5936);  slice_scatter_4447 = slice_scatter_4449 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24487: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5936, 5952)
        slice_24488: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_24487, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_745: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_24488, memory_format = torch.contiguous_format);  slice_24488 = None
        view_1494: "f32[32, 16]" = torch.ops.aten.view.default(clone_745, [32, 16]);  clone_745 = None
        mm_742: "f32[32, 8]" = torch.ops.aten.mm.default(view_1494, slice_7)
        view_1495: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_742, [2, 16, 8]);  mm_742 = None
        slice_24495: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4450, 1, 5936, 5952)
        slice_24496: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24495, 2, 0, 16)
        add_744: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24496, view_1495);  slice_24496 = view_1495 = None
        slice_scatter_4452: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24495, add_744, 2, 0, 16);  slice_24495 = add_744 = None
        slice_scatter_4453: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4450, slice_scatter_4452, 1, 5936, 5952);  slice_scatter_4450 = slice_scatter_4452 = None
        slice_24500: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4453, 1, 5936, 5952)
        slice_24501: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24500, 2, 0, 16)
        slice_scatter_4455: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24500, slice_24501, 2, 0, 16);  slice_24500 = slice_24501 = None
        slice_scatter_4456: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4453, slice_scatter_4455, 1, 5936, 5952);  slice_scatter_4453 = slice_scatter_4455 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24521: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_24487, 2, 16, 32);  slice_24487 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_746: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_24521, memory_format = torch.contiguous_format);  slice_24521 = None
        view_1496: "f32[32, 11]" = torch.ops.aten.view.default(clone_746, [32, 11]);  clone_746 = None
        mm_743: "f32[32, 8]" = torch.ops.aten.mm.default(view_1496, slice_37)
        view_1497: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_743, [2, 16, 8]);  mm_743 = None
        slice_24528: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4456, 1, 5936, 5952)
        slice_24529: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24528, 2, 0, 16)
        add_745: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24529, view_1497);  slice_24529 = view_1497 = None
        slice_scatter_4458: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24528, add_745, 2, 0, 16);  slice_24528 = add_745 = None
        slice_scatter_4459: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4456, slice_scatter_4458, 1, 5936, 5952);  slice_scatter_4456 = slice_scatter_4458 = None
        slice_24533: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4459, 1, 5936, 5952)
        slice_24534: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24533, 2, 0, 16)
        slice_scatter_4461: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24533, slice_24534, 2, 0, 16);  slice_24533 = slice_24534 = None
        slice_scatter_4462: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4459, slice_scatter_4461, 1, 5936, 5952);  slice_scatter_4459 = slice_scatter_4461 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24553: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5952, 5968)
        slice_24554: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_24553, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_747: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_24554, memory_format = torch.contiguous_format);  slice_24554 = None
        view_1498: "f32[32, 16]" = torch.ops.aten.view.default(clone_747, [32, 16]);  clone_747 = None
        mm_744: "f32[32, 8]" = torch.ops.aten.mm.default(view_1498, slice_7)
        view_1499: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_744, [2, 16, 8]);  mm_744 = None
        slice_24561: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4462, 1, 5952, 5968)
        slice_24562: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24561, 2, 0, 16)
        add_746: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24562, view_1499);  slice_24562 = view_1499 = None
        slice_scatter_4464: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24561, add_746, 2, 0, 16);  slice_24561 = add_746 = None
        slice_scatter_4465: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4462, slice_scatter_4464, 1, 5952, 5968);  slice_scatter_4462 = slice_scatter_4464 = None
        slice_24566: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4465, 1, 5952, 5968)
        slice_24567: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24566, 2, 0, 16)
        slice_scatter_4467: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24566, slice_24567, 2, 0, 16);  slice_24566 = slice_24567 = None
        slice_scatter_4468: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4465, slice_scatter_4467, 1, 5952, 5968);  slice_scatter_4465 = slice_scatter_4467 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24587: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_24553, 2, 16, 32);  slice_24553 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_748: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_24587, memory_format = torch.contiguous_format);  slice_24587 = None
        view_1500: "f32[32, 11]" = torch.ops.aten.view.default(clone_748, [32, 11]);  clone_748 = None
        mm_745: "f32[32, 8]" = torch.ops.aten.mm.default(view_1500, slice_37)
        view_1501: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_745, [2, 16, 8]);  mm_745 = None
        slice_24594: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4468, 1, 5952, 5968)
        slice_24595: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24594, 2, 0, 16)
        add_747: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24595, view_1501);  slice_24595 = view_1501 = None
        slice_scatter_4470: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24594, add_747, 2, 0, 16);  slice_24594 = add_747 = None
        slice_scatter_4471: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4468, slice_scatter_4470, 1, 5952, 5968);  slice_scatter_4468 = slice_scatter_4470 = None
        slice_24599: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4471, 1, 5952, 5968)
        slice_24600: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24599, 2, 0, 16)
        slice_scatter_4473: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24599, slice_24600, 2, 0, 16);  slice_24599 = slice_24600 = None
        slice_scatter_4474: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4471, slice_scatter_4473, 1, 5952, 5968);  slice_scatter_4471 = slice_scatter_4473 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24619: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5968, 5984)
        slice_24620: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_24619, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_749: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_24620, memory_format = torch.contiguous_format);  slice_24620 = None
        view_1502: "f32[32, 16]" = torch.ops.aten.view.default(clone_749, [32, 16]);  clone_749 = None
        mm_746: "f32[32, 8]" = torch.ops.aten.mm.default(view_1502, slice_7)
        view_1503: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_746, [2, 16, 8]);  mm_746 = None
        slice_24627: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4474, 1, 5968, 5984)
        slice_24628: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24627, 2, 0, 16)
        add_748: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24628, view_1503);  slice_24628 = view_1503 = None
        slice_scatter_4476: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24627, add_748, 2, 0, 16);  slice_24627 = add_748 = None
        slice_scatter_4477: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4474, slice_scatter_4476, 1, 5968, 5984);  slice_scatter_4474 = slice_scatter_4476 = None
        slice_24632: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4477, 1, 5968, 5984)
        slice_24633: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24632, 2, 0, 16)
        slice_scatter_4479: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24632, slice_24633, 2, 0, 16);  slice_24632 = slice_24633 = None
        slice_scatter_4480: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4477, slice_scatter_4479, 1, 5968, 5984);  slice_scatter_4477 = slice_scatter_4479 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24653: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_24619, 2, 16, 32);  slice_24619 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_750: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_24653, memory_format = torch.contiguous_format);  slice_24653 = None
        view_1504: "f32[32, 11]" = torch.ops.aten.view.default(clone_750, [32, 11]);  clone_750 = None
        mm_747: "f32[32, 8]" = torch.ops.aten.mm.default(view_1504, slice_37)
        view_1505: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_747, [2, 16, 8]);  mm_747 = None
        slice_24660: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4480, 1, 5968, 5984)
        slice_24661: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24660, 2, 0, 16)
        add_749: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24661, view_1505);  slice_24661 = view_1505 = None
        slice_scatter_4482: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24660, add_749, 2, 0, 16);  slice_24660 = add_749 = None
        slice_scatter_4483: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4480, slice_scatter_4482, 1, 5968, 5984);  slice_scatter_4480 = slice_scatter_4482 = None
        slice_24665: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4483, 1, 5968, 5984)
        slice_24666: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24665, 2, 0, 16)
        slice_scatter_4485: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24665, slice_24666, 2, 0, 16);  slice_24665 = slice_24666 = None
        slice_scatter_4486: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4483, slice_scatter_4485, 1, 5968, 5984);  slice_scatter_4483 = slice_scatter_4485 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24685: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 5984, 6000)
        slice_24686: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_24685, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_751: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_24686, memory_format = torch.contiguous_format);  slice_24686 = None
        view_1506: "f32[32, 16]" = torch.ops.aten.view.default(clone_751, [32, 16]);  clone_751 = None
        mm_748: "f32[32, 8]" = torch.ops.aten.mm.default(view_1506, slice_7)
        view_1507: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_748, [2, 16, 8]);  mm_748 = None
        slice_24693: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4486, 1, 5984, 6000)
        slice_24694: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24693, 2, 0, 16)
        add_750: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24694, view_1507);  slice_24694 = view_1507 = None
        slice_scatter_4488: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24693, add_750, 2, 0, 16);  slice_24693 = add_750 = None
        slice_scatter_4489: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4486, slice_scatter_4488, 1, 5984, 6000);  slice_scatter_4486 = slice_scatter_4488 = None
        slice_24698: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4489, 1, 5984, 6000)
        slice_24699: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24698, 2, 0, 16)
        slice_scatter_4491: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24698, slice_24699, 2, 0, 16);  slice_24698 = slice_24699 = None
        slice_scatter_4492: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4489, slice_scatter_4491, 1, 5984, 6000);  slice_scatter_4489 = slice_scatter_4491 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24719: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_24685, 2, 16, 32);  slice_24685 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_752: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_24719, memory_format = torch.contiguous_format);  slice_24719 = None
        view_1508: "f32[32, 11]" = torch.ops.aten.view.default(clone_752, [32, 11]);  clone_752 = None
        mm_749: "f32[32, 8]" = torch.ops.aten.mm.default(view_1508, slice_37)
        view_1509: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_749, [2, 16, 8]);  mm_749 = None
        slice_24726: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4492, 1, 5984, 6000)
        slice_24727: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24726, 2, 0, 16)
        add_751: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24727, view_1509);  slice_24727 = view_1509 = None
        slice_scatter_4494: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24726, add_751, 2, 0, 16);  slice_24726 = add_751 = None
        slice_scatter_4495: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4492, slice_scatter_4494, 1, 5984, 6000);  slice_scatter_4492 = slice_scatter_4494 = None
        slice_24731: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4495, 1, 5984, 6000)
        slice_24732: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24731, 2, 0, 16)
        slice_scatter_4497: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24731, slice_24732, 2, 0, 16);  slice_24731 = slice_24732 = None
        slice_scatter_4498: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4495, slice_scatter_4497, 1, 5984, 6000);  slice_scatter_4495 = slice_scatter_4497 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24751: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6000, 6016)
        slice_24752: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_24751, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_753: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_24752, memory_format = torch.contiguous_format);  slice_24752 = None
        view_1510: "f32[32, 16]" = torch.ops.aten.view.default(clone_753, [32, 16]);  clone_753 = None
        mm_750: "f32[32, 8]" = torch.ops.aten.mm.default(view_1510, slice_7)
        view_1511: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_750, [2, 16, 8]);  mm_750 = None
        slice_24759: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4498, 1, 6000, 6016)
        slice_24760: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24759, 2, 0, 16)
        add_752: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24760, view_1511);  slice_24760 = view_1511 = None
        slice_scatter_4500: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24759, add_752, 2, 0, 16);  slice_24759 = add_752 = None
        slice_scatter_4501: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4498, slice_scatter_4500, 1, 6000, 6016);  slice_scatter_4498 = slice_scatter_4500 = None
        slice_24764: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4501, 1, 6000, 6016)
        slice_24765: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24764, 2, 0, 16)
        slice_scatter_4503: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24764, slice_24765, 2, 0, 16);  slice_24764 = slice_24765 = None
        slice_scatter_4504: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4501, slice_scatter_4503, 1, 6000, 6016);  slice_scatter_4501 = slice_scatter_4503 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24785: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_24751, 2, 16, 32);  slice_24751 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_754: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_24785, memory_format = torch.contiguous_format);  slice_24785 = None
        view_1512: "f32[32, 11]" = torch.ops.aten.view.default(clone_754, [32, 11]);  clone_754 = None
        mm_751: "f32[32, 8]" = torch.ops.aten.mm.default(view_1512, slice_37)
        view_1513: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_751, [2, 16, 8]);  mm_751 = None
        slice_24792: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4504, 1, 6000, 6016)
        slice_24793: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24792, 2, 0, 16)
        add_753: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24793, view_1513);  slice_24793 = view_1513 = None
        slice_scatter_4506: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24792, add_753, 2, 0, 16);  slice_24792 = add_753 = None
        slice_scatter_4507: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4504, slice_scatter_4506, 1, 6000, 6016);  slice_scatter_4504 = slice_scatter_4506 = None
        slice_24797: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4507, 1, 6000, 6016)
        slice_24798: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24797, 2, 0, 16)
        slice_scatter_4509: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24797, slice_24798, 2, 0, 16);  slice_24797 = slice_24798 = None
        slice_scatter_4510: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4507, slice_scatter_4509, 1, 6000, 6016);  slice_scatter_4507 = slice_scatter_4509 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24817: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6016, 6032)
        slice_24818: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_24817, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_755: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_24818, memory_format = torch.contiguous_format);  slice_24818 = None
        view_1514: "f32[32, 16]" = torch.ops.aten.view.default(clone_755, [32, 16]);  clone_755 = None
        mm_752: "f32[32, 8]" = torch.ops.aten.mm.default(view_1514, slice_7)
        view_1515: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_752, [2, 16, 8]);  mm_752 = None
        slice_24825: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4510, 1, 6016, 6032)
        slice_24826: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24825, 2, 0, 16)
        add_754: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24826, view_1515);  slice_24826 = view_1515 = None
        slice_scatter_4512: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24825, add_754, 2, 0, 16);  slice_24825 = add_754 = None
        slice_scatter_4513: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4510, slice_scatter_4512, 1, 6016, 6032);  slice_scatter_4510 = slice_scatter_4512 = None
        slice_24830: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4513, 1, 6016, 6032)
        slice_24831: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24830, 2, 0, 16)
        slice_scatter_4515: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24830, slice_24831, 2, 0, 16);  slice_24830 = slice_24831 = None
        slice_scatter_4516: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4513, slice_scatter_4515, 1, 6016, 6032);  slice_scatter_4513 = slice_scatter_4515 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24851: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_24817, 2, 16, 32);  slice_24817 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_756: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_24851, memory_format = torch.contiguous_format);  slice_24851 = None
        view_1516: "f32[32, 11]" = torch.ops.aten.view.default(clone_756, [32, 11]);  clone_756 = None
        mm_753: "f32[32, 8]" = torch.ops.aten.mm.default(view_1516, slice_37)
        view_1517: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_753, [2, 16, 8]);  mm_753 = None
        slice_24858: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4516, 1, 6016, 6032)
        slice_24859: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24858, 2, 0, 16)
        add_755: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24859, view_1517);  slice_24859 = view_1517 = None
        slice_scatter_4518: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24858, add_755, 2, 0, 16);  slice_24858 = add_755 = None
        slice_scatter_4519: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4516, slice_scatter_4518, 1, 6016, 6032);  slice_scatter_4516 = slice_scatter_4518 = None
        slice_24863: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4519, 1, 6016, 6032)
        slice_24864: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24863, 2, 0, 16)
        slice_scatter_4521: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24863, slice_24864, 2, 0, 16);  slice_24863 = slice_24864 = None
        slice_scatter_4522: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4519, slice_scatter_4521, 1, 6016, 6032);  slice_scatter_4519 = slice_scatter_4521 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24883: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6032, 6048)
        slice_24884: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_24883, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_757: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_24884, memory_format = torch.contiguous_format);  slice_24884 = None
        view_1518: "f32[32, 16]" = torch.ops.aten.view.default(clone_757, [32, 16]);  clone_757 = None
        mm_754: "f32[32, 8]" = torch.ops.aten.mm.default(view_1518, slice_7)
        view_1519: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_754, [2, 16, 8]);  mm_754 = None
        slice_24891: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4522, 1, 6032, 6048)
        slice_24892: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24891, 2, 0, 16)
        add_756: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24892, view_1519);  slice_24892 = view_1519 = None
        slice_scatter_4524: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24891, add_756, 2, 0, 16);  slice_24891 = add_756 = None
        slice_scatter_4525: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4522, slice_scatter_4524, 1, 6032, 6048);  slice_scatter_4522 = slice_scatter_4524 = None
        slice_24896: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4525, 1, 6032, 6048)
        slice_24897: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24896, 2, 0, 16)
        slice_scatter_4527: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24896, slice_24897, 2, 0, 16);  slice_24896 = slice_24897 = None
        slice_scatter_4528: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4525, slice_scatter_4527, 1, 6032, 6048);  slice_scatter_4525 = slice_scatter_4527 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24917: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_24883, 2, 16, 32);  slice_24883 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_758: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_24917, memory_format = torch.contiguous_format);  slice_24917 = None
        view_1520: "f32[32, 11]" = torch.ops.aten.view.default(clone_758, [32, 11]);  clone_758 = None
        mm_755: "f32[32, 8]" = torch.ops.aten.mm.default(view_1520, slice_37)
        view_1521: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_755, [2, 16, 8]);  mm_755 = None
        slice_24924: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4528, 1, 6032, 6048)
        slice_24925: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24924, 2, 0, 16)
        add_757: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24925, view_1521);  slice_24925 = view_1521 = None
        slice_scatter_4530: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24924, add_757, 2, 0, 16);  slice_24924 = add_757 = None
        slice_scatter_4531: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4528, slice_scatter_4530, 1, 6032, 6048);  slice_scatter_4528 = slice_scatter_4530 = None
        slice_24929: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4531, 1, 6032, 6048)
        slice_24930: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24929, 2, 0, 16)
        slice_scatter_4533: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24929, slice_24930, 2, 0, 16);  slice_24929 = slice_24930 = None
        slice_scatter_4534: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4531, slice_scatter_4533, 1, 6032, 6048);  slice_scatter_4531 = slice_scatter_4533 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24949: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6048, 6064)
        slice_24950: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_24949, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_759: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_24950, memory_format = torch.contiguous_format);  slice_24950 = None
        view_1522: "f32[32, 16]" = torch.ops.aten.view.default(clone_759, [32, 16]);  clone_759 = None
        mm_756: "f32[32, 8]" = torch.ops.aten.mm.default(view_1522, slice_7)
        view_1523: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_756, [2, 16, 8]);  mm_756 = None
        slice_24957: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4534, 1, 6048, 6064)
        slice_24958: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24957, 2, 0, 16)
        add_758: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24958, view_1523);  slice_24958 = view_1523 = None
        slice_scatter_4536: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24957, add_758, 2, 0, 16);  slice_24957 = add_758 = None
        slice_scatter_4537: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4534, slice_scatter_4536, 1, 6048, 6064);  slice_scatter_4534 = slice_scatter_4536 = None
        slice_24962: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4537, 1, 6048, 6064)
        slice_24963: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24962, 2, 0, 16)
        slice_scatter_4539: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24962, slice_24963, 2, 0, 16);  slice_24962 = slice_24963 = None
        slice_scatter_4540: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4537, slice_scatter_4539, 1, 6048, 6064);  slice_scatter_4537 = slice_scatter_4539 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_24983: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_24949, 2, 16, 32);  slice_24949 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_760: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_24983, memory_format = torch.contiguous_format);  slice_24983 = None
        view_1524: "f32[32, 11]" = torch.ops.aten.view.default(clone_760, [32, 11]);  clone_760 = None
        mm_757: "f32[32, 8]" = torch.ops.aten.mm.default(view_1524, slice_37)
        view_1525: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_757, [2, 16, 8]);  mm_757 = None
        slice_24990: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4540, 1, 6048, 6064)
        slice_24991: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24990, 2, 0, 16)
        add_759: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_24991, view_1525);  slice_24991 = view_1525 = None
        slice_scatter_4542: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24990, add_759, 2, 0, 16);  slice_24990 = add_759 = None
        slice_scatter_4543: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4540, slice_scatter_4542, 1, 6048, 6064);  slice_scatter_4540 = slice_scatter_4542 = None
        slice_24995: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4543, 1, 6048, 6064)
        slice_24996: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_24995, 2, 0, 16)
        slice_scatter_4545: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_24995, slice_24996, 2, 0, 16);  slice_24995 = slice_24996 = None
        slice_scatter_4546: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4543, slice_scatter_4545, 1, 6048, 6064);  slice_scatter_4543 = slice_scatter_4545 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25015: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6064, 6080)
        slice_25016: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_25015, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_761: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_25016, memory_format = torch.contiguous_format);  slice_25016 = None
        view_1526: "f32[32, 16]" = torch.ops.aten.view.default(clone_761, [32, 16]);  clone_761 = None
        mm_758: "f32[32, 8]" = torch.ops.aten.mm.default(view_1526, slice_7)
        view_1527: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_758, [2, 16, 8]);  mm_758 = None
        slice_25023: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4546, 1, 6064, 6080)
        slice_25024: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25023, 2, 0, 16)
        add_760: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25024, view_1527);  slice_25024 = view_1527 = None
        slice_scatter_4548: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25023, add_760, 2, 0, 16);  slice_25023 = add_760 = None
        slice_scatter_4549: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4546, slice_scatter_4548, 1, 6064, 6080);  slice_scatter_4546 = slice_scatter_4548 = None
        slice_25028: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4549, 1, 6064, 6080)
        slice_25029: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25028, 2, 0, 16)
        slice_scatter_4551: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25028, slice_25029, 2, 0, 16);  slice_25028 = slice_25029 = None
        slice_scatter_4552: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4549, slice_scatter_4551, 1, 6064, 6080);  slice_scatter_4549 = slice_scatter_4551 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25049: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_25015, 2, 16, 32);  slice_25015 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_762: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_25049, memory_format = torch.contiguous_format);  slice_25049 = None
        view_1528: "f32[32, 11]" = torch.ops.aten.view.default(clone_762, [32, 11]);  clone_762 = None
        mm_759: "f32[32, 8]" = torch.ops.aten.mm.default(view_1528, slice_37)
        view_1529: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_759, [2, 16, 8]);  mm_759 = None
        slice_25056: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4552, 1, 6064, 6080)
        slice_25057: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25056, 2, 0, 16)
        add_761: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25057, view_1529);  slice_25057 = view_1529 = None
        slice_scatter_4554: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25056, add_761, 2, 0, 16);  slice_25056 = add_761 = None
        slice_scatter_4555: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4552, slice_scatter_4554, 1, 6064, 6080);  slice_scatter_4552 = slice_scatter_4554 = None
        slice_25061: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4555, 1, 6064, 6080)
        slice_25062: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25061, 2, 0, 16)
        slice_scatter_4557: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25061, slice_25062, 2, 0, 16);  slice_25061 = slice_25062 = None
        slice_scatter_4558: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4555, slice_scatter_4557, 1, 6064, 6080);  slice_scatter_4555 = slice_scatter_4557 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25081: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6080, 6096)
        slice_25082: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_25081, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_763: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_25082, memory_format = torch.contiguous_format);  slice_25082 = None
        view_1530: "f32[32, 16]" = torch.ops.aten.view.default(clone_763, [32, 16]);  clone_763 = None
        mm_760: "f32[32, 8]" = torch.ops.aten.mm.default(view_1530, slice_7)
        view_1531: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_760, [2, 16, 8]);  mm_760 = None
        slice_25089: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4558, 1, 6080, 6096)
        slice_25090: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25089, 2, 0, 16)
        add_762: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25090, view_1531);  slice_25090 = view_1531 = None
        slice_scatter_4560: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25089, add_762, 2, 0, 16);  slice_25089 = add_762 = None
        slice_scatter_4561: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4558, slice_scatter_4560, 1, 6080, 6096);  slice_scatter_4558 = slice_scatter_4560 = None
        slice_25094: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4561, 1, 6080, 6096)
        slice_25095: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25094, 2, 0, 16)
        slice_scatter_4563: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25094, slice_25095, 2, 0, 16);  slice_25094 = slice_25095 = None
        slice_scatter_4564: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4561, slice_scatter_4563, 1, 6080, 6096);  slice_scatter_4561 = slice_scatter_4563 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25115: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_25081, 2, 16, 32);  slice_25081 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_764: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_25115, memory_format = torch.contiguous_format);  slice_25115 = None
        view_1532: "f32[32, 11]" = torch.ops.aten.view.default(clone_764, [32, 11]);  clone_764 = None
        mm_761: "f32[32, 8]" = torch.ops.aten.mm.default(view_1532, slice_37)
        view_1533: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_761, [2, 16, 8]);  mm_761 = None
        slice_25122: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4564, 1, 6080, 6096)
        slice_25123: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25122, 2, 0, 16)
        add_763: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25123, view_1533);  slice_25123 = view_1533 = None
        slice_scatter_4566: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25122, add_763, 2, 0, 16);  slice_25122 = add_763 = None
        slice_scatter_4567: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4564, slice_scatter_4566, 1, 6080, 6096);  slice_scatter_4564 = slice_scatter_4566 = None
        slice_25127: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4567, 1, 6080, 6096)
        slice_25128: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25127, 2, 0, 16)
        slice_scatter_4569: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25127, slice_25128, 2, 0, 16);  slice_25127 = slice_25128 = None
        slice_scatter_4570: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4567, slice_scatter_4569, 1, 6080, 6096);  slice_scatter_4567 = slice_scatter_4569 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25147: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6096, 6112)
        slice_25148: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_25147, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_765: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_25148, memory_format = torch.contiguous_format);  slice_25148 = None
        view_1534: "f32[32, 16]" = torch.ops.aten.view.default(clone_765, [32, 16]);  clone_765 = None
        mm_762: "f32[32, 8]" = torch.ops.aten.mm.default(view_1534, slice_7)
        view_1535: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_762, [2, 16, 8]);  mm_762 = None
        slice_25155: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4570, 1, 6096, 6112)
        slice_25156: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25155, 2, 0, 16)
        add_764: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25156, view_1535);  slice_25156 = view_1535 = None
        slice_scatter_4572: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25155, add_764, 2, 0, 16);  slice_25155 = add_764 = None
        slice_scatter_4573: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4570, slice_scatter_4572, 1, 6096, 6112);  slice_scatter_4570 = slice_scatter_4572 = None
        slice_25160: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4573, 1, 6096, 6112)
        slice_25161: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25160, 2, 0, 16)
        slice_scatter_4575: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25160, slice_25161, 2, 0, 16);  slice_25160 = slice_25161 = None
        slice_scatter_4576: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4573, slice_scatter_4575, 1, 6096, 6112);  slice_scatter_4573 = slice_scatter_4575 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25181: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_25147, 2, 16, 32);  slice_25147 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_766: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_25181, memory_format = torch.contiguous_format);  slice_25181 = None
        view_1536: "f32[32, 11]" = torch.ops.aten.view.default(clone_766, [32, 11]);  clone_766 = None
        mm_763: "f32[32, 8]" = torch.ops.aten.mm.default(view_1536, slice_37)
        view_1537: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_763, [2, 16, 8]);  mm_763 = None
        slice_25188: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4576, 1, 6096, 6112)
        slice_25189: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25188, 2, 0, 16)
        add_765: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25189, view_1537);  slice_25189 = view_1537 = None
        slice_scatter_4578: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25188, add_765, 2, 0, 16);  slice_25188 = add_765 = None
        slice_scatter_4579: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4576, slice_scatter_4578, 1, 6096, 6112);  slice_scatter_4576 = slice_scatter_4578 = None
        slice_25193: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4579, 1, 6096, 6112)
        slice_25194: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25193, 2, 0, 16)
        slice_scatter_4581: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25193, slice_25194, 2, 0, 16);  slice_25193 = slice_25194 = None
        slice_scatter_4582: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4579, slice_scatter_4581, 1, 6096, 6112);  slice_scatter_4579 = slice_scatter_4581 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25213: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6112, 6128)
        slice_25214: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_25213, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_767: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_25214, memory_format = torch.contiguous_format);  slice_25214 = None
        view_1538: "f32[32, 16]" = torch.ops.aten.view.default(clone_767, [32, 16]);  clone_767 = None
        mm_764: "f32[32, 8]" = torch.ops.aten.mm.default(view_1538, slice_7)
        view_1539: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_764, [2, 16, 8]);  mm_764 = None
        slice_25221: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4582, 1, 6112, 6128)
        slice_25222: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25221, 2, 0, 16)
        add_766: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25222, view_1539);  slice_25222 = view_1539 = None
        slice_scatter_4584: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25221, add_766, 2, 0, 16);  slice_25221 = add_766 = None
        slice_scatter_4585: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4582, slice_scatter_4584, 1, 6112, 6128);  slice_scatter_4582 = slice_scatter_4584 = None
        slice_25226: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4585, 1, 6112, 6128)
        slice_25227: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25226, 2, 0, 16)
        slice_scatter_4587: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25226, slice_25227, 2, 0, 16);  slice_25226 = slice_25227 = None
        slice_scatter_4588: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4585, slice_scatter_4587, 1, 6112, 6128);  slice_scatter_4585 = slice_scatter_4587 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25247: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_25213, 2, 16, 32);  slice_25213 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_768: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_25247, memory_format = torch.contiguous_format);  slice_25247 = None
        view_1540: "f32[32, 11]" = torch.ops.aten.view.default(clone_768, [32, 11]);  clone_768 = None
        mm_765: "f32[32, 8]" = torch.ops.aten.mm.default(view_1540, slice_37)
        view_1541: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_765, [2, 16, 8]);  mm_765 = None
        slice_25254: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4588, 1, 6112, 6128)
        slice_25255: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25254, 2, 0, 16)
        add_767: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25255, view_1541);  slice_25255 = view_1541 = None
        slice_scatter_4590: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25254, add_767, 2, 0, 16);  slice_25254 = add_767 = None
        slice_scatter_4591: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4588, slice_scatter_4590, 1, 6112, 6128);  slice_scatter_4588 = slice_scatter_4590 = None
        slice_25259: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4591, 1, 6112, 6128)
        slice_25260: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25259, 2, 0, 16)
        slice_scatter_4593: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25259, slice_25260, 2, 0, 16);  slice_25259 = slice_25260 = None
        slice_scatter_4594: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4591, slice_scatter_4593, 1, 6112, 6128);  slice_scatter_4591 = slice_scatter_4593 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25279: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6128, 6144)
        slice_25280: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_25279, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_769: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_25280, memory_format = torch.contiguous_format);  slice_25280 = None
        view_1542: "f32[32, 16]" = torch.ops.aten.view.default(clone_769, [32, 16]);  clone_769 = None
        mm_766: "f32[32, 8]" = torch.ops.aten.mm.default(view_1542, slice_7)
        view_1543: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_766, [2, 16, 8]);  mm_766 = None
        slice_25287: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4594, 1, 6128, 6144)
        slice_25288: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25287, 2, 0, 16)
        add_768: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25288, view_1543);  slice_25288 = view_1543 = None
        slice_scatter_4596: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25287, add_768, 2, 0, 16);  slice_25287 = add_768 = None
        slice_scatter_4597: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4594, slice_scatter_4596, 1, 6128, 6144);  slice_scatter_4594 = slice_scatter_4596 = None
        slice_25292: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4597, 1, 6128, 6144)
        slice_25293: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25292, 2, 0, 16)
        slice_scatter_4599: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25292, slice_25293, 2, 0, 16);  slice_25292 = slice_25293 = None
        slice_scatter_4600: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4597, slice_scatter_4599, 1, 6128, 6144);  slice_scatter_4597 = slice_scatter_4599 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25313: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_25279, 2, 16, 32);  slice_25279 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_770: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_25313, memory_format = torch.contiguous_format);  slice_25313 = None
        view_1544: "f32[32, 11]" = torch.ops.aten.view.default(clone_770, [32, 11]);  clone_770 = None
        mm_767: "f32[32, 8]" = torch.ops.aten.mm.default(view_1544, slice_37)
        view_1545: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_767, [2, 16, 8]);  mm_767 = None
        slice_25320: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4600, 1, 6128, 6144)
        slice_25321: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25320, 2, 0, 16)
        add_769: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25321, view_1545);  slice_25321 = view_1545 = None
        slice_scatter_4602: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25320, add_769, 2, 0, 16);  slice_25320 = add_769 = None
        slice_scatter_4603: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4600, slice_scatter_4602, 1, 6128, 6144);  slice_scatter_4600 = slice_scatter_4602 = None
        slice_25325: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4603, 1, 6128, 6144)
        slice_25326: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25325, 2, 0, 16)
        slice_scatter_4605: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25325, slice_25326, 2, 0, 16);  slice_25325 = slice_25326 = None
        slice_scatter_4606: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4603, slice_scatter_4605, 1, 6128, 6144);  slice_scatter_4603 = slice_scatter_4605 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25345: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6144, 6160)
        slice_25346: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_25345, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_771: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_25346, memory_format = torch.contiguous_format);  slice_25346 = None
        view_1546: "f32[32, 16]" = torch.ops.aten.view.default(clone_771, [32, 16]);  clone_771 = None
        mm_768: "f32[32, 8]" = torch.ops.aten.mm.default(view_1546, slice_7)
        view_1547: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_768, [2, 16, 8]);  mm_768 = None
        slice_25353: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4606, 1, 6144, 6160)
        slice_25354: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25353, 2, 0, 16)
        add_770: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25354, view_1547);  slice_25354 = view_1547 = None
        slice_scatter_4608: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25353, add_770, 2, 0, 16);  slice_25353 = add_770 = None
        slice_scatter_4609: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4606, slice_scatter_4608, 1, 6144, 6160);  slice_scatter_4606 = slice_scatter_4608 = None
        slice_25358: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4609, 1, 6144, 6160)
        slice_25359: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25358, 2, 0, 16)
        slice_scatter_4611: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25358, slice_25359, 2, 0, 16);  slice_25358 = slice_25359 = None
        slice_scatter_4612: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4609, slice_scatter_4611, 1, 6144, 6160);  slice_scatter_4609 = slice_scatter_4611 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25379: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_25345, 2, 16, 32);  slice_25345 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_772: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_25379, memory_format = torch.contiguous_format);  slice_25379 = None
        view_1548: "f32[32, 11]" = torch.ops.aten.view.default(clone_772, [32, 11]);  clone_772 = None
        mm_769: "f32[32, 8]" = torch.ops.aten.mm.default(view_1548, slice_37)
        view_1549: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_769, [2, 16, 8]);  mm_769 = None
        slice_25386: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4612, 1, 6144, 6160)
        slice_25387: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25386, 2, 0, 16)
        add_771: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25387, view_1549);  slice_25387 = view_1549 = None
        slice_scatter_4614: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25386, add_771, 2, 0, 16);  slice_25386 = add_771 = None
        slice_scatter_4615: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4612, slice_scatter_4614, 1, 6144, 6160);  slice_scatter_4612 = slice_scatter_4614 = None
        slice_25391: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4615, 1, 6144, 6160)
        slice_25392: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25391, 2, 0, 16)
        slice_scatter_4617: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25391, slice_25392, 2, 0, 16);  slice_25391 = slice_25392 = None
        slice_scatter_4618: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4615, slice_scatter_4617, 1, 6144, 6160);  slice_scatter_4615 = slice_scatter_4617 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25411: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6160, 6176)
        slice_25412: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_25411, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_773: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_25412, memory_format = torch.contiguous_format);  slice_25412 = None
        view_1550: "f32[32, 16]" = torch.ops.aten.view.default(clone_773, [32, 16]);  clone_773 = None
        mm_770: "f32[32, 8]" = torch.ops.aten.mm.default(view_1550, slice_7)
        view_1551: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_770, [2, 16, 8]);  mm_770 = None
        slice_25419: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4618, 1, 6160, 6176)
        slice_25420: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25419, 2, 0, 16)
        add_772: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25420, view_1551);  slice_25420 = view_1551 = None
        slice_scatter_4620: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25419, add_772, 2, 0, 16);  slice_25419 = add_772 = None
        slice_scatter_4621: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4618, slice_scatter_4620, 1, 6160, 6176);  slice_scatter_4618 = slice_scatter_4620 = None
        slice_25424: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4621, 1, 6160, 6176)
        slice_25425: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25424, 2, 0, 16)
        slice_scatter_4623: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25424, slice_25425, 2, 0, 16);  slice_25424 = slice_25425 = None
        slice_scatter_4624: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4621, slice_scatter_4623, 1, 6160, 6176);  slice_scatter_4621 = slice_scatter_4623 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25445: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_25411, 2, 16, 32);  slice_25411 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_774: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_25445, memory_format = torch.contiguous_format);  slice_25445 = None
        view_1552: "f32[32, 11]" = torch.ops.aten.view.default(clone_774, [32, 11]);  clone_774 = None
        mm_771: "f32[32, 8]" = torch.ops.aten.mm.default(view_1552, slice_37)
        view_1553: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_771, [2, 16, 8]);  mm_771 = None
        slice_25452: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4624, 1, 6160, 6176)
        slice_25453: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25452, 2, 0, 16)
        add_773: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25453, view_1553);  slice_25453 = view_1553 = None
        slice_scatter_4626: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25452, add_773, 2, 0, 16);  slice_25452 = add_773 = None
        slice_scatter_4627: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4624, slice_scatter_4626, 1, 6160, 6176);  slice_scatter_4624 = slice_scatter_4626 = None
        slice_25457: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4627, 1, 6160, 6176)
        slice_25458: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25457, 2, 0, 16)
        slice_scatter_4629: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25457, slice_25458, 2, 0, 16);  slice_25457 = slice_25458 = None
        slice_scatter_4630: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4627, slice_scatter_4629, 1, 6160, 6176);  slice_scatter_4627 = slice_scatter_4629 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25477: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6176, 6192)
        slice_25478: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_25477, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_775: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_25478, memory_format = torch.contiguous_format);  slice_25478 = None
        view_1554: "f32[32, 16]" = torch.ops.aten.view.default(clone_775, [32, 16]);  clone_775 = None
        mm_772: "f32[32, 8]" = torch.ops.aten.mm.default(view_1554, slice_7)
        view_1555: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_772, [2, 16, 8]);  mm_772 = None
        slice_25485: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4630, 1, 6176, 6192)
        slice_25486: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25485, 2, 0, 16)
        add_774: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25486, view_1555);  slice_25486 = view_1555 = None
        slice_scatter_4632: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25485, add_774, 2, 0, 16);  slice_25485 = add_774 = None
        slice_scatter_4633: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4630, slice_scatter_4632, 1, 6176, 6192);  slice_scatter_4630 = slice_scatter_4632 = None
        slice_25490: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4633, 1, 6176, 6192)
        slice_25491: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25490, 2, 0, 16)
        slice_scatter_4635: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25490, slice_25491, 2, 0, 16);  slice_25490 = slice_25491 = None
        slice_scatter_4636: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4633, slice_scatter_4635, 1, 6176, 6192);  slice_scatter_4633 = slice_scatter_4635 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25511: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_25477, 2, 16, 32);  slice_25477 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_776: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_25511, memory_format = torch.contiguous_format);  slice_25511 = None
        view_1556: "f32[32, 11]" = torch.ops.aten.view.default(clone_776, [32, 11]);  clone_776 = None
        mm_773: "f32[32, 8]" = torch.ops.aten.mm.default(view_1556, slice_37)
        view_1557: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_773, [2, 16, 8]);  mm_773 = None
        slice_25518: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4636, 1, 6176, 6192)
        slice_25519: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25518, 2, 0, 16)
        add_775: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25519, view_1557);  slice_25519 = view_1557 = None
        slice_scatter_4638: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25518, add_775, 2, 0, 16);  slice_25518 = add_775 = None
        slice_scatter_4639: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4636, slice_scatter_4638, 1, 6176, 6192);  slice_scatter_4636 = slice_scatter_4638 = None
        slice_25523: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4639, 1, 6176, 6192)
        slice_25524: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25523, 2, 0, 16)
        slice_scatter_4641: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25523, slice_25524, 2, 0, 16);  slice_25523 = slice_25524 = None
        slice_scatter_4642: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4639, slice_scatter_4641, 1, 6176, 6192);  slice_scatter_4639 = slice_scatter_4641 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25543: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6192, 6208)
        slice_25544: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_25543, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_777: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_25544, memory_format = torch.contiguous_format);  slice_25544 = None
        view_1558: "f32[32, 16]" = torch.ops.aten.view.default(clone_777, [32, 16]);  clone_777 = None
        mm_774: "f32[32, 8]" = torch.ops.aten.mm.default(view_1558, slice_7)
        view_1559: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_774, [2, 16, 8]);  mm_774 = None
        slice_25551: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4642, 1, 6192, 6208)
        slice_25552: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25551, 2, 0, 16)
        add_776: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25552, view_1559);  slice_25552 = view_1559 = None
        slice_scatter_4644: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25551, add_776, 2, 0, 16);  slice_25551 = add_776 = None
        slice_scatter_4645: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4642, slice_scatter_4644, 1, 6192, 6208);  slice_scatter_4642 = slice_scatter_4644 = None
        slice_25556: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4645, 1, 6192, 6208)
        slice_25557: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25556, 2, 0, 16)
        slice_scatter_4647: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25556, slice_25557, 2, 0, 16);  slice_25556 = slice_25557 = None
        slice_scatter_4648: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4645, slice_scatter_4647, 1, 6192, 6208);  slice_scatter_4645 = slice_scatter_4647 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25577: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_25543, 2, 16, 32);  slice_25543 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_778: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_25577, memory_format = torch.contiguous_format);  slice_25577 = None
        view_1560: "f32[32, 11]" = torch.ops.aten.view.default(clone_778, [32, 11]);  clone_778 = None
        mm_775: "f32[32, 8]" = torch.ops.aten.mm.default(view_1560, slice_37)
        view_1561: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_775, [2, 16, 8]);  mm_775 = None
        slice_25584: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4648, 1, 6192, 6208)
        slice_25585: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25584, 2, 0, 16)
        add_777: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25585, view_1561);  slice_25585 = view_1561 = None
        slice_scatter_4650: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25584, add_777, 2, 0, 16);  slice_25584 = add_777 = None
        slice_scatter_4651: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4648, slice_scatter_4650, 1, 6192, 6208);  slice_scatter_4648 = slice_scatter_4650 = None
        slice_25589: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4651, 1, 6192, 6208)
        slice_25590: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25589, 2, 0, 16)
        slice_scatter_4653: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25589, slice_25590, 2, 0, 16);  slice_25589 = slice_25590 = None
        slice_scatter_4654: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4651, slice_scatter_4653, 1, 6192, 6208);  slice_scatter_4651 = slice_scatter_4653 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25609: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6208, 6224)
        slice_25610: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_25609, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_779: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_25610, memory_format = torch.contiguous_format);  slice_25610 = None
        view_1562: "f32[32, 16]" = torch.ops.aten.view.default(clone_779, [32, 16]);  clone_779 = None
        mm_776: "f32[32, 8]" = torch.ops.aten.mm.default(view_1562, slice_7)
        view_1563: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_776, [2, 16, 8]);  mm_776 = None
        slice_25617: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4654, 1, 6208, 6224)
        slice_25618: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25617, 2, 0, 16)
        add_778: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25618, view_1563);  slice_25618 = view_1563 = None
        slice_scatter_4656: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25617, add_778, 2, 0, 16);  slice_25617 = add_778 = None
        slice_scatter_4657: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4654, slice_scatter_4656, 1, 6208, 6224);  slice_scatter_4654 = slice_scatter_4656 = None
        slice_25622: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4657, 1, 6208, 6224)
        slice_25623: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25622, 2, 0, 16)
        slice_scatter_4659: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25622, slice_25623, 2, 0, 16);  slice_25622 = slice_25623 = None
        slice_scatter_4660: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4657, slice_scatter_4659, 1, 6208, 6224);  slice_scatter_4657 = slice_scatter_4659 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25643: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_25609, 2, 16, 32);  slice_25609 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_780: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_25643, memory_format = torch.contiguous_format);  slice_25643 = None
        view_1564: "f32[32, 11]" = torch.ops.aten.view.default(clone_780, [32, 11]);  clone_780 = None
        mm_777: "f32[32, 8]" = torch.ops.aten.mm.default(view_1564, slice_37)
        view_1565: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_777, [2, 16, 8]);  mm_777 = None
        slice_25650: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4660, 1, 6208, 6224)
        slice_25651: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25650, 2, 0, 16)
        add_779: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25651, view_1565);  slice_25651 = view_1565 = None
        slice_scatter_4662: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25650, add_779, 2, 0, 16);  slice_25650 = add_779 = None
        slice_scatter_4663: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4660, slice_scatter_4662, 1, 6208, 6224);  slice_scatter_4660 = slice_scatter_4662 = None
        slice_25655: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4663, 1, 6208, 6224)
        slice_25656: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25655, 2, 0, 16)
        slice_scatter_4665: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25655, slice_25656, 2, 0, 16);  slice_25655 = slice_25656 = None
        slice_scatter_4666: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4663, slice_scatter_4665, 1, 6208, 6224);  slice_scatter_4663 = slice_scatter_4665 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25675: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6224, 6240)
        slice_25676: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_25675, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_781: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_25676, memory_format = torch.contiguous_format);  slice_25676 = None
        view_1566: "f32[32, 16]" = torch.ops.aten.view.default(clone_781, [32, 16]);  clone_781 = None
        mm_778: "f32[32, 8]" = torch.ops.aten.mm.default(view_1566, slice_7)
        view_1567: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_778, [2, 16, 8]);  mm_778 = None
        slice_25683: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4666, 1, 6224, 6240)
        slice_25684: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25683, 2, 0, 16)
        add_780: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25684, view_1567);  slice_25684 = view_1567 = None
        slice_scatter_4668: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25683, add_780, 2, 0, 16);  slice_25683 = add_780 = None
        slice_scatter_4669: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4666, slice_scatter_4668, 1, 6224, 6240);  slice_scatter_4666 = slice_scatter_4668 = None
        slice_25688: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4669, 1, 6224, 6240)
        slice_25689: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25688, 2, 0, 16)
        slice_scatter_4671: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25688, slice_25689, 2, 0, 16);  slice_25688 = slice_25689 = None
        slice_scatter_4672: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4669, slice_scatter_4671, 1, 6224, 6240);  slice_scatter_4669 = slice_scatter_4671 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25709: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_25675, 2, 16, 32);  slice_25675 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_782: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_25709, memory_format = torch.contiguous_format);  slice_25709 = None
        view_1568: "f32[32, 11]" = torch.ops.aten.view.default(clone_782, [32, 11]);  clone_782 = None
        mm_779: "f32[32, 8]" = torch.ops.aten.mm.default(view_1568, slice_37)
        view_1569: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_779, [2, 16, 8]);  mm_779 = None
        slice_25716: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4672, 1, 6224, 6240)
        slice_25717: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25716, 2, 0, 16)
        add_781: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25717, view_1569);  slice_25717 = view_1569 = None
        slice_scatter_4674: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25716, add_781, 2, 0, 16);  slice_25716 = add_781 = None
        slice_scatter_4675: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4672, slice_scatter_4674, 1, 6224, 6240);  slice_scatter_4672 = slice_scatter_4674 = None
        slice_25721: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4675, 1, 6224, 6240)
        slice_25722: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25721, 2, 0, 16)
        slice_scatter_4677: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25721, slice_25722, 2, 0, 16);  slice_25721 = slice_25722 = None
        slice_scatter_4678: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4675, slice_scatter_4677, 1, 6224, 6240);  slice_scatter_4675 = slice_scatter_4677 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25741: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6240, 6256)
        slice_25742: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_25741, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_783: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_25742, memory_format = torch.contiguous_format);  slice_25742 = None
        view_1570: "f32[32, 16]" = torch.ops.aten.view.default(clone_783, [32, 16]);  clone_783 = None
        mm_780: "f32[32, 8]" = torch.ops.aten.mm.default(view_1570, slice_7)
        view_1571: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_780, [2, 16, 8]);  mm_780 = None
        slice_25749: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4678, 1, 6240, 6256)
        slice_25750: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25749, 2, 0, 16)
        add_782: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25750, view_1571);  slice_25750 = view_1571 = None
        slice_scatter_4680: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25749, add_782, 2, 0, 16);  slice_25749 = add_782 = None
        slice_scatter_4681: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4678, slice_scatter_4680, 1, 6240, 6256);  slice_scatter_4678 = slice_scatter_4680 = None
        slice_25754: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4681, 1, 6240, 6256)
        slice_25755: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25754, 2, 0, 16)
        slice_scatter_4683: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25754, slice_25755, 2, 0, 16);  slice_25754 = slice_25755 = None
        slice_scatter_4684: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4681, slice_scatter_4683, 1, 6240, 6256);  slice_scatter_4681 = slice_scatter_4683 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25775: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_25741, 2, 16, 32);  slice_25741 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_784: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_25775, memory_format = torch.contiguous_format);  slice_25775 = None
        view_1572: "f32[32, 11]" = torch.ops.aten.view.default(clone_784, [32, 11]);  clone_784 = None
        mm_781: "f32[32, 8]" = torch.ops.aten.mm.default(view_1572, slice_37)
        view_1573: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_781, [2, 16, 8]);  mm_781 = None
        slice_25782: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4684, 1, 6240, 6256)
        slice_25783: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25782, 2, 0, 16)
        add_783: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25783, view_1573);  slice_25783 = view_1573 = None
        slice_scatter_4686: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25782, add_783, 2, 0, 16);  slice_25782 = add_783 = None
        slice_scatter_4687: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4684, slice_scatter_4686, 1, 6240, 6256);  slice_scatter_4684 = slice_scatter_4686 = None
        slice_25787: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4687, 1, 6240, 6256)
        slice_25788: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25787, 2, 0, 16)
        slice_scatter_4689: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25787, slice_25788, 2, 0, 16);  slice_25787 = slice_25788 = None
        slice_scatter_4690: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4687, slice_scatter_4689, 1, 6240, 6256);  slice_scatter_4687 = slice_scatter_4689 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25807: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6256, 6272)
        slice_25808: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_25807, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_785: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_25808, memory_format = torch.contiguous_format);  slice_25808 = None
        view_1574: "f32[32, 16]" = torch.ops.aten.view.default(clone_785, [32, 16]);  clone_785 = None
        mm_782: "f32[32, 8]" = torch.ops.aten.mm.default(view_1574, slice_7)
        view_1575: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_782, [2, 16, 8]);  mm_782 = None
        slice_25815: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4690, 1, 6256, 6272)
        slice_25816: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25815, 2, 0, 16)
        add_784: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25816, view_1575);  slice_25816 = view_1575 = None
        slice_scatter_4692: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25815, add_784, 2, 0, 16);  slice_25815 = add_784 = None
        slice_scatter_4693: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4690, slice_scatter_4692, 1, 6256, 6272);  slice_scatter_4690 = slice_scatter_4692 = None
        slice_25820: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4693, 1, 6256, 6272)
        slice_25821: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25820, 2, 0, 16)
        slice_scatter_4695: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25820, slice_25821, 2, 0, 16);  slice_25820 = slice_25821 = None
        slice_scatter_4696: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4693, slice_scatter_4695, 1, 6256, 6272);  slice_scatter_4693 = slice_scatter_4695 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25841: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_25807, 2, 16, 32);  slice_25807 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_786: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_25841, memory_format = torch.contiguous_format);  slice_25841 = None
        view_1576: "f32[32, 11]" = torch.ops.aten.view.default(clone_786, [32, 11]);  clone_786 = None
        mm_783: "f32[32, 8]" = torch.ops.aten.mm.default(view_1576, slice_37)
        view_1577: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_783, [2, 16, 8]);  mm_783 = None
        slice_25848: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4696, 1, 6256, 6272)
        slice_25849: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25848, 2, 0, 16)
        add_785: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25849, view_1577);  slice_25849 = view_1577 = None
        slice_scatter_4698: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25848, add_785, 2, 0, 16);  slice_25848 = add_785 = None
        slice_scatter_4699: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4696, slice_scatter_4698, 1, 6256, 6272);  slice_scatter_4696 = slice_scatter_4698 = None
        slice_25853: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4699, 1, 6256, 6272)
        slice_25854: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25853, 2, 0, 16)
        slice_scatter_4701: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25853, slice_25854, 2, 0, 16);  slice_25853 = slice_25854 = None
        slice_scatter_4702: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4699, slice_scatter_4701, 1, 6256, 6272);  slice_scatter_4699 = slice_scatter_4701 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25873: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6272, 6288)
        slice_25874: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_25873, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_787: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_25874, memory_format = torch.contiguous_format);  slice_25874 = None
        view_1578: "f32[32, 16]" = torch.ops.aten.view.default(clone_787, [32, 16]);  clone_787 = None
        mm_784: "f32[32, 8]" = torch.ops.aten.mm.default(view_1578, slice_7)
        view_1579: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_784, [2, 16, 8]);  mm_784 = None
        slice_25881: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4702, 1, 6272, 6288)
        slice_25882: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25881, 2, 0, 16)
        add_786: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25882, view_1579);  slice_25882 = view_1579 = None
        slice_scatter_4704: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25881, add_786, 2, 0, 16);  slice_25881 = add_786 = None
        slice_scatter_4705: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4702, slice_scatter_4704, 1, 6272, 6288);  slice_scatter_4702 = slice_scatter_4704 = None
        slice_25886: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4705, 1, 6272, 6288)
        slice_25887: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25886, 2, 0, 16)
        slice_scatter_4707: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25886, slice_25887, 2, 0, 16);  slice_25886 = slice_25887 = None
        slice_scatter_4708: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4705, slice_scatter_4707, 1, 6272, 6288);  slice_scatter_4705 = slice_scatter_4707 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25907: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_25873, 2, 16, 32);  slice_25873 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_788: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_25907, memory_format = torch.contiguous_format);  slice_25907 = None
        view_1580: "f32[32, 11]" = torch.ops.aten.view.default(clone_788, [32, 11]);  clone_788 = None
        mm_785: "f32[32, 8]" = torch.ops.aten.mm.default(view_1580, slice_37)
        view_1581: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_785, [2, 16, 8]);  mm_785 = None
        slice_25914: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4708, 1, 6272, 6288)
        slice_25915: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25914, 2, 0, 16)
        add_787: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25915, view_1581);  slice_25915 = view_1581 = None
        slice_scatter_4710: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25914, add_787, 2, 0, 16);  slice_25914 = add_787 = None
        slice_scatter_4711: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4708, slice_scatter_4710, 1, 6272, 6288);  slice_scatter_4708 = slice_scatter_4710 = None
        slice_25919: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4711, 1, 6272, 6288)
        slice_25920: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25919, 2, 0, 16)
        slice_scatter_4713: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25919, slice_25920, 2, 0, 16);  slice_25919 = slice_25920 = None
        slice_scatter_4714: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4711, slice_scatter_4713, 1, 6272, 6288);  slice_scatter_4711 = slice_scatter_4713 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25939: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6288, 6304)
        slice_25940: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_25939, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_789: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_25940, memory_format = torch.contiguous_format);  slice_25940 = None
        view_1582: "f32[32, 16]" = torch.ops.aten.view.default(clone_789, [32, 16]);  clone_789 = None
        mm_786: "f32[32, 8]" = torch.ops.aten.mm.default(view_1582, slice_7)
        view_1583: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_786, [2, 16, 8]);  mm_786 = None
        slice_25947: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4714, 1, 6288, 6304)
        slice_25948: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25947, 2, 0, 16)
        add_788: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25948, view_1583);  slice_25948 = view_1583 = None
        slice_scatter_4716: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25947, add_788, 2, 0, 16);  slice_25947 = add_788 = None
        slice_scatter_4717: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4714, slice_scatter_4716, 1, 6288, 6304);  slice_scatter_4714 = slice_scatter_4716 = None
        slice_25952: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4717, 1, 6288, 6304)
        slice_25953: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25952, 2, 0, 16)
        slice_scatter_4719: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25952, slice_25953, 2, 0, 16);  slice_25952 = slice_25953 = None
        slice_scatter_4720: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4717, slice_scatter_4719, 1, 6288, 6304);  slice_scatter_4717 = slice_scatter_4719 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_25973: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_25939, 2, 16, 32);  slice_25939 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_790: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_25973, memory_format = torch.contiguous_format);  slice_25973 = None
        view_1584: "f32[32, 11]" = torch.ops.aten.view.default(clone_790, [32, 11]);  clone_790 = None
        mm_787: "f32[32, 8]" = torch.ops.aten.mm.default(view_1584, slice_37)
        view_1585: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_787, [2, 16, 8]);  mm_787 = None
        slice_25980: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4720, 1, 6288, 6304)
        slice_25981: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25980, 2, 0, 16)
        add_789: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_25981, view_1585);  slice_25981 = view_1585 = None
        slice_scatter_4722: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25980, add_789, 2, 0, 16);  slice_25980 = add_789 = None
        slice_scatter_4723: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4720, slice_scatter_4722, 1, 6288, 6304);  slice_scatter_4720 = slice_scatter_4722 = None
        slice_25985: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4723, 1, 6288, 6304)
        slice_25986: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_25985, 2, 0, 16)
        slice_scatter_4725: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_25985, slice_25986, 2, 0, 16);  slice_25985 = slice_25986 = None
        slice_scatter_4726: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4723, slice_scatter_4725, 1, 6288, 6304);  slice_scatter_4723 = slice_scatter_4725 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26005: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6304, 6320)
        slice_26006: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_26005, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_791: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_26006, memory_format = torch.contiguous_format);  slice_26006 = None
        view_1586: "f32[32, 16]" = torch.ops.aten.view.default(clone_791, [32, 16]);  clone_791 = None
        mm_788: "f32[32, 8]" = torch.ops.aten.mm.default(view_1586, slice_7)
        view_1587: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_788, [2, 16, 8]);  mm_788 = None
        slice_26013: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4726, 1, 6304, 6320)
        slice_26014: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26013, 2, 0, 16)
        add_790: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26014, view_1587);  slice_26014 = view_1587 = None
        slice_scatter_4728: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26013, add_790, 2, 0, 16);  slice_26013 = add_790 = None
        slice_scatter_4729: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4726, slice_scatter_4728, 1, 6304, 6320);  slice_scatter_4726 = slice_scatter_4728 = None
        slice_26018: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4729, 1, 6304, 6320)
        slice_26019: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26018, 2, 0, 16)
        slice_scatter_4731: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26018, slice_26019, 2, 0, 16);  slice_26018 = slice_26019 = None
        slice_scatter_4732: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4729, slice_scatter_4731, 1, 6304, 6320);  slice_scatter_4729 = slice_scatter_4731 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26039: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_26005, 2, 16, 32);  slice_26005 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_792: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_26039, memory_format = torch.contiguous_format);  slice_26039 = None
        view_1588: "f32[32, 11]" = torch.ops.aten.view.default(clone_792, [32, 11]);  clone_792 = None
        mm_789: "f32[32, 8]" = torch.ops.aten.mm.default(view_1588, slice_37)
        view_1589: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_789, [2, 16, 8]);  mm_789 = None
        slice_26046: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4732, 1, 6304, 6320)
        slice_26047: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26046, 2, 0, 16)
        add_791: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26047, view_1589);  slice_26047 = view_1589 = None
        slice_scatter_4734: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26046, add_791, 2, 0, 16);  slice_26046 = add_791 = None
        slice_scatter_4735: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4732, slice_scatter_4734, 1, 6304, 6320);  slice_scatter_4732 = slice_scatter_4734 = None
        slice_26051: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4735, 1, 6304, 6320)
        slice_26052: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26051, 2, 0, 16)
        slice_scatter_4737: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26051, slice_26052, 2, 0, 16);  slice_26051 = slice_26052 = None
        slice_scatter_4738: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4735, slice_scatter_4737, 1, 6304, 6320);  slice_scatter_4735 = slice_scatter_4737 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26071: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6320, 6336)
        slice_26072: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_26071, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_793: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_26072, memory_format = torch.contiguous_format);  slice_26072 = None
        view_1590: "f32[32, 16]" = torch.ops.aten.view.default(clone_793, [32, 16]);  clone_793 = None
        mm_790: "f32[32, 8]" = torch.ops.aten.mm.default(view_1590, slice_7)
        view_1591: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_790, [2, 16, 8]);  mm_790 = None
        slice_26079: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4738, 1, 6320, 6336)
        slice_26080: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26079, 2, 0, 16)
        add_792: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26080, view_1591);  slice_26080 = view_1591 = None
        slice_scatter_4740: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26079, add_792, 2, 0, 16);  slice_26079 = add_792 = None
        slice_scatter_4741: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4738, slice_scatter_4740, 1, 6320, 6336);  slice_scatter_4738 = slice_scatter_4740 = None
        slice_26084: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4741, 1, 6320, 6336)
        slice_26085: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26084, 2, 0, 16)
        slice_scatter_4743: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26084, slice_26085, 2, 0, 16);  slice_26084 = slice_26085 = None
        slice_scatter_4744: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4741, slice_scatter_4743, 1, 6320, 6336);  slice_scatter_4741 = slice_scatter_4743 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26105: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_26071, 2, 16, 32);  slice_26071 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_794: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_26105, memory_format = torch.contiguous_format);  slice_26105 = None
        view_1592: "f32[32, 11]" = torch.ops.aten.view.default(clone_794, [32, 11]);  clone_794 = None
        mm_791: "f32[32, 8]" = torch.ops.aten.mm.default(view_1592, slice_37)
        view_1593: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_791, [2, 16, 8]);  mm_791 = None
        slice_26112: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4744, 1, 6320, 6336)
        slice_26113: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26112, 2, 0, 16)
        add_793: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26113, view_1593);  slice_26113 = view_1593 = None
        slice_scatter_4746: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26112, add_793, 2, 0, 16);  slice_26112 = add_793 = None
        slice_scatter_4747: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4744, slice_scatter_4746, 1, 6320, 6336);  slice_scatter_4744 = slice_scatter_4746 = None
        slice_26117: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4747, 1, 6320, 6336)
        slice_26118: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26117, 2, 0, 16)
        slice_scatter_4749: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26117, slice_26118, 2, 0, 16);  slice_26117 = slice_26118 = None
        slice_scatter_4750: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4747, slice_scatter_4749, 1, 6320, 6336);  slice_scatter_4747 = slice_scatter_4749 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26137: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6336, 6352)
        slice_26138: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_26137, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_795: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_26138, memory_format = torch.contiguous_format);  slice_26138 = None
        view_1594: "f32[32, 16]" = torch.ops.aten.view.default(clone_795, [32, 16]);  clone_795 = None
        mm_792: "f32[32, 8]" = torch.ops.aten.mm.default(view_1594, slice_7)
        view_1595: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_792, [2, 16, 8]);  mm_792 = None
        slice_26145: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4750, 1, 6336, 6352)
        slice_26146: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26145, 2, 0, 16)
        add_794: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26146, view_1595);  slice_26146 = view_1595 = None
        slice_scatter_4752: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26145, add_794, 2, 0, 16);  slice_26145 = add_794 = None
        slice_scatter_4753: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4750, slice_scatter_4752, 1, 6336, 6352);  slice_scatter_4750 = slice_scatter_4752 = None
        slice_26150: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4753, 1, 6336, 6352)
        slice_26151: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26150, 2, 0, 16)
        slice_scatter_4755: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26150, slice_26151, 2, 0, 16);  slice_26150 = slice_26151 = None
        slice_scatter_4756: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4753, slice_scatter_4755, 1, 6336, 6352);  slice_scatter_4753 = slice_scatter_4755 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26171: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_26137, 2, 16, 32);  slice_26137 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_796: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_26171, memory_format = torch.contiguous_format);  slice_26171 = None
        view_1596: "f32[32, 11]" = torch.ops.aten.view.default(clone_796, [32, 11]);  clone_796 = None
        mm_793: "f32[32, 8]" = torch.ops.aten.mm.default(view_1596, slice_37)
        view_1597: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_793, [2, 16, 8]);  mm_793 = None
        slice_26178: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4756, 1, 6336, 6352)
        slice_26179: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26178, 2, 0, 16)
        add_795: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26179, view_1597);  slice_26179 = view_1597 = None
        slice_scatter_4758: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26178, add_795, 2, 0, 16);  slice_26178 = add_795 = None
        slice_scatter_4759: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4756, slice_scatter_4758, 1, 6336, 6352);  slice_scatter_4756 = slice_scatter_4758 = None
        slice_26183: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4759, 1, 6336, 6352)
        slice_26184: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26183, 2, 0, 16)
        slice_scatter_4761: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26183, slice_26184, 2, 0, 16);  slice_26183 = slice_26184 = None
        slice_scatter_4762: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4759, slice_scatter_4761, 1, 6336, 6352);  slice_scatter_4759 = slice_scatter_4761 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26203: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6352, 6368)
        slice_26204: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_26203, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_797: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_26204, memory_format = torch.contiguous_format);  slice_26204 = None
        view_1598: "f32[32, 16]" = torch.ops.aten.view.default(clone_797, [32, 16]);  clone_797 = None
        mm_794: "f32[32, 8]" = torch.ops.aten.mm.default(view_1598, slice_7)
        view_1599: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_794, [2, 16, 8]);  mm_794 = None
        slice_26211: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4762, 1, 6352, 6368)
        slice_26212: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26211, 2, 0, 16)
        add_796: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26212, view_1599);  slice_26212 = view_1599 = None
        slice_scatter_4764: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26211, add_796, 2, 0, 16);  slice_26211 = add_796 = None
        slice_scatter_4765: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4762, slice_scatter_4764, 1, 6352, 6368);  slice_scatter_4762 = slice_scatter_4764 = None
        slice_26216: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4765, 1, 6352, 6368)
        slice_26217: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26216, 2, 0, 16)
        slice_scatter_4767: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26216, slice_26217, 2, 0, 16);  slice_26216 = slice_26217 = None
        slice_scatter_4768: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4765, slice_scatter_4767, 1, 6352, 6368);  slice_scatter_4765 = slice_scatter_4767 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26237: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_26203, 2, 16, 32);  slice_26203 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_798: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_26237, memory_format = torch.contiguous_format);  slice_26237 = None
        view_1600: "f32[32, 11]" = torch.ops.aten.view.default(clone_798, [32, 11]);  clone_798 = None
        mm_795: "f32[32, 8]" = torch.ops.aten.mm.default(view_1600, slice_37)
        view_1601: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_795, [2, 16, 8]);  mm_795 = None
        slice_26244: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4768, 1, 6352, 6368)
        slice_26245: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26244, 2, 0, 16)
        add_797: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26245, view_1601);  slice_26245 = view_1601 = None
        slice_scatter_4770: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26244, add_797, 2, 0, 16);  slice_26244 = add_797 = None
        slice_scatter_4771: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4768, slice_scatter_4770, 1, 6352, 6368);  slice_scatter_4768 = slice_scatter_4770 = None
        slice_26249: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4771, 1, 6352, 6368)
        slice_26250: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26249, 2, 0, 16)
        slice_scatter_4773: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26249, slice_26250, 2, 0, 16);  slice_26249 = slice_26250 = None
        slice_scatter_4774: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4771, slice_scatter_4773, 1, 6352, 6368);  slice_scatter_4771 = slice_scatter_4773 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26269: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6368, 6384)
        slice_26270: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_26269, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_799: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_26270, memory_format = torch.contiguous_format);  slice_26270 = None
        view_1602: "f32[32, 16]" = torch.ops.aten.view.default(clone_799, [32, 16]);  clone_799 = None
        mm_796: "f32[32, 8]" = torch.ops.aten.mm.default(view_1602, slice_7)
        view_1603: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_796, [2, 16, 8]);  mm_796 = None
        slice_26277: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4774, 1, 6368, 6384)
        slice_26278: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26277, 2, 0, 16)
        add_798: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26278, view_1603);  slice_26278 = view_1603 = None
        slice_scatter_4776: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26277, add_798, 2, 0, 16);  slice_26277 = add_798 = None
        slice_scatter_4777: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4774, slice_scatter_4776, 1, 6368, 6384);  slice_scatter_4774 = slice_scatter_4776 = None
        slice_26282: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4777, 1, 6368, 6384)
        slice_26283: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26282, 2, 0, 16)
        slice_scatter_4779: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26282, slice_26283, 2, 0, 16);  slice_26282 = slice_26283 = None
        slice_scatter_4780: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4777, slice_scatter_4779, 1, 6368, 6384);  slice_scatter_4777 = slice_scatter_4779 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26303: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_26269, 2, 16, 32);  slice_26269 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_800: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_26303, memory_format = torch.contiguous_format);  slice_26303 = None
        view_1604: "f32[32, 11]" = torch.ops.aten.view.default(clone_800, [32, 11]);  clone_800 = None
        mm_797: "f32[32, 8]" = torch.ops.aten.mm.default(view_1604, slice_37)
        view_1605: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_797, [2, 16, 8]);  mm_797 = None
        slice_26310: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4780, 1, 6368, 6384)
        slice_26311: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26310, 2, 0, 16)
        add_799: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26311, view_1605);  slice_26311 = view_1605 = None
        slice_scatter_4782: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26310, add_799, 2, 0, 16);  slice_26310 = add_799 = None
        slice_scatter_4783: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4780, slice_scatter_4782, 1, 6368, 6384);  slice_scatter_4780 = slice_scatter_4782 = None
        slice_26315: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4783, 1, 6368, 6384)
        slice_26316: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26315, 2, 0, 16)
        slice_scatter_4785: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26315, slice_26316, 2, 0, 16);  slice_26315 = slice_26316 = None
        slice_scatter_4786: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4783, slice_scatter_4785, 1, 6368, 6384);  slice_scatter_4783 = slice_scatter_4785 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26335: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6384, 6400)
        slice_26336: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_26335, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_801: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_26336, memory_format = torch.contiguous_format);  slice_26336 = None
        view_1606: "f32[32, 16]" = torch.ops.aten.view.default(clone_801, [32, 16]);  clone_801 = None
        mm_798: "f32[32, 8]" = torch.ops.aten.mm.default(view_1606, slice_7)
        view_1607: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_798, [2, 16, 8]);  mm_798 = None
        slice_26343: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4786, 1, 6384, 6400)
        slice_26344: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26343, 2, 0, 16)
        add_800: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26344, view_1607);  slice_26344 = view_1607 = None
        slice_scatter_4788: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26343, add_800, 2, 0, 16);  slice_26343 = add_800 = None
        slice_scatter_4789: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4786, slice_scatter_4788, 1, 6384, 6400);  slice_scatter_4786 = slice_scatter_4788 = None
        slice_26348: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4789, 1, 6384, 6400)
        slice_26349: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26348, 2, 0, 16)
        slice_scatter_4791: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26348, slice_26349, 2, 0, 16);  slice_26348 = slice_26349 = None
        slice_scatter_4792: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4789, slice_scatter_4791, 1, 6384, 6400);  slice_scatter_4789 = slice_scatter_4791 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26369: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_26335, 2, 16, 32);  slice_26335 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_802: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_26369, memory_format = torch.contiguous_format);  slice_26369 = None
        view_1608: "f32[32, 11]" = torch.ops.aten.view.default(clone_802, [32, 11]);  clone_802 = None
        mm_799: "f32[32, 8]" = torch.ops.aten.mm.default(view_1608, slice_37)
        view_1609: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_799, [2, 16, 8]);  mm_799 = None
        slice_26376: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4792, 1, 6384, 6400)
        slice_26377: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26376, 2, 0, 16)
        add_801: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26377, view_1609);  slice_26377 = view_1609 = None
        slice_scatter_4794: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26376, add_801, 2, 0, 16);  slice_26376 = add_801 = None
        slice_scatter_4795: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4792, slice_scatter_4794, 1, 6384, 6400);  slice_scatter_4792 = slice_scatter_4794 = None
        slice_26381: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4795, 1, 6384, 6400)
        slice_26382: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26381, 2, 0, 16)
        slice_scatter_4797: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26381, slice_26382, 2, 0, 16);  slice_26381 = slice_26382 = None
        slice_scatter_4798: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4795, slice_scatter_4797, 1, 6384, 6400);  slice_scatter_4795 = slice_scatter_4797 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26401: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6400, 6416)
        slice_26402: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_26401, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_803: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_26402, memory_format = torch.contiguous_format);  slice_26402 = None
        view_1610: "f32[32, 16]" = torch.ops.aten.view.default(clone_803, [32, 16]);  clone_803 = None
        mm_800: "f32[32, 8]" = torch.ops.aten.mm.default(view_1610, slice_7)
        view_1611: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_800, [2, 16, 8]);  mm_800 = None
        slice_26409: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4798, 1, 6400, 6416)
        slice_26410: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26409, 2, 0, 16)
        add_802: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26410, view_1611);  slice_26410 = view_1611 = None
        slice_scatter_4800: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26409, add_802, 2, 0, 16);  slice_26409 = add_802 = None
        slice_scatter_4801: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4798, slice_scatter_4800, 1, 6400, 6416);  slice_scatter_4798 = slice_scatter_4800 = None
        slice_26414: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4801, 1, 6400, 6416)
        slice_26415: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26414, 2, 0, 16)
        slice_scatter_4803: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26414, slice_26415, 2, 0, 16);  slice_26414 = slice_26415 = None
        slice_scatter_4804: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4801, slice_scatter_4803, 1, 6400, 6416);  slice_scatter_4801 = slice_scatter_4803 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26435: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_26401, 2, 16, 32);  slice_26401 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_804: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_26435, memory_format = torch.contiguous_format);  slice_26435 = None
        view_1612: "f32[32, 11]" = torch.ops.aten.view.default(clone_804, [32, 11]);  clone_804 = None
        mm_801: "f32[32, 8]" = torch.ops.aten.mm.default(view_1612, slice_37)
        view_1613: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_801, [2, 16, 8]);  mm_801 = None
        slice_26442: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4804, 1, 6400, 6416)
        slice_26443: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26442, 2, 0, 16)
        add_803: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26443, view_1613);  slice_26443 = view_1613 = None
        slice_scatter_4806: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26442, add_803, 2, 0, 16);  slice_26442 = add_803 = None
        slice_scatter_4807: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4804, slice_scatter_4806, 1, 6400, 6416);  slice_scatter_4804 = slice_scatter_4806 = None
        slice_26447: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4807, 1, 6400, 6416)
        slice_26448: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26447, 2, 0, 16)
        slice_scatter_4809: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26447, slice_26448, 2, 0, 16);  slice_26447 = slice_26448 = None
        slice_scatter_4810: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4807, slice_scatter_4809, 1, 6400, 6416);  slice_scatter_4807 = slice_scatter_4809 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26467: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6416, 6432)
        slice_26468: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_26467, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_805: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_26468, memory_format = torch.contiguous_format);  slice_26468 = None
        view_1614: "f32[32, 16]" = torch.ops.aten.view.default(clone_805, [32, 16]);  clone_805 = None
        mm_802: "f32[32, 8]" = torch.ops.aten.mm.default(view_1614, slice_7)
        view_1615: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_802, [2, 16, 8]);  mm_802 = None
        slice_26475: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4810, 1, 6416, 6432)
        slice_26476: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26475, 2, 0, 16)
        add_804: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26476, view_1615);  slice_26476 = view_1615 = None
        slice_scatter_4812: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26475, add_804, 2, 0, 16);  slice_26475 = add_804 = None
        slice_scatter_4813: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4810, slice_scatter_4812, 1, 6416, 6432);  slice_scatter_4810 = slice_scatter_4812 = None
        slice_26480: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4813, 1, 6416, 6432)
        slice_26481: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26480, 2, 0, 16)
        slice_scatter_4815: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26480, slice_26481, 2, 0, 16);  slice_26480 = slice_26481 = None
        slice_scatter_4816: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4813, slice_scatter_4815, 1, 6416, 6432);  slice_scatter_4813 = slice_scatter_4815 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26501: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_26467, 2, 16, 32);  slice_26467 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_806: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_26501, memory_format = torch.contiguous_format);  slice_26501 = None
        view_1616: "f32[32, 11]" = torch.ops.aten.view.default(clone_806, [32, 11]);  clone_806 = None
        mm_803: "f32[32, 8]" = torch.ops.aten.mm.default(view_1616, slice_37)
        view_1617: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_803, [2, 16, 8]);  mm_803 = None
        slice_26508: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4816, 1, 6416, 6432)
        slice_26509: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26508, 2, 0, 16)
        add_805: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26509, view_1617);  slice_26509 = view_1617 = None
        slice_scatter_4818: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26508, add_805, 2, 0, 16);  slice_26508 = add_805 = None
        slice_scatter_4819: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4816, slice_scatter_4818, 1, 6416, 6432);  slice_scatter_4816 = slice_scatter_4818 = None
        slice_26513: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4819, 1, 6416, 6432)
        slice_26514: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26513, 2, 0, 16)
        slice_scatter_4821: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26513, slice_26514, 2, 0, 16);  slice_26513 = slice_26514 = None
        slice_scatter_4822: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4819, slice_scatter_4821, 1, 6416, 6432);  slice_scatter_4819 = slice_scatter_4821 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26533: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6432, 6448)
        slice_26534: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_26533, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_807: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_26534, memory_format = torch.contiguous_format);  slice_26534 = None
        view_1618: "f32[32, 16]" = torch.ops.aten.view.default(clone_807, [32, 16]);  clone_807 = None
        mm_804: "f32[32, 8]" = torch.ops.aten.mm.default(view_1618, slice_7)
        view_1619: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_804, [2, 16, 8]);  mm_804 = None
        slice_26541: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4822, 1, 6432, 6448)
        slice_26542: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26541, 2, 0, 16)
        add_806: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26542, view_1619);  slice_26542 = view_1619 = None
        slice_scatter_4824: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26541, add_806, 2, 0, 16);  slice_26541 = add_806 = None
        slice_scatter_4825: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4822, slice_scatter_4824, 1, 6432, 6448);  slice_scatter_4822 = slice_scatter_4824 = None
        slice_26546: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4825, 1, 6432, 6448)
        slice_26547: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26546, 2, 0, 16)
        slice_scatter_4827: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26546, slice_26547, 2, 0, 16);  slice_26546 = slice_26547 = None
        slice_scatter_4828: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4825, slice_scatter_4827, 1, 6432, 6448);  slice_scatter_4825 = slice_scatter_4827 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26567: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_26533, 2, 16, 32);  slice_26533 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_808: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_26567, memory_format = torch.contiguous_format);  slice_26567 = None
        view_1620: "f32[32, 11]" = torch.ops.aten.view.default(clone_808, [32, 11]);  clone_808 = None
        mm_805: "f32[32, 8]" = torch.ops.aten.mm.default(view_1620, slice_37)
        view_1621: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_805, [2, 16, 8]);  mm_805 = None
        slice_26574: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4828, 1, 6432, 6448)
        slice_26575: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26574, 2, 0, 16)
        add_807: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26575, view_1621);  slice_26575 = view_1621 = None
        slice_scatter_4830: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26574, add_807, 2, 0, 16);  slice_26574 = add_807 = None
        slice_scatter_4831: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4828, slice_scatter_4830, 1, 6432, 6448);  slice_scatter_4828 = slice_scatter_4830 = None
        slice_26579: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4831, 1, 6432, 6448)
        slice_26580: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26579, 2, 0, 16)
        slice_scatter_4833: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26579, slice_26580, 2, 0, 16);  slice_26579 = slice_26580 = None
        slice_scatter_4834: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4831, slice_scatter_4833, 1, 6432, 6448);  slice_scatter_4831 = slice_scatter_4833 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26599: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6448, 6464)
        slice_26600: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_26599, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_809: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_26600, memory_format = torch.contiguous_format);  slice_26600 = None
        view_1622: "f32[32, 16]" = torch.ops.aten.view.default(clone_809, [32, 16]);  clone_809 = None
        mm_806: "f32[32, 8]" = torch.ops.aten.mm.default(view_1622, slice_7)
        view_1623: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_806, [2, 16, 8]);  mm_806 = None
        slice_26607: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4834, 1, 6448, 6464)
        slice_26608: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26607, 2, 0, 16)
        add_808: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26608, view_1623);  slice_26608 = view_1623 = None
        slice_scatter_4836: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26607, add_808, 2, 0, 16);  slice_26607 = add_808 = None
        slice_scatter_4837: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4834, slice_scatter_4836, 1, 6448, 6464);  slice_scatter_4834 = slice_scatter_4836 = None
        slice_26612: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4837, 1, 6448, 6464)
        slice_26613: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26612, 2, 0, 16)
        slice_scatter_4839: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26612, slice_26613, 2, 0, 16);  slice_26612 = slice_26613 = None
        slice_scatter_4840: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4837, slice_scatter_4839, 1, 6448, 6464);  slice_scatter_4837 = slice_scatter_4839 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26633: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_26599, 2, 16, 32);  slice_26599 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_810: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_26633, memory_format = torch.contiguous_format);  slice_26633 = None
        view_1624: "f32[32, 11]" = torch.ops.aten.view.default(clone_810, [32, 11]);  clone_810 = None
        mm_807: "f32[32, 8]" = torch.ops.aten.mm.default(view_1624, slice_37)
        view_1625: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_807, [2, 16, 8]);  mm_807 = None
        slice_26640: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4840, 1, 6448, 6464)
        slice_26641: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26640, 2, 0, 16)
        add_809: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26641, view_1625);  slice_26641 = view_1625 = None
        slice_scatter_4842: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26640, add_809, 2, 0, 16);  slice_26640 = add_809 = None
        slice_scatter_4843: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4840, slice_scatter_4842, 1, 6448, 6464);  slice_scatter_4840 = slice_scatter_4842 = None
        slice_26645: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4843, 1, 6448, 6464)
        slice_26646: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26645, 2, 0, 16)
        slice_scatter_4845: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26645, slice_26646, 2, 0, 16);  slice_26645 = slice_26646 = None
        slice_scatter_4846: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4843, slice_scatter_4845, 1, 6448, 6464);  slice_scatter_4843 = slice_scatter_4845 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26665: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6464, 6480)
        slice_26666: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_26665, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_811: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_26666, memory_format = torch.contiguous_format);  slice_26666 = None
        view_1626: "f32[32, 16]" = torch.ops.aten.view.default(clone_811, [32, 16]);  clone_811 = None
        mm_808: "f32[32, 8]" = torch.ops.aten.mm.default(view_1626, slice_7)
        view_1627: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_808, [2, 16, 8]);  mm_808 = None
        slice_26673: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4846, 1, 6464, 6480)
        slice_26674: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26673, 2, 0, 16)
        add_810: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26674, view_1627);  slice_26674 = view_1627 = None
        slice_scatter_4848: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26673, add_810, 2, 0, 16);  slice_26673 = add_810 = None
        slice_scatter_4849: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4846, slice_scatter_4848, 1, 6464, 6480);  slice_scatter_4846 = slice_scatter_4848 = None
        slice_26678: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4849, 1, 6464, 6480)
        slice_26679: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26678, 2, 0, 16)
        slice_scatter_4851: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26678, slice_26679, 2, 0, 16);  slice_26678 = slice_26679 = None
        slice_scatter_4852: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4849, slice_scatter_4851, 1, 6464, 6480);  slice_scatter_4849 = slice_scatter_4851 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26699: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_26665, 2, 16, 32);  slice_26665 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_812: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_26699, memory_format = torch.contiguous_format);  slice_26699 = None
        view_1628: "f32[32, 11]" = torch.ops.aten.view.default(clone_812, [32, 11]);  clone_812 = None
        mm_809: "f32[32, 8]" = torch.ops.aten.mm.default(view_1628, slice_37)
        view_1629: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_809, [2, 16, 8]);  mm_809 = None
        slice_26706: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4852, 1, 6464, 6480)
        slice_26707: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26706, 2, 0, 16)
        add_811: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26707, view_1629);  slice_26707 = view_1629 = None
        slice_scatter_4854: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26706, add_811, 2, 0, 16);  slice_26706 = add_811 = None
        slice_scatter_4855: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4852, slice_scatter_4854, 1, 6464, 6480);  slice_scatter_4852 = slice_scatter_4854 = None
        slice_26711: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4855, 1, 6464, 6480)
        slice_26712: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26711, 2, 0, 16)
        slice_scatter_4857: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26711, slice_26712, 2, 0, 16);  slice_26711 = slice_26712 = None
        slice_scatter_4858: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4855, slice_scatter_4857, 1, 6464, 6480);  slice_scatter_4855 = slice_scatter_4857 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26731: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6480, 6496)
        slice_26732: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_26731, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_813: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_26732, memory_format = torch.contiguous_format);  slice_26732 = None
        view_1630: "f32[32, 16]" = torch.ops.aten.view.default(clone_813, [32, 16]);  clone_813 = None
        mm_810: "f32[32, 8]" = torch.ops.aten.mm.default(view_1630, slice_7)
        view_1631: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_810, [2, 16, 8]);  mm_810 = None
        slice_26739: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4858, 1, 6480, 6496)
        slice_26740: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26739, 2, 0, 16)
        add_812: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26740, view_1631);  slice_26740 = view_1631 = None
        slice_scatter_4860: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26739, add_812, 2, 0, 16);  slice_26739 = add_812 = None
        slice_scatter_4861: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4858, slice_scatter_4860, 1, 6480, 6496);  slice_scatter_4858 = slice_scatter_4860 = None
        slice_26744: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4861, 1, 6480, 6496)
        slice_26745: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26744, 2, 0, 16)
        slice_scatter_4863: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26744, slice_26745, 2, 0, 16);  slice_26744 = slice_26745 = None
        slice_scatter_4864: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4861, slice_scatter_4863, 1, 6480, 6496);  slice_scatter_4861 = slice_scatter_4863 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26765: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_26731, 2, 16, 32);  slice_26731 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_814: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_26765, memory_format = torch.contiguous_format);  slice_26765 = None
        view_1632: "f32[32, 11]" = torch.ops.aten.view.default(clone_814, [32, 11]);  clone_814 = None
        mm_811: "f32[32, 8]" = torch.ops.aten.mm.default(view_1632, slice_37)
        view_1633: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_811, [2, 16, 8]);  mm_811 = None
        slice_26772: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4864, 1, 6480, 6496)
        slice_26773: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26772, 2, 0, 16)
        add_813: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26773, view_1633);  slice_26773 = view_1633 = None
        slice_scatter_4866: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26772, add_813, 2, 0, 16);  slice_26772 = add_813 = None
        slice_scatter_4867: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4864, slice_scatter_4866, 1, 6480, 6496);  slice_scatter_4864 = slice_scatter_4866 = None
        slice_26777: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4867, 1, 6480, 6496)
        slice_26778: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26777, 2, 0, 16)
        slice_scatter_4869: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26777, slice_26778, 2, 0, 16);  slice_26777 = slice_26778 = None
        slice_scatter_4870: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4867, slice_scatter_4869, 1, 6480, 6496);  slice_scatter_4867 = slice_scatter_4869 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26797: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6496, 6512)
        slice_26798: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_26797, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_815: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_26798, memory_format = torch.contiguous_format);  slice_26798 = None
        view_1634: "f32[32, 16]" = torch.ops.aten.view.default(clone_815, [32, 16]);  clone_815 = None
        mm_812: "f32[32, 8]" = torch.ops.aten.mm.default(view_1634, slice_7)
        view_1635: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_812, [2, 16, 8]);  mm_812 = None
        slice_26805: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4870, 1, 6496, 6512)
        slice_26806: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26805, 2, 0, 16)
        add_814: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26806, view_1635);  slice_26806 = view_1635 = None
        slice_scatter_4872: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26805, add_814, 2, 0, 16);  slice_26805 = add_814 = None
        slice_scatter_4873: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4870, slice_scatter_4872, 1, 6496, 6512);  slice_scatter_4870 = slice_scatter_4872 = None
        slice_26810: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4873, 1, 6496, 6512)
        slice_26811: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26810, 2, 0, 16)
        slice_scatter_4875: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26810, slice_26811, 2, 0, 16);  slice_26810 = slice_26811 = None
        slice_scatter_4876: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4873, slice_scatter_4875, 1, 6496, 6512);  slice_scatter_4873 = slice_scatter_4875 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26831: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_26797, 2, 16, 32);  slice_26797 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_816: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_26831, memory_format = torch.contiguous_format);  slice_26831 = None
        view_1636: "f32[32, 11]" = torch.ops.aten.view.default(clone_816, [32, 11]);  clone_816 = None
        mm_813: "f32[32, 8]" = torch.ops.aten.mm.default(view_1636, slice_37)
        view_1637: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_813, [2, 16, 8]);  mm_813 = None
        slice_26838: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4876, 1, 6496, 6512)
        slice_26839: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26838, 2, 0, 16)
        add_815: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26839, view_1637);  slice_26839 = view_1637 = None
        slice_scatter_4878: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26838, add_815, 2, 0, 16);  slice_26838 = add_815 = None
        slice_scatter_4879: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4876, slice_scatter_4878, 1, 6496, 6512);  slice_scatter_4876 = slice_scatter_4878 = None
        slice_26843: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4879, 1, 6496, 6512)
        slice_26844: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26843, 2, 0, 16)
        slice_scatter_4881: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26843, slice_26844, 2, 0, 16);  slice_26843 = slice_26844 = None
        slice_scatter_4882: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4879, slice_scatter_4881, 1, 6496, 6512);  slice_scatter_4879 = slice_scatter_4881 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26863: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6512, 6528)
        slice_26864: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_26863, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_817: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_26864, memory_format = torch.contiguous_format);  slice_26864 = None
        view_1638: "f32[32, 16]" = torch.ops.aten.view.default(clone_817, [32, 16]);  clone_817 = None
        mm_814: "f32[32, 8]" = torch.ops.aten.mm.default(view_1638, slice_7)
        view_1639: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_814, [2, 16, 8]);  mm_814 = None
        slice_26871: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4882, 1, 6512, 6528)
        slice_26872: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26871, 2, 0, 16)
        add_816: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26872, view_1639);  slice_26872 = view_1639 = None
        slice_scatter_4884: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26871, add_816, 2, 0, 16);  slice_26871 = add_816 = None
        slice_scatter_4885: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4882, slice_scatter_4884, 1, 6512, 6528);  slice_scatter_4882 = slice_scatter_4884 = None
        slice_26876: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4885, 1, 6512, 6528)
        slice_26877: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26876, 2, 0, 16)
        slice_scatter_4887: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26876, slice_26877, 2, 0, 16);  slice_26876 = slice_26877 = None
        slice_scatter_4888: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4885, slice_scatter_4887, 1, 6512, 6528);  slice_scatter_4885 = slice_scatter_4887 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26897: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_26863, 2, 16, 32);  slice_26863 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_818: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_26897, memory_format = torch.contiguous_format);  slice_26897 = None
        view_1640: "f32[32, 11]" = torch.ops.aten.view.default(clone_818, [32, 11]);  clone_818 = None
        mm_815: "f32[32, 8]" = torch.ops.aten.mm.default(view_1640, slice_37)
        view_1641: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_815, [2, 16, 8]);  mm_815 = None
        slice_26904: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4888, 1, 6512, 6528)
        slice_26905: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26904, 2, 0, 16)
        add_817: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26905, view_1641);  slice_26905 = view_1641 = None
        slice_scatter_4890: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26904, add_817, 2, 0, 16);  slice_26904 = add_817 = None
        slice_scatter_4891: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4888, slice_scatter_4890, 1, 6512, 6528);  slice_scatter_4888 = slice_scatter_4890 = None
        slice_26909: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4891, 1, 6512, 6528)
        slice_26910: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26909, 2, 0, 16)
        slice_scatter_4893: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26909, slice_26910, 2, 0, 16);  slice_26909 = slice_26910 = None
        slice_scatter_4894: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4891, slice_scatter_4893, 1, 6512, 6528);  slice_scatter_4891 = slice_scatter_4893 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26929: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6528, 6544)
        slice_26930: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_26929, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_819: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_26930, memory_format = torch.contiguous_format);  slice_26930 = None
        view_1642: "f32[32, 16]" = torch.ops.aten.view.default(clone_819, [32, 16]);  clone_819 = None
        mm_816: "f32[32, 8]" = torch.ops.aten.mm.default(view_1642, slice_7)
        view_1643: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_816, [2, 16, 8]);  mm_816 = None
        slice_26937: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4894, 1, 6528, 6544)
        slice_26938: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26937, 2, 0, 16)
        add_818: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26938, view_1643);  slice_26938 = view_1643 = None
        slice_scatter_4896: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26937, add_818, 2, 0, 16);  slice_26937 = add_818 = None
        slice_scatter_4897: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4894, slice_scatter_4896, 1, 6528, 6544);  slice_scatter_4894 = slice_scatter_4896 = None
        slice_26942: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4897, 1, 6528, 6544)
        slice_26943: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26942, 2, 0, 16)
        slice_scatter_4899: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26942, slice_26943, 2, 0, 16);  slice_26942 = slice_26943 = None
        slice_scatter_4900: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4897, slice_scatter_4899, 1, 6528, 6544);  slice_scatter_4897 = slice_scatter_4899 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26963: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_26929, 2, 16, 32);  slice_26929 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_820: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_26963, memory_format = torch.contiguous_format);  slice_26963 = None
        view_1644: "f32[32, 11]" = torch.ops.aten.view.default(clone_820, [32, 11]);  clone_820 = None
        mm_817: "f32[32, 8]" = torch.ops.aten.mm.default(view_1644, slice_37)
        view_1645: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_817, [2, 16, 8]);  mm_817 = None
        slice_26970: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4900, 1, 6528, 6544)
        slice_26971: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26970, 2, 0, 16)
        add_819: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_26971, view_1645);  slice_26971 = view_1645 = None
        slice_scatter_4902: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26970, add_819, 2, 0, 16);  slice_26970 = add_819 = None
        slice_scatter_4903: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4900, slice_scatter_4902, 1, 6528, 6544);  slice_scatter_4900 = slice_scatter_4902 = None
        slice_26975: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4903, 1, 6528, 6544)
        slice_26976: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_26975, 2, 0, 16)
        slice_scatter_4905: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_26975, slice_26976, 2, 0, 16);  slice_26975 = slice_26976 = None
        slice_scatter_4906: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4903, slice_scatter_4905, 1, 6528, 6544);  slice_scatter_4903 = slice_scatter_4905 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_26995: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6544, 6560)
        slice_26996: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_26995, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_821: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_26996, memory_format = torch.contiguous_format);  slice_26996 = None
        view_1646: "f32[32, 16]" = torch.ops.aten.view.default(clone_821, [32, 16]);  clone_821 = None
        mm_818: "f32[32, 8]" = torch.ops.aten.mm.default(view_1646, slice_7)
        view_1647: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_818, [2, 16, 8]);  mm_818 = None
        slice_27003: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4906, 1, 6544, 6560)
        slice_27004: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27003, 2, 0, 16)
        add_820: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27004, view_1647);  slice_27004 = view_1647 = None
        slice_scatter_4908: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27003, add_820, 2, 0, 16);  slice_27003 = add_820 = None
        slice_scatter_4909: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4906, slice_scatter_4908, 1, 6544, 6560);  slice_scatter_4906 = slice_scatter_4908 = None
        slice_27008: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4909, 1, 6544, 6560)
        slice_27009: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27008, 2, 0, 16)
        slice_scatter_4911: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27008, slice_27009, 2, 0, 16);  slice_27008 = slice_27009 = None
        slice_scatter_4912: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4909, slice_scatter_4911, 1, 6544, 6560);  slice_scatter_4909 = slice_scatter_4911 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27029: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_26995, 2, 16, 32);  slice_26995 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_822: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_27029, memory_format = torch.contiguous_format);  slice_27029 = None
        view_1648: "f32[32, 11]" = torch.ops.aten.view.default(clone_822, [32, 11]);  clone_822 = None
        mm_819: "f32[32, 8]" = torch.ops.aten.mm.default(view_1648, slice_37)
        view_1649: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_819, [2, 16, 8]);  mm_819 = None
        slice_27036: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4912, 1, 6544, 6560)
        slice_27037: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27036, 2, 0, 16)
        add_821: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27037, view_1649);  slice_27037 = view_1649 = None
        slice_scatter_4914: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27036, add_821, 2, 0, 16);  slice_27036 = add_821 = None
        slice_scatter_4915: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4912, slice_scatter_4914, 1, 6544, 6560);  slice_scatter_4912 = slice_scatter_4914 = None
        slice_27041: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4915, 1, 6544, 6560)
        slice_27042: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27041, 2, 0, 16)
        slice_scatter_4917: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27041, slice_27042, 2, 0, 16);  slice_27041 = slice_27042 = None
        slice_scatter_4918: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4915, slice_scatter_4917, 1, 6544, 6560);  slice_scatter_4915 = slice_scatter_4917 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27061: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6560, 6576)
        slice_27062: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_27061, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_823: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_27062, memory_format = torch.contiguous_format);  slice_27062 = None
        view_1650: "f32[32, 16]" = torch.ops.aten.view.default(clone_823, [32, 16]);  clone_823 = None
        mm_820: "f32[32, 8]" = torch.ops.aten.mm.default(view_1650, slice_7)
        view_1651: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_820, [2, 16, 8]);  mm_820 = None
        slice_27069: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4918, 1, 6560, 6576)
        slice_27070: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27069, 2, 0, 16)
        add_822: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27070, view_1651);  slice_27070 = view_1651 = None
        slice_scatter_4920: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27069, add_822, 2, 0, 16);  slice_27069 = add_822 = None
        slice_scatter_4921: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4918, slice_scatter_4920, 1, 6560, 6576);  slice_scatter_4918 = slice_scatter_4920 = None
        slice_27074: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4921, 1, 6560, 6576)
        slice_27075: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27074, 2, 0, 16)
        slice_scatter_4923: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27074, slice_27075, 2, 0, 16);  slice_27074 = slice_27075 = None
        slice_scatter_4924: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4921, slice_scatter_4923, 1, 6560, 6576);  slice_scatter_4921 = slice_scatter_4923 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27095: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_27061, 2, 16, 32);  slice_27061 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_824: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_27095, memory_format = torch.contiguous_format);  slice_27095 = None
        view_1652: "f32[32, 11]" = torch.ops.aten.view.default(clone_824, [32, 11]);  clone_824 = None
        mm_821: "f32[32, 8]" = torch.ops.aten.mm.default(view_1652, slice_37)
        view_1653: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_821, [2, 16, 8]);  mm_821 = None
        slice_27102: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4924, 1, 6560, 6576)
        slice_27103: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27102, 2, 0, 16)
        add_823: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27103, view_1653);  slice_27103 = view_1653 = None
        slice_scatter_4926: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27102, add_823, 2, 0, 16);  slice_27102 = add_823 = None
        slice_scatter_4927: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4924, slice_scatter_4926, 1, 6560, 6576);  slice_scatter_4924 = slice_scatter_4926 = None
        slice_27107: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4927, 1, 6560, 6576)
        slice_27108: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27107, 2, 0, 16)
        slice_scatter_4929: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27107, slice_27108, 2, 0, 16);  slice_27107 = slice_27108 = None
        slice_scatter_4930: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4927, slice_scatter_4929, 1, 6560, 6576);  slice_scatter_4927 = slice_scatter_4929 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27127: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6576, 6592)
        slice_27128: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_27127, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_825: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_27128, memory_format = torch.contiguous_format);  slice_27128 = None
        view_1654: "f32[32, 16]" = torch.ops.aten.view.default(clone_825, [32, 16]);  clone_825 = None
        mm_822: "f32[32, 8]" = torch.ops.aten.mm.default(view_1654, slice_7)
        view_1655: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_822, [2, 16, 8]);  mm_822 = None
        slice_27135: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4930, 1, 6576, 6592)
        slice_27136: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27135, 2, 0, 16)
        add_824: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27136, view_1655);  slice_27136 = view_1655 = None
        slice_scatter_4932: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27135, add_824, 2, 0, 16);  slice_27135 = add_824 = None
        slice_scatter_4933: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4930, slice_scatter_4932, 1, 6576, 6592);  slice_scatter_4930 = slice_scatter_4932 = None
        slice_27140: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4933, 1, 6576, 6592)
        slice_27141: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27140, 2, 0, 16)
        slice_scatter_4935: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27140, slice_27141, 2, 0, 16);  slice_27140 = slice_27141 = None
        slice_scatter_4936: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4933, slice_scatter_4935, 1, 6576, 6592);  slice_scatter_4933 = slice_scatter_4935 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27161: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_27127, 2, 16, 32);  slice_27127 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_826: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_27161, memory_format = torch.contiguous_format);  slice_27161 = None
        view_1656: "f32[32, 11]" = torch.ops.aten.view.default(clone_826, [32, 11]);  clone_826 = None
        mm_823: "f32[32, 8]" = torch.ops.aten.mm.default(view_1656, slice_37)
        view_1657: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_823, [2, 16, 8]);  mm_823 = None
        slice_27168: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4936, 1, 6576, 6592)
        slice_27169: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27168, 2, 0, 16)
        add_825: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27169, view_1657);  slice_27169 = view_1657 = None
        slice_scatter_4938: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27168, add_825, 2, 0, 16);  slice_27168 = add_825 = None
        slice_scatter_4939: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4936, slice_scatter_4938, 1, 6576, 6592);  slice_scatter_4936 = slice_scatter_4938 = None
        slice_27173: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4939, 1, 6576, 6592)
        slice_27174: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27173, 2, 0, 16)
        slice_scatter_4941: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27173, slice_27174, 2, 0, 16);  slice_27173 = slice_27174 = None
        slice_scatter_4942: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4939, slice_scatter_4941, 1, 6576, 6592);  slice_scatter_4939 = slice_scatter_4941 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27193: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6592, 6608)
        slice_27194: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_27193, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_827: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_27194, memory_format = torch.contiguous_format);  slice_27194 = None
        view_1658: "f32[32, 16]" = torch.ops.aten.view.default(clone_827, [32, 16]);  clone_827 = None
        mm_824: "f32[32, 8]" = torch.ops.aten.mm.default(view_1658, slice_7)
        view_1659: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_824, [2, 16, 8]);  mm_824 = None
        slice_27201: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4942, 1, 6592, 6608)
        slice_27202: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27201, 2, 0, 16)
        add_826: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27202, view_1659);  slice_27202 = view_1659 = None
        slice_scatter_4944: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27201, add_826, 2, 0, 16);  slice_27201 = add_826 = None
        slice_scatter_4945: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4942, slice_scatter_4944, 1, 6592, 6608);  slice_scatter_4942 = slice_scatter_4944 = None
        slice_27206: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4945, 1, 6592, 6608)
        slice_27207: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27206, 2, 0, 16)
        slice_scatter_4947: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27206, slice_27207, 2, 0, 16);  slice_27206 = slice_27207 = None
        slice_scatter_4948: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4945, slice_scatter_4947, 1, 6592, 6608);  slice_scatter_4945 = slice_scatter_4947 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27227: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_27193, 2, 16, 32);  slice_27193 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_828: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_27227, memory_format = torch.contiguous_format);  slice_27227 = None
        view_1660: "f32[32, 11]" = torch.ops.aten.view.default(clone_828, [32, 11]);  clone_828 = None
        mm_825: "f32[32, 8]" = torch.ops.aten.mm.default(view_1660, slice_37)
        view_1661: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_825, [2, 16, 8]);  mm_825 = None
        slice_27234: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4948, 1, 6592, 6608)
        slice_27235: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27234, 2, 0, 16)
        add_827: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27235, view_1661);  slice_27235 = view_1661 = None
        slice_scatter_4950: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27234, add_827, 2, 0, 16);  slice_27234 = add_827 = None
        slice_scatter_4951: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4948, slice_scatter_4950, 1, 6592, 6608);  slice_scatter_4948 = slice_scatter_4950 = None
        slice_27239: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4951, 1, 6592, 6608)
        slice_27240: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27239, 2, 0, 16)
        slice_scatter_4953: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27239, slice_27240, 2, 0, 16);  slice_27239 = slice_27240 = None
        slice_scatter_4954: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4951, slice_scatter_4953, 1, 6592, 6608);  slice_scatter_4951 = slice_scatter_4953 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27259: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6608, 6624)
        slice_27260: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_27259, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_829: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_27260, memory_format = torch.contiguous_format);  slice_27260 = None
        view_1662: "f32[32, 16]" = torch.ops.aten.view.default(clone_829, [32, 16]);  clone_829 = None
        mm_826: "f32[32, 8]" = torch.ops.aten.mm.default(view_1662, slice_7)
        view_1663: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_826, [2, 16, 8]);  mm_826 = None
        slice_27267: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4954, 1, 6608, 6624)
        slice_27268: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27267, 2, 0, 16)
        add_828: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27268, view_1663);  slice_27268 = view_1663 = None
        slice_scatter_4956: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27267, add_828, 2, 0, 16);  slice_27267 = add_828 = None
        slice_scatter_4957: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4954, slice_scatter_4956, 1, 6608, 6624);  slice_scatter_4954 = slice_scatter_4956 = None
        slice_27272: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4957, 1, 6608, 6624)
        slice_27273: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27272, 2, 0, 16)
        slice_scatter_4959: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27272, slice_27273, 2, 0, 16);  slice_27272 = slice_27273 = None
        slice_scatter_4960: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4957, slice_scatter_4959, 1, 6608, 6624);  slice_scatter_4957 = slice_scatter_4959 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27293: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_27259, 2, 16, 32);  slice_27259 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_830: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_27293, memory_format = torch.contiguous_format);  slice_27293 = None
        view_1664: "f32[32, 11]" = torch.ops.aten.view.default(clone_830, [32, 11]);  clone_830 = None
        mm_827: "f32[32, 8]" = torch.ops.aten.mm.default(view_1664, slice_37)
        view_1665: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_827, [2, 16, 8]);  mm_827 = None
        slice_27300: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4960, 1, 6608, 6624)
        slice_27301: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27300, 2, 0, 16)
        add_829: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27301, view_1665);  slice_27301 = view_1665 = None
        slice_scatter_4962: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27300, add_829, 2, 0, 16);  slice_27300 = add_829 = None
        slice_scatter_4963: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4960, slice_scatter_4962, 1, 6608, 6624);  slice_scatter_4960 = slice_scatter_4962 = None
        slice_27305: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4963, 1, 6608, 6624)
        slice_27306: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27305, 2, 0, 16)
        slice_scatter_4965: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27305, slice_27306, 2, 0, 16);  slice_27305 = slice_27306 = None
        slice_scatter_4966: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4963, slice_scatter_4965, 1, 6608, 6624);  slice_scatter_4963 = slice_scatter_4965 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27325: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6624, 6640)
        slice_27326: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_27325, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_831: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_27326, memory_format = torch.contiguous_format);  slice_27326 = None
        view_1666: "f32[32, 16]" = torch.ops.aten.view.default(clone_831, [32, 16]);  clone_831 = None
        mm_828: "f32[32, 8]" = torch.ops.aten.mm.default(view_1666, slice_7)
        view_1667: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_828, [2, 16, 8]);  mm_828 = None
        slice_27333: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4966, 1, 6624, 6640)
        slice_27334: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27333, 2, 0, 16)
        add_830: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27334, view_1667);  slice_27334 = view_1667 = None
        slice_scatter_4968: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27333, add_830, 2, 0, 16);  slice_27333 = add_830 = None
        slice_scatter_4969: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4966, slice_scatter_4968, 1, 6624, 6640);  slice_scatter_4966 = slice_scatter_4968 = None
        slice_27338: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4969, 1, 6624, 6640)
        slice_27339: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27338, 2, 0, 16)
        slice_scatter_4971: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27338, slice_27339, 2, 0, 16);  slice_27338 = slice_27339 = None
        slice_scatter_4972: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4969, slice_scatter_4971, 1, 6624, 6640);  slice_scatter_4969 = slice_scatter_4971 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27359: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_27325, 2, 16, 32);  slice_27325 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_832: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_27359, memory_format = torch.contiguous_format);  slice_27359 = None
        view_1668: "f32[32, 11]" = torch.ops.aten.view.default(clone_832, [32, 11]);  clone_832 = None
        mm_829: "f32[32, 8]" = torch.ops.aten.mm.default(view_1668, slice_37)
        view_1669: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_829, [2, 16, 8]);  mm_829 = None
        slice_27366: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4972, 1, 6624, 6640)
        slice_27367: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27366, 2, 0, 16)
        add_831: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27367, view_1669);  slice_27367 = view_1669 = None
        slice_scatter_4974: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27366, add_831, 2, 0, 16);  slice_27366 = add_831 = None
        slice_scatter_4975: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4972, slice_scatter_4974, 1, 6624, 6640);  slice_scatter_4972 = slice_scatter_4974 = None
        slice_27371: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4975, 1, 6624, 6640)
        slice_27372: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27371, 2, 0, 16)
        slice_scatter_4977: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27371, slice_27372, 2, 0, 16);  slice_27371 = slice_27372 = None
        slice_scatter_4978: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4975, slice_scatter_4977, 1, 6624, 6640);  slice_scatter_4975 = slice_scatter_4977 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27391: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6640, 6656)
        slice_27392: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_27391, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_833: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_27392, memory_format = torch.contiguous_format);  slice_27392 = None
        view_1670: "f32[32, 16]" = torch.ops.aten.view.default(clone_833, [32, 16]);  clone_833 = None
        mm_830: "f32[32, 8]" = torch.ops.aten.mm.default(view_1670, slice_7)
        view_1671: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_830, [2, 16, 8]);  mm_830 = None
        slice_27399: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4978, 1, 6640, 6656)
        slice_27400: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27399, 2, 0, 16)
        add_832: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27400, view_1671);  slice_27400 = view_1671 = None
        slice_scatter_4980: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27399, add_832, 2, 0, 16);  slice_27399 = add_832 = None
        slice_scatter_4981: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4978, slice_scatter_4980, 1, 6640, 6656);  slice_scatter_4978 = slice_scatter_4980 = None
        slice_27404: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4981, 1, 6640, 6656)
        slice_27405: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27404, 2, 0, 16)
        slice_scatter_4983: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27404, slice_27405, 2, 0, 16);  slice_27404 = slice_27405 = None
        slice_scatter_4984: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4981, slice_scatter_4983, 1, 6640, 6656);  slice_scatter_4981 = slice_scatter_4983 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27425: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_27391, 2, 16, 32);  slice_27391 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_834: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_27425, memory_format = torch.contiguous_format);  slice_27425 = None
        view_1672: "f32[32, 11]" = torch.ops.aten.view.default(clone_834, [32, 11]);  clone_834 = None
        mm_831: "f32[32, 8]" = torch.ops.aten.mm.default(view_1672, slice_37)
        view_1673: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_831, [2, 16, 8]);  mm_831 = None
        slice_27432: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4984, 1, 6640, 6656)
        slice_27433: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27432, 2, 0, 16)
        add_833: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27433, view_1673);  slice_27433 = view_1673 = None
        slice_scatter_4986: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27432, add_833, 2, 0, 16);  slice_27432 = add_833 = None
        slice_scatter_4987: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4984, slice_scatter_4986, 1, 6640, 6656);  slice_scatter_4984 = slice_scatter_4986 = None
        slice_27437: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4987, 1, 6640, 6656)
        slice_27438: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27437, 2, 0, 16)
        slice_scatter_4989: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27437, slice_27438, 2, 0, 16);  slice_27437 = slice_27438 = None
        slice_scatter_4990: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4987, slice_scatter_4989, 1, 6640, 6656);  slice_scatter_4987 = slice_scatter_4989 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27457: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6656, 6672)
        slice_27458: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_27457, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_835: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_27458, memory_format = torch.contiguous_format);  slice_27458 = None
        view_1674: "f32[32, 16]" = torch.ops.aten.view.default(clone_835, [32, 16]);  clone_835 = None
        mm_832: "f32[32, 8]" = torch.ops.aten.mm.default(view_1674, slice_7)
        view_1675: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_832, [2, 16, 8]);  mm_832 = None
        slice_27465: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4990, 1, 6656, 6672)
        slice_27466: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27465, 2, 0, 16)
        add_834: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27466, view_1675);  slice_27466 = view_1675 = None
        slice_scatter_4992: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27465, add_834, 2, 0, 16);  slice_27465 = add_834 = None
        slice_scatter_4993: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4990, slice_scatter_4992, 1, 6656, 6672);  slice_scatter_4990 = slice_scatter_4992 = None
        slice_27470: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4993, 1, 6656, 6672)
        slice_27471: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27470, 2, 0, 16)
        slice_scatter_4995: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27470, slice_27471, 2, 0, 16);  slice_27470 = slice_27471 = None
        slice_scatter_4996: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4993, slice_scatter_4995, 1, 6656, 6672);  slice_scatter_4993 = slice_scatter_4995 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27491: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_27457, 2, 16, 32);  slice_27457 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_836: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_27491, memory_format = torch.contiguous_format);  slice_27491 = None
        view_1676: "f32[32, 11]" = torch.ops.aten.view.default(clone_836, [32, 11]);  clone_836 = None
        mm_833: "f32[32, 8]" = torch.ops.aten.mm.default(view_1676, slice_37)
        view_1677: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_833, [2, 16, 8]);  mm_833 = None
        slice_27498: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4996, 1, 6656, 6672)
        slice_27499: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27498, 2, 0, 16)
        add_835: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27499, view_1677);  slice_27499 = view_1677 = None
        slice_scatter_4998: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27498, add_835, 2, 0, 16);  slice_27498 = add_835 = None
        slice_scatter_4999: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4996, slice_scatter_4998, 1, 6656, 6672);  slice_scatter_4996 = slice_scatter_4998 = None
        slice_27503: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_4999, 1, 6656, 6672)
        slice_27504: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27503, 2, 0, 16)
        slice_scatter_5001: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27503, slice_27504, 2, 0, 16);  slice_27503 = slice_27504 = None
        slice_scatter_5002: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_4999, slice_scatter_5001, 1, 6656, 6672);  slice_scatter_4999 = slice_scatter_5001 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27523: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6672, 6688)
        slice_27524: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_27523, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_837: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_27524, memory_format = torch.contiguous_format);  slice_27524 = None
        view_1678: "f32[32, 16]" = torch.ops.aten.view.default(clone_837, [32, 16]);  clone_837 = None
        mm_834: "f32[32, 8]" = torch.ops.aten.mm.default(view_1678, slice_7)
        view_1679: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_834, [2, 16, 8]);  mm_834 = None
        slice_27531: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5002, 1, 6672, 6688)
        slice_27532: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27531, 2, 0, 16)
        add_836: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27532, view_1679);  slice_27532 = view_1679 = None
        slice_scatter_5004: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27531, add_836, 2, 0, 16);  slice_27531 = add_836 = None
        slice_scatter_5005: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5002, slice_scatter_5004, 1, 6672, 6688);  slice_scatter_5002 = slice_scatter_5004 = None
        slice_27536: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5005, 1, 6672, 6688)
        slice_27537: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27536, 2, 0, 16)
        slice_scatter_5007: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27536, slice_27537, 2, 0, 16);  slice_27536 = slice_27537 = None
        slice_scatter_5008: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5005, slice_scatter_5007, 1, 6672, 6688);  slice_scatter_5005 = slice_scatter_5007 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27557: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_27523, 2, 16, 32);  slice_27523 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_838: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_27557, memory_format = torch.contiguous_format);  slice_27557 = None
        view_1680: "f32[32, 11]" = torch.ops.aten.view.default(clone_838, [32, 11]);  clone_838 = None
        mm_835: "f32[32, 8]" = torch.ops.aten.mm.default(view_1680, slice_37)
        view_1681: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_835, [2, 16, 8]);  mm_835 = None
        slice_27564: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5008, 1, 6672, 6688)
        slice_27565: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27564, 2, 0, 16)
        add_837: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27565, view_1681);  slice_27565 = view_1681 = None
        slice_scatter_5010: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27564, add_837, 2, 0, 16);  slice_27564 = add_837 = None
        slice_scatter_5011: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5008, slice_scatter_5010, 1, 6672, 6688);  slice_scatter_5008 = slice_scatter_5010 = None
        slice_27569: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5011, 1, 6672, 6688)
        slice_27570: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27569, 2, 0, 16)
        slice_scatter_5013: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27569, slice_27570, 2, 0, 16);  slice_27569 = slice_27570 = None
        slice_scatter_5014: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5011, slice_scatter_5013, 1, 6672, 6688);  slice_scatter_5011 = slice_scatter_5013 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27589: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6688, 6704)
        slice_27590: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_27589, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_839: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_27590, memory_format = torch.contiguous_format);  slice_27590 = None
        view_1682: "f32[32, 16]" = torch.ops.aten.view.default(clone_839, [32, 16]);  clone_839 = None
        mm_836: "f32[32, 8]" = torch.ops.aten.mm.default(view_1682, slice_7)
        view_1683: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_836, [2, 16, 8]);  mm_836 = None
        slice_27597: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5014, 1, 6688, 6704)
        slice_27598: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27597, 2, 0, 16)
        add_838: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27598, view_1683);  slice_27598 = view_1683 = None
        slice_scatter_5016: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27597, add_838, 2, 0, 16);  slice_27597 = add_838 = None
        slice_scatter_5017: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5014, slice_scatter_5016, 1, 6688, 6704);  slice_scatter_5014 = slice_scatter_5016 = None
        slice_27602: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5017, 1, 6688, 6704)
        slice_27603: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27602, 2, 0, 16)
        slice_scatter_5019: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27602, slice_27603, 2, 0, 16);  slice_27602 = slice_27603 = None
        slice_scatter_5020: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5017, slice_scatter_5019, 1, 6688, 6704);  slice_scatter_5017 = slice_scatter_5019 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27623: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_27589, 2, 16, 32);  slice_27589 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_840: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_27623, memory_format = torch.contiguous_format);  slice_27623 = None
        view_1684: "f32[32, 11]" = torch.ops.aten.view.default(clone_840, [32, 11]);  clone_840 = None
        mm_837: "f32[32, 8]" = torch.ops.aten.mm.default(view_1684, slice_37)
        view_1685: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_837, [2, 16, 8]);  mm_837 = None
        slice_27630: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5020, 1, 6688, 6704)
        slice_27631: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27630, 2, 0, 16)
        add_839: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27631, view_1685);  slice_27631 = view_1685 = None
        slice_scatter_5022: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27630, add_839, 2, 0, 16);  slice_27630 = add_839 = None
        slice_scatter_5023: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5020, slice_scatter_5022, 1, 6688, 6704);  slice_scatter_5020 = slice_scatter_5022 = None
        slice_27635: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5023, 1, 6688, 6704)
        slice_27636: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27635, 2, 0, 16)
        slice_scatter_5025: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27635, slice_27636, 2, 0, 16);  slice_27635 = slice_27636 = None
        slice_scatter_5026: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5023, slice_scatter_5025, 1, 6688, 6704);  slice_scatter_5023 = slice_scatter_5025 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27655: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6704, 6720)
        slice_27656: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_27655, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_841: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_27656, memory_format = torch.contiguous_format);  slice_27656 = None
        view_1686: "f32[32, 16]" = torch.ops.aten.view.default(clone_841, [32, 16]);  clone_841 = None
        mm_838: "f32[32, 8]" = torch.ops.aten.mm.default(view_1686, slice_7)
        view_1687: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_838, [2, 16, 8]);  mm_838 = None
        slice_27663: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5026, 1, 6704, 6720)
        slice_27664: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27663, 2, 0, 16)
        add_840: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27664, view_1687);  slice_27664 = view_1687 = None
        slice_scatter_5028: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27663, add_840, 2, 0, 16);  slice_27663 = add_840 = None
        slice_scatter_5029: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5026, slice_scatter_5028, 1, 6704, 6720);  slice_scatter_5026 = slice_scatter_5028 = None
        slice_27668: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5029, 1, 6704, 6720)
        slice_27669: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27668, 2, 0, 16)
        slice_scatter_5031: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27668, slice_27669, 2, 0, 16);  slice_27668 = slice_27669 = None
        slice_scatter_5032: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5029, slice_scatter_5031, 1, 6704, 6720);  slice_scatter_5029 = slice_scatter_5031 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27689: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_27655, 2, 16, 32);  slice_27655 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_842: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_27689, memory_format = torch.contiguous_format);  slice_27689 = None
        view_1688: "f32[32, 11]" = torch.ops.aten.view.default(clone_842, [32, 11]);  clone_842 = None
        mm_839: "f32[32, 8]" = torch.ops.aten.mm.default(view_1688, slice_37)
        view_1689: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_839, [2, 16, 8]);  mm_839 = None
        slice_27696: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5032, 1, 6704, 6720)
        slice_27697: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27696, 2, 0, 16)
        add_841: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27697, view_1689);  slice_27697 = view_1689 = None
        slice_scatter_5034: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27696, add_841, 2, 0, 16);  slice_27696 = add_841 = None
        slice_scatter_5035: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5032, slice_scatter_5034, 1, 6704, 6720);  slice_scatter_5032 = slice_scatter_5034 = None
        slice_27701: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5035, 1, 6704, 6720)
        slice_27702: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27701, 2, 0, 16)
        slice_scatter_5037: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27701, slice_27702, 2, 0, 16);  slice_27701 = slice_27702 = None
        slice_scatter_5038: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5035, slice_scatter_5037, 1, 6704, 6720);  slice_scatter_5035 = slice_scatter_5037 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27721: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6720, 6736)
        slice_27722: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_27721, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_843: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_27722, memory_format = torch.contiguous_format);  slice_27722 = None
        view_1690: "f32[32, 16]" = torch.ops.aten.view.default(clone_843, [32, 16]);  clone_843 = None
        mm_840: "f32[32, 8]" = torch.ops.aten.mm.default(view_1690, slice_7)
        view_1691: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_840, [2, 16, 8]);  mm_840 = None
        slice_27729: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5038, 1, 6720, 6736)
        slice_27730: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27729, 2, 0, 16)
        add_842: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27730, view_1691);  slice_27730 = view_1691 = None
        slice_scatter_5040: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27729, add_842, 2, 0, 16);  slice_27729 = add_842 = None
        slice_scatter_5041: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5038, slice_scatter_5040, 1, 6720, 6736);  slice_scatter_5038 = slice_scatter_5040 = None
        slice_27734: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5041, 1, 6720, 6736)
        slice_27735: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27734, 2, 0, 16)
        slice_scatter_5043: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27734, slice_27735, 2, 0, 16);  slice_27734 = slice_27735 = None
        slice_scatter_5044: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5041, slice_scatter_5043, 1, 6720, 6736);  slice_scatter_5041 = slice_scatter_5043 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27755: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_27721, 2, 16, 32);  slice_27721 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_844: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_27755, memory_format = torch.contiguous_format);  slice_27755 = None
        view_1692: "f32[32, 11]" = torch.ops.aten.view.default(clone_844, [32, 11]);  clone_844 = None
        mm_841: "f32[32, 8]" = torch.ops.aten.mm.default(view_1692, slice_37)
        view_1693: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_841, [2, 16, 8]);  mm_841 = None
        slice_27762: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5044, 1, 6720, 6736)
        slice_27763: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27762, 2, 0, 16)
        add_843: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27763, view_1693);  slice_27763 = view_1693 = None
        slice_scatter_5046: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27762, add_843, 2, 0, 16);  slice_27762 = add_843 = None
        slice_scatter_5047: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5044, slice_scatter_5046, 1, 6720, 6736);  slice_scatter_5044 = slice_scatter_5046 = None
        slice_27767: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5047, 1, 6720, 6736)
        slice_27768: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27767, 2, 0, 16)
        slice_scatter_5049: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27767, slice_27768, 2, 0, 16);  slice_27767 = slice_27768 = None
        slice_scatter_5050: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5047, slice_scatter_5049, 1, 6720, 6736);  slice_scatter_5047 = slice_scatter_5049 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27787: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6736, 6752)
        slice_27788: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_27787, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_845: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_27788, memory_format = torch.contiguous_format);  slice_27788 = None
        view_1694: "f32[32, 16]" = torch.ops.aten.view.default(clone_845, [32, 16]);  clone_845 = None
        mm_842: "f32[32, 8]" = torch.ops.aten.mm.default(view_1694, slice_7)
        view_1695: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_842, [2, 16, 8]);  mm_842 = None
        slice_27795: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5050, 1, 6736, 6752)
        slice_27796: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27795, 2, 0, 16)
        add_844: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27796, view_1695);  slice_27796 = view_1695 = None
        slice_scatter_5052: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27795, add_844, 2, 0, 16);  slice_27795 = add_844 = None
        slice_scatter_5053: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5050, slice_scatter_5052, 1, 6736, 6752);  slice_scatter_5050 = slice_scatter_5052 = None
        slice_27800: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5053, 1, 6736, 6752)
        slice_27801: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27800, 2, 0, 16)
        slice_scatter_5055: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27800, slice_27801, 2, 0, 16);  slice_27800 = slice_27801 = None
        slice_scatter_5056: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5053, slice_scatter_5055, 1, 6736, 6752);  slice_scatter_5053 = slice_scatter_5055 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27821: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_27787, 2, 16, 32);  slice_27787 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_846: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_27821, memory_format = torch.contiguous_format);  slice_27821 = None
        view_1696: "f32[32, 11]" = torch.ops.aten.view.default(clone_846, [32, 11]);  clone_846 = None
        mm_843: "f32[32, 8]" = torch.ops.aten.mm.default(view_1696, slice_37)
        view_1697: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_843, [2, 16, 8]);  mm_843 = None
        slice_27828: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5056, 1, 6736, 6752)
        slice_27829: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27828, 2, 0, 16)
        add_845: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27829, view_1697);  slice_27829 = view_1697 = None
        slice_scatter_5058: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27828, add_845, 2, 0, 16);  slice_27828 = add_845 = None
        slice_scatter_5059: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5056, slice_scatter_5058, 1, 6736, 6752);  slice_scatter_5056 = slice_scatter_5058 = None
        slice_27833: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5059, 1, 6736, 6752)
        slice_27834: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27833, 2, 0, 16)
        slice_scatter_5061: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27833, slice_27834, 2, 0, 16);  slice_27833 = slice_27834 = None
        slice_scatter_5062: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5059, slice_scatter_5061, 1, 6736, 6752);  slice_scatter_5059 = slice_scatter_5061 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27853: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6752, 6768)
        slice_27854: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_27853, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_847: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_27854, memory_format = torch.contiguous_format);  slice_27854 = None
        view_1698: "f32[32, 16]" = torch.ops.aten.view.default(clone_847, [32, 16]);  clone_847 = None
        mm_844: "f32[32, 8]" = torch.ops.aten.mm.default(view_1698, slice_7)
        view_1699: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_844, [2, 16, 8]);  mm_844 = None
        slice_27861: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5062, 1, 6752, 6768)
        slice_27862: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27861, 2, 0, 16)
        add_846: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27862, view_1699);  slice_27862 = view_1699 = None
        slice_scatter_5064: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27861, add_846, 2, 0, 16);  slice_27861 = add_846 = None
        slice_scatter_5065: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5062, slice_scatter_5064, 1, 6752, 6768);  slice_scatter_5062 = slice_scatter_5064 = None
        slice_27866: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5065, 1, 6752, 6768)
        slice_27867: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27866, 2, 0, 16)
        slice_scatter_5067: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27866, slice_27867, 2, 0, 16);  slice_27866 = slice_27867 = None
        slice_scatter_5068: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5065, slice_scatter_5067, 1, 6752, 6768);  slice_scatter_5065 = slice_scatter_5067 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27887: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_27853, 2, 16, 32);  slice_27853 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_848: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_27887, memory_format = torch.contiguous_format);  slice_27887 = None
        view_1700: "f32[32, 11]" = torch.ops.aten.view.default(clone_848, [32, 11]);  clone_848 = None
        mm_845: "f32[32, 8]" = torch.ops.aten.mm.default(view_1700, slice_37)
        view_1701: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_845, [2, 16, 8]);  mm_845 = None
        slice_27894: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5068, 1, 6752, 6768)
        slice_27895: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27894, 2, 0, 16)
        add_847: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27895, view_1701);  slice_27895 = view_1701 = None
        slice_scatter_5070: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27894, add_847, 2, 0, 16);  slice_27894 = add_847 = None
        slice_scatter_5071: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5068, slice_scatter_5070, 1, 6752, 6768);  slice_scatter_5068 = slice_scatter_5070 = None
        slice_27899: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5071, 1, 6752, 6768)
        slice_27900: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27899, 2, 0, 16)
        slice_scatter_5073: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27899, slice_27900, 2, 0, 16);  slice_27899 = slice_27900 = None
        slice_scatter_5074: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5071, slice_scatter_5073, 1, 6752, 6768);  slice_scatter_5071 = slice_scatter_5073 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27919: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6768, 6784)
        slice_27920: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_27919, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_849: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_27920, memory_format = torch.contiguous_format);  slice_27920 = None
        view_1702: "f32[32, 16]" = torch.ops.aten.view.default(clone_849, [32, 16]);  clone_849 = None
        mm_846: "f32[32, 8]" = torch.ops.aten.mm.default(view_1702, slice_7)
        view_1703: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_846, [2, 16, 8]);  mm_846 = None
        slice_27927: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5074, 1, 6768, 6784)
        slice_27928: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27927, 2, 0, 16)
        add_848: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27928, view_1703);  slice_27928 = view_1703 = None
        slice_scatter_5076: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27927, add_848, 2, 0, 16);  slice_27927 = add_848 = None
        slice_scatter_5077: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5074, slice_scatter_5076, 1, 6768, 6784);  slice_scatter_5074 = slice_scatter_5076 = None
        slice_27932: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5077, 1, 6768, 6784)
        slice_27933: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27932, 2, 0, 16)
        slice_scatter_5079: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27932, slice_27933, 2, 0, 16);  slice_27932 = slice_27933 = None
        slice_scatter_5080: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5077, slice_scatter_5079, 1, 6768, 6784);  slice_scatter_5077 = slice_scatter_5079 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27953: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_27919, 2, 16, 32);  slice_27919 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_850: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_27953, memory_format = torch.contiguous_format);  slice_27953 = None
        view_1704: "f32[32, 11]" = torch.ops.aten.view.default(clone_850, [32, 11]);  clone_850 = None
        mm_847: "f32[32, 8]" = torch.ops.aten.mm.default(view_1704, slice_37)
        view_1705: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_847, [2, 16, 8]);  mm_847 = None
        slice_27960: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5080, 1, 6768, 6784)
        slice_27961: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27960, 2, 0, 16)
        add_849: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27961, view_1705);  slice_27961 = view_1705 = None
        slice_scatter_5082: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27960, add_849, 2, 0, 16);  slice_27960 = add_849 = None
        slice_scatter_5083: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5080, slice_scatter_5082, 1, 6768, 6784);  slice_scatter_5080 = slice_scatter_5082 = None
        slice_27965: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5083, 1, 6768, 6784)
        slice_27966: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27965, 2, 0, 16)
        slice_scatter_5085: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27965, slice_27966, 2, 0, 16);  slice_27965 = slice_27966 = None
        slice_scatter_5086: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5083, slice_scatter_5085, 1, 6768, 6784);  slice_scatter_5083 = slice_scatter_5085 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_27985: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6784, 6800)
        slice_27986: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_27985, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_851: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_27986, memory_format = torch.contiguous_format);  slice_27986 = None
        view_1706: "f32[32, 16]" = torch.ops.aten.view.default(clone_851, [32, 16]);  clone_851 = None
        mm_848: "f32[32, 8]" = torch.ops.aten.mm.default(view_1706, slice_7)
        view_1707: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_848, [2, 16, 8]);  mm_848 = None
        slice_27993: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5086, 1, 6784, 6800)
        slice_27994: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27993, 2, 0, 16)
        add_850: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_27994, view_1707);  slice_27994 = view_1707 = None
        slice_scatter_5088: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27993, add_850, 2, 0, 16);  slice_27993 = add_850 = None
        slice_scatter_5089: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5086, slice_scatter_5088, 1, 6784, 6800);  slice_scatter_5086 = slice_scatter_5088 = None
        slice_27998: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5089, 1, 6784, 6800)
        slice_27999: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_27998, 2, 0, 16)
        slice_scatter_5091: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_27998, slice_27999, 2, 0, 16);  slice_27998 = slice_27999 = None
        slice_scatter_5092: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5089, slice_scatter_5091, 1, 6784, 6800);  slice_scatter_5089 = slice_scatter_5091 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28019: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_27985, 2, 16, 32);  slice_27985 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_852: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_28019, memory_format = torch.contiguous_format);  slice_28019 = None
        view_1708: "f32[32, 11]" = torch.ops.aten.view.default(clone_852, [32, 11]);  clone_852 = None
        mm_849: "f32[32, 8]" = torch.ops.aten.mm.default(view_1708, slice_37)
        view_1709: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_849, [2, 16, 8]);  mm_849 = None
        slice_28026: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5092, 1, 6784, 6800)
        slice_28027: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28026, 2, 0, 16)
        add_851: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28027, view_1709);  slice_28027 = view_1709 = None
        slice_scatter_5094: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28026, add_851, 2, 0, 16);  slice_28026 = add_851 = None
        slice_scatter_5095: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5092, slice_scatter_5094, 1, 6784, 6800);  slice_scatter_5092 = slice_scatter_5094 = None
        slice_28031: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5095, 1, 6784, 6800)
        slice_28032: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28031, 2, 0, 16)
        slice_scatter_5097: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28031, slice_28032, 2, 0, 16);  slice_28031 = slice_28032 = None
        slice_scatter_5098: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5095, slice_scatter_5097, 1, 6784, 6800);  slice_scatter_5095 = slice_scatter_5097 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28051: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6800, 6816)
        slice_28052: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_28051, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_853: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_28052, memory_format = torch.contiguous_format);  slice_28052 = None
        view_1710: "f32[32, 16]" = torch.ops.aten.view.default(clone_853, [32, 16]);  clone_853 = None
        mm_850: "f32[32, 8]" = torch.ops.aten.mm.default(view_1710, slice_7)
        view_1711: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_850, [2, 16, 8]);  mm_850 = None
        slice_28059: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5098, 1, 6800, 6816)
        slice_28060: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28059, 2, 0, 16)
        add_852: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28060, view_1711);  slice_28060 = view_1711 = None
        slice_scatter_5100: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28059, add_852, 2, 0, 16);  slice_28059 = add_852 = None
        slice_scatter_5101: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5098, slice_scatter_5100, 1, 6800, 6816);  slice_scatter_5098 = slice_scatter_5100 = None
        slice_28064: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5101, 1, 6800, 6816)
        slice_28065: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28064, 2, 0, 16)
        slice_scatter_5103: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28064, slice_28065, 2, 0, 16);  slice_28064 = slice_28065 = None
        slice_scatter_5104: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5101, slice_scatter_5103, 1, 6800, 6816);  slice_scatter_5101 = slice_scatter_5103 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28085: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_28051, 2, 16, 32);  slice_28051 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_854: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_28085, memory_format = torch.contiguous_format);  slice_28085 = None
        view_1712: "f32[32, 11]" = torch.ops.aten.view.default(clone_854, [32, 11]);  clone_854 = None
        mm_851: "f32[32, 8]" = torch.ops.aten.mm.default(view_1712, slice_37)
        view_1713: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_851, [2, 16, 8]);  mm_851 = None
        slice_28092: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5104, 1, 6800, 6816)
        slice_28093: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28092, 2, 0, 16)
        add_853: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28093, view_1713);  slice_28093 = view_1713 = None
        slice_scatter_5106: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28092, add_853, 2, 0, 16);  slice_28092 = add_853 = None
        slice_scatter_5107: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5104, slice_scatter_5106, 1, 6800, 6816);  slice_scatter_5104 = slice_scatter_5106 = None
        slice_28097: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5107, 1, 6800, 6816)
        slice_28098: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28097, 2, 0, 16)
        slice_scatter_5109: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28097, slice_28098, 2, 0, 16);  slice_28097 = slice_28098 = None
        slice_scatter_5110: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5107, slice_scatter_5109, 1, 6800, 6816);  slice_scatter_5107 = slice_scatter_5109 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28117: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6816, 6832)
        slice_28118: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_28117, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_855: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_28118, memory_format = torch.contiguous_format);  slice_28118 = None
        view_1714: "f32[32, 16]" = torch.ops.aten.view.default(clone_855, [32, 16]);  clone_855 = None
        mm_852: "f32[32, 8]" = torch.ops.aten.mm.default(view_1714, slice_7)
        view_1715: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_852, [2, 16, 8]);  mm_852 = None
        slice_28125: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5110, 1, 6816, 6832)
        slice_28126: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28125, 2, 0, 16)
        add_854: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28126, view_1715);  slice_28126 = view_1715 = None
        slice_scatter_5112: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28125, add_854, 2, 0, 16);  slice_28125 = add_854 = None
        slice_scatter_5113: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5110, slice_scatter_5112, 1, 6816, 6832);  slice_scatter_5110 = slice_scatter_5112 = None
        slice_28130: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5113, 1, 6816, 6832)
        slice_28131: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28130, 2, 0, 16)
        slice_scatter_5115: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28130, slice_28131, 2, 0, 16);  slice_28130 = slice_28131 = None
        slice_scatter_5116: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5113, slice_scatter_5115, 1, 6816, 6832);  slice_scatter_5113 = slice_scatter_5115 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28151: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_28117, 2, 16, 32);  slice_28117 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_856: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_28151, memory_format = torch.contiguous_format);  slice_28151 = None
        view_1716: "f32[32, 11]" = torch.ops.aten.view.default(clone_856, [32, 11]);  clone_856 = None
        mm_853: "f32[32, 8]" = torch.ops.aten.mm.default(view_1716, slice_37)
        view_1717: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_853, [2, 16, 8]);  mm_853 = None
        slice_28158: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5116, 1, 6816, 6832)
        slice_28159: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28158, 2, 0, 16)
        add_855: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28159, view_1717);  slice_28159 = view_1717 = None
        slice_scatter_5118: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28158, add_855, 2, 0, 16);  slice_28158 = add_855 = None
        slice_scatter_5119: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5116, slice_scatter_5118, 1, 6816, 6832);  slice_scatter_5116 = slice_scatter_5118 = None
        slice_28163: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5119, 1, 6816, 6832)
        slice_28164: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28163, 2, 0, 16)
        slice_scatter_5121: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28163, slice_28164, 2, 0, 16);  slice_28163 = slice_28164 = None
        slice_scatter_5122: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5119, slice_scatter_5121, 1, 6816, 6832);  slice_scatter_5119 = slice_scatter_5121 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28183: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6832, 6848)
        slice_28184: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_28183, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_857: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_28184, memory_format = torch.contiguous_format);  slice_28184 = None
        view_1718: "f32[32, 16]" = torch.ops.aten.view.default(clone_857, [32, 16]);  clone_857 = None
        mm_854: "f32[32, 8]" = torch.ops.aten.mm.default(view_1718, slice_7)
        view_1719: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_854, [2, 16, 8]);  mm_854 = None
        slice_28191: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5122, 1, 6832, 6848)
        slice_28192: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28191, 2, 0, 16)
        add_856: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28192, view_1719);  slice_28192 = view_1719 = None
        slice_scatter_5124: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28191, add_856, 2, 0, 16);  slice_28191 = add_856 = None
        slice_scatter_5125: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5122, slice_scatter_5124, 1, 6832, 6848);  slice_scatter_5122 = slice_scatter_5124 = None
        slice_28196: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5125, 1, 6832, 6848)
        slice_28197: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28196, 2, 0, 16)
        slice_scatter_5127: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28196, slice_28197, 2, 0, 16);  slice_28196 = slice_28197 = None
        slice_scatter_5128: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5125, slice_scatter_5127, 1, 6832, 6848);  slice_scatter_5125 = slice_scatter_5127 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28217: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_28183, 2, 16, 32);  slice_28183 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_858: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_28217, memory_format = torch.contiguous_format);  slice_28217 = None
        view_1720: "f32[32, 11]" = torch.ops.aten.view.default(clone_858, [32, 11]);  clone_858 = None
        mm_855: "f32[32, 8]" = torch.ops.aten.mm.default(view_1720, slice_37)
        view_1721: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_855, [2, 16, 8]);  mm_855 = None
        slice_28224: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5128, 1, 6832, 6848)
        slice_28225: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28224, 2, 0, 16)
        add_857: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28225, view_1721);  slice_28225 = view_1721 = None
        slice_scatter_5130: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28224, add_857, 2, 0, 16);  slice_28224 = add_857 = None
        slice_scatter_5131: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5128, slice_scatter_5130, 1, 6832, 6848);  slice_scatter_5128 = slice_scatter_5130 = None
        slice_28229: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5131, 1, 6832, 6848)
        slice_28230: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28229, 2, 0, 16)
        slice_scatter_5133: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28229, slice_28230, 2, 0, 16);  slice_28229 = slice_28230 = None
        slice_scatter_5134: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5131, slice_scatter_5133, 1, 6832, 6848);  slice_scatter_5131 = slice_scatter_5133 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28249: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6848, 6864)
        slice_28250: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_28249, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_859: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_28250, memory_format = torch.contiguous_format);  slice_28250 = None
        view_1722: "f32[32, 16]" = torch.ops.aten.view.default(clone_859, [32, 16]);  clone_859 = None
        mm_856: "f32[32, 8]" = torch.ops.aten.mm.default(view_1722, slice_7)
        view_1723: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_856, [2, 16, 8]);  mm_856 = None
        slice_28257: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5134, 1, 6848, 6864)
        slice_28258: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28257, 2, 0, 16)
        add_858: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28258, view_1723);  slice_28258 = view_1723 = None
        slice_scatter_5136: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28257, add_858, 2, 0, 16);  slice_28257 = add_858 = None
        slice_scatter_5137: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5134, slice_scatter_5136, 1, 6848, 6864);  slice_scatter_5134 = slice_scatter_5136 = None
        slice_28262: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5137, 1, 6848, 6864)
        slice_28263: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28262, 2, 0, 16)
        slice_scatter_5139: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28262, slice_28263, 2, 0, 16);  slice_28262 = slice_28263 = None
        slice_scatter_5140: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5137, slice_scatter_5139, 1, 6848, 6864);  slice_scatter_5137 = slice_scatter_5139 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28283: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_28249, 2, 16, 32);  slice_28249 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_860: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_28283, memory_format = torch.contiguous_format);  slice_28283 = None
        view_1724: "f32[32, 11]" = torch.ops.aten.view.default(clone_860, [32, 11]);  clone_860 = None
        mm_857: "f32[32, 8]" = torch.ops.aten.mm.default(view_1724, slice_37)
        view_1725: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_857, [2, 16, 8]);  mm_857 = None
        slice_28290: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5140, 1, 6848, 6864)
        slice_28291: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28290, 2, 0, 16)
        add_859: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28291, view_1725);  slice_28291 = view_1725 = None
        slice_scatter_5142: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28290, add_859, 2, 0, 16);  slice_28290 = add_859 = None
        slice_scatter_5143: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5140, slice_scatter_5142, 1, 6848, 6864);  slice_scatter_5140 = slice_scatter_5142 = None
        slice_28295: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5143, 1, 6848, 6864)
        slice_28296: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28295, 2, 0, 16)
        slice_scatter_5145: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28295, slice_28296, 2, 0, 16);  slice_28295 = slice_28296 = None
        slice_scatter_5146: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5143, slice_scatter_5145, 1, 6848, 6864);  slice_scatter_5143 = slice_scatter_5145 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28315: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6864, 6880)
        slice_28316: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_28315, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_861: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_28316, memory_format = torch.contiguous_format);  slice_28316 = None
        view_1726: "f32[32, 16]" = torch.ops.aten.view.default(clone_861, [32, 16]);  clone_861 = None
        mm_858: "f32[32, 8]" = torch.ops.aten.mm.default(view_1726, slice_7)
        view_1727: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_858, [2, 16, 8]);  mm_858 = None
        slice_28323: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5146, 1, 6864, 6880)
        slice_28324: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28323, 2, 0, 16)
        add_860: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28324, view_1727);  slice_28324 = view_1727 = None
        slice_scatter_5148: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28323, add_860, 2, 0, 16);  slice_28323 = add_860 = None
        slice_scatter_5149: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5146, slice_scatter_5148, 1, 6864, 6880);  slice_scatter_5146 = slice_scatter_5148 = None
        slice_28328: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5149, 1, 6864, 6880)
        slice_28329: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28328, 2, 0, 16)
        slice_scatter_5151: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28328, slice_28329, 2, 0, 16);  slice_28328 = slice_28329 = None
        slice_scatter_5152: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5149, slice_scatter_5151, 1, 6864, 6880);  slice_scatter_5149 = slice_scatter_5151 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28349: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_28315, 2, 16, 32);  slice_28315 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_862: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_28349, memory_format = torch.contiguous_format);  slice_28349 = None
        view_1728: "f32[32, 11]" = torch.ops.aten.view.default(clone_862, [32, 11]);  clone_862 = None
        mm_859: "f32[32, 8]" = torch.ops.aten.mm.default(view_1728, slice_37)
        view_1729: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_859, [2, 16, 8]);  mm_859 = None
        slice_28356: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5152, 1, 6864, 6880)
        slice_28357: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28356, 2, 0, 16)
        add_861: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28357, view_1729);  slice_28357 = view_1729 = None
        slice_scatter_5154: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28356, add_861, 2, 0, 16);  slice_28356 = add_861 = None
        slice_scatter_5155: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5152, slice_scatter_5154, 1, 6864, 6880);  slice_scatter_5152 = slice_scatter_5154 = None
        slice_28361: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5155, 1, 6864, 6880)
        slice_28362: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28361, 2, 0, 16)
        slice_scatter_5157: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28361, slice_28362, 2, 0, 16);  slice_28361 = slice_28362 = None
        slice_scatter_5158: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5155, slice_scatter_5157, 1, 6864, 6880);  slice_scatter_5155 = slice_scatter_5157 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28381: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6880, 6896)
        slice_28382: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_28381, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_863: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_28382, memory_format = torch.contiguous_format);  slice_28382 = None
        view_1730: "f32[32, 16]" = torch.ops.aten.view.default(clone_863, [32, 16]);  clone_863 = None
        mm_860: "f32[32, 8]" = torch.ops.aten.mm.default(view_1730, slice_7)
        view_1731: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_860, [2, 16, 8]);  mm_860 = None
        slice_28389: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5158, 1, 6880, 6896)
        slice_28390: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28389, 2, 0, 16)
        add_862: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28390, view_1731);  slice_28390 = view_1731 = None
        slice_scatter_5160: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28389, add_862, 2, 0, 16);  slice_28389 = add_862 = None
        slice_scatter_5161: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5158, slice_scatter_5160, 1, 6880, 6896);  slice_scatter_5158 = slice_scatter_5160 = None
        slice_28394: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5161, 1, 6880, 6896)
        slice_28395: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28394, 2, 0, 16)
        slice_scatter_5163: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28394, slice_28395, 2, 0, 16);  slice_28394 = slice_28395 = None
        slice_scatter_5164: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5161, slice_scatter_5163, 1, 6880, 6896);  slice_scatter_5161 = slice_scatter_5163 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28415: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_28381, 2, 16, 32);  slice_28381 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_864: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_28415, memory_format = torch.contiguous_format);  slice_28415 = None
        view_1732: "f32[32, 11]" = torch.ops.aten.view.default(clone_864, [32, 11]);  clone_864 = None
        mm_861: "f32[32, 8]" = torch.ops.aten.mm.default(view_1732, slice_37)
        view_1733: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_861, [2, 16, 8]);  mm_861 = None
        slice_28422: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5164, 1, 6880, 6896)
        slice_28423: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28422, 2, 0, 16)
        add_863: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28423, view_1733);  slice_28423 = view_1733 = None
        slice_scatter_5166: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28422, add_863, 2, 0, 16);  slice_28422 = add_863 = None
        slice_scatter_5167: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5164, slice_scatter_5166, 1, 6880, 6896);  slice_scatter_5164 = slice_scatter_5166 = None
        slice_28427: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5167, 1, 6880, 6896)
        slice_28428: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28427, 2, 0, 16)
        slice_scatter_5169: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28427, slice_28428, 2, 0, 16);  slice_28427 = slice_28428 = None
        slice_scatter_5170: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5167, slice_scatter_5169, 1, 6880, 6896);  slice_scatter_5167 = slice_scatter_5169 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28447: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6896, 6912)
        slice_28448: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_28447, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_865: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_28448, memory_format = torch.contiguous_format);  slice_28448 = None
        view_1734: "f32[32, 16]" = torch.ops.aten.view.default(clone_865, [32, 16]);  clone_865 = None
        mm_862: "f32[32, 8]" = torch.ops.aten.mm.default(view_1734, slice_7)
        view_1735: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_862, [2, 16, 8]);  mm_862 = None
        slice_28455: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5170, 1, 6896, 6912)
        slice_28456: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28455, 2, 0, 16)
        add_864: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28456, view_1735);  slice_28456 = view_1735 = None
        slice_scatter_5172: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28455, add_864, 2, 0, 16);  slice_28455 = add_864 = None
        slice_scatter_5173: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5170, slice_scatter_5172, 1, 6896, 6912);  slice_scatter_5170 = slice_scatter_5172 = None
        slice_28460: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5173, 1, 6896, 6912)
        slice_28461: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28460, 2, 0, 16)
        slice_scatter_5175: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28460, slice_28461, 2, 0, 16);  slice_28460 = slice_28461 = None
        slice_scatter_5176: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5173, slice_scatter_5175, 1, 6896, 6912);  slice_scatter_5173 = slice_scatter_5175 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28481: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_28447, 2, 16, 32);  slice_28447 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_866: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_28481, memory_format = torch.contiguous_format);  slice_28481 = None
        view_1736: "f32[32, 11]" = torch.ops.aten.view.default(clone_866, [32, 11]);  clone_866 = None
        mm_863: "f32[32, 8]" = torch.ops.aten.mm.default(view_1736, slice_37)
        view_1737: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_863, [2, 16, 8]);  mm_863 = None
        slice_28488: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5176, 1, 6896, 6912)
        slice_28489: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28488, 2, 0, 16)
        add_865: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28489, view_1737);  slice_28489 = view_1737 = None
        slice_scatter_5178: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28488, add_865, 2, 0, 16);  slice_28488 = add_865 = None
        slice_scatter_5179: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5176, slice_scatter_5178, 1, 6896, 6912);  slice_scatter_5176 = slice_scatter_5178 = None
        slice_28493: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5179, 1, 6896, 6912)
        slice_28494: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28493, 2, 0, 16)
        slice_scatter_5181: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28493, slice_28494, 2, 0, 16);  slice_28493 = slice_28494 = None
        slice_scatter_5182: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5179, slice_scatter_5181, 1, 6896, 6912);  slice_scatter_5179 = slice_scatter_5181 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28513: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6912, 6928)
        slice_28514: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_28513, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_867: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_28514, memory_format = torch.contiguous_format);  slice_28514 = None
        view_1738: "f32[32, 16]" = torch.ops.aten.view.default(clone_867, [32, 16]);  clone_867 = None
        mm_864: "f32[32, 8]" = torch.ops.aten.mm.default(view_1738, slice_7)
        view_1739: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_864, [2, 16, 8]);  mm_864 = None
        slice_28521: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5182, 1, 6912, 6928)
        slice_28522: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28521, 2, 0, 16)
        add_866: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28522, view_1739);  slice_28522 = view_1739 = None
        slice_scatter_5184: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28521, add_866, 2, 0, 16);  slice_28521 = add_866 = None
        slice_scatter_5185: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5182, slice_scatter_5184, 1, 6912, 6928);  slice_scatter_5182 = slice_scatter_5184 = None
        slice_28526: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5185, 1, 6912, 6928)
        slice_28527: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28526, 2, 0, 16)
        slice_scatter_5187: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28526, slice_28527, 2, 0, 16);  slice_28526 = slice_28527 = None
        slice_scatter_5188: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5185, slice_scatter_5187, 1, 6912, 6928);  slice_scatter_5185 = slice_scatter_5187 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28547: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_28513, 2, 16, 32);  slice_28513 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_868: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_28547, memory_format = torch.contiguous_format);  slice_28547 = None
        view_1740: "f32[32, 11]" = torch.ops.aten.view.default(clone_868, [32, 11]);  clone_868 = None
        mm_865: "f32[32, 8]" = torch.ops.aten.mm.default(view_1740, slice_37)
        view_1741: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_865, [2, 16, 8]);  mm_865 = None
        slice_28554: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5188, 1, 6912, 6928)
        slice_28555: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28554, 2, 0, 16)
        add_867: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28555, view_1741);  slice_28555 = view_1741 = None
        slice_scatter_5190: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28554, add_867, 2, 0, 16);  slice_28554 = add_867 = None
        slice_scatter_5191: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5188, slice_scatter_5190, 1, 6912, 6928);  slice_scatter_5188 = slice_scatter_5190 = None
        slice_28559: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5191, 1, 6912, 6928)
        slice_28560: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28559, 2, 0, 16)
        slice_scatter_5193: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28559, slice_28560, 2, 0, 16);  slice_28559 = slice_28560 = None
        slice_scatter_5194: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5191, slice_scatter_5193, 1, 6912, 6928);  slice_scatter_5191 = slice_scatter_5193 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28579: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6928, 6944)
        slice_28580: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_28579, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_869: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_28580, memory_format = torch.contiguous_format);  slice_28580 = None
        view_1742: "f32[32, 16]" = torch.ops.aten.view.default(clone_869, [32, 16]);  clone_869 = None
        mm_866: "f32[32, 8]" = torch.ops.aten.mm.default(view_1742, slice_7)
        view_1743: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_866, [2, 16, 8]);  mm_866 = None
        slice_28587: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5194, 1, 6928, 6944)
        slice_28588: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28587, 2, 0, 16)
        add_868: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28588, view_1743);  slice_28588 = view_1743 = None
        slice_scatter_5196: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28587, add_868, 2, 0, 16);  slice_28587 = add_868 = None
        slice_scatter_5197: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5194, slice_scatter_5196, 1, 6928, 6944);  slice_scatter_5194 = slice_scatter_5196 = None
        slice_28592: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5197, 1, 6928, 6944)
        slice_28593: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28592, 2, 0, 16)
        slice_scatter_5199: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28592, slice_28593, 2, 0, 16);  slice_28592 = slice_28593 = None
        slice_scatter_5200: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5197, slice_scatter_5199, 1, 6928, 6944);  slice_scatter_5197 = slice_scatter_5199 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28613: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_28579, 2, 16, 32);  slice_28579 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_870: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_28613, memory_format = torch.contiguous_format);  slice_28613 = None
        view_1744: "f32[32, 11]" = torch.ops.aten.view.default(clone_870, [32, 11]);  clone_870 = None
        mm_867: "f32[32, 8]" = torch.ops.aten.mm.default(view_1744, slice_37)
        view_1745: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_867, [2, 16, 8]);  mm_867 = None
        slice_28620: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5200, 1, 6928, 6944)
        slice_28621: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28620, 2, 0, 16)
        add_869: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28621, view_1745);  slice_28621 = view_1745 = None
        slice_scatter_5202: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28620, add_869, 2, 0, 16);  slice_28620 = add_869 = None
        slice_scatter_5203: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5200, slice_scatter_5202, 1, 6928, 6944);  slice_scatter_5200 = slice_scatter_5202 = None
        slice_28625: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5203, 1, 6928, 6944)
        slice_28626: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28625, 2, 0, 16)
        slice_scatter_5205: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28625, slice_28626, 2, 0, 16);  slice_28625 = slice_28626 = None
        slice_scatter_5206: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5203, slice_scatter_5205, 1, 6928, 6944);  slice_scatter_5203 = slice_scatter_5205 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28645: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6944, 6960)
        slice_28646: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_28645, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_871: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_28646, memory_format = torch.contiguous_format);  slice_28646 = None
        view_1746: "f32[32, 16]" = torch.ops.aten.view.default(clone_871, [32, 16]);  clone_871 = None
        mm_868: "f32[32, 8]" = torch.ops.aten.mm.default(view_1746, slice_7)
        view_1747: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_868, [2, 16, 8]);  mm_868 = None
        slice_28653: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5206, 1, 6944, 6960)
        slice_28654: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28653, 2, 0, 16)
        add_870: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28654, view_1747);  slice_28654 = view_1747 = None
        slice_scatter_5208: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28653, add_870, 2, 0, 16);  slice_28653 = add_870 = None
        slice_scatter_5209: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5206, slice_scatter_5208, 1, 6944, 6960);  slice_scatter_5206 = slice_scatter_5208 = None
        slice_28658: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5209, 1, 6944, 6960)
        slice_28659: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28658, 2, 0, 16)
        slice_scatter_5211: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28658, slice_28659, 2, 0, 16);  slice_28658 = slice_28659 = None
        slice_scatter_5212: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5209, slice_scatter_5211, 1, 6944, 6960);  slice_scatter_5209 = slice_scatter_5211 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28679: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_28645, 2, 16, 32);  slice_28645 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_872: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_28679, memory_format = torch.contiguous_format);  slice_28679 = None
        view_1748: "f32[32, 11]" = torch.ops.aten.view.default(clone_872, [32, 11]);  clone_872 = None
        mm_869: "f32[32, 8]" = torch.ops.aten.mm.default(view_1748, slice_37)
        view_1749: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_869, [2, 16, 8]);  mm_869 = None
        slice_28686: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5212, 1, 6944, 6960)
        slice_28687: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28686, 2, 0, 16)
        add_871: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28687, view_1749);  slice_28687 = view_1749 = None
        slice_scatter_5214: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28686, add_871, 2, 0, 16);  slice_28686 = add_871 = None
        slice_scatter_5215: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5212, slice_scatter_5214, 1, 6944, 6960);  slice_scatter_5212 = slice_scatter_5214 = None
        slice_28691: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5215, 1, 6944, 6960)
        slice_28692: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28691, 2, 0, 16)
        slice_scatter_5217: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28691, slice_28692, 2, 0, 16);  slice_28691 = slice_28692 = None
        slice_scatter_5218: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5215, slice_scatter_5217, 1, 6944, 6960);  slice_scatter_5215 = slice_scatter_5217 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28711: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6960, 6976)
        slice_28712: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_28711, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_873: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_28712, memory_format = torch.contiguous_format);  slice_28712 = None
        view_1750: "f32[32, 16]" = torch.ops.aten.view.default(clone_873, [32, 16]);  clone_873 = None
        mm_870: "f32[32, 8]" = torch.ops.aten.mm.default(view_1750, slice_7)
        view_1751: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_870, [2, 16, 8]);  mm_870 = None
        slice_28719: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5218, 1, 6960, 6976)
        slice_28720: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28719, 2, 0, 16)
        add_872: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28720, view_1751);  slice_28720 = view_1751 = None
        slice_scatter_5220: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28719, add_872, 2, 0, 16);  slice_28719 = add_872 = None
        slice_scatter_5221: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5218, slice_scatter_5220, 1, 6960, 6976);  slice_scatter_5218 = slice_scatter_5220 = None
        slice_28724: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5221, 1, 6960, 6976)
        slice_28725: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28724, 2, 0, 16)
        slice_scatter_5223: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28724, slice_28725, 2, 0, 16);  slice_28724 = slice_28725 = None
        slice_scatter_5224: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5221, slice_scatter_5223, 1, 6960, 6976);  slice_scatter_5221 = slice_scatter_5223 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28745: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_28711, 2, 16, 32);  slice_28711 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_874: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_28745, memory_format = torch.contiguous_format);  slice_28745 = None
        view_1752: "f32[32, 11]" = torch.ops.aten.view.default(clone_874, [32, 11]);  clone_874 = None
        mm_871: "f32[32, 8]" = torch.ops.aten.mm.default(view_1752, slice_37)
        view_1753: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_871, [2, 16, 8]);  mm_871 = None
        slice_28752: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5224, 1, 6960, 6976)
        slice_28753: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28752, 2, 0, 16)
        add_873: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28753, view_1753);  slice_28753 = view_1753 = None
        slice_scatter_5226: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28752, add_873, 2, 0, 16);  slice_28752 = add_873 = None
        slice_scatter_5227: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5224, slice_scatter_5226, 1, 6960, 6976);  slice_scatter_5224 = slice_scatter_5226 = None
        slice_28757: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5227, 1, 6960, 6976)
        slice_28758: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28757, 2, 0, 16)
        slice_scatter_5229: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28757, slice_28758, 2, 0, 16);  slice_28757 = slice_28758 = None
        slice_scatter_5230: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5227, slice_scatter_5229, 1, 6960, 6976);  slice_scatter_5227 = slice_scatter_5229 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28777: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6976, 6992)
        slice_28778: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_28777, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_875: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_28778, memory_format = torch.contiguous_format);  slice_28778 = None
        view_1754: "f32[32, 16]" = torch.ops.aten.view.default(clone_875, [32, 16]);  clone_875 = None
        mm_872: "f32[32, 8]" = torch.ops.aten.mm.default(view_1754, slice_7)
        view_1755: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_872, [2, 16, 8]);  mm_872 = None
        slice_28785: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5230, 1, 6976, 6992)
        slice_28786: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28785, 2, 0, 16)
        add_874: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28786, view_1755);  slice_28786 = view_1755 = None
        slice_scatter_5232: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28785, add_874, 2, 0, 16);  slice_28785 = add_874 = None
        slice_scatter_5233: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5230, slice_scatter_5232, 1, 6976, 6992);  slice_scatter_5230 = slice_scatter_5232 = None
        slice_28790: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5233, 1, 6976, 6992)
        slice_28791: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28790, 2, 0, 16)
        slice_scatter_5235: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28790, slice_28791, 2, 0, 16);  slice_28790 = slice_28791 = None
        slice_scatter_5236: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5233, slice_scatter_5235, 1, 6976, 6992);  slice_scatter_5233 = slice_scatter_5235 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28811: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_28777, 2, 16, 32);  slice_28777 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_876: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_28811, memory_format = torch.contiguous_format);  slice_28811 = None
        view_1756: "f32[32, 11]" = torch.ops.aten.view.default(clone_876, [32, 11]);  clone_876 = None
        mm_873: "f32[32, 8]" = torch.ops.aten.mm.default(view_1756, slice_37)
        view_1757: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_873, [2, 16, 8]);  mm_873 = None
        slice_28818: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5236, 1, 6976, 6992)
        slice_28819: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28818, 2, 0, 16)
        add_875: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28819, view_1757);  slice_28819 = view_1757 = None
        slice_scatter_5238: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28818, add_875, 2, 0, 16);  slice_28818 = add_875 = None
        slice_scatter_5239: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5236, slice_scatter_5238, 1, 6976, 6992);  slice_scatter_5236 = slice_scatter_5238 = None
        slice_28823: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5239, 1, 6976, 6992)
        slice_28824: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28823, 2, 0, 16)
        slice_scatter_5241: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28823, slice_28824, 2, 0, 16);  slice_28823 = slice_28824 = None
        slice_scatter_5242: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5239, slice_scatter_5241, 1, 6976, 6992);  slice_scatter_5239 = slice_scatter_5241 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28843: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 6992, 7008)
        slice_28844: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_28843, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_877: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_28844, memory_format = torch.contiguous_format);  slice_28844 = None
        view_1758: "f32[32, 16]" = torch.ops.aten.view.default(clone_877, [32, 16]);  clone_877 = None
        mm_874: "f32[32, 8]" = torch.ops.aten.mm.default(view_1758, slice_7)
        view_1759: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_874, [2, 16, 8]);  mm_874 = None
        slice_28851: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5242, 1, 6992, 7008)
        slice_28852: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28851, 2, 0, 16)
        add_876: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28852, view_1759);  slice_28852 = view_1759 = None
        slice_scatter_5244: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28851, add_876, 2, 0, 16);  slice_28851 = add_876 = None
        slice_scatter_5245: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5242, slice_scatter_5244, 1, 6992, 7008);  slice_scatter_5242 = slice_scatter_5244 = None
        slice_28856: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5245, 1, 6992, 7008)
        slice_28857: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28856, 2, 0, 16)
        slice_scatter_5247: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28856, slice_28857, 2, 0, 16);  slice_28856 = slice_28857 = None
        slice_scatter_5248: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5245, slice_scatter_5247, 1, 6992, 7008);  slice_scatter_5245 = slice_scatter_5247 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28877: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_28843, 2, 16, 32);  slice_28843 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_878: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_28877, memory_format = torch.contiguous_format);  slice_28877 = None
        view_1760: "f32[32, 11]" = torch.ops.aten.view.default(clone_878, [32, 11]);  clone_878 = None
        mm_875: "f32[32, 8]" = torch.ops.aten.mm.default(view_1760, slice_37)
        view_1761: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_875, [2, 16, 8]);  mm_875 = None
        slice_28884: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5248, 1, 6992, 7008)
        slice_28885: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28884, 2, 0, 16)
        add_877: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28885, view_1761);  slice_28885 = view_1761 = None
        slice_scatter_5250: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28884, add_877, 2, 0, 16);  slice_28884 = add_877 = None
        slice_scatter_5251: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5248, slice_scatter_5250, 1, 6992, 7008);  slice_scatter_5248 = slice_scatter_5250 = None
        slice_28889: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5251, 1, 6992, 7008)
        slice_28890: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28889, 2, 0, 16)
        slice_scatter_5253: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28889, slice_28890, 2, 0, 16);  slice_28889 = slice_28890 = None
        slice_scatter_5254: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5251, slice_scatter_5253, 1, 6992, 7008);  slice_scatter_5251 = slice_scatter_5253 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28909: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7008, 7024)
        slice_28910: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_28909, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_879: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_28910, memory_format = torch.contiguous_format);  slice_28910 = None
        view_1762: "f32[32, 16]" = torch.ops.aten.view.default(clone_879, [32, 16]);  clone_879 = None
        mm_876: "f32[32, 8]" = torch.ops.aten.mm.default(view_1762, slice_7)
        view_1763: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_876, [2, 16, 8]);  mm_876 = None
        slice_28917: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5254, 1, 7008, 7024)
        slice_28918: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28917, 2, 0, 16)
        add_878: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28918, view_1763);  slice_28918 = view_1763 = None
        slice_scatter_5256: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28917, add_878, 2, 0, 16);  slice_28917 = add_878 = None
        slice_scatter_5257: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5254, slice_scatter_5256, 1, 7008, 7024);  slice_scatter_5254 = slice_scatter_5256 = None
        slice_28922: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5257, 1, 7008, 7024)
        slice_28923: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28922, 2, 0, 16)
        slice_scatter_5259: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28922, slice_28923, 2, 0, 16);  slice_28922 = slice_28923 = None
        slice_scatter_5260: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5257, slice_scatter_5259, 1, 7008, 7024);  slice_scatter_5257 = slice_scatter_5259 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28943: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_28909, 2, 16, 32);  slice_28909 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_880: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_28943, memory_format = torch.contiguous_format);  slice_28943 = None
        view_1764: "f32[32, 11]" = torch.ops.aten.view.default(clone_880, [32, 11]);  clone_880 = None
        mm_877: "f32[32, 8]" = torch.ops.aten.mm.default(view_1764, slice_37)
        view_1765: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_877, [2, 16, 8]);  mm_877 = None
        slice_28950: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5260, 1, 7008, 7024)
        slice_28951: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28950, 2, 0, 16)
        add_879: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28951, view_1765);  slice_28951 = view_1765 = None
        slice_scatter_5262: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28950, add_879, 2, 0, 16);  slice_28950 = add_879 = None
        slice_scatter_5263: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5260, slice_scatter_5262, 1, 7008, 7024);  slice_scatter_5260 = slice_scatter_5262 = None
        slice_28955: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5263, 1, 7008, 7024)
        slice_28956: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28955, 2, 0, 16)
        slice_scatter_5265: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28955, slice_28956, 2, 0, 16);  slice_28955 = slice_28956 = None
        slice_scatter_5266: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5263, slice_scatter_5265, 1, 7008, 7024);  slice_scatter_5263 = slice_scatter_5265 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_28975: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7024, 7040)
        slice_28976: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_28975, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_881: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_28976, memory_format = torch.contiguous_format);  slice_28976 = None
        view_1766: "f32[32, 16]" = torch.ops.aten.view.default(clone_881, [32, 16]);  clone_881 = None
        mm_878: "f32[32, 8]" = torch.ops.aten.mm.default(view_1766, slice_7)
        view_1767: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_878, [2, 16, 8]);  mm_878 = None
        slice_28983: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5266, 1, 7024, 7040)
        slice_28984: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28983, 2, 0, 16)
        add_880: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_28984, view_1767);  slice_28984 = view_1767 = None
        slice_scatter_5268: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28983, add_880, 2, 0, 16);  slice_28983 = add_880 = None
        slice_scatter_5269: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5266, slice_scatter_5268, 1, 7024, 7040);  slice_scatter_5266 = slice_scatter_5268 = None
        slice_28988: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5269, 1, 7024, 7040)
        slice_28989: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_28988, 2, 0, 16)
        slice_scatter_5271: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_28988, slice_28989, 2, 0, 16);  slice_28988 = slice_28989 = None
        slice_scatter_5272: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5269, slice_scatter_5271, 1, 7024, 7040);  slice_scatter_5269 = slice_scatter_5271 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29009: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_28975, 2, 16, 32);  slice_28975 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_882: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_29009, memory_format = torch.contiguous_format);  slice_29009 = None
        view_1768: "f32[32, 11]" = torch.ops.aten.view.default(clone_882, [32, 11]);  clone_882 = None
        mm_879: "f32[32, 8]" = torch.ops.aten.mm.default(view_1768, slice_37)
        view_1769: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_879, [2, 16, 8]);  mm_879 = None
        slice_29016: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5272, 1, 7024, 7040)
        slice_29017: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29016, 2, 0, 16)
        add_881: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29017, view_1769);  slice_29017 = view_1769 = None
        slice_scatter_5274: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29016, add_881, 2, 0, 16);  slice_29016 = add_881 = None
        slice_scatter_5275: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5272, slice_scatter_5274, 1, 7024, 7040);  slice_scatter_5272 = slice_scatter_5274 = None
        slice_29021: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5275, 1, 7024, 7040)
        slice_29022: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29021, 2, 0, 16)
        slice_scatter_5277: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29021, slice_29022, 2, 0, 16);  slice_29021 = slice_29022 = None
        slice_scatter_5278: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5275, slice_scatter_5277, 1, 7024, 7040);  slice_scatter_5275 = slice_scatter_5277 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29041: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7040, 7056)
        slice_29042: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_29041, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_883: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_29042, memory_format = torch.contiguous_format);  slice_29042 = None
        view_1770: "f32[32, 16]" = torch.ops.aten.view.default(clone_883, [32, 16]);  clone_883 = None
        mm_880: "f32[32, 8]" = torch.ops.aten.mm.default(view_1770, slice_7)
        view_1771: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_880, [2, 16, 8]);  mm_880 = None
        slice_29049: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5278, 1, 7040, 7056)
        slice_29050: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29049, 2, 0, 16)
        add_882: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29050, view_1771);  slice_29050 = view_1771 = None
        slice_scatter_5280: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29049, add_882, 2, 0, 16);  slice_29049 = add_882 = None
        slice_scatter_5281: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5278, slice_scatter_5280, 1, 7040, 7056);  slice_scatter_5278 = slice_scatter_5280 = None
        slice_29054: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5281, 1, 7040, 7056)
        slice_29055: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29054, 2, 0, 16)
        slice_scatter_5283: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29054, slice_29055, 2, 0, 16);  slice_29054 = slice_29055 = None
        slice_scatter_5284: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5281, slice_scatter_5283, 1, 7040, 7056);  slice_scatter_5281 = slice_scatter_5283 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29075: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_29041, 2, 16, 32);  slice_29041 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_884: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_29075, memory_format = torch.contiguous_format);  slice_29075 = None
        view_1772: "f32[32, 11]" = torch.ops.aten.view.default(clone_884, [32, 11]);  clone_884 = None
        mm_881: "f32[32, 8]" = torch.ops.aten.mm.default(view_1772, slice_37)
        view_1773: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_881, [2, 16, 8]);  mm_881 = None
        slice_29082: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5284, 1, 7040, 7056)
        slice_29083: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29082, 2, 0, 16)
        add_883: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29083, view_1773);  slice_29083 = view_1773 = None
        slice_scatter_5286: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29082, add_883, 2, 0, 16);  slice_29082 = add_883 = None
        slice_scatter_5287: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5284, slice_scatter_5286, 1, 7040, 7056);  slice_scatter_5284 = slice_scatter_5286 = None
        slice_29087: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5287, 1, 7040, 7056)
        slice_29088: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29087, 2, 0, 16)
        slice_scatter_5289: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29087, slice_29088, 2, 0, 16);  slice_29087 = slice_29088 = None
        slice_scatter_5290: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5287, slice_scatter_5289, 1, 7040, 7056);  slice_scatter_5287 = slice_scatter_5289 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29107: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7056, 7072)
        slice_29108: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_29107, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_885: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_29108, memory_format = torch.contiguous_format);  slice_29108 = None
        view_1774: "f32[32, 16]" = torch.ops.aten.view.default(clone_885, [32, 16]);  clone_885 = None
        mm_882: "f32[32, 8]" = torch.ops.aten.mm.default(view_1774, slice_7)
        view_1775: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_882, [2, 16, 8]);  mm_882 = None
        slice_29115: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5290, 1, 7056, 7072)
        slice_29116: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29115, 2, 0, 16)
        add_884: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29116, view_1775);  slice_29116 = view_1775 = None
        slice_scatter_5292: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29115, add_884, 2, 0, 16);  slice_29115 = add_884 = None
        slice_scatter_5293: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5290, slice_scatter_5292, 1, 7056, 7072);  slice_scatter_5290 = slice_scatter_5292 = None
        slice_29120: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5293, 1, 7056, 7072)
        slice_29121: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29120, 2, 0, 16)
        slice_scatter_5295: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29120, slice_29121, 2, 0, 16);  slice_29120 = slice_29121 = None
        slice_scatter_5296: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5293, slice_scatter_5295, 1, 7056, 7072);  slice_scatter_5293 = slice_scatter_5295 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29141: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_29107, 2, 16, 32);  slice_29107 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_886: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_29141, memory_format = torch.contiguous_format);  slice_29141 = None
        view_1776: "f32[32, 11]" = torch.ops.aten.view.default(clone_886, [32, 11]);  clone_886 = None
        mm_883: "f32[32, 8]" = torch.ops.aten.mm.default(view_1776, slice_37)
        view_1777: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_883, [2, 16, 8]);  mm_883 = None
        slice_29148: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5296, 1, 7056, 7072)
        slice_29149: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29148, 2, 0, 16)
        add_885: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29149, view_1777);  slice_29149 = view_1777 = None
        slice_scatter_5298: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29148, add_885, 2, 0, 16);  slice_29148 = add_885 = None
        slice_scatter_5299: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5296, slice_scatter_5298, 1, 7056, 7072);  slice_scatter_5296 = slice_scatter_5298 = None
        slice_29153: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5299, 1, 7056, 7072)
        slice_29154: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29153, 2, 0, 16)
        slice_scatter_5301: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29153, slice_29154, 2, 0, 16);  slice_29153 = slice_29154 = None
        slice_scatter_5302: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5299, slice_scatter_5301, 1, 7056, 7072);  slice_scatter_5299 = slice_scatter_5301 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29173: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7072, 7088)
        slice_29174: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_29173, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_887: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_29174, memory_format = torch.contiguous_format);  slice_29174 = None
        view_1778: "f32[32, 16]" = torch.ops.aten.view.default(clone_887, [32, 16]);  clone_887 = None
        mm_884: "f32[32, 8]" = torch.ops.aten.mm.default(view_1778, slice_7)
        view_1779: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_884, [2, 16, 8]);  mm_884 = None
        slice_29181: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5302, 1, 7072, 7088)
        slice_29182: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29181, 2, 0, 16)
        add_886: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29182, view_1779);  slice_29182 = view_1779 = None
        slice_scatter_5304: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29181, add_886, 2, 0, 16);  slice_29181 = add_886 = None
        slice_scatter_5305: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5302, slice_scatter_5304, 1, 7072, 7088);  slice_scatter_5302 = slice_scatter_5304 = None
        slice_29186: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5305, 1, 7072, 7088)
        slice_29187: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29186, 2, 0, 16)
        slice_scatter_5307: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29186, slice_29187, 2, 0, 16);  slice_29186 = slice_29187 = None
        slice_scatter_5308: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5305, slice_scatter_5307, 1, 7072, 7088);  slice_scatter_5305 = slice_scatter_5307 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29207: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_29173, 2, 16, 32);  slice_29173 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_888: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_29207, memory_format = torch.contiguous_format);  slice_29207 = None
        view_1780: "f32[32, 11]" = torch.ops.aten.view.default(clone_888, [32, 11]);  clone_888 = None
        mm_885: "f32[32, 8]" = torch.ops.aten.mm.default(view_1780, slice_37)
        view_1781: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_885, [2, 16, 8]);  mm_885 = None
        slice_29214: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5308, 1, 7072, 7088)
        slice_29215: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29214, 2, 0, 16)
        add_887: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29215, view_1781);  slice_29215 = view_1781 = None
        slice_scatter_5310: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29214, add_887, 2, 0, 16);  slice_29214 = add_887 = None
        slice_scatter_5311: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5308, slice_scatter_5310, 1, 7072, 7088);  slice_scatter_5308 = slice_scatter_5310 = None
        slice_29219: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5311, 1, 7072, 7088)
        slice_29220: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29219, 2, 0, 16)
        slice_scatter_5313: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29219, slice_29220, 2, 0, 16);  slice_29219 = slice_29220 = None
        slice_scatter_5314: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5311, slice_scatter_5313, 1, 7072, 7088);  slice_scatter_5311 = slice_scatter_5313 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29239: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7088, 7104)
        slice_29240: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_29239, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_889: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_29240, memory_format = torch.contiguous_format);  slice_29240 = None
        view_1782: "f32[32, 16]" = torch.ops.aten.view.default(clone_889, [32, 16]);  clone_889 = None
        mm_886: "f32[32, 8]" = torch.ops.aten.mm.default(view_1782, slice_7)
        view_1783: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_886, [2, 16, 8]);  mm_886 = None
        slice_29247: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5314, 1, 7088, 7104)
        slice_29248: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29247, 2, 0, 16)
        add_888: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29248, view_1783);  slice_29248 = view_1783 = None
        slice_scatter_5316: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29247, add_888, 2, 0, 16);  slice_29247 = add_888 = None
        slice_scatter_5317: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5314, slice_scatter_5316, 1, 7088, 7104);  slice_scatter_5314 = slice_scatter_5316 = None
        slice_29252: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5317, 1, 7088, 7104)
        slice_29253: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29252, 2, 0, 16)
        slice_scatter_5319: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29252, slice_29253, 2, 0, 16);  slice_29252 = slice_29253 = None
        slice_scatter_5320: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5317, slice_scatter_5319, 1, 7088, 7104);  slice_scatter_5317 = slice_scatter_5319 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29273: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_29239, 2, 16, 32);  slice_29239 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_890: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_29273, memory_format = torch.contiguous_format);  slice_29273 = None
        view_1784: "f32[32, 11]" = torch.ops.aten.view.default(clone_890, [32, 11]);  clone_890 = None
        mm_887: "f32[32, 8]" = torch.ops.aten.mm.default(view_1784, slice_37)
        view_1785: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_887, [2, 16, 8]);  mm_887 = None
        slice_29280: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5320, 1, 7088, 7104)
        slice_29281: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29280, 2, 0, 16)
        add_889: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29281, view_1785);  slice_29281 = view_1785 = None
        slice_scatter_5322: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29280, add_889, 2, 0, 16);  slice_29280 = add_889 = None
        slice_scatter_5323: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5320, slice_scatter_5322, 1, 7088, 7104);  slice_scatter_5320 = slice_scatter_5322 = None
        slice_29285: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5323, 1, 7088, 7104)
        slice_29286: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29285, 2, 0, 16)
        slice_scatter_5325: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29285, slice_29286, 2, 0, 16);  slice_29285 = slice_29286 = None
        slice_scatter_5326: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5323, slice_scatter_5325, 1, 7088, 7104);  slice_scatter_5323 = slice_scatter_5325 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29305: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7104, 7120)
        slice_29306: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_29305, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_891: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_29306, memory_format = torch.contiguous_format);  slice_29306 = None
        view_1786: "f32[32, 16]" = torch.ops.aten.view.default(clone_891, [32, 16]);  clone_891 = None
        mm_888: "f32[32, 8]" = torch.ops.aten.mm.default(view_1786, slice_7)
        view_1787: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_888, [2, 16, 8]);  mm_888 = None
        slice_29313: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5326, 1, 7104, 7120)
        slice_29314: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29313, 2, 0, 16)
        add_890: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29314, view_1787);  slice_29314 = view_1787 = None
        slice_scatter_5328: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29313, add_890, 2, 0, 16);  slice_29313 = add_890 = None
        slice_scatter_5329: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5326, slice_scatter_5328, 1, 7104, 7120);  slice_scatter_5326 = slice_scatter_5328 = None
        slice_29318: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5329, 1, 7104, 7120)
        slice_29319: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29318, 2, 0, 16)
        slice_scatter_5331: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29318, slice_29319, 2, 0, 16);  slice_29318 = slice_29319 = None
        slice_scatter_5332: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5329, slice_scatter_5331, 1, 7104, 7120);  slice_scatter_5329 = slice_scatter_5331 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29339: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_29305, 2, 16, 32);  slice_29305 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_892: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_29339, memory_format = torch.contiguous_format);  slice_29339 = None
        view_1788: "f32[32, 11]" = torch.ops.aten.view.default(clone_892, [32, 11]);  clone_892 = None
        mm_889: "f32[32, 8]" = torch.ops.aten.mm.default(view_1788, slice_37)
        view_1789: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_889, [2, 16, 8]);  mm_889 = None
        slice_29346: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5332, 1, 7104, 7120)
        slice_29347: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29346, 2, 0, 16)
        add_891: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29347, view_1789);  slice_29347 = view_1789 = None
        slice_scatter_5334: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29346, add_891, 2, 0, 16);  slice_29346 = add_891 = None
        slice_scatter_5335: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5332, slice_scatter_5334, 1, 7104, 7120);  slice_scatter_5332 = slice_scatter_5334 = None
        slice_29351: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5335, 1, 7104, 7120)
        slice_29352: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29351, 2, 0, 16)
        slice_scatter_5337: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29351, slice_29352, 2, 0, 16);  slice_29351 = slice_29352 = None
        slice_scatter_5338: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5335, slice_scatter_5337, 1, 7104, 7120);  slice_scatter_5335 = slice_scatter_5337 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29371: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7120, 7136)
        slice_29372: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_29371, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_893: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_29372, memory_format = torch.contiguous_format);  slice_29372 = None
        view_1790: "f32[32, 16]" = torch.ops.aten.view.default(clone_893, [32, 16]);  clone_893 = None
        mm_890: "f32[32, 8]" = torch.ops.aten.mm.default(view_1790, slice_7)
        view_1791: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_890, [2, 16, 8]);  mm_890 = None
        slice_29379: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5338, 1, 7120, 7136)
        slice_29380: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29379, 2, 0, 16)
        add_892: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29380, view_1791);  slice_29380 = view_1791 = None
        slice_scatter_5340: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29379, add_892, 2, 0, 16);  slice_29379 = add_892 = None
        slice_scatter_5341: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5338, slice_scatter_5340, 1, 7120, 7136);  slice_scatter_5338 = slice_scatter_5340 = None
        slice_29384: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5341, 1, 7120, 7136)
        slice_29385: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29384, 2, 0, 16)
        slice_scatter_5343: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29384, slice_29385, 2, 0, 16);  slice_29384 = slice_29385 = None
        slice_scatter_5344: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5341, slice_scatter_5343, 1, 7120, 7136);  slice_scatter_5341 = slice_scatter_5343 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29405: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_29371, 2, 16, 32);  slice_29371 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_894: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_29405, memory_format = torch.contiguous_format);  slice_29405 = None
        view_1792: "f32[32, 11]" = torch.ops.aten.view.default(clone_894, [32, 11]);  clone_894 = None
        mm_891: "f32[32, 8]" = torch.ops.aten.mm.default(view_1792, slice_37)
        view_1793: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_891, [2, 16, 8]);  mm_891 = None
        slice_29412: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5344, 1, 7120, 7136)
        slice_29413: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29412, 2, 0, 16)
        add_893: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29413, view_1793);  slice_29413 = view_1793 = None
        slice_scatter_5346: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29412, add_893, 2, 0, 16);  slice_29412 = add_893 = None
        slice_scatter_5347: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5344, slice_scatter_5346, 1, 7120, 7136);  slice_scatter_5344 = slice_scatter_5346 = None
        slice_29417: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5347, 1, 7120, 7136)
        slice_29418: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29417, 2, 0, 16)
        slice_scatter_5349: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29417, slice_29418, 2, 0, 16);  slice_29417 = slice_29418 = None
        slice_scatter_5350: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5347, slice_scatter_5349, 1, 7120, 7136);  slice_scatter_5347 = slice_scatter_5349 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29437: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7136, 7152)
        slice_29438: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_29437, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_895: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_29438, memory_format = torch.contiguous_format);  slice_29438 = None
        view_1794: "f32[32, 16]" = torch.ops.aten.view.default(clone_895, [32, 16]);  clone_895 = None
        mm_892: "f32[32, 8]" = torch.ops.aten.mm.default(view_1794, slice_7)
        view_1795: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_892, [2, 16, 8]);  mm_892 = None
        slice_29445: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5350, 1, 7136, 7152)
        slice_29446: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29445, 2, 0, 16)
        add_894: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29446, view_1795);  slice_29446 = view_1795 = None
        slice_scatter_5352: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29445, add_894, 2, 0, 16);  slice_29445 = add_894 = None
        slice_scatter_5353: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5350, slice_scatter_5352, 1, 7136, 7152);  slice_scatter_5350 = slice_scatter_5352 = None
        slice_29450: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5353, 1, 7136, 7152)
        slice_29451: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29450, 2, 0, 16)
        slice_scatter_5355: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29450, slice_29451, 2, 0, 16);  slice_29450 = slice_29451 = None
        slice_scatter_5356: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5353, slice_scatter_5355, 1, 7136, 7152);  slice_scatter_5353 = slice_scatter_5355 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29471: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_29437, 2, 16, 32);  slice_29437 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_896: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_29471, memory_format = torch.contiguous_format);  slice_29471 = None
        view_1796: "f32[32, 11]" = torch.ops.aten.view.default(clone_896, [32, 11]);  clone_896 = None
        mm_893: "f32[32, 8]" = torch.ops.aten.mm.default(view_1796, slice_37)
        view_1797: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_893, [2, 16, 8]);  mm_893 = None
        slice_29478: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5356, 1, 7136, 7152)
        slice_29479: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29478, 2, 0, 16)
        add_895: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29479, view_1797);  slice_29479 = view_1797 = None
        slice_scatter_5358: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29478, add_895, 2, 0, 16);  slice_29478 = add_895 = None
        slice_scatter_5359: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5356, slice_scatter_5358, 1, 7136, 7152);  slice_scatter_5356 = slice_scatter_5358 = None
        slice_29483: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5359, 1, 7136, 7152)
        slice_29484: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29483, 2, 0, 16)
        slice_scatter_5361: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29483, slice_29484, 2, 0, 16);  slice_29483 = slice_29484 = None
        slice_scatter_5362: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5359, slice_scatter_5361, 1, 7136, 7152);  slice_scatter_5359 = slice_scatter_5361 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29503: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7152, 7168)
        slice_29504: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_29503, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_897: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_29504, memory_format = torch.contiguous_format);  slice_29504 = None
        view_1798: "f32[32, 16]" = torch.ops.aten.view.default(clone_897, [32, 16]);  clone_897 = None
        mm_894: "f32[32, 8]" = torch.ops.aten.mm.default(view_1798, slice_7)
        view_1799: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_894, [2, 16, 8]);  mm_894 = None
        slice_29511: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5362, 1, 7152, 7168)
        slice_29512: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29511, 2, 0, 16)
        add_896: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29512, view_1799);  slice_29512 = view_1799 = None
        slice_scatter_5364: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29511, add_896, 2, 0, 16);  slice_29511 = add_896 = None
        slice_scatter_5365: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5362, slice_scatter_5364, 1, 7152, 7168);  slice_scatter_5362 = slice_scatter_5364 = None
        slice_29516: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5365, 1, 7152, 7168)
        slice_29517: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29516, 2, 0, 16)
        slice_scatter_5367: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29516, slice_29517, 2, 0, 16);  slice_29516 = slice_29517 = None
        slice_scatter_5368: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5365, slice_scatter_5367, 1, 7152, 7168);  slice_scatter_5365 = slice_scatter_5367 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29537: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_29503, 2, 16, 32);  slice_29503 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_898: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_29537, memory_format = torch.contiguous_format);  slice_29537 = None
        view_1800: "f32[32, 11]" = torch.ops.aten.view.default(clone_898, [32, 11]);  clone_898 = None
        mm_895: "f32[32, 8]" = torch.ops.aten.mm.default(view_1800, slice_37)
        view_1801: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_895, [2, 16, 8]);  mm_895 = None
        slice_29544: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5368, 1, 7152, 7168)
        slice_29545: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29544, 2, 0, 16)
        add_897: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29545, view_1801);  slice_29545 = view_1801 = None
        slice_scatter_5370: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29544, add_897, 2, 0, 16);  slice_29544 = add_897 = None
        slice_scatter_5371: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5368, slice_scatter_5370, 1, 7152, 7168);  slice_scatter_5368 = slice_scatter_5370 = None
        slice_29549: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5371, 1, 7152, 7168)
        slice_29550: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29549, 2, 0, 16)
        slice_scatter_5373: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29549, slice_29550, 2, 0, 16);  slice_29549 = slice_29550 = None
        slice_scatter_5374: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5371, slice_scatter_5373, 1, 7152, 7168);  slice_scatter_5371 = slice_scatter_5373 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29569: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7168, 7184)
        slice_29570: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_29569, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_899: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_29570, memory_format = torch.contiguous_format);  slice_29570 = None
        view_1802: "f32[32, 16]" = torch.ops.aten.view.default(clone_899, [32, 16]);  clone_899 = None
        mm_896: "f32[32, 8]" = torch.ops.aten.mm.default(view_1802, slice_7)
        view_1803: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_896, [2, 16, 8]);  mm_896 = None
        slice_29577: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5374, 1, 7168, 7184)
        slice_29578: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29577, 2, 0, 16)
        add_898: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29578, view_1803);  slice_29578 = view_1803 = None
        slice_scatter_5376: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29577, add_898, 2, 0, 16);  slice_29577 = add_898 = None
        slice_scatter_5377: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5374, slice_scatter_5376, 1, 7168, 7184);  slice_scatter_5374 = slice_scatter_5376 = None
        slice_29582: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5377, 1, 7168, 7184)
        slice_29583: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29582, 2, 0, 16)
        slice_scatter_5379: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29582, slice_29583, 2, 0, 16);  slice_29582 = slice_29583 = None
        slice_scatter_5380: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5377, slice_scatter_5379, 1, 7168, 7184);  slice_scatter_5377 = slice_scatter_5379 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29603: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_29569, 2, 16, 32);  slice_29569 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_900: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_29603, memory_format = torch.contiguous_format);  slice_29603 = None
        view_1804: "f32[32, 11]" = torch.ops.aten.view.default(clone_900, [32, 11]);  clone_900 = None
        mm_897: "f32[32, 8]" = torch.ops.aten.mm.default(view_1804, slice_37)
        view_1805: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_897, [2, 16, 8]);  mm_897 = None
        slice_29610: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5380, 1, 7168, 7184)
        slice_29611: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29610, 2, 0, 16)
        add_899: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29611, view_1805);  slice_29611 = view_1805 = None
        slice_scatter_5382: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29610, add_899, 2, 0, 16);  slice_29610 = add_899 = None
        slice_scatter_5383: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5380, slice_scatter_5382, 1, 7168, 7184);  slice_scatter_5380 = slice_scatter_5382 = None
        slice_29615: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5383, 1, 7168, 7184)
        slice_29616: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29615, 2, 0, 16)
        slice_scatter_5385: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29615, slice_29616, 2, 0, 16);  slice_29615 = slice_29616 = None
        slice_scatter_5386: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5383, slice_scatter_5385, 1, 7168, 7184);  slice_scatter_5383 = slice_scatter_5385 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29635: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7184, 7200)
        slice_29636: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_29635, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_901: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_29636, memory_format = torch.contiguous_format);  slice_29636 = None
        view_1806: "f32[32, 16]" = torch.ops.aten.view.default(clone_901, [32, 16]);  clone_901 = None
        mm_898: "f32[32, 8]" = torch.ops.aten.mm.default(view_1806, slice_7)
        view_1807: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_898, [2, 16, 8]);  mm_898 = None
        slice_29643: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5386, 1, 7184, 7200)
        slice_29644: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29643, 2, 0, 16)
        add_900: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29644, view_1807);  slice_29644 = view_1807 = None
        slice_scatter_5388: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29643, add_900, 2, 0, 16);  slice_29643 = add_900 = None
        slice_scatter_5389: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5386, slice_scatter_5388, 1, 7184, 7200);  slice_scatter_5386 = slice_scatter_5388 = None
        slice_29648: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5389, 1, 7184, 7200)
        slice_29649: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29648, 2, 0, 16)
        slice_scatter_5391: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29648, slice_29649, 2, 0, 16);  slice_29648 = slice_29649 = None
        slice_scatter_5392: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5389, slice_scatter_5391, 1, 7184, 7200);  slice_scatter_5389 = slice_scatter_5391 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29669: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_29635, 2, 16, 32);  slice_29635 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_902: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_29669, memory_format = torch.contiguous_format);  slice_29669 = None
        view_1808: "f32[32, 11]" = torch.ops.aten.view.default(clone_902, [32, 11]);  clone_902 = None
        mm_899: "f32[32, 8]" = torch.ops.aten.mm.default(view_1808, slice_37)
        view_1809: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_899, [2, 16, 8]);  mm_899 = None
        slice_29676: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5392, 1, 7184, 7200)
        slice_29677: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29676, 2, 0, 16)
        add_901: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29677, view_1809);  slice_29677 = view_1809 = None
        slice_scatter_5394: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29676, add_901, 2, 0, 16);  slice_29676 = add_901 = None
        slice_scatter_5395: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5392, slice_scatter_5394, 1, 7184, 7200);  slice_scatter_5392 = slice_scatter_5394 = None
        slice_29681: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5395, 1, 7184, 7200)
        slice_29682: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29681, 2, 0, 16)
        slice_scatter_5397: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29681, slice_29682, 2, 0, 16);  slice_29681 = slice_29682 = None
        slice_scatter_5398: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5395, slice_scatter_5397, 1, 7184, 7200);  slice_scatter_5395 = slice_scatter_5397 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29701: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7200, 7216)
        slice_29702: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_29701, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_903: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_29702, memory_format = torch.contiguous_format);  slice_29702 = None
        view_1810: "f32[32, 16]" = torch.ops.aten.view.default(clone_903, [32, 16]);  clone_903 = None
        mm_900: "f32[32, 8]" = torch.ops.aten.mm.default(view_1810, slice_7)
        view_1811: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_900, [2, 16, 8]);  mm_900 = None
        slice_29709: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5398, 1, 7200, 7216)
        slice_29710: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29709, 2, 0, 16)
        add_902: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29710, view_1811);  slice_29710 = view_1811 = None
        slice_scatter_5400: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29709, add_902, 2, 0, 16);  slice_29709 = add_902 = None
        slice_scatter_5401: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5398, slice_scatter_5400, 1, 7200, 7216);  slice_scatter_5398 = slice_scatter_5400 = None
        slice_29714: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5401, 1, 7200, 7216)
        slice_29715: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29714, 2, 0, 16)
        slice_scatter_5403: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29714, slice_29715, 2, 0, 16);  slice_29714 = slice_29715 = None
        slice_scatter_5404: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5401, slice_scatter_5403, 1, 7200, 7216);  slice_scatter_5401 = slice_scatter_5403 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29735: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_29701, 2, 16, 32);  slice_29701 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_904: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_29735, memory_format = torch.contiguous_format);  slice_29735 = None
        view_1812: "f32[32, 11]" = torch.ops.aten.view.default(clone_904, [32, 11]);  clone_904 = None
        mm_901: "f32[32, 8]" = torch.ops.aten.mm.default(view_1812, slice_37)
        view_1813: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_901, [2, 16, 8]);  mm_901 = None
        slice_29742: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5404, 1, 7200, 7216)
        slice_29743: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29742, 2, 0, 16)
        add_903: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29743, view_1813);  slice_29743 = view_1813 = None
        slice_scatter_5406: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29742, add_903, 2, 0, 16);  slice_29742 = add_903 = None
        slice_scatter_5407: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5404, slice_scatter_5406, 1, 7200, 7216);  slice_scatter_5404 = slice_scatter_5406 = None
        slice_29747: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5407, 1, 7200, 7216)
        slice_29748: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29747, 2, 0, 16)
        slice_scatter_5409: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29747, slice_29748, 2, 0, 16);  slice_29747 = slice_29748 = None
        slice_scatter_5410: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5407, slice_scatter_5409, 1, 7200, 7216);  slice_scatter_5407 = slice_scatter_5409 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29767: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7216, 7232)
        slice_29768: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_29767, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_905: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_29768, memory_format = torch.contiguous_format);  slice_29768 = None
        view_1814: "f32[32, 16]" = torch.ops.aten.view.default(clone_905, [32, 16]);  clone_905 = None
        mm_902: "f32[32, 8]" = torch.ops.aten.mm.default(view_1814, slice_7)
        view_1815: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_902, [2, 16, 8]);  mm_902 = None
        slice_29775: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5410, 1, 7216, 7232)
        slice_29776: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29775, 2, 0, 16)
        add_904: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29776, view_1815);  slice_29776 = view_1815 = None
        slice_scatter_5412: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29775, add_904, 2, 0, 16);  slice_29775 = add_904 = None
        slice_scatter_5413: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5410, slice_scatter_5412, 1, 7216, 7232);  slice_scatter_5410 = slice_scatter_5412 = None
        slice_29780: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5413, 1, 7216, 7232)
        slice_29781: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29780, 2, 0, 16)
        slice_scatter_5415: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29780, slice_29781, 2, 0, 16);  slice_29780 = slice_29781 = None
        slice_scatter_5416: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5413, slice_scatter_5415, 1, 7216, 7232);  slice_scatter_5413 = slice_scatter_5415 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29801: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_29767, 2, 16, 32);  slice_29767 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_906: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_29801, memory_format = torch.contiguous_format);  slice_29801 = None
        view_1816: "f32[32, 11]" = torch.ops.aten.view.default(clone_906, [32, 11]);  clone_906 = None
        mm_903: "f32[32, 8]" = torch.ops.aten.mm.default(view_1816, slice_37)
        view_1817: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_903, [2, 16, 8]);  mm_903 = None
        slice_29808: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5416, 1, 7216, 7232)
        slice_29809: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29808, 2, 0, 16)
        add_905: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29809, view_1817);  slice_29809 = view_1817 = None
        slice_scatter_5418: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29808, add_905, 2, 0, 16);  slice_29808 = add_905 = None
        slice_scatter_5419: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5416, slice_scatter_5418, 1, 7216, 7232);  slice_scatter_5416 = slice_scatter_5418 = None
        slice_29813: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5419, 1, 7216, 7232)
        slice_29814: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29813, 2, 0, 16)
        slice_scatter_5421: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29813, slice_29814, 2, 0, 16);  slice_29813 = slice_29814 = None
        slice_scatter_5422: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5419, slice_scatter_5421, 1, 7216, 7232);  slice_scatter_5419 = slice_scatter_5421 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29833: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7232, 7248)
        slice_29834: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_29833, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_907: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_29834, memory_format = torch.contiguous_format);  slice_29834 = None
        view_1818: "f32[32, 16]" = torch.ops.aten.view.default(clone_907, [32, 16]);  clone_907 = None
        mm_904: "f32[32, 8]" = torch.ops.aten.mm.default(view_1818, slice_7)
        view_1819: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_904, [2, 16, 8]);  mm_904 = None
        slice_29841: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5422, 1, 7232, 7248)
        slice_29842: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29841, 2, 0, 16)
        add_906: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29842, view_1819);  slice_29842 = view_1819 = None
        slice_scatter_5424: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29841, add_906, 2, 0, 16);  slice_29841 = add_906 = None
        slice_scatter_5425: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5422, slice_scatter_5424, 1, 7232, 7248);  slice_scatter_5422 = slice_scatter_5424 = None
        slice_29846: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5425, 1, 7232, 7248)
        slice_29847: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29846, 2, 0, 16)
        slice_scatter_5427: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29846, slice_29847, 2, 0, 16);  slice_29846 = slice_29847 = None
        slice_scatter_5428: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5425, slice_scatter_5427, 1, 7232, 7248);  slice_scatter_5425 = slice_scatter_5427 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29867: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_29833, 2, 16, 32);  slice_29833 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_908: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_29867, memory_format = torch.contiguous_format);  slice_29867 = None
        view_1820: "f32[32, 11]" = torch.ops.aten.view.default(clone_908, [32, 11]);  clone_908 = None
        mm_905: "f32[32, 8]" = torch.ops.aten.mm.default(view_1820, slice_37)
        view_1821: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_905, [2, 16, 8]);  mm_905 = None
        slice_29874: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5428, 1, 7232, 7248)
        slice_29875: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29874, 2, 0, 16)
        add_907: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29875, view_1821);  slice_29875 = view_1821 = None
        slice_scatter_5430: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29874, add_907, 2, 0, 16);  slice_29874 = add_907 = None
        slice_scatter_5431: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5428, slice_scatter_5430, 1, 7232, 7248);  slice_scatter_5428 = slice_scatter_5430 = None
        slice_29879: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5431, 1, 7232, 7248)
        slice_29880: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29879, 2, 0, 16)
        slice_scatter_5433: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29879, slice_29880, 2, 0, 16);  slice_29879 = slice_29880 = None
        slice_scatter_5434: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5431, slice_scatter_5433, 1, 7232, 7248);  slice_scatter_5431 = slice_scatter_5433 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29899: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7248, 7264)
        slice_29900: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_29899, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_909: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_29900, memory_format = torch.contiguous_format);  slice_29900 = None
        view_1822: "f32[32, 16]" = torch.ops.aten.view.default(clone_909, [32, 16]);  clone_909 = None
        mm_906: "f32[32, 8]" = torch.ops.aten.mm.default(view_1822, slice_7)
        view_1823: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_906, [2, 16, 8]);  mm_906 = None
        slice_29907: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5434, 1, 7248, 7264)
        slice_29908: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29907, 2, 0, 16)
        add_908: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29908, view_1823);  slice_29908 = view_1823 = None
        slice_scatter_5436: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29907, add_908, 2, 0, 16);  slice_29907 = add_908 = None
        slice_scatter_5437: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5434, slice_scatter_5436, 1, 7248, 7264);  slice_scatter_5434 = slice_scatter_5436 = None
        slice_29912: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5437, 1, 7248, 7264)
        slice_29913: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29912, 2, 0, 16)
        slice_scatter_5439: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29912, slice_29913, 2, 0, 16);  slice_29912 = slice_29913 = None
        slice_scatter_5440: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5437, slice_scatter_5439, 1, 7248, 7264);  slice_scatter_5437 = slice_scatter_5439 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29933: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_29899, 2, 16, 32);  slice_29899 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_910: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_29933, memory_format = torch.contiguous_format);  slice_29933 = None
        view_1824: "f32[32, 11]" = torch.ops.aten.view.default(clone_910, [32, 11]);  clone_910 = None
        mm_907: "f32[32, 8]" = torch.ops.aten.mm.default(view_1824, slice_37)
        view_1825: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_907, [2, 16, 8]);  mm_907 = None
        slice_29940: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5440, 1, 7248, 7264)
        slice_29941: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29940, 2, 0, 16)
        add_909: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29941, view_1825);  slice_29941 = view_1825 = None
        slice_scatter_5442: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29940, add_909, 2, 0, 16);  slice_29940 = add_909 = None
        slice_scatter_5443: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5440, slice_scatter_5442, 1, 7248, 7264);  slice_scatter_5440 = slice_scatter_5442 = None
        slice_29945: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5443, 1, 7248, 7264)
        slice_29946: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29945, 2, 0, 16)
        slice_scatter_5445: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29945, slice_29946, 2, 0, 16);  slice_29945 = slice_29946 = None
        slice_scatter_5446: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5443, slice_scatter_5445, 1, 7248, 7264);  slice_scatter_5443 = slice_scatter_5445 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29965: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7264, 7280)
        slice_29966: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_29965, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_911: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_29966, memory_format = torch.contiguous_format);  slice_29966 = None
        view_1826: "f32[32, 16]" = torch.ops.aten.view.default(clone_911, [32, 16]);  clone_911 = None
        mm_908: "f32[32, 8]" = torch.ops.aten.mm.default(view_1826, slice_7)
        view_1827: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_908, [2, 16, 8]);  mm_908 = None
        slice_29973: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5446, 1, 7264, 7280)
        slice_29974: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29973, 2, 0, 16)
        add_910: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_29974, view_1827);  slice_29974 = view_1827 = None
        slice_scatter_5448: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29973, add_910, 2, 0, 16);  slice_29973 = add_910 = None
        slice_scatter_5449: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5446, slice_scatter_5448, 1, 7264, 7280);  slice_scatter_5446 = slice_scatter_5448 = None
        slice_29978: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5449, 1, 7264, 7280)
        slice_29979: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_29978, 2, 0, 16)
        slice_scatter_5451: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_29978, slice_29979, 2, 0, 16);  slice_29978 = slice_29979 = None
        slice_scatter_5452: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5449, slice_scatter_5451, 1, 7264, 7280);  slice_scatter_5449 = slice_scatter_5451 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_29999: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_29965, 2, 16, 32);  slice_29965 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_912: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_29999, memory_format = torch.contiguous_format);  slice_29999 = None
        view_1828: "f32[32, 11]" = torch.ops.aten.view.default(clone_912, [32, 11]);  clone_912 = None
        mm_909: "f32[32, 8]" = torch.ops.aten.mm.default(view_1828, slice_37)
        view_1829: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_909, [2, 16, 8]);  mm_909 = None
        slice_30006: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5452, 1, 7264, 7280)
        slice_30007: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30006, 2, 0, 16)
        add_911: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30007, view_1829);  slice_30007 = view_1829 = None
        slice_scatter_5454: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30006, add_911, 2, 0, 16);  slice_30006 = add_911 = None
        slice_scatter_5455: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5452, slice_scatter_5454, 1, 7264, 7280);  slice_scatter_5452 = slice_scatter_5454 = None
        slice_30011: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5455, 1, 7264, 7280)
        slice_30012: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30011, 2, 0, 16)
        slice_scatter_5457: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30011, slice_30012, 2, 0, 16);  slice_30011 = slice_30012 = None
        slice_scatter_5458: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5455, slice_scatter_5457, 1, 7264, 7280);  slice_scatter_5455 = slice_scatter_5457 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30031: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7280, 7296)
        slice_30032: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_30031, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_913: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_30032, memory_format = torch.contiguous_format);  slice_30032 = None
        view_1830: "f32[32, 16]" = torch.ops.aten.view.default(clone_913, [32, 16]);  clone_913 = None
        mm_910: "f32[32, 8]" = torch.ops.aten.mm.default(view_1830, slice_7)
        view_1831: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_910, [2, 16, 8]);  mm_910 = None
        slice_30039: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5458, 1, 7280, 7296)
        slice_30040: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30039, 2, 0, 16)
        add_912: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30040, view_1831);  slice_30040 = view_1831 = None
        slice_scatter_5460: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30039, add_912, 2, 0, 16);  slice_30039 = add_912 = None
        slice_scatter_5461: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5458, slice_scatter_5460, 1, 7280, 7296);  slice_scatter_5458 = slice_scatter_5460 = None
        slice_30044: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5461, 1, 7280, 7296)
        slice_30045: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30044, 2, 0, 16)
        slice_scatter_5463: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30044, slice_30045, 2, 0, 16);  slice_30044 = slice_30045 = None
        slice_scatter_5464: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5461, slice_scatter_5463, 1, 7280, 7296);  slice_scatter_5461 = slice_scatter_5463 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30065: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_30031, 2, 16, 32);  slice_30031 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_914: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_30065, memory_format = torch.contiguous_format);  slice_30065 = None
        view_1832: "f32[32, 11]" = torch.ops.aten.view.default(clone_914, [32, 11]);  clone_914 = None
        mm_911: "f32[32, 8]" = torch.ops.aten.mm.default(view_1832, slice_37)
        view_1833: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_911, [2, 16, 8]);  mm_911 = None
        slice_30072: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5464, 1, 7280, 7296)
        slice_30073: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30072, 2, 0, 16)
        add_913: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30073, view_1833);  slice_30073 = view_1833 = None
        slice_scatter_5466: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30072, add_913, 2, 0, 16);  slice_30072 = add_913 = None
        slice_scatter_5467: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5464, slice_scatter_5466, 1, 7280, 7296);  slice_scatter_5464 = slice_scatter_5466 = None
        slice_30077: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5467, 1, 7280, 7296)
        slice_30078: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30077, 2, 0, 16)
        slice_scatter_5469: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30077, slice_30078, 2, 0, 16);  slice_30077 = slice_30078 = None
        slice_scatter_5470: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5467, slice_scatter_5469, 1, 7280, 7296);  slice_scatter_5467 = slice_scatter_5469 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30097: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7296, 7312)
        slice_30098: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_30097, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_915: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_30098, memory_format = torch.contiguous_format);  slice_30098 = None
        view_1834: "f32[32, 16]" = torch.ops.aten.view.default(clone_915, [32, 16]);  clone_915 = None
        mm_912: "f32[32, 8]" = torch.ops.aten.mm.default(view_1834, slice_7)
        view_1835: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_912, [2, 16, 8]);  mm_912 = None
        slice_30105: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5470, 1, 7296, 7312)
        slice_30106: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30105, 2, 0, 16)
        add_914: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30106, view_1835);  slice_30106 = view_1835 = None
        slice_scatter_5472: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30105, add_914, 2, 0, 16);  slice_30105 = add_914 = None
        slice_scatter_5473: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5470, slice_scatter_5472, 1, 7296, 7312);  slice_scatter_5470 = slice_scatter_5472 = None
        slice_30110: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5473, 1, 7296, 7312)
        slice_30111: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30110, 2, 0, 16)
        slice_scatter_5475: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30110, slice_30111, 2, 0, 16);  slice_30110 = slice_30111 = None
        slice_scatter_5476: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5473, slice_scatter_5475, 1, 7296, 7312);  slice_scatter_5473 = slice_scatter_5475 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30131: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_30097, 2, 16, 32);  slice_30097 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_916: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_30131, memory_format = torch.contiguous_format);  slice_30131 = None
        view_1836: "f32[32, 11]" = torch.ops.aten.view.default(clone_916, [32, 11]);  clone_916 = None
        mm_913: "f32[32, 8]" = torch.ops.aten.mm.default(view_1836, slice_37)
        view_1837: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_913, [2, 16, 8]);  mm_913 = None
        slice_30138: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5476, 1, 7296, 7312)
        slice_30139: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30138, 2, 0, 16)
        add_915: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30139, view_1837);  slice_30139 = view_1837 = None
        slice_scatter_5478: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30138, add_915, 2, 0, 16);  slice_30138 = add_915 = None
        slice_scatter_5479: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5476, slice_scatter_5478, 1, 7296, 7312);  slice_scatter_5476 = slice_scatter_5478 = None
        slice_30143: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5479, 1, 7296, 7312)
        slice_30144: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30143, 2, 0, 16)
        slice_scatter_5481: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30143, slice_30144, 2, 0, 16);  slice_30143 = slice_30144 = None
        slice_scatter_5482: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5479, slice_scatter_5481, 1, 7296, 7312);  slice_scatter_5479 = slice_scatter_5481 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30163: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7312, 7328)
        slice_30164: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_30163, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_917: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_30164, memory_format = torch.contiguous_format);  slice_30164 = None
        view_1838: "f32[32, 16]" = torch.ops.aten.view.default(clone_917, [32, 16]);  clone_917 = None
        mm_914: "f32[32, 8]" = torch.ops.aten.mm.default(view_1838, slice_7)
        view_1839: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_914, [2, 16, 8]);  mm_914 = None
        slice_30171: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5482, 1, 7312, 7328)
        slice_30172: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30171, 2, 0, 16)
        add_916: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30172, view_1839);  slice_30172 = view_1839 = None
        slice_scatter_5484: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30171, add_916, 2, 0, 16);  slice_30171 = add_916 = None
        slice_scatter_5485: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5482, slice_scatter_5484, 1, 7312, 7328);  slice_scatter_5482 = slice_scatter_5484 = None
        slice_30176: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5485, 1, 7312, 7328)
        slice_30177: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30176, 2, 0, 16)
        slice_scatter_5487: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30176, slice_30177, 2, 0, 16);  slice_30176 = slice_30177 = None
        slice_scatter_5488: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5485, slice_scatter_5487, 1, 7312, 7328);  slice_scatter_5485 = slice_scatter_5487 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30197: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_30163, 2, 16, 32);  slice_30163 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_918: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_30197, memory_format = torch.contiguous_format);  slice_30197 = None
        view_1840: "f32[32, 11]" = torch.ops.aten.view.default(clone_918, [32, 11]);  clone_918 = None
        mm_915: "f32[32, 8]" = torch.ops.aten.mm.default(view_1840, slice_37)
        view_1841: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_915, [2, 16, 8]);  mm_915 = None
        slice_30204: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5488, 1, 7312, 7328)
        slice_30205: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30204, 2, 0, 16)
        add_917: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30205, view_1841);  slice_30205 = view_1841 = None
        slice_scatter_5490: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30204, add_917, 2, 0, 16);  slice_30204 = add_917 = None
        slice_scatter_5491: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5488, slice_scatter_5490, 1, 7312, 7328);  slice_scatter_5488 = slice_scatter_5490 = None
        slice_30209: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5491, 1, 7312, 7328)
        slice_30210: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30209, 2, 0, 16)
        slice_scatter_5493: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30209, slice_30210, 2, 0, 16);  slice_30209 = slice_30210 = None
        slice_scatter_5494: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5491, slice_scatter_5493, 1, 7312, 7328);  slice_scatter_5491 = slice_scatter_5493 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30229: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7328, 7344)
        slice_30230: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_30229, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_919: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_30230, memory_format = torch.contiguous_format);  slice_30230 = None
        view_1842: "f32[32, 16]" = torch.ops.aten.view.default(clone_919, [32, 16]);  clone_919 = None
        mm_916: "f32[32, 8]" = torch.ops.aten.mm.default(view_1842, slice_7)
        view_1843: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_916, [2, 16, 8]);  mm_916 = None
        slice_30237: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5494, 1, 7328, 7344)
        slice_30238: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30237, 2, 0, 16)
        add_918: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30238, view_1843);  slice_30238 = view_1843 = None
        slice_scatter_5496: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30237, add_918, 2, 0, 16);  slice_30237 = add_918 = None
        slice_scatter_5497: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5494, slice_scatter_5496, 1, 7328, 7344);  slice_scatter_5494 = slice_scatter_5496 = None
        slice_30242: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5497, 1, 7328, 7344)
        slice_30243: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30242, 2, 0, 16)
        slice_scatter_5499: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30242, slice_30243, 2, 0, 16);  slice_30242 = slice_30243 = None
        slice_scatter_5500: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5497, slice_scatter_5499, 1, 7328, 7344);  slice_scatter_5497 = slice_scatter_5499 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30263: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_30229, 2, 16, 32);  slice_30229 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_920: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_30263, memory_format = torch.contiguous_format);  slice_30263 = None
        view_1844: "f32[32, 11]" = torch.ops.aten.view.default(clone_920, [32, 11]);  clone_920 = None
        mm_917: "f32[32, 8]" = torch.ops.aten.mm.default(view_1844, slice_37)
        view_1845: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_917, [2, 16, 8]);  mm_917 = None
        slice_30270: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5500, 1, 7328, 7344)
        slice_30271: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30270, 2, 0, 16)
        add_919: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30271, view_1845);  slice_30271 = view_1845 = None
        slice_scatter_5502: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30270, add_919, 2, 0, 16);  slice_30270 = add_919 = None
        slice_scatter_5503: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5500, slice_scatter_5502, 1, 7328, 7344);  slice_scatter_5500 = slice_scatter_5502 = None
        slice_30275: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5503, 1, 7328, 7344)
        slice_30276: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30275, 2, 0, 16)
        slice_scatter_5505: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30275, slice_30276, 2, 0, 16);  slice_30275 = slice_30276 = None
        slice_scatter_5506: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5503, slice_scatter_5505, 1, 7328, 7344);  slice_scatter_5503 = slice_scatter_5505 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30295: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7344, 7360)
        slice_30296: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_30295, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_921: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_30296, memory_format = torch.contiguous_format);  slice_30296 = None
        view_1846: "f32[32, 16]" = torch.ops.aten.view.default(clone_921, [32, 16]);  clone_921 = None
        mm_918: "f32[32, 8]" = torch.ops.aten.mm.default(view_1846, slice_7)
        view_1847: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_918, [2, 16, 8]);  mm_918 = None
        slice_30303: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5506, 1, 7344, 7360)
        slice_30304: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30303, 2, 0, 16)
        add_920: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30304, view_1847);  slice_30304 = view_1847 = None
        slice_scatter_5508: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30303, add_920, 2, 0, 16);  slice_30303 = add_920 = None
        slice_scatter_5509: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5506, slice_scatter_5508, 1, 7344, 7360);  slice_scatter_5506 = slice_scatter_5508 = None
        slice_30308: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5509, 1, 7344, 7360)
        slice_30309: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30308, 2, 0, 16)
        slice_scatter_5511: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30308, slice_30309, 2, 0, 16);  slice_30308 = slice_30309 = None
        slice_scatter_5512: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5509, slice_scatter_5511, 1, 7344, 7360);  slice_scatter_5509 = slice_scatter_5511 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30329: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_30295, 2, 16, 32);  slice_30295 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_922: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_30329, memory_format = torch.contiguous_format);  slice_30329 = None
        view_1848: "f32[32, 11]" = torch.ops.aten.view.default(clone_922, [32, 11]);  clone_922 = None
        mm_919: "f32[32, 8]" = torch.ops.aten.mm.default(view_1848, slice_37)
        view_1849: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_919, [2, 16, 8]);  mm_919 = None
        slice_30336: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5512, 1, 7344, 7360)
        slice_30337: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30336, 2, 0, 16)
        add_921: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30337, view_1849);  slice_30337 = view_1849 = None
        slice_scatter_5514: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30336, add_921, 2, 0, 16);  slice_30336 = add_921 = None
        slice_scatter_5515: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5512, slice_scatter_5514, 1, 7344, 7360);  slice_scatter_5512 = slice_scatter_5514 = None
        slice_30341: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5515, 1, 7344, 7360)
        slice_30342: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30341, 2, 0, 16)
        slice_scatter_5517: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30341, slice_30342, 2, 0, 16);  slice_30341 = slice_30342 = None
        slice_scatter_5518: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5515, slice_scatter_5517, 1, 7344, 7360);  slice_scatter_5515 = slice_scatter_5517 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30361: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7360, 7376)
        slice_30362: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_30361, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_923: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_30362, memory_format = torch.contiguous_format);  slice_30362 = None
        view_1850: "f32[32, 16]" = torch.ops.aten.view.default(clone_923, [32, 16]);  clone_923 = None
        mm_920: "f32[32, 8]" = torch.ops.aten.mm.default(view_1850, slice_7)
        view_1851: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_920, [2, 16, 8]);  mm_920 = None
        slice_30369: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5518, 1, 7360, 7376)
        slice_30370: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30369, 2, 0, 16)
        add_922: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30370, view_1851);  slice_30370 = view_1851 = None
        slice_scatter_5520: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30369, add_922, 2, 0, 16);  slice_30369 = add_922 = None
        slice_scatter_5521: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5518, slice_scatter_5520, 1, 7360, 7376);  slice_scatter_5518 = slice_scatter_5520 = None
        slice_30374: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5521, 1, 7360, 7376)
        slice_30375: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30374, 2, 0, 16)
        slice_scatter_5523: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30374, slice_30375, 2, 0, 16);  slice_30374 = slice_30375 = None
        slice_scatter_5524: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5521, slice_scatter_5523, 1, 7360, 7376);  slice_scatter_5521 = slice_scatter_5523 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30395: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_30361, 2, 16, 32);  slice_30361 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_924: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_30395, memory_format = torch.contiguous_format);  slice_30395 = None
        view_1852: "f32[32, 11]" = torch.ops.aten.view.default(clone_924, [32, 11]);  clone_924 = None
        mm_921: "f32[32, 8]" = torch.ops.aten.mm.default(view_1852, slice_37)
        view_1853: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_921, [2, 16, 8]);  mm_921 = None
        slice_30402: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5524, 1, 7360, 7376)
        slice_30403: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30402, 2, 0, 16)
        add_923: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30403, view_1853);  slice_30403 = view_1853 = None
        slice_scatter_5526: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30402, add_923, 2, 0, 16);  slice_30402 = add_923 = None
        slice_scatter_5527: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5524, slice_scatter_5526, 1, 7360, 7376);  slice_scatter_5524 = slice_scatter_5526 = None
        slice_30407: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5527, 1, 7360, 7376)
        slice_30408: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30407, 2, 0, 16)
        slice_scatter_5529: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30407, slice_30408, 2, 0, 16);  slice_30407 = slice_30408 = None
        slice_scatter_5530: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5527, slice_scatter_5529, 1, 7360, 7376);  slice_scatter_5527 = slice_scatter_5529 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30427: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7376, 7392)
        slice_30428: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_30427, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_925: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_30428, memory_format = torch.contiguous_format);  slice_30428 = None
        view_1854: "f32[32, 16]" = torch.ops.aten.view.default(clone_925, [32, 16]);  clone_925 = None
        mm_922: "f32[32, 8]" = torch.ops.aten.mm.default(view_1854, slice_7)
        view_1855: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_922, [2, 16, 8]);  mm_922 = None
        slice_30435: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5530, 1, 7376, 7392)
        slice_30436: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30435, 2, 0, 16)
        add_924: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30436, view_1855);  slice_30436 = view_1855 = None
        slice_scatter_5532: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30435, add_924, 2, 0, 16);  slice_30435 = add_924 = None
        slice_scatter_5533: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5530, slice_scatter_5532, 1, 7376, 7392);  slice_scatter_5530 = slice_scatter_5532 = None
        slice_30440: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5533, 1, 7376, 7392)
        slice_30441: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30440, 2, 0, 16)
        slice_scatter_5535: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30440, slice_30441, 2, 0, 16);  slice_30440 = slice_30441 = None
        slice_scatter_5536: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5533, slice_scatter_5535, 1, 7376, 7392);  slice_scatter_5533 = slice_scatter_5535 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30461: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_30427, 2, 16, 32);  slice_30427 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_926: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_30461, memory_format = torch.contiguous_format);  slice_30461 = None
        view_1856: "f32[32, 11]" = torch.ops.aten.view.default(clone_926, [32, 11]);  clone_926 = None
        mm_923: "f32[32, 8]" = torch.ops.aten.mm.default(view_1856, slice_37)
        view_1857: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_923, [2, 16, 8]);  mm_923 = None
        slice_30468: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5536, 1, 7376, 7392)
        slice_30469: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30468, 2, 0, 16)
        add_925: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30469, view_1857);  slice_30469 = view_1857 = None
        slice_scatter_5538: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30468, add_925, 2, 0, 16);  slice_30468 = add_925 = None
        slice_scatter_5539: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5536, slice_scatter_5538, 1, 7376, 7392);  slice_scatter_5536 = slice_scatter_5538 = None
        slice_30473: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5539, 1, 7376, 7392)
        slice_30474: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30473, 2, 0, 16)
        slice_scatter_5541: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30473, slice_30474, 2, 0, 16);  slice_30473 = slice_30474 = None
        slice_scatter_5542: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5539, slice_scatter_5541, 1, 7376, 7392);  slice_scatter_5539 = slice_scatter_5541 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30493: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7392, 7408)
        slice_30494: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_30493, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_927: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_30494, memory_format = torch.contiguous_format);  slice_30494 = None
        view_1858: "f32[32, 16]" = torch.ops.aten.view.default(clone_927, [32, 16]);  clone_927 = None
        mm_924: "f32[32, 8]" = torch.ops.aten.mm.default(view_1858, slice_7)
        view_1859: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_924, [2, 16, 8]);  mm_924 = None
        slice_30501: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5542, 1, 7392, 7408)
        slice_30502: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30501, 2, 0, 16)
        add_926: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30502, view_1859);  slice_30502 = view_1859 = None
        slice_scatter_5544: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30501, add_926, 2, 0, 16);  slice_30501 = add_926 = None
        slice_scatter_5545: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5542, slice_scatter_5544, 1, 7392, 7408);  slice_scatter_5542 = slice_scatter_5544 = None
        slice_30506: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5545, 1, 7392, 7408)
        slice_30507: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30506, 2, 0, 16)
        slice_scatter_5547: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30506, slice_30507, 2, 0, 16);  slice_30506 = slice_30507 = None
        slice_scatter_5548: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5545, slice_scatter_5547, 1, 7392, 7408);  slice_scatter_5545 = slice_scatter_5547 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30527: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_30493, 2, 16, 32);  slice_30493 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_928: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_30527, memory_format = torch.contiguous_format);  slice_30527 = None
        view_1860: "f32[32, 11]" = torch.ops.aten.view.default(clone_928, [32, 11]);  clone_928 = None
        mm_925: "f32[32, 8]" = torch.ops.aten.mm.default(view_1860, slice_37)
        view_1861: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_925, [2, 16, 8]);  mm_925 = None
        slice_30534: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5548, 1, 7392, 7408)
        slice_30535: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30534, 2, 0, 16)
        add_927: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30535, view_1861);  slice_30535 = view_1861 = None
        slice_scatter_5550: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30534, add_927, 2, 0, 16);  slice_30534 = add_927 = None
        slice_scatter_5551: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5548, slice_scatter_5550, 1, 7392, 7408);  slice_scatter_5548 = slice_scatter_5550 = None
        slice_30539: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5551, 1, 7392, 7408)
        slice_30540: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30539, 2, 0, 16)
        slice_scatter_5553: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30539, slice_30540, 2, 0, 16);  slice_30539 = slice_30540 = None
        slice_scatter_5554: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5551, slice_scatter_5553, 1, 7392, 7408);  slice_scatter_5551 = slice_scatter_5553 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30559: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7408, 7424)
        slice_30560: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_30559, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_929: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_30560, memory_format = torch.contiguous_format);  slice_30560 = None
        view_1862: "f32[32, 16]" = torch.ops.aten.view.default(clone_929, [32, 16]);  clone_929 = None
        mm_926: "f32[32, 8]" = torch.ops.aten.mm.default(view_1862, slice_7)
        view_1863: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_926, [2, 16, 8]);  mm_926 = None
        slice_30567: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5554, 1, 7408, 7424)
        slice_30568: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30567, 2, 0, 16)
        add_928: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30568, view_1863);  slice_30568 = view_1863 = None
        slice_scatter_5556: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30567, add_928, 2, 0, 16);  slice_30567 = add_928 = None
        slice_scatter_5557: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5554, slice_scatter_5556, 1, 7408, 7424);  slice_scatter_5554 = slice_scatter_5556 = None
        slice_30572: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5557, 1, 7408, 7424)
        slice_30573: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30572, 2, 0, 16)
        slice_scatter_5559: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30572, slice_30573, 2, 0, 16);  slice_30572 = slice_30573 = None
        slice_scatter_5560: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5557, slice_scatter_5559, 1, 7408, 7424);  slice_scatter_5557 = slice_scatter_5559 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30593: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_30559, 2, 16, 32);  slice_30559 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_930: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_30593, memory_format = torch.contiguous_format);  slice_30593 = None
        view_1864: "f32[32, 11]" = torch.ops.aten.view.default(clone_930, [32, 11]);  clone_930 = None
        mm_927: "f32[32, 8]" = torch.ops.aten.mm.default(view_1864, slice_37)
        view_1865: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_927, [2, 16, 8]);  mm_927 = None
        slice_30600: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5560, 1, 7408, 7424)
        slice_30601: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30600, 2, 0, 16)
        add_929: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30601, view_1865);  slice_30601 = view_1865 = None
        slice_scatter_5562: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30600, add_929, 2, 0, 16);  slice_30600 = add_929 = None
        slice_scatter_5563: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5560, slice_scatter_5562, 1, 7408, 7424);  slice_scatter_5560 = slice_scatter_5562 = None
        slice_30605: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5563, 1, 7408, 7424)
        slice_30606: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30605, 2, 0, 16)
        slice_scatter_5565: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30605, slice_30606, 2, 0, 16);  slice_30605 = slice_30606 = None
        slice_scatter_5566: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5563, slice_scatter_5565, 1, 7408, 7424);  slice_scatter_5563 = slice_scatter_5565 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30625: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7424, 7440)
        slice_30626: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_30625, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_931: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_30626, memory_format = torch.contiguous_format);  slice_30626 = None
        view_1866: "f32[32, 16]" = torch.ops.aten.view.default(clone_931, [32, 16]);  clone_931 = None
        mm_928: "f32[32, 8]" = torch.ops.aten.mm.default(view_1866, slice_7)
        view_1867: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_928, [2, 16, 8]);  mm_928 = None
        slice_30633: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5566, 1, 7424, 7440)
        slice_30634: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30633, 2, 0, 16)
        add_930: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30634, view_1867);  slice_30634 = view_1867 = None
        slice_scatter_5568: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30633, add_930, 2, 0, 16);  slice_30633 = add_930 = None
        slice_scatter_5569: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5566, slice_scatter_5568, 1, 7424, 7440);  slice_scatter_5566 = slice_scatter_5568 = None
        slice_30638: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5569, 1, 7424, 7440)
        slice_30639: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30638, 2, 0, 16)
        slice_scatter_5571: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30638, slice_30639, 2, 0, 16);  slice_30638 = slice_30639 = None
        slice_scatter_5572: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5569, slice_scatter_5571, 1, 7424, 7440);  slice_scatter_5569 = slice_scatter_5571 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30659: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_30625, 2, 16, 32);  slice_30625 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_932: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_30659, memory_format = torch.contiguous_format);  slice_30659 = None
        view_1868: "f32[32, 11]" = torch.ops.aten.view.default(clone_932, [32, 11]);  clone_932 = None
        mm_929: "f32[32, 8]" = torch.ops.aten.mm.default(view_1868, slice_37)
        view_1869: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_929, [2, 16, 8]);  mm_929 = None
        slice_30666: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5572, 1, 7424, 7440)
        slice_30667: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30666, 2, 0, 16)
        add_931: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30667, view_1869);  slice_30667 = view_1869 = None
        slice_scatter_5574: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30666, add_931, 2, 0, 16);  slice_30666 = add_931 = None
        slice_scatter_5575: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5572, slice_scatter_5574, 1, 7424, 7440);  slice_scatter_5572 = slice_scatter_5574 = None
        slice_30671: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5575, 1, 7424, 7440)
        slice_30672: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30671, 2, 0, 16)
        slice_scatter_5577: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30671, slice_30672, 2, 0, 16);  slice_30671 = slice_30672 = None
        slice_scatter_5578: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5575, slice_scatter_5577, 1, 7424, 7440);  slice_scatter_5575 = slice_scatter_5577 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30691: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7440, 7456)
        slice_30692: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_30691, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_933: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_30692, memory_format = torch.contiguous_format);  slice_30692 = None
        view_1870: "f32[32, 16]" = torch.ops.aten.view.default(clone_933, [32, 16]);  clone_933 = None
        mm_930: "f32[32, 8]" = torch.ops.aten.mm.default(view_1870, slice_7)
        view_1871: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_930, [2, 16, 8]);  mm_930 = None
        slice_30699: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5578, 1, 7440, 7456)
        slice_30700: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30699, 2, 0, 16)
        add_932: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30700, view_1871);  slice_30700 = view_1871 = None
        slice_scatter_5580: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30699, add_932, 2, 0, 16);  slice_30699 = add_932 = None
        slice_scatter_5581: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5578, slice_scatter_5580, 1, 7440, 7456);  slice_scatter_5578 = slice_scatter_5580 = None
        slice_30704: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5581, 1, 7440, 7456)
        slice_30705: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30704, 2, 0, 16)
        slice_scatter_5583: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30704, slice_30705, 2, 0, 16);  slice_30704 = slice_30705 = None
        slice_scatter_5584: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5581, slice_scatter_5583, 1, 7440, 7456);  slice_scatter_5581 = slice_scatter_5583 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30725: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_30691, 2, 16, 32);  slice_30691 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_934: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_30725, memory_format = torch.contiguous_format);  slice_30725 = None
        view_1872: "f32[32, 11]" = torch.ops.aten.view.default(clone_934, [32, 11]);  clone_934 = None
        mm_931: "f32[32, 8]" = torch.ops.aten.mm.default(view_1872, slice_37)
        view_1873: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_931, [2, 16, 8]);  mm_931 = None
        slice_30732: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5584, 1, 7440, 7456)
        slice_30733: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30732, 2, 0, 16)
        add_933: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30733, view_1873);  slice_30733 = view_1873 = None
        slice_scatter_5586: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30732, add_933, 2, 0, 16);  slice_30732 = add_933 = None
        slice_scatter_5587: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5584, slice_scatter_5586, 1, 7440, 7456);  slice_scatter_5584 = slice_scatter_5586 = None
        slice_30737: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5587, 1, 7440, 7456)
        slice_30738: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30737, 2, 0, 16)
        slice_scatter_5589: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30737, slice_30738, 2, 0, 16);  slice_30737 = slice_30738 = None
        slice_scatter_5590: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5587, slice_scatter_5589, 1, 7440, 7456);  slice_scatter_5587 = slice_scatter_5589 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30757: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7456, 7472)
        slice_30758: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_30757, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_935: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_30758, memory_format = torch.contiguous_format);  slice_30758 = None
        view_1874: "f32[32, 16]" = torch.ops.aten.view.default(clone_935, [32, 16]);  clone_935 = None
        mm_932: "f32[32, 8]" = torch.ops.aten.mm.default(view_1874, slice_7)
        view_1875: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_932, [2, 16, 8]);  mm_932 = None
        slice_30765: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5590, 1, 7456, 7472)
        slice_30766: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30765, 2, 0, 16)
        add_934: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30766, view_1875);  slice_30766 = view_1875 = None
        slice_scatter_5592: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30765, add_934, 2, 0, 16);  slice_30765 = add_934 = None
        slice_scatter_5593: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5590, slice_scatter_5592, 1, 7456, 7472);  slice_scatter_5590 = slice_scatter_5592 = None
        slice_30770: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5593, 1, 7456, 7472)
        slice_30771: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30770, 2, 0, 16)
        slice_scatter_5595: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30770, slice_30771, 2, 0, 16);  slice_30770 = slice_30771 = None
        slice_scatter_5596: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5593, slice_scatter_5595, 1, 7456, 7472);  slice_scatter_5593 = slice_scatter_5595 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30791: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_30757, 2, 16, 32);  slice_30757 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_936: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_30791, memory_format = torch.contiguous_format);  slice_30791 = None
        view_1876: "f32[32, 11]" = torch.ops.aten.view.default(clone_936, [32, 11]);  clone_936 = None
        mm_933: "f32[32, 8]" = torch.ops.aten.mm.default(view_1876, slice_37)
        view_1877: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_933, [2, 16, 8]);  mm_933 = None
        slice_30798: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5596, 1, 7456, 7472)
        slice_30799: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30798, 2, 0, 16)
        add_935: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30799, view_1877);  slice_30799 = view_1877 = None
        slice_scatter_5598: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30798, add_935, 2, 0, 16);  slice_30798 = add_935 = None
        slice_scatter_5599: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5596, slice_scatter_5598, 1, 7456, 7472);  slice_scatter_5596 = slice_scatter_5598 = None
        slice_30803: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5599, 1, 7456, 7472)
        slice_30804: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30803, 2, 0, 16)
        slice_scatter_5601: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30803, slice_30804, 2, 0, 16);  slice_30803 = slice_30804 = None
        slice_scatter_5602: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5599, slice_scatter_5601, 1, 7456, 7472);  slice_scatter_5599 = slice_scatter_5601 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30823: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7472, 7488)
        slice_30824: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_30823, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_937: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_30824, memory_format = torch.contiguous_format);  slice_30824 = None
        view_1878: "f32[32, 16]" = torch.ops.aten.view.default(clone_937, [32, 16]);  clone_937 = None
        mm_934: "f32[32, 8]" = torch.ops.aten.mm.default(view_1878, slice_7)
        view_1879: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_934, [2, 16, 8]);  mm_934 = None
        slice_30831: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5602, 1, 7472, 7488)
        slice_30832: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30831, 2, 0, 16)
        add_936: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30832, view_1879);  slice_30832 = view_1879 = None
        slice_scatter_5604: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30831, add_936, 2, 0, 16);  slice_30831 = add_936 = None
        slice_scatter_5605: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5602, slice_scatter_5604, 1, 7472, 7488);  slice_scatter_5602 = slice_scatter_5604 = None
        slice_30836: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5605, 1, 7472, 7488)
        slice_30837: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30836, 2, 0, 16)
        slice_scatter_5607: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30836, slice_30837, 2, 0, 16);  slice_30836 = slice_30837 = None
        slice_scatter_5608: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5605, slice_scatter_5607, 1, 7472, 7488);  slice_scatter_5605 = slice_scatter_5607 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30857: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_30823, 2, 16, 32);  slice_30823 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_938: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_30857, memory_format = torch.contiguous_format);  slice_30857 = None
        view_1880: "f32[32, 11]" = torch.ops.aten.view.default(clone_938, [32, 11]);  clone_938 = None
        mm_935: "f32[32, 8]" = torch.ops.aten.mm.default(view_1880, slice_37)
        view_1881: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_935, [2, 16, 8]);  mm_935 = None
        slice_30864: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5608, 1, 7472, 7488)
        slice_30865: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30864, 2, 0, 16)
        add_937: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30865, view_1881);  slice_30865 = view_1881 = None
        slice_scatter_5610: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30864, add_937, 2, 0, 16);  slice_30864 = add_937 = None
        slice_scatter_5611: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5608, slice_scatter_5610, 1, 7472, 7488);  slice_scatter_5608 = slice_scatter_5610 = None
        slice_30869: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5611, 1, 7472, 7488)
        slice_30870: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30869, 2, 0, 16)
        slice_scatter_5613: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30869, slice_30870, 2, 0, 16);  slice_30869 = slice_30870 = None
        slice_scatter_5614: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5611, slice_scatter_5613, 1, 7472, 7488);  slice_scatter_5611 = slice_scatter_5613 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30889: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7488, 7504)
        slice_30890: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_30889, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_939: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_30890, memory_format = torch.contiguous_format);  slice_30890 = None
        view_1882: "f32[32, 16]" = torch.ops.aten.view.default(clone_939, [32, 16]);  clone_939 = None
        mm_936: "f32[32, 8]" = torch.ops.aten.mm.default(view_1882, slice_7)
        view_1883: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_936, [2, 16, 8]);  mm_936 = None
        slice_30897: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5614, 1, 7488, 7504)
        slice_30898: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30897, 2, 0, 16)
        add_938: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30898, view_1883);  slice_30898 = view_1883 = None
        slice_scatter_5616: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30897, add_938, 2, 0, 16);  slice_30897 = add_938 = None
        slice_scatter_5617: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5614, slice_scatter_5616, 1, 7488, 7504);  slice_scatter_5614 = slice_scatter_5616 = None
        slice_30902: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5617, 1, 7488, 7504)
        slice_30903: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30902, 2, 0, 16)
        slice_scatter_5619: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30902, slice_30903, 2, 0, 16);  slice_30902 = slice_30903 = None
        slice_scatter_5620: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5617, slice_scatter_5619, 1, 7488, 7504);  slice_scatter_5617 = slice_scatter_5619 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30923: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_30889, 2, 16, 32);  slice_30889 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_940: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_30923, memory_format = torch.contiguous_format);  slice_30923 = None
        view_1884: "f32[32, 11]" = torch.ops.aten.view.default(clone_940, [32, 11]);  clone_940 = None
        mm_937: "f32[32, 8]" = torch.ops.aten.mm.default(view_1884, slice_37)
        view_1885: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_937, [2, 16, 8]);  mm_937 = None
        slice_30930: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5620, 1, 7488, 7504)
        slice_30931: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30930, 2, 0, 16)
        add_939: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30931, view_1885);  slice_30931 = view_1885 = None
        slice_scatter_5622: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30930, add_939, 2, 0, 16);  slice_30930 = add_939 = None
        slice_scatter_5623: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5620, slice_scatter_5622, 1, 7488, 7504);  slice_scatter_5620 = slice_scatter_5622 = None
        slice_30935: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5623, 1, 7488, 7504)
        slice_30936: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30935, 2, 0, 16)
        slice_scatter_5625: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30935, slice_30936, 2, 0, 16);  slice_30935 = slice_30936 = None
        slice_scatter_5626: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5623, slice_scatter_5625, 1, 7488, 7504);  slice_scatter_5623 = slice_scatter_5625 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30955: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7504, 7520)
        slice_30956: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_30955, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_941: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_30956, memory_format = torch.contiguous_format);  slice_30956 = None
        view_1886: "f32[32, 16]" = torch.ops.aten.view.default(clone_941, [32, 16]);  clone_941 = None
        mm_938: "f32[32, 8]" = torch.ops.aten.mm.default(view_1886, slice_7)
        view_1887: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_938, [2, 16, 8]);  mm_938 = None
        slice_30963: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5626, 1, 7504, 7520)
        slice_30964: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30963, 2, 0, 16)
        add_940: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30964, view_1887);  slice_30964 = view_1887 = None
        slice_scatter_5628: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30963, add_940, 2, 0, 16);  slice_30963 = add_940 = None
        slice_scatter_5629: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5626, slice_scatter_5628, 1, 7504, 7520);  slice_scatter_5626 = slice_scatter_5628 = None
        slice_30968: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5629, 1, 7504, 7520)
        slice_30969: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30968, 2, 0, 16)
        slice_scatter_5631: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30968, slice_30969, 2, 0, 16);  slice_30968 = slice_30969 = None
        slice_scatter_5632: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5629, slice_scatter_5631, 1, 7504, 7520);  slice_scatter_5629 = slice_scatter_5631 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_30989: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_30955, 2, 16, 32);  slice_30955 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_942: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_30989, memory_format = torch.contiguous_format);  slice_30989 = None
        view_1888: "f32[32, 11]" = torch.ops.aten.view.default(clone_942, [32, 11]);  clone_942 = None
        mm_939: "f32[32, 8]" = torch.ops.aten.mm.default(view_1888, slice_37)
        view_1889: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_939, [2, 16, 8]);  mm_939 = None
        slice_30996: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5632, 1, 7504, 7520)
        slice_30997: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_30996, 2, 0, 16)
        add_941: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_30997, view_1889);  slice_30997 = view_1889 = None
        slice_scatter_5634: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_30996, add_941, 2, 0, 16);  slice_30996 = add_941 = None
        slice_scatter_5635: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5632, slice_scatter_5634, 1, 7504, 7520);  slice_scatter_5632 = slice_scatter_5634 = None
        slice_31001: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5635, 1, 7504, 7520)
        slice_31002: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31001, 2, 0, 16)
        slice_scatter_5637: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31001, slice_31002, 2, 0, 16);  slice_31001 = slice_31002 = None
        slice_scatter_5638: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5635, slice_scatter_5637, 1, 7504, 7520);  slice_scatter_5635 = slice_scatter_5637 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31021: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7520, 7536)
        slice_31022: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_31021, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_943: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_31022, memory_format = torch.contiguous_format);  slice_31022 = None
        view_1890: "f32[32, 16]" = torch.ops.aten.view.default(clone_943, [32, 16]);  clone_943 = None
        mm_940: "f32[32, 8]" = torch.ops.aten.mm.default(view_1890, slice_7)
        view_1891: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_940, [2, 16, 8]);  mm_940 = None
        slice_31029: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5638, 1, 7520, 7536)
        slice_31030: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31029, 2, 0, 16)
        add_942: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31030, view_1891);  slice_31030 = view_1891 = None
        slice_scatter_5640: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31029, add_942, 2, 0, 16);  slice_31029 = add_942 = None
        slice_scatter_5641: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5638, slice_scatter_5640, 1, 7520, 7536);  slice_scatter_5638 = slice_scatter_5640 = None
        slice_31034: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5641, 1, 7520, 7536)
        slice_31035: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31034, 2, 0, 16)
        slice_scatter_5643: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31034, slice_31035, 2, 0, 16);  slice_31034 = slice_31035 = None
        slice_scatter_5644: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5641, slice_scatter_5643, 1, 7520, 7536);  slice_scatter_5641 = slice_scatter_5643 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31055: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_31021, 2, 16, 32);  slice_31021 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_944: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_31055, memory_format = torch.contiguous_format);  slice_31055 = None
        view_1892: "f32[32, 11]" = torch.ops.aten.view.default(clone_944, [32, 11]);  clone_944 = None
        mm_941: "f32[32, 8]" = torch.ops.aten.mm.default(view_1892, slice_37)
        view_1893: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_941, [2, 16, 8]);  mm_941 = None
        slice_31062: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5644, 1, 7520, 7536)
        slice_31063: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31062, 2, 0, 16)
        add_943: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31063, view_1893);  slice_31063 = view_1893 = None
        slice_scatter_5646: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31062, add_943, 2, 0, 16);  slice_31062 = add_943 = None
        slice_scatter_5647: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5644, slice_scatter_5646, 1, 7520, 7536);  slice_scatter_5644 = slice_scatter_5646 = None
        slice_31067: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5647, 1, 7520, 7536)
        slice_31068: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31067, 2, 0, 16)
        slice_scatter_5649: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31067, slice_31068, 2, 0, 16);  slice_31067 = slice_31068 = None
        slice_scatter_5650: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5647, slice_scatter_5649, 1, 7520, 7536);  slice_scatter_5647 = slice_scatter_5649 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31087: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7536, 7552)
        slice_31088: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_31087, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_945: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_31088, memory_format = torch.contiguous_format);  slice_31088 = None
        view_1894: "f32[32, 16]" = torch.ops.aten.view.default(clone_945, [32, 16]);  clone_945 = None
        mm_942: "f32[32, 8]" = torch.ops.aten.mm.default(view_1894, slice_7)
        view_1895: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_942, [2, 16, 8]);  mm_942 = None
        slice_31095: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5650, 1, 7536, 7552)
        slice_31096: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31095, 2, 0, 16)
        add_944: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31096, view_1895);  slice_31096 = view_1895 = None
        slice_scatter_5652: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31095, add_944, 2, 0, 16);  slice_31095 = add_944 = None
        slice_scatter_5653: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5650, slice_scatter_5652, 1, 7536, 7552);  slice_scatter_5650 = slice_scatter_5652 = None
        slice_31100: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5653, 1, 7536, 7552)
        slice_31101: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31100, 2, 0, 16)
        slice_scatter_5655: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31100, slice_31101, 2, 0, 16);  slice_31100 = slice_31101 = None
        slice_scatter_5656: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5653, slice_scatter_5655, 1, 7536, 7552);  slice_scatter_5653 = slice_scatter_5655 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31121: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_31087, 2, 16, 32);  slice_31087 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_946: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_31121, memory_format = torch.contiguous_format);  slice_31121 = None
        view_1896: "f32[32, 11]" = torch.ops.aten.view.default(clone_946, [32, 11]);  clone_946 = None
        mm_943: "f32[32, 8]" = torch.ops.aten.mm.default(view_1896, slice_37)
        view_1897: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_943, [2, 16, 8]);  mm_943 = None
        slice_31128: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5656, 1, 7536, 7552)
        slice_31129: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31128, 2, 0, 16)
        add_945: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31129, view_1897);  slice_31129 = view_1897 = None
        slice_scatter_5658: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31128, add_945, 2, 0, 16);  slice_31128 = add_945 = None
        slice_scatter_5659: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5656, slice_scatter_5658, 1, 7536, 7552);  slice_scatter_5656 = slice_scatter_5658 = None
        slice_31133: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5659, 1, 7536, 7552)
        slice_31134: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31133, 2, 0, 16)
        slice_scatter_5661: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31133, slice_31134, 2, 0, 16);  slice_31133 = slice_31134 = None
        slice_scatter_5662: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5659, slice_scatter_5661, 1, 7536, 7552);  slice_scatter_5659 = slice_scatter_5661 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31153: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7552, 7568)
        slice_31154: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_31153, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_947: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_31154, memory_format = torch.contiguous_format);  slice_31154 = None
        view_1898: "f32[32, 16]" = torch.ops.aten.view.default(clone_947, [32, 16]);  clone_947 = None
        mm_944: "f32[32, 8]" = torch.ops.aten.mm.default(view_1898, slice_7)
        view_1899: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_944, [2, 16, 8]);  mm_944 = None
        slice_31161: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5662, 1, 7552, 7568)
        slice_31162: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31161, 2, 0, 16)
        add_946: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31162, view_1899);  slice_31162 = view_1899 = None
        slice_scatter_5664: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31161, add_946, 2, 0, 16);  slice_31161 = add_946 = None
        slice_scatter_5665: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5662, slice_scatter_5664, 1, 7552, 7568);  slice_scatter_5662 = slice_scatter_5664 = None
        slice_31166: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5665, 1, 7552, 7568)
        slice_31167: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31166, 2, 0, 16)
        slice_scatter_5667: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31166, slice_31167, 2, 0, 16);  slice_31166 = slice_31167 = None
        slice_scatter_5668: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5665, slice_scatter_5667, 1, 7552, 7568);  slice_scatter_5665 = slice_scatter_5667 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31187: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_31153, 2, 16, 32);  slice_31153 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_948: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_31187, memory_format = torch.contiguous_format);  slice_31187 = None
        view_1900: "f32[32, 11]" = torch.ops.aten.view.default(clone_948, [32, 11]);  clone_948 = None
        mm_945: "f32[32, 8]" = torch.ops.aten.mm.default(view_1900, slice_37)
        view_1901: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_945, [2, 16, 8]);  mm_945 = None
        slice_31194: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5668, 1, 7552, 7568)
        slice_31195: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31194, 2, 0, 16)
        add_947: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31195, view_1901);  slice_31195 = view_1901 = None
        slice_scatter_5670: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31194, add_947, 2, 0, 16);  slice_31194 = add_947 = None
        slice_scatter_5671: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5668, slice_scatter_5670, 1, 7552, 7568);  slice_scatter_5668 = slice_scatter_5670 = None
        slice_31199: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5671, 1, 7552, 7568)
        slice_31200: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31199, 2, 0, 16)
        slice_scatter_5673: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31199, slice_31200, 2, 0, 16);  slice_31199 = slice_31200 = None
        slice_scatter_5674: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5671, slice_scatter_5673, 1, 7552, 7568);  slice_scatter_5671 = slice_scatter_5673 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31219: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7568, 7584)
        slice_31220: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_31219, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_949: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_31220, memory_format = torch.contiguous_format);  slice_31220 = None
        view_1902: "f32[32, 16]" = torch.ops.aten.view.default(clone_949, [32, 16]);  clone_949 = None
        mm_946: "f32[32, 8]" = torch.ops.aten.mm.default(view_1902, slice_7)
        view_1903: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_946, [2, 16, 8]);  mm_946 = None
        slice_31227: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5674, 1, 7568, 7584)
        slice_31228: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31227, 2, 0, 16)
        add_948: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31228, view_1903);  slice_31228 = view_1903 = None
        slice_scatter_5676: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31227, add_948, 2, 0, 16);  slice_31227 = add_948 = None
        slice_scatter_5677: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5674, slice_scatter_5676, 1, 7568, 7584);  slice_scatter_5674 = slice_scatter_5676 = None
        slice_31232: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5677, 1, 7568, 7584)
        slice_31233: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31232, 2, 0, 16)
        slice_scatter_5679: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31232, slice_31233, 2, 0, 16);  slice_31232 = slice_31233 = None
        slice_scatter_5680: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5677, slice_scatter_5679, 1, 7568, 7584);  slice_scatter_5677 = slice_scatter_5679 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31253: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_31219, 2, 16, 32);  slice_31219 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_950: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_31253, memory_format = torch.contiguous_format);  slice_31253 = None
        view_1904: "f32[32, 11]" = torch.ops.aten.view.default(clone_950, [32, 11]);  clone_950 = None
        mm_947: "f32[32, 8]" = torch.ops.aten.mm.default(view_1904, slice_37)
        view_1905: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_947, [2, 16, 8]);  mm_947 = None
        slice_31260: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5680, 1, 7568, 7584)
        slice_31261: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31260, 2, 0, 16)
        add_949: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31261, view_1905);  slice_31261 = view_1905 = None
        slice_scatter_5682: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31260, add_949, 2, 0, 16);  slice_31260 = add_949 = None
        slice_scatter_5683: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5680, slice_scatter_5682, 1, 7568, 7584);  slice_scatter_5680 = slice_scatter_5682 = None
        slice_31265: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5683, 1, 7568, 7584)
        slice_31266: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31265, 2, 0, 16)
        slice_scatter_5685: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31265, slice_31266, 2, 0, 16);  slice_31265 = slice_31266 = None
        slice_scatter_5686: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5683, slice_scatter_5685, 1, 7568, 7584);  slice_scatter_5683 = slice_scatter_5685 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31285: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7584, 7600)
        slice_31286: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_31285, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_951: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_31286, memory_format = torch.contiguous_format);  slice_31286 = None
        view_1906: "f32[32, 16]" = torch.ops.aten.view.default(clone_951, [32, 16]);  clone_951 = None
        mm_948: "f32[32, 8]" = torch.ops.aten.mm.default(view_1906, slice_7)
        view_1907: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_948, [2, 16, 8]);  mm_948 = None
        slice_31293: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5686, 1, 7584, 7600)
        slice_31294: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31293, 2, 0, 16)
        add_950: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31294, view_1907);  slice_31294 = view_1907 = None
        slice_scatter_5688: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31293, add_950, 2, 0, 16);  slice_31293 = add_950 = None
        slice_scatter_5689: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5686, slice_scatter_5688, 1, 7584, 7600);  slice_scatter_5686 = slice_scatter_5688 = None
        slice_31298: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5689, 1, 7584, 7600)
        slice_31299: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31298, 2, 0, 16)
        slice_scatter_5691: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31298, slice_31299, 2, 0, 16);  slice_31298 = slice_31299 = None
        slice_scatter_5692: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5689, slice_scatter_5691, 1, 7584, 7600);  slice_scatter_5689 = slice_scatter_5691 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31319: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_31285, 2, 16, 32);  slice_31285 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_952: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_31319, memory_format = torch.contiguous_format);  slice_31319 = None
        view_1908: "f32[32, 11]" = torch.ops.aten.view.default(clone_952, [32, 11]);  clone_952 = None
        mm_949: "f32[32, 8]" = torch.ops.aten.mm.default(view_1908, slice_37)
        view_1909: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_949, [2, 16, 8]);  mm_949 = None
        slice_31326: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5692, 1, 7584, 7600)
        slice_31327: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31326, 2, 0, 16)
        add_951: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31327, view_1909);  slice_31327 = view_1909 = None
        slice_scatter_5694: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31326, add_951, 2, 0, 16);  slice_31326 = add_951 = None
        slice_scatter_5695: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5692, slice_scatter_5694, 1, 7584, 7600);  slice_scatter_5692 = slice_scatter_5694 = None
        slice_31331: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5695, 1, 7584, 7600)
        slice_31332: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31331, 2, 0, 16)
        slice_scatter_5697: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31331, slice_31332, 2, 0, 16);  slice_31331 = slice_31332 = None
        slice_scatter_5698: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5695, slice_scatter_5697, 1, 7584, 7600);  slice_scatter_5695 = slice_scatter_5697 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31351: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7600, 7616)
        slice_31352: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_31351, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_953: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_31352, memory_format = torch.contiguous_format);  slice_31352 = None
        view_1910: "f32[32, 16]" = torch.ops.aten.view.default(clone_953, [32, 16]);  clone_953 = None
        mm_950: "f32[32, 8]" = torch.ops.aten.mm.default(view_1910, slice_7)
        view_1911: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_950, [2, 16, 8]);  mm_950 = None
        slice_31359: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5698, 1, 7600, 7616)
        slice_31360: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31359, 2, 0, 16)
        add_952: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31360, view_1911);  slice_31360 = view_1911 = None
        slice_scatter_5700: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31359, add_952, 2, 0, 16);  slice_31359 = add_952 = None
        slice_scatter_5701: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5698, slice_scatter_5700, 1, 7600, 7616);  slice_scatter_5698 = slice_scatter_5700 = None
        slice_31364: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5701, 1, 7600, 7616)
        slice_31365: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31364, 2, 0, 16)
        slice_scatter_5703: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31364, slice_31365, 2, 0, 16);  slice_31364 = slice_31365 = None
        slice_scatter_5704: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5701, slice_scatter_5703, 1, 7600, 7616);  slice_scatter_5701 = slice_scatter_5703 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31385: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_31351, 2, 16, 32);  slice_31351 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_954: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_31385, memory_format = torch.contiguous_format);  slice_31385 = None
        view_1912: "f32[32, 11]" = torch.ops.aten.view.default(clone_954, [32, 11]);  clone_954 = None
        mm_951: "f32[32, 8]" = torch.ops.aten.mm.default(view_1912, slice_37)
        view_1913: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_951, [2, 16, 8]);  mm_951 = None
        slice_31392: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5704, 1, 7600, 7616)
        slice_31393: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31392, 2, 0, 16)
        add_953: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31393, view_1913);  slice_31393 = view_1913 = None
        slice_scatter_5706: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31392, add_953, 2, 0, 16);  slice_31392 = add_953 = None
        slice_scatter_5707: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5704, slice_scatter_5706, 1, 7600, 7616);  slice_scatter_5704 = slice_scatter_5706 = None
        slice_31397: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5707, 1, 7600, 7616)
        slice_31398: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31397, 2, 0, 16)
        slice_scatter_5709: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31397, slice_31398, 2, 0, 16);  slice_31397 = slice_31398 = None
        slice_scatter_5710: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5707, slice_scatter_5709, 1, 7600, 7616);  slice_scatter_5707 = slice_scatter_5709 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31417: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7616, 7632)
        slice_31418: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_31417, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_955: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_31418, memory_format = torch.contiguous_format);  slice_31418 = None
        view_1914: "f32[32, 16]" = torch.ops.aten.view.default(clone_955, [32, 16]);  clone_955 = None
        mm_952: "f32[32, 8]" = torch.ops.aten.mm.default(view_1914, slice_7)
        view_1915: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_952, [2, 16, 8]);  mm_952 = None
        slice_31425: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5710, 1, 7616, 7632)
        slice_31426: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31425, 2, 0, 16)
        add_954: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31426, view_1915);  slice_31426 = view_1915 = None
        slice_scatter_5712: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31425, add_954, 2, 0, 16);  slice_31425 = add_954 = None
        slice_scatter_5713: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5710, slice_scatter_5712, 1, 7616, 7632);  slice_scatter_5710 = slice_scatter_5712 = None
        slice_31430: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5713, 1, 7616, 7632)
        slice_31431: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31430, 2, 0, 16)
        slice_scatter_5715: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31430, slice_31431, 2, 0, 16);  slice_31430 = slice_31431 = None
        slice_scatter_5716: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5713, slice_scatter_5715, 1, 7616, 7632);  slice_scatter_5713 = slice_scatter_5715 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31451: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_31417, 2, 16, 32);  slice_31417 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_956: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_31451, memory_format = torch.contiguous_format);  slice_31451 = None
        view_1916: "f32[32, 11]" = torch.ops.aten.view.default(clone_956, [32, 11]);  clone_956 = None
        mm_953: "f32[32, 8]" = torch.ops.aten.mm.default(view_1916, slice_37)
        view_1917: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_953, [2, 16, 8]);  mm_953 = None
        slice_31458: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5716, 1, 7616, 7632)
        slice_31459: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31458, 2, 0, 16)
        add_955: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31459, view_1917);  slice_31459 = view_1917 = None
        slice_scatter_5718: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31458, add_955, 2, 0, 16);  slice_31458 = add_955 = None
        slice_scatter_5719: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5716, slice_scatter_5718, 1, 7616, 7632);  slice_scatter_5716 = slice_scatter_5718 = None
        slice_31463: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5719, 1, 7616, 7632)
        slice_31464: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31463, 2, 0, 16)
        slice_scatter_5721: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31463, slice_31464, 2, 0, 16);  slice_31463 = slice_31464 = None
        slice_scatter_5722: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5719, slice_scatter_5721, 1, 7616, 7632);  slice_scatter_5719 = slice_scatter_5721 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31483: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7632, 7648)
        slice_31484: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_31483, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_957: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_31484, memory_format = torch.contiguous_format);  slice_31484 = None
        view_1918: "f32[32, 16]" = torch.ops.aten.view.default(clone_957, [32, 16]);  clone_957 = None
        mm_954: "f32[32, 8]" = torch.ops.aten.mm.default(view_1918, slice_7)
        view_1919: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_954, [2, 16, 8]);  mm_954 = None
        slice_31491: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5722, 1, 7632, 7648)
        slice_31492: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31491, 2, 0, 16)
        add_956: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31492, view_1919);  slice_31492 = view_1919 = None
        slice_scatter_5724: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31491, add_956, 2, 0, 16);  slice_31491 = add_956 = None
        slice_scatter_5725: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5722, slice_scatter_5724, 1, 7632, 7648);  slice_scatter_5722 = slice_scatter_5724 = None
        slice_31496: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5725, 1, 7632, 7648)
        slice_31497: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31496, 2, 0, 16)
        slice_scatter_5727: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31496, slice_31497, 2, 0, 16);  slice_31496 = slice_31497 = None
        slice_scatter_5728: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5725, slice_scatter_5727, 1, 7632, 7648);  slice_scatter_5725 = slice_scatter_5727 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31517: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_31483, 2, 16, 32);  slice_31483 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_958: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_31517, memory_format = torch.contiguous_format);  slice_31517 = None
        view_1920: "f32[32, 11]" = torch.ops.aten.view.default(clone_958, [32, 11]);  clone_958 = None
        mm_955: "f32[32, 8]" = torch.ops.aten.mm.default(view_1920, slice_37)
        view_1921: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_955, [2, 16, 8]);  mm_955 = None
        slice_31524: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5728, 1, 7632, 7648)
        slice_31525: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31524, 2, 0, 16)
        add_957: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31525, view_1921);  slice_31525 = view_1921 = None
        slice_scatter_5730: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31524, add_957, 2, 0, 16);  slice_31524 = add_957 = None
        slice_scatter_5731: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5728, slice_scatter_5730, 1, 7632, 7648);  slice_scatter_5728 = slice_scatter_5730 = None
        slice_31529: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5731, 1, 7632, 7648)
        slice_31530: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31529, 2, 0, 16)
        slice_scatter_5733: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31529, slice_31530, 2, 0, 16);  slice_31529 = slice_31530 = None
        slice_scatter_5734: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5731, slice_scatter_5733, 1, 7632, 7648);  slice_scatter_5731 = slice_scatter_5733 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31549: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7648, 7664)
        slice_31550: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_31549, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_959: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_31550, memory_format = torch.contiguous_format);  slice_31550 = None
        view_1922: "f32[32, 16]" = torch.ops.aten.view.default(clone_959, [32, 16]);  clone_959 = None
        mm_956: "f32[32, 8]" = torch.ops.aten.mm.default(view_1922, slice_7)
        view_1923: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_956, [2, 16, 8]);  mm_956 = None
        slice_31557: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5734, 1, 7648, 7664)
        slice_31558: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31557, 2, 0, 16)
        add_958: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31558, view_1923);  slice_31558 = view_1923 = None
        slice_scatter_5736: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31557, add_958, 2, 0, 16);  slice_31557 = add_958 = None
        slice_scatter_5737: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5734, slice_scatter_5736, 1, 7648, 7664);  slice_scatter_5734 = slice_scatter_5736 = None
        slice_31562: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5737, 1, 7648, 7664)
        slice_31563: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31562, 2, 0, 16)
        slice_scatter_5739: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31562, slice_31563, 2, 0, 16);  slice_31562 = slice_31563 = None
        slice_scatter_5740: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5737, slice_scatter_5739, 1, 7648, 7664);  slice_scatter_5737 = slice_scatter_5739 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31583: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_31549, 2, 16, 32);  slice_31549 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_960: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_31583, memory_format = torch.contiguous_format);  slice_31583 = None
        view_1924: "f32[32, 11]" = torch.ops.aten.view.default(clone_960, [32, 11]);  clone_960 = None
        mm_957: "f32[32, 8]" = torch.ops.aten.mm.default(view_1924, slice_37)
        view_1925: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_957, [2, 16, 8]);  mm_957 = None
        slice_31590: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5740, 1, 7648, 7664)
        slice_31591: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31590, 2, 0, 16)
        add_959: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31591, view_1925);  slice_31591 = view_1925 = None
        slice_scatter_5742: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31590, add_959, 2, 0, 16);  slice_31590 = add_959 = None
        slice_scatter_5743: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5740, slice_scatter_5742, 1, 7648, 7664);  slice_scatter_5740 = slice_scatter_5742 = None
        slice_31595: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5743, 1, 7648, 7664)
        slice_31596: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31595, 2, 0, 16)
        slice_scatter_5745: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31595, slice_31596, 2, 0, 16);  slice_31595 = slice_31596 = None
        slice_scatter_5746: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5743, slice_scatter_5745, 1, 7648, 7664);  slice_scatter_5743 = slice_scatter_5745 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31615: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7664, 7680)
        slice_31616: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_31615, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_961: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_31616, memory_format = torch.contiguous_format);  slice_31616 = None
        view_1926: "f32[32, 16]" = torch.ops.aten.view.default(clone_961, [32, 16]);  clone_961 = None
        mm_958: "f32[32, 8]" = torch.ops.aten.mm.default(view_1926, slice_7)
        view_1927: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_958, [2, 16, 8]);  mm_958 = None
        slice_31623: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5746, 1, 7664, 7680)
        slice_31624: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31623, 2, 0, 16)
        add_960: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31624, view_1927);  slice_31624 = view_1927 = None
        slice_scatter_5748: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31623, add_960, 2, 0, 16);  slice_31623 = add_960 = None
        slice_scatter_5749: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5746, slice_scatter_5748, 1, 7664, 7680);  slice_scatter_5746 = slice_scatter_5748 = None
        slice_31628: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5749, 1, 7664, 7680)
        slice_31629: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31628, 2, 0, 16)
        slice_scatter_5751: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31628, slice_31629, 2, 0, 16);  slice_31628 = slice_31629 = None
        slice_scatter_5752: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5749, slice_scatter_5751, 1, 7664, 7680);  slice_scatter_5749 = slice_scatter_5751 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31649: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_31615, 2, 16, 32);  slice_31615 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_962: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_31649, memory_format = torch.contiguous_format);  slice_31649 = None
        view_1928: "f32[32, 11]" = torch.ops.aten.view.default(clone_962, [32, 11]);  clone_962 = None
        mm_959: "f32[32, 8]" = torch.ops.aten.mm.default(view_1928, slice_37)
        view_1929: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_959, [2, 16, 8]);  mm_959 = None
        slice_31656: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5752, 1, 7664, 7680)
        slice_31657: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31656, 2, 0, 16)
        add_961: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31657, view_1929);  slice_31657 = view_1929 = None
        slice_scatter_5754: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31656, add_961, 2, 0, 16);  slice_31656 = add_961 = None
        slice_scatter_5755: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5752, slice_scatter_5754, 1, 7664, 7680);  slice_scatter_5752 = slice_scatter_5754 = None
        slice_31661: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5755, 1, 7664, 7680)
        slice_31662: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31661, 2, 0, 16)
        slice_scatter_5757: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31661, slice_31662, 2, 0, 16);  slice_31661 = slice_31662 = None
        slice_scatter_5758: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5755, slice_scatter_5757, 1, 7664, 7680);  slice_scatter_5755 = slice_scatter_5757 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31681: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7680, 7696)
        slice_31682: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_31681, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_963: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_31682, memory_format = torch.contiguous_format);  slice_31682 = None
        view_1930: "f32[32, 16]" = torch.ops.aten.view.default(clone_963, [32, 16]);  clone_963 = None
        mm_960: "f32[32, 8]" = torch.ops.aten.mm.default(view_1930, slice_7)
        view_1931: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_960, [2, 16, 8]);  mm_960 = None
        slice_31689: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5758, 1, 7680, 7696)
        slice_31690: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31689, 2, 0, 16)
        add_962: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31690, view_1931);  slice_31690 = view_1931 = None
        slice_scatter_5760: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31689, add_962, 2, 0, 16);  slice_31689 = add_962 = None
        slice_scatter_5761: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5758, slice_scatter_5760, 1, 7680, 7696);  slice_scatter_5758 = slice_scatter_5760 = None
        slice_31694: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5761, 1, 7680, 7696)
        slice_31695: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31694, 2, 0, 16)
        slice_scatter_5763: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31694, slice_31695, 2, 0, 16);  slice_31694 = slice_31695 = None
        slice_scatter_5764: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5761, slice_scatter_5763, 1, 7680, 7696);  slice_scatter_5761 = slice_scatter_5763 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31715: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_31681, 2, 16, 32);  slice_31681 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_964: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_31715, memory_format = torch.contiguous_format);  slice_31715 = None
        view_1932: "f32[32, 11]" = torch.ops.aten.view.default(clone_964, [32, 11]);  clone_964 = None
        mm_961: "f32[32, 8]" = torch.ops.aten.mm.default(view_1932, slice_37)
        view_1933: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_961, [2, 16, 8]);  mm_961 = None
        slice_31722: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5764, 1, 7680, 7696)
        slice_31723: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31722, 2, 0, 16)
        add_963: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31723, view_1933);  slice_31723 = view_1933 = None
        slice_scatter_5766: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31722, add_963, 2, 0, 16);  slice_31722 = add_963 = None
        slice_scatter_5767: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5764, slice_scatter_5766, 1, 7680, 7696);  slice_scatter_5764 = slice_scatter_5766 = None
        slice_31727: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5767, 1, 7680, 7696)
        slice_31728: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31727, 2, 0, 16)
        slice_scatter_5769: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31727, slice_31728, 2, 0, 16);  slice_31727 = slice_31728 = None
        slice_scatter_5770: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5767, slice_scatter_5769, 1, 7680, 7696);  slice_scatter_5767 = slice_scatter_5769 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31747: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7696, 7712)
        slice_31748: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_31747, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_965: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_31748, memory_format = torch.contiguous_format);  slice_31748 = None
        view_1934: "f32[32, 16]" = torch.ops.aten.view.default(clone_965, [32, 16]);  clone_965 = None
        mm_962: "f32[32, 8]" = torch.ops.aten.mm.default(view_1934, slice_7)
        view_1935: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_962, [2, 16, 8]);  mm_962 = None
        slice_31755: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5770, 1, 7696, 7712)
        slice_31756: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31755, 2, 0, 16)
        add_964: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31756, view_1935);  slice_31756 = view_1935 = None
        slice_scatter_5772: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31755, add_964, 2, 0, 16);  slice_31755 = add_964 = None
        slice_scatter_5773: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5770, slice_scatter_5772, 1, 7696, 7712);  slice_scatter_5770 = slice_scatter_5772 = None
        slice_31760: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5773, 1, 7696, 7712)
        slice_31761: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31760, 2, 0, 16)
        slice_scatter_5775: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31760, slice_31761, 2, 0, 16);  slice_31760 = slice_31761 = None
        slice_scatter_5776: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5773, slice_scatter_5775, 1, 7696, 7712);  slice_scatter_5773 = slice_scatter_5775 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31781: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_31747, 2, 16, 32);  slice_31747 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_966: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_31781, memory_format = torch.contiguous_format);  slice_31781 = None
        view_1936: "f32[32, 11]" = torch.ops.aten.view.default(clone_966, [32, 11]);  clone_966 = None
        mm_963: "f32[32, 8]" = torch.ops.aten.mm.default(view_1936, slice_37)
        view_1937: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_963, [2, 16, 8]);  mm_963 = None
        slice_31788: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5776, 1, 7696, 7712)
        slice_31789: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31788, 2, 0, 16)
        add_965: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31789, view_1937);  slice_31789 = view_1937 = None
        slice_scatter_5778: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31788, add_965, 2, 0, 16);  slice_31788 = add_965 = None
        slice_scatter_5779: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5776, slice_scatter_5778, 1, 7696, 7712);  slice_scatter_5776 = slice_scatter_5778 = None
        slice_31793: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5779, 1, 7696, 7712)
        slice_31794: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31793, 2, 0, 16)
        slice_scatter_5781: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31793, slice_31794, 2, 0, 16);  slice_31793 = slice_31794 = None
        slice_scatter_5782: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5779, slice_scatter_5781, 1, 7696, 7712);  slice_scatter_5779 = slice_scatter_5781 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31813: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7712, 7728)
        slice_31814: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_31813, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_967: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_31814, memory_format = torch.contiguous_format);  slice_31814 = None
        view_1938: "f32[32, 16]" = torch.ops.aten.view.default(clone_967, [32, 16]);  clone_967 = None
        mm_964: "f32[32, 8]" = torch.ops.aten.mm.default(view_1938, slice_7)
        view_1939: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_964, [2, 16, 8]);  mm_964 = None
        slice_31821: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5782, 1, 7712, 7728)
        slice_31822: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31821, 2, 0, 16)
        add_966: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31822, view_1939);  slice_31822 = view_1939 = None
        slice_scatter_5784: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31821, add_966, 2, 0, 16);  slice_31821 = add_966 = None
        slice_scatter_5785: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5782, slice_scatter_5784, 1, 7712, 7728);  slice_scatter_5782 = slice_scatter_5784 = None
        slice_31826: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5785, 1, 7712, 7728)
        slice_31827: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31826, 2, 0, 16)
        slice_scatter_5787: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31826, slice_31827, 2, 0, 16);  slice_31826 = slice_31827 = None
        slice_scatter_5788: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5785, slice_scatter_5787, 1, 7712, 7728);  slice_scatter_5785 = slice_scatter_5787 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31847: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_31813, 2, 16, 32);  slice_31813 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_968: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_31847, memory_format = torch.contiguous_format);  slice_31847 = None
        view_1940: "f32[32, 11]" = torch.ops.aten.view.default(clone_968, [32, 11]);  clone_968 = None
        mm_965: "f32[32, 8]" = torch.ops.aten.mm.default(view_1940, slice_37)
        view_1941: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_965, [2, 16, 8]);  mm_965 = None
        slice_31854: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5788, 1, 7712, 7728)
        slice_31855: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31854, 2, 0, 16)
        add_967: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31855, view_1941);  slice_31855 = view_1941 = None
        slice_scatter_5790: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31854, add_967, 2, 0, 16);  slice_31854 = add_967 = None
        slice_scatter_5791: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5788, slice_scatter_5790, 1, 7712, 7728);  slice_scatter_5788 = slice_scatter_5790 = None
        slice_31859: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5791, 1, 7712, 7728)
        slice_31860: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31859, 2, 0, 16)
        slice_scatter_5793: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31859, slice_31860, 2, 0, 16);  slice_31859 = slice_31860 = None
        slice_scatter_5794: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5791, slice_scatter_5793, 1, 7712, 7728);  slice_scatter_5791 = slice_scatter_5793 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31879: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7728, 7744)
        slice_31880: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_31879, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_969: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_31880, memory_format = torch.contiguous_format);  slice_31880 = None
        view_1942: "f32[32, 16]" = torch.ops.aten.view.default(clone_969, [32, 16]);  clone_969 = None
        mm_966: "f32[32, 8]" = torch.ops.aten.mm.default(view_1942, slice_7)
        view_1943: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_966, [2, 16, 8]);  mm_966 = None
        slice_31887: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5794, 1, 7728, 7744)
        slice_31888: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31887, 2, 0, 16)
        add_968: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31888, view_1943);  slice_31888 = view_1943 = None
        slice_scatter_5796: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31887, add_968, 2, 0, 16);  slice_31887 = add_968 = None
        slice_scatter_5797: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5794, slice_scatter_5796, 1, 7728, 7744);  slice_scatter_5794 = slice_scatter_5796 = None
        slice_31892: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5797, 1, 7728, 7744)
        slice_31893: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31892, 2, 0, 16)
        slice_scatter_5799: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31892, slice_31893, 2, 0, 16);  slice_31892 = slice_31893 = None
        slice_scatter_5800: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5797, slice_scatter_5799, 1, 7728, 7744);  slice_scatter_5797 = slice_scatter_5799 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31913: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_31879, 2, 16, 32);  slice_31879 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_970: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_31913, memory_format = torch.contiguous_format);  slice_31913 = None
        view_1944: "f32[32, 11]" = torch.ops.aten.view.default(clone_970, [32, 11]);  clone_970 = None
        mm_967: "f32[32, 8]" = torch.ops.aten.mm.default(view_1944, slice_37)
        view_1945: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_967, [2, 16, 8]);  mm_967 = None
        slice_31920: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5800, 1, 7728, 7744)
        slice_31921: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31920, 2, 0, 16)
        add_969: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31921, view_1945);  slice_31921 = view_1945 = None
        slice_scatter_5802: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31920, add_969, 2, 0, 16);  slice_31920 = add_969 = None
        slice_scatter_5803: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5800, slice_scatter_5802, 1, 7728, 7744);  slice_scatter_5800 = slice_scatter_5802 = None
        slice_31925: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5803, 1, 7728, 7744)
        slice_31926: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31925, 2, 0, 16)
        slice_scatter_5805: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31925, slice_31926, 2, 0, 16);  slice_31925 = slice_31926 = None
        slice_scatter_5806: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5803, slice_scatter_5805, 1, 7728, 7744);  slice_scatter_5803 = slice_scatter_5805 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31945: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7744, 7760)
        slice_31946: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_31945, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_971: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_31946, memory_format = torch.contiguous_format);  slice_31946 = None
        view_1946: "f32[32, 16]" = torch.ops.aten.view.default(clone_971, [32, 16]);  clone_971 = None
        mm_968: "f32[32, 8]" = torch.ops.aten.mm.default(view_1946, slice_7)
        view_1947: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_968, [2, 16, 8]);  mm_968 = None
        slice_31953: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5806, 1, 7744, 7760)
        slice_31954: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31953, 2, 0, 16)
        add_970: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31954, view_1947);  slice_31954 = view_1947 = None
        slice_scatter_5808: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31953, add_970, 2, 0, 16);  slice_31953 = add_970 = None
        slice_scatter_5809: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5806, slice_scatter_5808, 1, 7744, 7760);  slice_scatter_5806 = slice_scatter_5808 = None
        slice_31958: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5809, 1, 7744, 7760)
        slice_31959: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31958, 2, 0, 16)
        slice_scatter_5811: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31958, slice_31959, 2, 0, 16);  slice_31958 = slice_31959 = None
        slice_scatter_5812: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5809, slice_scatter_5811, 1, 7744, 7760);  slice_scatter_5809 = slice_scatter_5811 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_31979: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_31945, 2, 16, 32);  slice_31945 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_972: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_31979, memory_format = torch.contiguous_format);  slice_31979 = None
        view_1948: "f32[32, 11]" = torch.ops.aten.view.default(clone_972, [32, 11]);  clone_972 = None
        mm_969: "f32[32, 8]" = torch.ops.aten.mm.default(view_1948, slice_37)
        view_1949: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_969, [2, 16, 8]);  mm_969 = None
        slice_31986: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5812, 1, 7744, 7760)
        slice_31987: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31986, 2, 0, 16)
        add_971: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_31987, view_1949);  slice_31987 = view_1949 = None
        slice_scatter_5814: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31986, add_971, 2, 0, 16);  slice_31986 = add_971 = None
        slice_scatter_5815: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5812, slice_scatter_5814, 1, 7744, 7760);  slice_scatter_5812 = slice_scatter_5814 = None
        slice_31991: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5815, 1, 7744, 7760)
        slice_31992: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_31991, 2, 0, 16)
        slice_scatter_5817: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_31991, slice_31992, 2, 0, 16);  slice_31991 = slice_31992 = None
        slice_scatter_5818: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5815, slice_scatter_5817, 1, 7744, 7760);  slice_scatter_5815 = slice_scatter_5817 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32011: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7760, 7776)
        slice_32012: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_32011, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_973: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_32012, memory_format = torch.contiguous_format);  slice_32012 = None
        view_1950: "f32[32, 16]" = torch.ops.aten.view.default(clone_973, [32, 16]);  clone_973 = None
        mm_970: "f32[32, 8]" = torch.ops.aten.mm.default(view_1950, slice_7)
        view_1951: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_970, [2, 16, 8]);  mm_970 = None
        slice_32019: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5818, 1, 7760, 7776)
        slice_32020: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32019, 2, 0, 16)
        add_972: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32020, view_1951);  slice_32020 = view_1951 = None
        slice_scatter_5820: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32019, add_972, 2, 0, 16);  slice_32019 = add_972 = None
        slice_scatter_5821: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5818, slice_scatter_5820, 1, 7760, 7776);  slice_scatter_5818 = slice_scatter_5820 = None
        slice_32024: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5821, 1, 7760, 7776)
        slice_32025: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32024, 2, 0, 16)
        slice_scatter_5823: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32024, slice_32025, 2, 0, 16);  slice_32024 = slice_32025 = None
        slice_scatter_5824: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5821, slice_scatter_5823, 1, 7760, 7776);  slice_scatter_5821 = slice_scatter_5823 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32045: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_32011, 2, 16, 32);  slice_32011 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_974: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_32045, memory_format = torch.contiguous_format);  slice_32045 = None
        view_1952: "f32[32, 11]" = torch.ops.aten.view.default(clone_974, [32, 11]);  clone_974 = None
        mm_971: "f32[32, 8]" = torch.ops.aten.mm.default(view_1952, slice_37)
        view_1953: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_971, [2, 16, 8]);  mm_971 = None
        slice_32052: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5824, 1, 7760, 7776)
        slice_32053: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32052, 2, 0, 16)
        add_973: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32053, view_1953);  slice_32053 = view_1953 = None
        slice_scatter_5826: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32052, add_973, 2, 0, 16);  slice_32052 = add_973 = None
        slice_scatter_5827: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5824, slice_scatter_5826, 1, 7760, 7776);  slice_scatter_5824 = slice_scatter_5826 = None
        slice_32057: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5827, 1, 7760, 7776)
        slice_32058: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32057, 2, 0, 16)
        slice_scatter_5829: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32057, slice_32058, 2, 0, 16);  slice_32057 = slice_32058 = None
        slice_scatter_5830: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5827, slice_scatter_5829, 1, 7760, 7776);  slice_scatter_5827 = slice_scatter_5829 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32077: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7776, 7792)
        slice_32078: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_32077, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_975: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_32078, memory_format = torch.contiguous_format);  slice_32078 = None
        view_1954: "f32[32, 16]" = torch.ops.aten.view.default(clone_975, [32, 16]);  clone_975 = None
        mm_972: "f32[32, 8]" = torch.ops.aten.mm.default(view_1954, slice_7)
        view_1955: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_972, [2, 16, 8]);  mm_972 = None
        slice_32085: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5830, 1, 7776, 7792)
        slice_32086: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32085, 2, 0, 16)
        add_974: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32086, view_1955);  slice_32086 = view_1955 = None
        slice_scatter_5832: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32085, add_974, 2, 0, 16);  slice_32085 = add_974 = None
        slice_scatter_5833: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5830, slice_scatter_5832, 1, 7776, 7792);  slice_scatter_5830 = slice_scatter_5832 = None
        slice_32090: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5833, 1, 7776, 7792)
        slice_32091: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32090, 2, 0, 16)
        slice_scatter_5835: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32090, slice_32091, 2, 0, 16);  slice_32090 = slice_32091 = None
        slice_scatter_5836: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5833, slice_scatter_5835, 1, 7776, 7792);  slice_scatter_5833 = slice_scatter_5835 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32111: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_32077, 2, 16, 32);  slice_32077 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_976: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_32111, memory_format = torch.contiguous_format);  slice_32111 = None
        view_1956: "f32[32, 11]" = torch.ops.aten.view.default(clone_976, [32, 11]);  clone_976 = None
        mm_973: "f32[32, 8]" = torch.ops.aten.mm.default(view_1956, slice_37)
        view_1957: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_973, [2, 16, 8]);  mm_973 = None
        slice_32118: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5836, 1, 7776, 7792)
        slice_32119: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32118, 2, 0, 16)
        add_975: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32119, view_1957);  slice_32119 = view_1957 = None
        slice_scatter_5838: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32118, add_975, 2, 0, 16);  slice_32118 = add_975 = None
        slice_scatter_5839: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5836, slice_scatter_5838, 1, 7776, 7792);  slice_scatter_5836 = slice_scatter_5838 = None
        slice_32123: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5839, 1, 7776, 7792)
        slice_32124: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32123, 2, 0, 16)
        slice_scatter_5841: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32123, slice_32124, 2, 0, 16);  slice_32123 = slice_32124 = None
        slice_scatter_5842: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5839, slice_scatter_5841, 1, 7776, 7792);  slice_scatter_5839 = slice_scatter_5841 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32143: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7792, 7808)
        slice_32144: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_32143, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_977: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_32144, memory_format = torch.contiguous_format);  slice_32144 = None
        view_1958: "f32[32, 16]" = torch.ops.aten.view.default(clone_977, [32, 16]);  clone_977 = None
        mm_974: "f32[32, 8]" = torch.ops.aten.mm.default(view_1958, slice_7)
        view_1959: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_974, [2, 16, 8]);  mm_974 = None
        slice_32151: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5842, 1, 7792, 7808)
        slice_32152: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32151, 2, 0, 16)
        add_976: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32152, view_1959);  slice_32152 = view_1959 = None
        slice_scatter_5844: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32151, add_976, 2, 0, 16);  slice_32151 = add_976 = None
        slice_scatter_5845: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5842, slice_scatter_5844, 1, 7792, 7808);  slice_scatter_5842 = slice_scatter_5844 = None
        slice_32156: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5845, 1, 7792, 7808)
        slice_32157: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32156, 2, 0, 16)
        slice_scatter_5847: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32156, slice_32157, 2, 0, 16);  slice_32156 = slice_32157 = None
        slice_scatter_5848: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5845, slice_scatter_5847, 1, 7792, 7808);  slice_scatter_5845 = slice_scatter_5847 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32177: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_32143, 2, 16, 32);  slice_32143 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_978: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_32177, memory_format = torch.contiguous_format);  slice_32177 = None
        view_1960: "f32[32, 11]" = torch.ops.aten.view.default(clone_978, [32, 11]);  clone_978 = None
        mm_975: "f32[32, 8]" = torch.ops.aten.mm.default(view_1960, slice_37)
        view_1961: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_975, [2, 16, 8]);  mm_975 = None
        slice_32184: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5848, 1, 7792, 7808)
        slice_32185: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32184, 2, 0, 16)
        add_977: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32185, view_1961);  slice_32185 = view_1961 = None
        slice_scatter_5850: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32184, add_977, 2, 0, 16);  slice_32184 = add_977 = None
        slice_scatter_5851: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5848, slice_scatter_5850, 1, 7792, 7808);  slice_scatter_5848 = slice_scatter_5850 = None
        slice_32189: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5851, 1, 7792, 7808)
        slice_32190: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32189, 2, 0, 16)
        slice_scatter_5853: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32189, slice_32190, 2, 0, 16);  slice_32189 = slice_32190 = None
        slice_scatter_5854: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5851, slice_scatter_5853, 1, 7792, 7808);  slice_scatter_5851 = slice_scatter_5853 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32209: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7808, 7824)
        slice_32210: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_32209, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_979: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_32210, memory_format = torch.contiguous_format);  slice_32210 = None
        view_1962: "f32[32, 16]" = torch.ops.aten.view.default(clone_979, [32, 16]);  clone_979 = None
        mm_976: "f32[32, 8]" = torch.ops.aten.mm.default(view_1962, slice_7)
        view_1963: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_976, [2, 16, 8]);  mm_976 = None
        slice_32217: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5854, 1, 7808, 7824)
        slice_32218: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32217, 2, 0, 16)
        add_978: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32218, view_1963);  slice_32218 = view_1963 = None
        slice_scatter_5856: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32217, add_978, 2, 0, 16);  slice_32217 = add_978 = None
        slice_scatter_5857: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5854, slice_scatter_5856, 1, 7808, 7824);  slice_scatter_5854 = slice_scatter_5856 = None
        slice_32222: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5857, 1, 7808, 7824)
        slice_32223: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32222, 2, 0, 16)
        slice_scatter_5859: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32222, slice_32223, 2, 0, 16);  slice_32222 = slice_32223 = None
        slice_scatter_5860: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5857, slice_scatter_5859, 1, 7808, 7824);  slice_scatter_5857 = slice_scatter_5859 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32243: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_32209, 2, 16, 32);  slice_32209 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_980: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_32243, memory_format = torch.contiguous_format);  slice_32243 = None
        view_1964: "f32[32, 11]" = torch.ops.aten.view.default(clone_980, [32, 11]);  clone_980 = None
        mm_977: "f32[32, 8]" = torch.ops.aten.mm.default(view_1964, slice_37)
        view_1965: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_977, [2, 16, 8]);  mm_977 = None
        slice_32250: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5860, 1, 7808, 7824)
        slice_32251: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32250, 2, 0, 16)
        add_979: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32251, view_1965);  slice_32251 = view_1965 = None
        slice_scatter_5862: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32250, add_979, 2, 0, 16);  slice_32250 = add_979 = None
        slice_scatter_5863: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5860, slice_scatter_5862, 1, 7808, 7824);  slice_scatter_5860 = slice_scatter_5862 = None
        slice_32255: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5863, 1, 7808, 7824)
        slice_32256: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32255, 2, 0, 16)
        slice_scatter_5865: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32255, slice_32256, 2, 0, 16);  slice_32255 = slice_32256 = None
        slice_scatter_5866: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5863, slice_scatter_5865, 1, 7808, 7824);  slice_scatter_5863 = slice_scatter_5865 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32275: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7824, 7840)
        slice_32276: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_32275, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_981: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_32276, memory_format = torch.contiguous_format);  slice_32276 = None
        view_1966: "f32[32, 16]" = torch.ops.aten.view.default(clone_981, [32, 16]);  clone_981 = None
        mm_978: "f32[32, 8]" = torch.ops.aten.mm.default(view_1966, slice_7)
        view_1967: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_978, [2, 16, 8]);  mm_978 = None
        slice_32283: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5866, 1, 7824, 7840)
        slice_32284: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32283, 2, 0, 16)
        add_980: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32284, view_1967);  slice_32284 = view_1967 = None
        slice_scatter_5868: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32283, add_980, 2, 0, 16);  slice_32283 = add_980 = None
        slice_scatter_5869: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5866, slice_scatter_5868, 1, 7824, 7840);  slice_scatter_5866 = slice_scatter_5868 = None
        slice_32288: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5869, 1, 7824, 7840)
        slice_32289: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32288, 2, 0, 16)
        slice_scatter_5871: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32288, slice_32289, 2, 0, 16);  slice_32288 = slice_32289 = None
        slice_scatter_5872: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5869, slice_scatter_5871, 1, 7824, 7840);  slice_scatter_5869 = slice_scatter_5871 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32309: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_32275, 2, 16, 32);  slice_32275 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_982: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_32309, memory_format = torch.contiguous_format);  slice_32309 = None
        view_1968: "f32[32, 11]" = torch.ops.aten.view.default(clone_982, [32, 11]);  clone_982 = None
        mm_979: "f32[32, 8]" = torch.ops.aten.mm.default(view_1968, slice_37)
        view_1969: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_979, [2, 16, 8]);  mm_979 = None
        slice_32316: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5872, 1, 7824, 7840)
        slice_32317: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32316, 2, 0, 16)
        add_981: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32317, view_1969);  slice_32317 = view_1969 = None
        slice_scatter_5874: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32316, add_981, 2, 0, 16);  slice_32316 = add_981 = None
        slice_scatter_5875: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5872, slice_scatter_5874, 1, 7824, 7840);  slice_scatter_5872 = slice_scatter_5874 = None
        slice_32321: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5875, 1, 7824, 7840)
        slice_32322: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32321, 2, 0, 16)
        slice_scatter_5877: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32321, slice_32322, 2, 0, 16);  slice_32321 = slice_32322 = None
        slice_scatter_5878: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5875, slice_scatter_5877, 1, 7824, 7840);  slice_scatter_5875 = slice_scatter_5877 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32341: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7840, 7856)
        slice_32342: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_32341, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_983: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_32342, memory_format = torch.contiguous_format);  slice_32342 = None
        view_1970: "f32[32, 16]" = torch.ops.aten.view.default(clone_983, [32, 16]);  clone_983 = None
        mm_980: "f32[32, 8]" = torch.ops.aten.mm.default(view_1970, slice_7)
        view_1971: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_980, [2, 16, 8]);  mm_980 = None
        slice_32349: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5878, 1, 7840, 7856)
        slice_32350: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32349, 2, 0, 16)
        add_982: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32350, view_1971);  slice_32350 = view_1971 = None
        slice_scatter_5880: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32349, add_982, 2, 0, 16);  slice_32349 = add_982 = None
        slice_scatter_5881: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5878, slice_scatter_5880, 1, 7840, 7856);  slice_scatter_5878 = slice_scatter_5880 = None
        slice_32354: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5881, 1, 7840, 7856)
        slice_32355: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32354, 2, 0, 16)
        slice_scatter_5883: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32354, slice_32355, 2, 0, 16);  slice_32354 = slice_32355 = None
        slice_scatter_5884: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5881, slice_scatter_5883, 1, 7840, 7856);  slice_scatter_5881 = slice_scatter_5883 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32375: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_32341, 2, 16, 32);  slice_32341 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_984: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_32375, memory_format = torch.contiguous_format);  slice_32375 = None
        view_1972: "f32[32, 11]" = torch.ops.aten.view.default(clone_984, [32, 11]);  clone_984 = None
        mm_981: "f32[32, 8]" = torch.ops.aten.mm.default(view_1972, slice_37)
        view_1973: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_981, [2, 16, 8]);  mm_981 = None
        slice_32382: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5884, 1, 7840, 7856)
        slice_32383: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32382, 2, 0, 16)
        add_983: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32383, view_1973);  slice_32383 = view_1973 = None
        slice_scatter_5886: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32382, add_983, 2, 0, 16);  slice_32382 = add_983 = None
        slice_scatter_5887: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5884, slice_scatter_5886, 1, 7840, 7856);  slice_scatter_5884 = slice_scatter_5886 = None
        slice_32387: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5887, 1, 7840, 7856)
        slice_32388: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32387, 2, 0, 16)
        slice_scatter_5889: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32387, slice_32388, 2, 0, 16);  slice_32387 = slice_32388 = None
        slice_scatter_5890: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5887, slice_scatter_5889, 1, 7840, 7856);  slice_scatter_5887 = slice_scatter_5889 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32407: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7856, 7872)
        slice_32408: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_32407, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_985: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_32408, memory_format = torch.contiguous_format);  slice_32408 = None
        view_1974: "f32[32, 16]" = torch.ops.aten.view.default(clone_985, [32, 16]);  clone_985 = None
        mm_982: "f32[32, 8]" = torch.ops.aten.mm.default(view_1974, slice_7)
        view_1975: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_982, [2, 16, 8]);  mm_982 = None
        slice_32415: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5890, 1, 7856, 7872)
        slice_32416: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32415, 2, 0, 16)
        add_984: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32416, view_1975);  slice_32416 = view_1975 = None
        slice_scatter_5892: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32415, add_984, 2, 0, 16);  slice_32415 = add_984 = None
        slice_scatter_5893: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5890, slice_scatter_5892, 1, 7856, 7872);  slice_scatter_5890 = slice_scatter_5892 = None
        slice_32420: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5893, 1, 7856, 7872)
        slice_32421: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32420, 2, 0, 16)
        slice_scatter_5895: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32420, slice_32421, 2, 0, 16);  slice_32420 = slice_32421 = None
        slice_scatter_5896: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5893, slice_scatter_5895, 1, 7856, 7872);  slice_scatter_5893 = slice_scatter_5895 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32441: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_32407, 2, 16, 32);  slice_32407 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_986: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_32441, memory_format = torch.contiguous_format);  slice_32441 = None
        view_1976: "f32[32, 11]" = torch.ops.aten.view.default(clone_986, [32, 11]);  clone_986 = None
        mm_983: "f32[32, 8]" = torch.ops.aten.mm.default(view_1976, slice_37)
        view_1977: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_983, [2, 16, 8]);  mm_983 = None
        slice_32448: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5896, 1, 7856, 7872)
        slice_32449: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32448, 2, 0, 16)
        add_985: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32449, view_1977);  slice_32449 = view_1977 = None
        slice_scatter_5898: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32448, add_985, 2, 0, 16);  slice_32448 = add_985 = None
        slice_scatter_5899: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5896, slice_scatter_5898, 1, 7856, 7872);  slice_scatter_5896 = slice_scatter_5898 = None
        slice_32453: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5899, 1, 7856, 7872)
        slice_32454: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32453, 2, 0, 16)
        slice_scatter_5901: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32453, slice_32454, 2, 0, 16);  slice_32453 = slice_32454 = None
        slice_scatter_5902: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5899, slice_scatter_5901, 1, 7856, 7872);  slice_scatter_5899 = slice_scatter_5901 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32473: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7872, 7888)
        slice_32474: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_32473, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_987: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_32474, memory_format = torch.contiguous_format);  slice_32474 = None
        view_1978: "f32[32, 16]" = torch.ops.aten.view.default(clone_987, [32, 16]);  clone_987 = None
        mm_984: "f32[32, 8]" = torch.ops.aten.mm.default(view_1978, slice_7)
        view_1979: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_984, [2, 16, 8]);  mm_984 = None
        slice_32481: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5902, 1, 7872, 7888)
        slice_32482: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32481, 2, 0, 16)
        add_986: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32482, view_1979);  slice_32482 = view_1979 = None
        slice_scatter_5904: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32481, add_986, 2, 0, 16);  slice_32481 = add_986 = None
        slice_scatter_5905: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5902, slice_scatter_5904, 1, 7872, 7888);  slice_scatter_5902 = slice_scatter_5904 = None
        slice_32486: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5905, 1, 7872, 7888)
        slice_32487: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32486, 2, 0, 16)
        slice_scatter_5907: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32486, slice_32487, 2, 0, 16);  slice_32486 = slice_32487 = None
        slice_scatter_5908: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5905, slice_scatter_5907, 1, 7872, 7888);  slice_scatter_5905 = slice_scatter_5907 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32507: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_32473, 2, 16, 32);  slice_32473 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_988: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_32507, memory_format = torch.contiguous_format);  slice_32507 = None
        view_1980: "f32[32, 11]" = torch.ops.aten.view.default(clone_988, [32, 11]);  clone_988 = None
        mm_985: "f32[32, 8]" = torch.ops.aten.mm.default(view_1980, slice_37)
        view_1981: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_985, [2, 16, 8]);  mm_985 = None
        slice_32514: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5908, 1, 7872, 7888)
        slice_32515: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32514, 2, 0, 16)
        add_987: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32515, view_1981);  slice_32515 = view_1981 = None
        slice_scatter_5910: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32514, add_987, 2, 0, 16);  slice_32514 = add_987 = None
        slice_scatter_5911: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5908, slice_scatter_5910, 1, 7872, 7888);  slice_scatter_5908 = slice_scatter_5910 = None
        slice_32519: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5911, 1, 7872, 7888)
        slice_32520: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32519, 2, 0, 16)
        slice_scatter_5913: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32519, slice_32520, 2, 0, 16);  slice_32519 = slice_32520 = None
        slice_scatter_5914: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5911, slice_scatter_5913, 1, 7872, 7888);  slice_scatter_5911 = slice_scatter_5913 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32539: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7888, 7904)
        slice_32540: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_32539, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_989: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_32540, memory_format = torch.contiguous_format);  slice_32540 = None
        view_1982: "f32[32, 16]" = torch.ops.aten.view.default(clone_989, [32, 16]);  clone_989 = None
        mm_986: "f32[32, 8]" = torch.ops.aten.mm.default(view_1982, slice_7)
        view_1983: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_986, [2, 16, 8]);  mm_986 = None
        slice_32547: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5914, 1, 7888, 7904)
        slice_32548: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32547, 2, 0, 16)
        add_988: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32548, view_1983);  slice_32548 = view_1983 = None
        slice_scatter_5916: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32547, add_988, 2, 0, 16);  slice_32547 = add_988 = None
        slice_scatter_5917: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5914, slice_scatter_5916, 1, 7888, 7904);  slice_scatter_5914 = slice_scatter_5916 = None
        slice_32552: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5917, 1, 7888, 7904)
        slice_32553: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32552, 2, 0, 16)
        slice_scatter_5919: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32552, slice_32553, 2, 0, 16);  slice_32552 = slice_32553 = None
        slice_scatter_5920: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5917, slice_scatter_5919, 1, 7888, 7904);  slice_scatter_5917 = slice_scatter_5919 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32573: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_32539, 2, 16, 32);  slice_32539 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_990: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_32573, memory_format = torch.contiguous_format);  slice_32573 = None
        view_1984: "f32[32, 11]" = torch.ops.aten.view.default(clone_990, [32, 11]);  clone_990 = None
        mm_987: "f32[32, 8]" = torch.ops.aten.mm.default(view_1984, slice_37)
        view_1985: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_987, [2, 16, 8]);  mm_987 = None
        slice_32580: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5920, 1, 7888, 7904)
        slice_32581: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32580, 2, 0, 16)
        add_989: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32581, view_1985);  slice_32581 = view_1985 = None
        slice_scatter_5922: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32580, add_989, 2, 0, 16);  slice_32580 = add_989 = None
        slice_scatter_5923: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5920, slice_scatter_5922, 1, 7888, 7904);  slice_scatter_5920 = slice_scatter_5922 = None
        slice_32585: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5923, 1, 7888, 7904)
        slice_32586: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32585, 2, 0, 16)
        slice_scatter_5925: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32585, slice_32586, 2, 0, 16);  slice_32585 = slice_32586 = None
        slice_scatter_5926: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5923, slice_scatter_5925, 1, 7888, 7904);  slice_scatter_5923 = slice_scatter_5925 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32605: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7904, 7920)
        slice_32606: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_32605, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_991: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_32606, memory_format = torch.contiguous_format);  slice_32606 = None
        view_1986: "f32[32, 16]" = torch.ops.aten.view.default(clone_991, [32, 16]);  clone_991 = None
        mm_988: "f32[32, 8]" = torch.ops.aten.mm.default(view_1986, slice_7)
        view_1987: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_988, [2, 16, 8]);  mm_988 = None
        slice_32613: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5926, 1, 7904, 7920)
        slice_32614: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32613, 2, 0, 16)
        add_990: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32614, view_1987);  slice_32614 = view_1987 = None
        slice_scatter_5928: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32613, add_990, 2, 0, 16);  slice_32613 = add_990 = None
        slice_scatter_5929: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5926, slice_scatter_5928, 1, 7904, 7920);  slice_scatter_5926 = slice_scatter_5928 = None
        slice_32618: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5929, 1, 7904, 7920)
        slice_32619: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32618, 2, 0, 16)
        slice_scatter_5931: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32618, slice_32619, 2, 0, 16);  slice_32618 = slice_32619 = None
        slice_scatter_5932: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5929, slice_scatter_5931, 1, 7904, 7920);  slice_scatter_5929 = slice_scatter_5931 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32639: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_32605, 2, 16, 32);  slice_32605 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_992: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_32639, memory_format = torch.contiguous_format);  slice_32639 = None
        view_1988: "f32[32, 11]" = torch.ops.aten.view.default(clone_992, [32, 11]);  clone_992 = None
        mm_989: "f32[32, 8]" = torch.ops.aten.mm.default(view_1988, slice_37)
        view_1989: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_989, [2, 16, 8]);  mm_989 = None
        slice_32646: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5932, 1, 7904, 7920)
        slice_32647: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32646, 2, 0, 16)
        add_991: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32647, view_1989);  slice_32647 = view_1989 = None
        slice_scatter_5934: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32646, add_991, 2, 0, 16);  slice_32646 = add_991 = None
        slice_scatter_5935: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5932, slice_scatter_5934, 1, 7904, 7920);  slice_scatter_5932 = slice_scatter_5934 = None
        slice_32651: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5935, 1, 7904, 7920)
        slice_32652: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32651, 2, 0, 16)
        slice_scatter_5937: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32651, slice_32652, 2, 0, 16);  slice_32651 = slice_32652 = None
        slice_scatter_5938: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5935, slice_scatter_5937, 1, 7904, 7920);  slice_scatter_5935 = slice_scatter_5937 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32671: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7920, 7936)
        slice_32672: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_32671, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_993: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_32672, memory_format = torch.contiguous_format);  slice_32672 = None
        view_1990: "f32[32, 16]" = torch.ops.aten.view.default(clone_993, [32, 16]);  clone_993 = None
        mm_990: "f32[32, 8]" = torch.ops.aten.mm.default(view_1990, slice_7)
        view_1991: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_990, [2, 16, 8]);  mm_990 = None
        slice_32679: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5938, 1, 7920, 7936)
        slice_32680: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32679, 2, 0, 16)
        add_992: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32680, view_1991);  slice_32680 = view_1991 = None
        slice_scatter_5940: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32679, add_992, 2, 0, 16);  slice_32679 = add_992 = None
        slice_scatter_5941: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5938, slice_scatter_5940, 1, 7920, 7936);  slice_scatter_5938 = slice_scatter_5940 = None
        slice_32684: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5941, 1, 7920, 7936)
        slice_32685: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32684, 2, 0, 16)
        slice_scatter_5943: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32684, slice_32685, 2, 0, 16);  slice_32684 = slice_32685 = None
        slice_scatter_5944: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5941, slice_scatter_5943, 1, 7920, 7936);  slice_scatter_5941 = slice_scatter_5943 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32705: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_32671, 2, 16, 32);  slice_32671 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_994: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_32705, memory_format = torch.contiguous_format);  slice_32705 = None
        view_1992: "f32[32, 11]" = torch.ops.aten.view.default(clone_994, [32, 11]);  clone_994 = None
        mm_991: "f32[32, 8]" = torch.ops.aten.mm.default(view_1992, slice_37)
        view_1993: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_991, [2, 16, 8]);  mm_991 = None
        slice_32712: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5944, 1, 7920, 7936)
        slice_32713: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32712, 2, 0, 16)
        add_993: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32713, view_1993);  slice_32713 = view_1993 = None
        slice_scatter_5946: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32712, add_993, 2, 0, 16);  slice_32712 = add_993 = None
        slice_scatter_5947: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5944, slice_scatter_5946, 1, 7920, 7936);  slice_scatter_5944 = slice_scatter_5946 = None
        slice_32717: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5947, 1, 7920, 7936)
        slice_32718: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32717, 2, 0, 16)
        slice_scatter_5949: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32717, slice_32718, 2, 0, 16);  slice_32717 = slice_32718 = None
        slice_scatter_5950: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5947, slice_scatter_5949, 1, 7920, 7936);  slice_scatter_5947 = slice_scatter_5949 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32737: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7936, 7952)
        slice_32738: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_32737, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_995: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_32738, memory_format = torch.contiguous_format);  slice_32738 = None
        view_1994: "f32[32, 16]" = torch.ops.aten.view.default(clone_995, [32, 16]);  clone_995 = None
        mm_992: "f32[32, 8]" = torch.ops.aten.mm.default(view_1994, slice_7)
        view_1995: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_992, [2, 16, 8]);  mm_992 = None
        slice_32745: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5950, 1, 7936, 7952)
        slice_32746: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32745, 2, 0, 16)
        add_994: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32746, view_1995);  slice_32746 = view_1995 = None
        slice_scatter_5952: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32745, add_994, 2, 0, 16);  slice_32745 = add_994 = None
        slice_scatter_5953: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5950, slice_scatter_5952, 1, 7936, 7952);  slice_scatter_5950 = slice_scatter_5952 = None
        slice_32750: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5953, 1, 7936, 7952)
        slice_32751: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32750, 2, 0, 16)
        slice_scatter_5955: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32750, slice_32751, 2, 0, 16);  slice_32750 = slice_32751 = None
        slice_scatter_5956: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5953, slice_scatter_5955, 1, 7936, 7952);  slice_scatter_5953 = slice_scatter_5955 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32771: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_32737, 2, 16, 32);  slice_32737 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_996: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_32771, memory_format = torch.contiguous_format);  slice_32771 = None
        view_1996: "f32[32, 11]" = torch.ops.aten.view.default(clone_996, [32, 11]);  clone_996 = None
        mm_993: "f32[32, 8]" = torch.ops.aten.mm.default(view_1996, slice_37)
        view_1997: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_993, [2, 16, 8]);  mm_993 = None
        slice_32778: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5956, 1, 7936, 7952)
        slice_32779: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32778, 2, 0, 16)
        add_995: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32779, view_1997);  slice_32779 = view_1997 = None
        slice_scatter_5958: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32778, add_995, 2, 0, 16);  slice_32778 = add_995 = None
        slice_scatter_5959: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5956, slice_scatter_5958, 1, 7936, 7952);  slice_scatter_5956 = slice_scatter_5958 = None
        slice_32783: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5959, 1, 7936, 7952)
        slice_32784: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32783, 2, 0, 16)
        slice_scatter_5961: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32783, slice_32784, 2, 0, 16);  slice_32783 = slice_32784 = None
        slice_scatter_5962: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5959, slice_scatter_5961, 1, 7936, 7952);  slice_scatter_5959 = slice_scatter_5961 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32803: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7952, 7968)
        slice_32804: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_32803, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_997: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_32804, memory_format = torch.contiguous_format);  slice_32804 = None
        view_1998: "f32[32, 16]" = torch.ops.aten.view.default(clone_997, [32, 16]);  clone_997 = None
        mm_994: "f32[32, 8]" = torch.ops.aten.mm.default(view_1998, slice_7)
        view_1999: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_994, [2, 16, 8]);  mm_994 = None
        slice_32811: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5962, 1, 7952, 7968)
        slice_32812: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32811, 2, 0, 16)
        add_996: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32812, view_1999);  slice_32812 = view_1999 = None
        slice_scatter_5964: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32811, add_996, 2, 0, 16);  slice_32811 = add_996 = None
        slice_scatter_5965: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5962, slice_scatter_5964, 1, 7952, 7968);  slice_scatter_5962 = slice_scatter_5964 = None
        slice_32816: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5965, 1, 7952, 7968)
        slice_32817: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32816, 2, 0, 16)
        slice_scatter_5967: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32816, slice_32817, 2, 0, 16);  slice_32816 = slice_32817 = None
        slice_scatter_5968: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5965, slice_scatter_5967, 1, 7952, 7968);  slice_scatter_5965 = slice_scatter_5967 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32837: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_32803, 2, 16, 32);  slice_32803 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_998: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_32837, memory_format = torch.contiguous_format);  slice_32837 = None
        view_2000: "f32[32, 11]" = torch.ops.aten.view.default(clone_998, [32, 11]);  clone_998 = None
        mm_995: "f32[32, 8]" = torch.ops.aten.mm.default(view_2000, slice_37)
        view_2001: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_995, [2, 16, 8]);  mm_995 = None
        slice_32844: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5968, 1, 7952, 7968)
        slice_32845: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32844, 2, 0, 16)
        add_997: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32845, view_2001);  slice_32845 = view_2001 = None
        slice_scatter_5970: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32844, add_997, 2, 0, 16);  slice_32844 = add_997 = None
        slice_scatter_5971: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5968, slice_scatter_5970, 1, 7952, 7968);  slice_scatter_5968 = slice_scatter_5970 = None
        slice_32849: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5971, 1, 7952, 7968)
        slice_32850: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32849, 2, 0, 16)
        slice_scatter_5973: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32849, slice_32850, 2, 0, 16);  slice_32849 = slice_32850 = None
        slice_scatter_5974: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5971, slice_scatter_5973, 1, 7952, 7968);  slice_scatter_5971 = slice_scatter_5973 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32869: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7968, 7984)
        slice_32870: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_32869, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_999: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_32870, memory_format = torch.contiguous_format);  slice_32870 = None
        view_2002: "f32[32, 16]" = torch.ops.aten.view.default(clone_999, [32, 16]);  clone_999 = None
        mm_996: "f32[32, 8]" = torch.ops.aten.mm.default(view_2002, slice_7)
        view_2003: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_996, [2, 16, 8]);  mm_996 = None
        slice_32877: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5974, 1, 7968, 7984)
        slice_32878: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32877, 2, 0, 16)
        add_998: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32878, view_2003);  slice_32878 = view_2003 = None
        slice_scatter_5976: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32877, add_998, 2, 0, 16);  slice_32877 = add_998 = None
        slice_scatter_5977: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5974, slice_scatter_5976, 1, 7968, 7984);  slice_scatter_5974 = slice_scatter_5976 = None
        slice_32882: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5977, 1, 7968, 7984)
        slice_32883: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32882, 2, 0, 16)
        slice_scatter_5979: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32882, slice_32883, 2, 0, 16);  slice_32882 = slice_32883 = None
        slice_scatter_5980: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5977, slice_scatter_5979, 1, 7968, 7984);  slice_scatter_5977 = slice_scatter_5979 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32903: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_32869, 2, 16, 32);  slice_32869 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1000: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_32903, memory_format = torch.contiguous_format);  slice_32903 = None
        view_2004: "f32[32, 11]" = torch.ops.aten.view.default(clone_1000, [32, 11]);  clone_1000 = None
        mm_997: "f32[32, 8]" = torch.ops.aten.mm.default(view_2004, slice_37)
        view_2005: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_997, [2, 16, 8]);  mm_997 = None
        slice_32910: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5980, 1, 7968, 7984)
        slice_32911: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32910, 2, 0, 16)
        add_999: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32911, view_2005);  slice_32911 = view_2005 = None
        slice_scatter_5982: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32910, add_999, 2, 0, 16);  slice_32910 = add_999 = None
        slice_scatter_5983: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5980, slice_scatter_5982, 1, 7968, 7984);  slice_scatter_5980 = slice_scatter_5982 = None
        slice_32915: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5983, 1, 7968, 7984)
        slice_32916: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32915, 2, 0, 16)
        slice_scatter_5985: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32915, slice_32916, 2, 0, 16);  slice_32915 = slice_32916 = None
        slice_scatter_5986: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5983, slice_scatter_5985, 1, 7968, 7984);  slice_scatter_5983 = slice_scatter_5985 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32935: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 7984, 8000)
        slice_32936: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_32935, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1001: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_32936, memory_format = torch.contiguous_format);  slice_32936 = None
        view_2006: "f32[32, 16]" = torch.ops.aten.view.default(clone_1001, [32, 16]);  clone_1001 = None
        mm_998: "f32[32, 8]" = torch.ops.aten.mm.default(view_2006, slice_7)
        view_2007: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_998, [2, 16, 8]);  mm_998 = None
        slice_32943: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5986, 1, 7984, 8000)
        slice_32944: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32943, 2, 0, 16)
        add_1000: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32944, view_2007);  slice_32944 = view_2007 = None
        slice_scatter_5988: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32943, add_1000, 2, 0, 16);  slice_32943 = add_1000 = None
        slice_scatter_5989: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5986, slice_scatter_5988, 1, 7984, 8000);  slice_scatter_5986 = slice_scatter_5988 = None
        slice_32948: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5989, 1, 7984, 8000)
        slice_32949: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32948, 2, 0, 16)
        slice_scatter_5991: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32948, slice_32949, 2, 0, 16);  slice_32948 = slice_32949 = None
        slice_scatter_5992: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5989, slice_scatter_5991, 1, 7984, 8000);  slice_scatter_5989 = slice_scatter_5991 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_32969: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_32935, 2, 16, 32);  slice_32935 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1002: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_32969, memory_format = torch.contiguous_format);  slice_32969 = None
        view_2008: "f32[32, 11]" = torch.ops.aten.view.default(clone_1002, [32, 11]);  clone_1002 = None
        mm_999: "f32[32, 8]" = torch.ops.aten.mm.default(view_2008, slice_37)
        view_2009: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_999, [2, 16, 8]);  mm_999 = None
        slice_32976: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5992, 1, 7984, 8000)
        slice_32977: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32976, 2, 0, 16)
        add_1001: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_32977, view_2009);  slice_32977 = view_2009 = None
        slice_scatter_5994: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32976, add_1001, 2, 0, 16);  slice_32976 = add_1001 = None
        slice_scatter_5995: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5992, slice_scatter_5994, 1, 7984, 8000);  slice_scatter_5992 = slice_scatter_5994 = None
        slice_32981: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5995, 1, 7984, 8000)
        slice_32982: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_32981, 2, 0, 16)
        slice_scatter_5997: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_32981, slice_32982, 2, 0, 16);  slice_32981 = slice_32982 = None
        slice_scatter_5998: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5995, slice_scatter_5997, 1, 7984, 8000);  slice_scatter_5995 = slice_scatter_5997 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33001: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8000, 8016)
        slice_33002: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_33001, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1003: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_33002, memory_format = torch.contiguous_format);  slice_33002 = None
        view_2010: "f32[32, 16]" = torch.ops.aten.view.default(clone_1003, [32, 16]);  clone_1003 = None
        mm_1000: "f32[32, 8]" = torch.ops.aten.mm.default(view_2010, slice_7)
        view_2011: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1000, [2, 16, 8]);  mm_1000 = None
        slice_33009: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_5998, 1, 8000, 8016)
        slice_33010: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33009, 2, 0, 16)
        add_1002: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33010, view_2011);  slice_33010 = view_2011 = None
        slice_scatter_6000: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33009, add_1002, 2, 0, 16);  slice_33009 = add_1002 = None
        slice_scatter_6001: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_5998, slice_scatter_6000, 1, 8000, 8016);  slice_scatter_5998 = slice_scatter_6000 = None
        slice_33014: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6001, 1, 8000, 8016)
        slice_33015: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33014, 2, 0, 16)
        slice_scatter_6003: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33014, slice_33015, 2, 0, 16);  slice_33014 = slice_33015 = None
        slice_scatter_6004: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6001, slice_scatter_6003, 1, 8000, 8016);  slice_scatter_6001 = slice_scatter_6003 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33035: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_33001, 2, 16, 32);  slice_33001 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1004: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_33035, memory_format = torch.contiguous_format);  slice_33035 = None
        view_2012: "f32[32, 11]" = torch.ops.aten.view.default(clone_1004, [32, 11]);  clone_1004 = None
        mm_1001: "f32[32, 8]" = torch.ops.aten.mm.default(view_2012, slice_37)
        view_2013: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1001, [2, 16, 8]);  mm_1001 = None
        slice_33042: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6004, 1, 8000, 8016)
        slice_33043: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33042, 2, 0, 16)
        add_1003: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33043, view_2013);  slice_33043 = view_2013 = None
        slice_scatter_6006: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33042, add_1003, 2, 0, 16);  slice_33042 = add_1003 = None
        slice_scatter_6007: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6004, slice_scatter_6006, 1, 8000, 8016);  slice_scatter_6004 = slice_scatter_6006 = None
        slice_33047: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6007, 1, 8000, 8016)
        slice_33048: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33047, 2, 0, 16)
        slice_scatter_6009: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33047, slice_33048, 2, 0, 16);  slice_33047 = slice_33048 = None
        slice_scatter_6010: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6007, slice_scatter_6009, 1, 8000, 8016);  slice_scatter_6007 = slice_scatter_6009 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33067: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8016, 8032)
        slice_33068: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_33067, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1005: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_33068, memory_format = torch.contiguous_format);  slice_33068 = None
        view_2014: "f32[32, 16]" = torch.ops.aten.view.default(clone_1005, [32, 16]);  clone_1005 = None
        mm_1002: "f32[32, 8]" = torch.ops.aten.mm.default(view_2014, slice_7)
        view_2015: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1002, [2, 16, 8]);  mm_1002 = None
        slice_33075: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6010, 1, 8016, 8032)
        slice_33076: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33075, 2, 0, 16)
        add_1004: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33076, view_2015);  slice_33076 = view_2015 = None
        slice_scatter_6012: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33075, add_1004, 2, 0, 16);  slice_33075 = add_1004 = None
        slice_scatter_6013: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6010, slice_scatter_6012, 1, 8016, 8032);  slice_scatter_6010 = slice_scatter_6012 = None
        slice_33080: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6013, 1, 8016, 8032)
        slice_33081: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33080, 2, 0, 16)
        slice_scatter_6015: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33080, slice_33081, 2, 0, 16);  slice_33080 = slice_33081 = None
        slice_scatter_6016: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6013, slice_scatter_6015, 1, 8016, 8032);  slice_scatter_6013 = slice_scatter_6015 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33101: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_33067, 2, 16, 32);  slice_33067 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1006: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_33101, memory_format = torch.contiguous_format);  slice_33101 = None
        view_2016: "f32[32, 11]" = torch.ops.aten.view.default(clone_1006, [32, 11]);  clone_1006 = None
        mm_1003: "f32[32, 8]" = torch.ops.aten.mm.default(view_2016, slice_37)
        view_2017: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1003, [2, 16, 8]);  mm_1003 = None
        slice_33108: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6016, 1, 8016, 8032)
        slice_33109: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33108, 2, 0, 16)
        add_1005: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33109, view_2017);  slice_33109 = view_2017 = None
        slice_scatter_6018: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33108, add_1005, 2, 0, 16);  slice_33108 = add_1005 = None
        slice_scatter_6019: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6016, slice_scatter_6018, 1, 8016, 8032);  slice_scatter_6016 = slice_scatter_6018 = None
        slice_33113: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6019, 1, 8016, 8032)
        slice_33114: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33113, 2, 0, 16)
        slice_scatter_6021: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33113, slice_33114, 2, 0, 16);  slice_33113 = slice_33114 = None
        slice_scatter_6022: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6019, slice_scatter_6021, 1, 8016, 8032);  slice_scatter_6019 = slice_scatter_6021 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33133: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8032, 8048)
        slice_33134: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_33133, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1007: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_33134, memory_format = torch.contiguous_format);  slice_33134 = None
        view_2018: "f32[32, 16]" = torch.ops.aten.view.default(clone_1007, [32, 16]);  clone_1007 = None
        mm_1004: "f32[32, 8]" = torch.ops.aten.mm.default(view_2018, slice_7)
        view_2019: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1004, [2, 16, 8]);  mm_1004 = None
        slice_33141: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6022, 1, 8032, 8048)
        slice_33142: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33141, 2, 0, 16)
        add_1006: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33142, view_2019);  slice_33142 = view_2019 = None
        slice_scatter_6024: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33141, add_1006, 2, 0, 16);  slice_33141 = add_1006 = None
        slice_scatter_6025: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6022, slice_scatter_6024, 1, 8032, 8048);  slice_scatter_6022 = slice_scatter_6024 = None
        slice_33146: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6025, 1, 8032, 8048)
        slice_33147: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33146, 2, 0, 16)
        slice_scatter_6027: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33146, slice_33147, 2, 0, 16);  slice_33146 = slice_33147 = None
        slice_scatter_6028: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6025, slice_scatter_6027, 1, 8032, 8048);  slice_scatter_6025 = slice_scatter_6027 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33167: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_33133, 2, 16, 32);  slice_33133 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1008: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_33167, memory_format = torch.contiguous_format);  slice_33167 = None
        view_2020: "f32[32, 11]" = torch.ops.aten.view.default(clone_1008, [32, 11]);  clone_1008 = None
        mm_1005: "f32[32, 8]" = torch.ops.aten.mm.default(view_2020, slice_37)
        view_2021: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1005, [2, 16, 8]);  mm_1005 = None
        slice_33174: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6028, 1, 8032, 8048)
        slice_33175: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33174, 2, 0, 16)
        add_1007: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33175, view_2021);  slice_33175 = view_2021 = None
        slice_scatter_6030: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33174, add_1007, 2, 0, 16);  slice_33174 = add_1007 = None
        slice_scatter_6031: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6028, slice_scatter_6030, 1, 8032, 8048);  slice_scatter_6028 = slice_scatter_6030 = None
        slice_33179: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6031, 1, 8032, 8048)
        slice_33180: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33179, 2, 0, 16)
        slice_scatter_6033: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33179, slice_33180, 2, 0, 16);  slice_33179 = slice_33180 = None
        slice_scatter_6034: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6031, slice_scatter_6033, 1, 8032, 8048);  slice_scatter_6031 = slice_scatter_6033 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33199: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8048, 8064)
        slice_33200: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_33199, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1009: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_33200, memory_format = torch.contiguous_format);  slice_33200 = None
        view_2022: "f32[32, 16]" = torch.ops.aten.view.default(clone_1009, [32, 16]);  clone_1009 = None
        mm_1006: "f32[32, 8]" = torch.ops.aten.mm.default(view_2022, slice_7)
        view_2023: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1006, [2, 16, 8]);  mm_1006 = None
        slice_33207: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6034, 1, 8048, 8064)
        slice_33208: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33207, 2, 0, 16)
        add_1008: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33208, view_2023);  slice_33208 = view_2023 = None
        slice_scatter_6036: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33207, add_1008, 2, 0, 16);  slice_33207 = add_1008 = None
        slice_scatter_6037: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6034, slice_scatter_6036, 1, 8048, 8064);  slice_scatter_6034 = slice_scatter_6036 = None
        slice_33212: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6037, 1, 8048, 8064)
        slice_33213: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33212, 2, 0, 16)
        slice_scatter_6039: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33212, slice_33213, 2, 0, 16);  slice_33212 = slice_33213 = None
        slice_scatter_6040: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6037, slice_scatter_6039, 1, 8048, 8064);  slice_scatter_6037 = slice_scatter_6039 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33233: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_33199, 2, 16, 32);  slice_33199 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1010: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_33233, memory_format = torch.contiguous_format);  slice_33233 = None
        view_2024: "f32[32, 11]" = torch.ops.aten.view.default(clone_1010, [32, 11]);  clone_1010 = None
        mm_1007: "f32[32, 8]" = torch.ops.aten.mm.default(view_2024, slice_37)
        view_2025: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1007, [2, 16, 8]);  mm_1007 = None
        slice_33240: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6040, 1, 8048, 8064)
        slice_33241: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33240, 2, 0, 16)
        add_1009: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33241, view_2025);  slice_33241 = view_2025 = None
        slice_scatter_6042: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33240, add_1009, 2, 0, 16);  slice_33240 = add_1009 = None
        slice_scatter_6043: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6040, slice_scatter_6042, 1, 8048, 8064);  slice_scatter_6040 = slice_scatter_6042 = None
        slice_33245: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6043, 1, 8048, 8064)
        slice_33246: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33245, 2, 0, 16)
        slice_scatter_6045: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33245, slice_33246, 2, 0, 16);  slice_33245 = slice_33246 = None
        slice_scatter_6046: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6043, slice_scatter_6045, 1, 8048, 8064);  slice_scatter_6043 = slice_scatter_6045 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33265: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8064, 8080)
        slice_33266: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_33265, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1011: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_33266, memory_format = torch.contiguous_format);  slice_33266 = None
        view_2026: "f32[32, 16]" = torch.ops.aten.view.default(clone_1011, [32, 16]);  clone_1011 = None
        mm_1008: "f32[32, 8]" = torch.ops.aten.mm.default(view_2026, slice_7)
        view_2027: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1008, [2, 16, 8]);  mm_1008 = None
        slice_33273: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6046, 1, 8064, 8080)
        slice_33274: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33273, 2, 0, 16)
        add_1010: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33274, view_2027);  slice_33274 = view_2027 = None
        slice_scatter_6048: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33273, add_1010, 2, 0, 16);  slice_33273 = add_1010 = None
        slice_scatter_6049: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6046, slice_scatter_6048, 1, 8064, 8080);  slice_scatter_6046 = slice_scatter_6048 = None
        slice_33278: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6049, 1, 8064, 8080)
        slice_33279: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33278, 2, 0, 16)
        slice_scatter_6051: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33278, slice_33279, 2, 0, 16);  slice_33278 = slice_33279 = None
        slice_scatter_6052: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6049, slice_scatter_6051, 1, 8064, 8080);  slice_scatter_6049 = slice_scatter_6051 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33299: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_33265, 2, 16, 32);  slice_33265 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1012: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_33299, memory_format = torch.contiguous_format);  slice_33299 = None
        view_2028: "f32[32, 11]" = torch.ops.aten.view.default(clone_1012, [32, 11]);  clone_1012 = None
        mm_1009: "f32[32, 8]" = torch.ops.aten.mm.default(view_2028, slice_37)
        view_2029: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1009, [2, 16, 8]);  mm_1009 = None
        slice_33306: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6052, 1, 8064, 8080)
        slice_33307: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33306, 2, 0, 16)
        add_1011: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33307, view_2029);  slice_33307 = view_2029 = None
        slice_scatter_6054: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33306, add_1011, 2, 0, 16);  slice_33306 = add_1011 = None
        slice_scatter_6055: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6052, slice_scatter_6054, 1, 8064, 8080);  slice_scatter_6052 = slice_scatter_6054 = None
        slice_33311: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6055, 1, 8064, 8080)
        slice_33312: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33311, 2, 0, 16)
        slice_scatter_6057: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33311, slice_33312, 2, 0, 16);  slice_33311 = slice_33312 = None
        slice_scatter_6058: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6055, slice_scatter_6057, 1, 8064, 8080);  slice_scatter_6055 = slice_scatter_6057 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33331: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8080, 8096)
        slice_33332: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_33331, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1013: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_33332, memory_format = torch.contiguous_format);  slice_33332 = None
        view_2030: "f32[32, 16]" = torch.ops.aten.view.default(clone_1013, [32, 16]);  clone_1013 = None
        mm_1010: "f32[32, 8]" = torch.ops.aten.mm.default(view_2030, slice_7)
        view_2031: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1010, [2, 16, 8]);  mm_1010 = None
        slice_33339: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6058, 1, 8080, 8096)
        slice_33340: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33339, 2, 0, 16)
        add_1012: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33340, view_2031);  slice_33340 = view_2031 = None
        slice_scatter_6060: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33339, add_1012, 2, 0, 16);  slice_33339 = add_1012 = None
        slice_scatter_6061: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6058, slice_scatter_6060, 1, 8080, 8096);  slice_scatter_6058 = slice_scatter_6060 = None
        slice_33344: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6061, 1, 8080, 8096)
        slice_33345: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33344, 2, 0, 16)
        slice_scatter_6063: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33344, slice_33345, 2, 0, 16);  slice_33344 = slice_33345 = None
        slice_scatter_6064: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6061, slice_scatter_6063, 1, 8080, 8096);  slice_scatter_6061 = slice_scatter_6063 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33365: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_33331, 2, 16, 32);  slice_33331 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1014: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_33365, memory_format = torch.contiguous_format);  slice_33365 = None
        view_2032: "f32[32, 11]" = torch.ops.aten.view.default(clone_1014, [32, 11]);  clone_1014 = None
        mm_1011: "f32[32, 8]" = torch.ops.aten.mm.default(view_2032, slice_37)
        view_2033: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1011, [2, 16, 8]);  mm_1011 = None
        slice_33372: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6064, 1, 8080, 8096)
        slice_33373: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33372, 2, 0, 16)
        add_1013: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33373, view_2033);  slice_33373 = view_2033 = None
        slice_scatter_6066: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33372, add_1013, 2, 0, 16);  slice_33372 = add_1013 = None
        slice_scatter_6067: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6064, slice_scatter_6066, 1, 8080, 8096);  slice_scatter_6064 = slice_scatter_6066 = None
        slice_33377: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6067, 1, 8080, 8096)
        slice_33378: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33377, 2, 0, 16)
        slice_scatter_6069: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33377, slice_33378, 2, 0, 16);  slice_33377 = slice_33378 = None
        slice_scatter_6070: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6067, slice_scatter_6069, 1, 8080, 8096);  slice_scatter_6067 = slice_scatter_6069 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33397: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8096, 8112)
        slice_33398: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_33397, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1015: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_33398, memory_format = torch.contiguous_format);  slice_33398 = None
        view_2034: "f32[32, 16]" = torch.ops.aten.view.default(clone_1015, [32, 16]);  clone_1015 = None
        mm_1012: "f32[32, 8]" = torch.ops.aten.mm.default(view_2034, slice_7)
        view_2035: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1012, [2, 16, 8]);  mm_1012 = None
        slice_33405: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6070, 1, 8096, 8112)
        slice_33406: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33405, 2, 0, 16)
        add_1014: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33406, view_2035);  slice_33406 = view_2035 = None
        slice_scatter_6072: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33405, add_1014, 2, 0, 16);  slice_33405 = add_1014 = None
        slice_scatter_6073: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6070, slice_scatter_6072, 1, 8096, 8112);  slice_scatter_6070 = slice_scatter_6072 = None
        slice_33410: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6073, 1, 8096, 8112)
        slice_33411: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33410, 2, 0, 16)
        slice_scatter_6075: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33410, slice_33411, 2, 0, 16);  slice_33410 = slice_33411 = None
        slice_scatter_6076: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6073, slice_scatter_6075, 1, 8096, 8112);  slice_scatter_6073 = slice_scatter_6075 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33431: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_33397, 2, 16, 32);  slice_33397 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1016: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_33431, memory_format = torch.contiguous_format);  slice_33431 = None
        view_2036: "f32[32, 11]" = torch.ops.aten.view.default(clone_1016, [32, 11]);  clone_1016 = None
        mm_1013: "f32[32, 8]" = torch.ops.aten.mm.default(view_2036, slice_37)
        view_2037: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1013, [2, 16, 8]);  mm_1013 = None
        slice_33438: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6076, 1, 8096, 8112)
        slice_33439: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33438, 2, 0, 16)
        add_1015: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33439, view_2037);  slice_33439 = view_2037 = None
        slice_scatter_6078: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33438, add_1015, 2, 0, 16);  slice_33438 = add_1015 = None
        slice_scatter_6079: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6076, slice_scatter_6078, 1, 8096, 8112);  slice_scatter_6076 = slice_scatter_6078 = None
        slice_33443: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6079, 1, 8096, 8112)
        slice_33444: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33443, 2, 0, 16)
        slice_scatter_6081: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33443, slice_33444, 2, 0, 16);  slice_33443 = slice_33444 = None
        slice_scatter_6082: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6079, slice_scatter_6081, 1, 8096, 8112);  slice_scatter_6079 = slice_scatter_6081 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33463: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8112, 8128)
        slice_33464: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_33463, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1017: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_33464, memory_format = torch.contiguous_format);  slice_33464 = None
        view_2038: "f32[32, 16]" = torch.ops.aten.view.default(clone_1017, [32, 16]);  clone_1017 = None
        mm_1014: "f32[32, 8]" = torch.ops.aten.mm.default(view_2038, slice_7)
        view_2039: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1014, [2, 16, 8]);  mm_1014 = None
        slice_33471: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6082, 1, 8112, 8128)
        slice_33472: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33471, 2, 0, 16)
        add_1016: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33472, view_2039);  slice_33472 = view_2039 = None
        slice_scatter_6084: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33471, add_1016, 2, 0, 16);  slice_33471 = add_1016 = None
        slice_scatter_6085: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6082, slice_scatter_6084, 1, 8112, 8128);  slice_scatter_6082 = slice_scatter_6084 = None
        slice_33476: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6085, 1, 8112, 8128)
        slice_33477: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33476, 2, 0, 16)
        slice_scatter_6087: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33476, slice_33477, 2, 0, 16);  slice_33476 = slice_33477 = None
        slice_scatter_6088: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6085, slice_scatter_6087, 1, 8112, 8128);  slice_scatter_6085 = slice_scatter_6087 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33497: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_33463, 2, 16, 32);  slice_33463 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1018: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_33497, memory_format = torch.contiguous_format);  slice_33497 = None
        view_2040: "f32[32, 11]" = torch.ops.aten.view.default(clone_1018, [32, 11]);  clone_1018 = None
        mm_1015: "f32[32, 8]" = torch.ops.aten.mm.default(view_2040, slice_37)
        view_2041: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1015, [2, 16, 8]);  mm_1015 = None
        slice_33504: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6088, 1, 8112, 8128)
        slice_33505: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33504, 2, 0, 16)
        add_1017: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33505, view_2041);  slice_33505 = view_2041 = None
        slice_scatter_6090: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33504, add_1017, 2, 0, 16);  slice_33504 = add_1017 = None
        slice_scatter_6091: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6088, slice_scatter_6090, 1, 8112, 8128);  slice_scatter_6088 = slice_scatter_6090 = None
        slice_33509: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6091, 1, 8112, 8128)
        slice_33510: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33509, 2, 0, 16)
        slice_scatter_6093: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33509, slice_33510, 2, 0, 16);  slice_33509 = slice_33510 = None
        slice_scatter_6094: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6091, slice_scatter_6093, 1, 8112, 8128);  slice_scatter_6091 = slice_scatter_6093 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33529: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8128, 8144)
        slice_33530: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_33529, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1019: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_33530, memory_format = torch.contiguous_format);  slice_33530 = None
        view_2042: "f32[32, 16]" = torch.ops.aten.view.default(clone_1019, [32, 16]);  clone_1019 = None
        mm_1016: "f32[32, 8]" = torch.ops.aten.mm.default(view_2042, slice_7)
        view_2043: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1016, [2, 16, 8]);  mm_1016 = None
        slice_33537: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6094, 1, 8128, 8144)
        slice_33538: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33537, 2, 0, 16)
        add_1018: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33538, view_2043);  slice_33538 = view_2043 = None
        slice_scatter_6096: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33537, add_1018, 2, 0, 16);  slice_33537 = add_1018 = None
        slice_scatter_6097: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6094, slice_scatter_6096, 1, 8128, 8144);  slice_scatter_6094 = slice_scatter_6096 = None
        slice_33542: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6097, 1, 8128, 8144)
        slice_33543: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33542, 2, 0, 16)
        slice_scatter_6099: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33542, slice_33543, 2, 0, 16);  slice_33542 = slice_33543 = None
        slice_scatter_6100: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6097, slice_scatter_6099, 1, 8128, 8144);  slice_scatter_6097 = slice_scatter_6099 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33563: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_33529, 2, 16, 32);  slice_33529 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1020: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_33563, memory_format = torch.contiguous_format);  slice_33563 = None
        view_2044: "f32[32, 11]" = torch.ops.aten.view.default(clone_1020, [32, 11]);  clone_1020 = None
        mm_1017: "f32[32, 8]" = torch.ops.aten.mm.default(view_2044, slice_37)
        view_2045: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1017, [2, 16, 8]);  mm_1017 = None
        slice_33570: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6100, 1, 8128, 8144)
        slice_33571: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33570, 2, 0, 16)
        add_1019: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33571, view_2045);  slice_33571 = view_2045 = None
        slice_scatter_6102: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33570, add_1019, 2, 0, 16);  slice_33570 = add_1019 = None
        slice_scatter_6103: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6100, slice_scatter_6102, 1, 8128, 8144);  slice_scatter_6100 = slice_scatter_6102 = None
        slice_33575: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6103, 1, 8128, 8144)
        slice_33576: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33575, 2, 0, 16)
        slice_scatter_6105: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33575, slice_33576, 2, 0, 16);  slice_33575 = slice_33576 = None
        slice_scatter_6106: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6103, slice_scatter_6105, 1, 8128, 8144);  slice_scatter_6103 = slice_scatter_6105 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33595: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8144, 8160)
        slice_33596: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_33595, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1021: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_33596, memory_format = torch.contiguous_format);  slice_33596 = None
        view_2046: "f32[32, 16]" = torch.ops.aten.view.default(clone_1021, [32, 16]);  clone_1021 = None
        mm_1018: "f32[32, 8]" = torch.ops.aten.mm.default(view_2046, slice_7)
        view_2047: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1018, [2, 16, 8]);  mm_1018 = None
        slice_33603: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6106, 1, 8144, 8160)
        slice_33604: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33603, 2, 0, 16)
        add_1020: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33604, view_2047);  slice_33604 = view_2047 = None
        slice_scatter_6108: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33603, add_1020, 2, 0, 16);  slice_33603 = add_1020 = None
        slice_scatter_6109: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6106, slice_scatter_6108, 1, 8144, 8160);  slice_scatter_6106 = slice_scatter_6108 = None
        slice_33608: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6109, 1, 8144, 8160)
        slice_33609: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33608, 2, 0, 16)
        slice_scatter_6111: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33608, slice_33609, 2, 0, 16);  slice_33608 = slice_33609 = None
        slice_scatter_6112: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6109, slice_scatter_6111, 1, 8144, 8160);  slice_scatter_6109 = slice_scatter_6111 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33629: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_33595, 2, 16, 32);  slice_33595 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1022: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_33629, memory_format = torch.contiguous_format);  slice_33629 = None
        view_2048: "f32[32, 11]" = torch.ops.aten.view.default(clone_1022, [32, 11]);  clone_1022 = None
        mm_1019: "f32[32, 8]" = torch.ops.aten.mm.default(view_2048, slice_37)
        view_2049: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1019, [2, 16, 8]);  mm_1019 = None
        slice_33636: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6112, 1, 8144, 8160)
        slice_33637: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33636, 2, 0, 16)
        add_1021: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33637, view_2049);  slice_33637 = view_2049 = None
        slice_scatter_6114: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33636, add_1021, 2, 0, 16);  slice_33636 = add_1021 = None
        slice_scatter_6115: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6112, slice_scatter_6114, 1, 8144, 8160);  slice_scatter_6112 = slice_scatter_6114 = None
        slice_33641: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6115, 1, 8144, 8160)
        slice_33642: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33641, 2, 0, 16)
        slice_scatter_6117: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33641, slice_33642, 2, 0, 16);  slice_33641 = slice_33642 = None
        slice_scatter_6118: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6115, slice_scatter_6117, 1, 8144, 8160);  slice_scatter_6115 = slice_scatter_6117 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33661: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8160, 8176)
        slice_33662: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_33661, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1023: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_33662, memory_format = torch.contiguous_format);  slice_33662 = None
        view_2050: "f32[32, 16]" = torch.ops.aten.view.default(clone_1023, [32, 16]);  clone_1023 = None
        mm_1020: "f32[32, 8]" = torch.ops.aten.mm.default(view_2050, slice_7)
        view_2051: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1020, [2, 16, 8]);  mm_1020 = None
        slice_33669: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6118, 1, 8160, 8176)
        slice_33670: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33669, 2, 0, 16)
        add_1022: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33670, view_2051);  slice_33670 = view_2051 = None
        slice_scatter_6120: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33669, add_1022, 2, 0, 16);  slice_33669 = add_1022 = None
        slice_scatter_6121: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6118, slice_scatter_6120, 1, 8160, 8176);  slice_scatter_6118 = slice_scatter_6120 = None
        slice_33674: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6121, 1, 8160, 8176)
        slice_33675: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33674, 2, 0, 16)
        slice_scatter_6123: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33674, slice_33675, 2, 0, 16);  slice_33674 = slice_33675 = None
        slice_scatter_6124: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6121, slice_scatter_6123, 1, 8160, 8176);  slice_scatter_6121 = slice_scatter_6123 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33695: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_33661, 2, 16, 32);  slice_33661 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1024: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_33695, memory_format = torch.contiguous_format);  slice_33695 = None
        view_2052: "f32[32, 11]" = torch.ops.aten.view.default(clone_1024, [32, 11]);  clone_1024 = None
        mm_1021: "f32[32, 8]" = torch.ops.aten.mm.default(view_2052, slice_37)
        view_2053: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1021, [2, 16, 8]);  mm_1021 = None
        slice_33702: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6124, 1, 8160, 8176)
        slice_33703: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33702, 2, 0, 16)
        add_1023: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33703, view_2053);  slice_33703 = view_2053 = None
        slice_scatter_6126: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33702, add_1023, 2, 0, 16);  slice_33702 = add_1023 = None
        slice_scatter_6127: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6124, slice_scatter_6126, 1, 8160, 8176);  slice_scatter_6124 = slice_scatter_6126 = None
        slice_33707: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6127, 1, 8160, 8176)
        slice_33708: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33707, 2, 0, 16)
        slice_scatter_6129: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33707, slice_33708, 2, 0, 16);  slice_33707 = slice_33708 = None
        slice_scatter_6130: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6127, slice_scatter_6129, 1, 8160, 8176);  slice_scatter_6127 = slice_scatter_6129 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33727: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8176, 8192)
        slice_33728: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_33727, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1025: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_33728, memory_format = torch.contiguous_format);  slice_33728 = None
        view_2054: "f32[32, 16]" = torch.ops.aten.view.default(clone_1025, [32, 16]);  clone_1025 = None
        mm_1022: "f32[32, 8]" = torch.ops.aten.mm.default(view_2054, slice_7)
        view_2055: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1022, [2, 16, 8]);  mm_1022 = None
        slice_33735: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6130, 1, 8176, 8192)
        slice_33736: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33735, 2, 0, 16)
        add_1024: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33736, view_2055);  slice_33736 = view_2055 = None
        slice_scatter_6132: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33735, add_1024, 2, 0, 16);  slice_33735 = add_1024 = None
        slice_scatter_6133: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6130, slice_scatter_6132, 1, 8176, 8192);  slice_scatter_6130 = slice_scatter_6132 = None
        slice_33740: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6133, 1, 8176, 8192)
        slice_33741: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33740, 2, 0, 16)
        slice_scatter_6135: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33740, slice_33741, 2, 0, 16);  slice_33740 = slice_33741 = None
        slice_scatter_6136: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6133, slice_scatter_6135, 1, 8176, 8192);  slice_scatter_6133 = slice_scatter_6135 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33761: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_33727, 2, 16, 32);  slice_33727 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1026: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_33761, memory_format = torch.contiguous_format);  slice_33761 = None
        view_2056: "f32[32, 11]" = torch.ops.aten.view.default(clone_1026, [32, 11]);  clone_1026 = None
        mm_1023: "f32[32, 8]" = torch.ops.aten.mm.default(view_2056, slice_37)
        view_2057: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1023, [2, 16, 8]);  mm_1023 = None
        slice_33768: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6136, 1, 8176, 8192)
        slice_33769: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33768, 2, 0, 16)
        add_1025: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33769, view_2057);  slice_33769 = view_2057 = None
        slice_scatter_6138: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33768, add_1025, 2, 0, 16);  slice_33768 = add_1025 = None
        slice_scatter_6139: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6136, slice_scatter_6138, 1, 8176, 8192);  slice_scatter_6136 = slice_scatter_6138 = None
        slice_33773: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6139, 1, 8176, 8192)
        slice_33774: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33773, 2, 0, 16)
        slice_scatter_6141: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33773, slice_33774, 2, 0, 16);  slice_33773 = slice_33774 = None
        slice_scatter_6142: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6139, slice_scatter_6141, 1, 8176, 8192);  slice_scatter_6139 = slice_scatter_6141 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33793: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8192, 8208)
        slice_33794: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_33793, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1027: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_33794, memory_format = torch.contiguous_format);  slice_33794 = None
        view_2058: "f32[32, 16]" = torch.ops.aten.view.default(clone_1027, [32, 16]);  clone_1027 = None
        mm_1024: "f32[32, 8]" = torch.ops.aten.mm.default(view_2058, slice_7)
        view_2059: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1024, [2, 16, 8]);  mm_1024 = None
        slice_33801: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6142, 1, 8192, 8208)
        slice_33802: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33801, 2, 0, 16)
        add_1026: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33802, view_2059);  slice_33802 = view_2059 = None
        slice_scatter_6144: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33801, add_1026, 2, 0, 16);  slice_33801 = add_1026 = None
        slice_scatter_6145: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6142, slice_scatter_6144, 1, 8192, 8208);  slice_scatter_6142 = slice_scatter_6144 = None
        slice_33806: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6145, 1, 8192, 8208)
        slice_33807: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33806, 2, 0, 16)
        slice_scatter_6147: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33806, slice_33807, 2, 0, 16);  slice_33806 = slice_33807 = None
        slice_scatter_6148: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6145, slice_scatter_6147, 1, 8192, 8208);  slice_scatter_6145 = slice_scatter_6147 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33827: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_33793, 2, 16, 32);  slice_33793 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1028: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_33827, memory_format = torch.contiguous_format);  slice_33827 = None
        view_2060: "f32[32, 11]" = torch.ops.aten.view.default(clone_1028, [32, 11]);  clone_1028 = None
        mm_1025: "f32[32, 8]" = torch.ops.aten.mm.default(view_2060, slice_37)
        view_2061: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1025, [2, 16, 8]);  mm_1025 = None
        slice_33834: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6148, 1, 8192, 8208)
        slice_33835: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33834, 2, 0, 16)
        add_1027: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33835, view_2061);  slice_33835 = view_2061 = None
        slice_scatter_6150: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33834, add_1027, 2, 0, 16);  slice_33834 = add_1027 = None
        slice_scatter_6151: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6148, slice_scatter_6150, 1, 8192, 8208);  slice_scatter_6148 = slice_scatter_6150 = None
        slice_33839: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6151, 1, 8192, 8208)
        slice_33840: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33839, 2, 0, 16)
        slice_scatter_6153: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33839, slice_33840, 2, 0, 16);  slice_33839 = slice_33840 = None
        slice_scatter_6154: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6151, slice_scatter_6153, 1, 8192, 8208);  slice_scatter_6151 = slice_scatter_6153 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33859: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8208, 8224)
        slice_33860: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_33859, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1029: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_33860, memory_format = torch.contiguous_format);  slice_33860 = None
        view_2062: "f32[32, 16]" = torch.ops.aten.view.default(clone_1029, [32, 16]);  clone_1029 = None
        mm_1026: "f32[32, 8]" = torch.ops.aten.mm.default(view_2062, slice_7)
        view_2063: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1026, [2, 16, 8]);  mm_1026 = None
        slice_33867: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6154, 1, 8208, 8224)
        slice_33868: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33867, 2, 0, 16)
        add_1028: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33868, view_2063);  slice_33868 = view_2063 = None
        slice_scatter_6156: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33867, add_1028, 2, 0, 16);  slice_33867 = add_1028 = None
        slice_scatter_6157: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6154, slice_scatter_6156, 1, 8208, 8224);  slice_scatter_6154 = slice_scatter_6156 = None
        slice_33872: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6157, 1, 8208, 8224)
        slice_33873: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33872, 2, 0, 16)
        slice_scatter_6159: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33872, slice_33873, 2, 0, 16);  slice_33872 = slice_33873 = None
        slice_scatter_6160: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6157, slice_scatter_6159, 1, 8208, 8224);  slice_scatter_6157 = slice_scatter_6159 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33893: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_33859, 2, 16, 32);  slice_33859 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1030: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_33893, memory_format = torch.contiguous_format);  slice_33893 = None
        view_2064: "f32[32, 11]" = torch.ops.aten.view.default(clone_1030, [32, 11]);  clone_1030 = None
        mm_1027: "f32[32, 8]" = torch.ops.aten.mm.default(view_2064, slice_37)
        view_2065: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1027, [2, 16, 8]);  mm_1027 = None
        slice_33900: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6160, 1, 8208, 8224)
        slice_33901: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33900, 2, 0, 16)
        add_1029: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33901, view_2065);  slice_33901 = view_2065 = None
        slice_scatter_6162: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33900, add_1029, 2, 0, 16);  slice_33900 = add_1029 = None
        slice_scatter_6163: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6160, slice_scatter_6162, 1, 8208, 8224);  slice_scatter_6160 = slice_scatter_6162 = None
        slice_33905: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6163, 1, 8208, 8224)
        slice_33906: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33905, 2, 0, 16)
        slice_scatter_6165: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33905, slice_33906, 2, 0, 16);  slice_33905 = slice_33906 = None
        slice_scatter_6166: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6163, slice_scatter_6165, 1, 8208, 8224);  slice_scatter_6163 = slice_scatter_6165 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33925: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8224, 8240)
        slice_33926: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_33925, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1031: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_33926, memory_format = torch.contiguous_format);  slice_33926 = None
        view_2066: "f32[32, 16]" = torch.ops.aten.view.default(clone_1031, [32, 16]);  clone_1031 = None
        mm_1028: "f32[32, 8]" = torch.ops.aten.mm.default(view_2066, slice_7)
        view_2067: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1028, [2, 16, 8]);  mm_1028 = None
        slice_33933: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6166, 1, 8224, 8240)
        slice_33934: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33933, 2, 0, 16)
        add_1030: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33934, view_2067);  slice_33934 = view_2067 = None
        slice_scatter_6168: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33933, add_1030, 2, 0, 16);  slice_33933 = add_1030 = None
        slice_scatter_6169: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6166, slice_scatter_6168, 1, 8224, 8240);  slice_scatter_6166 = slice_scatter_6168 = None
        slice_33938: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6169, 1, 8224, 8240)
        slice_33939: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33938, 2, 0, 16)
        slice_scatter_6171: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33938, slice_33939, 2, 0, 16);  slice_33938 = slice_33939 = None
        slice_scatter_6172: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6169, slice_scatter_6171, 1, 8224, 8240);  slice_scatter_6169 = slice_scatter_6171 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33959: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_33925, 2, 16, 32);  slice_33925 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1032: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_33959, memory_format = torch.contiguous_format);  slice_33959 = None
        view_2068: "f32[32, 11]" = torch.ops.aten.view.default(clone_1032, [32, 11]);  clone_1032 = None
        mm_1029: "f32[32, 8]" = torch.ops.aten.mm.default(view_2068, slice_37)
        view_2069: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1029, [2, 16, 8]);  mm_1029 = None
        slice_33966: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6172, 1, 8224, 8240)
        slice_33967: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33966, 2, 0, 16)
        add_1031: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_33967, view_2069);  slice_33967 = view_2069 = None
        slice_scatter_6174: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33966, add_1031, 2, 0, 16);  slice_33966 = add_1031 = None
        slice_scatter_6175: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6172, slice_scatter_6174, 1, 8224, 8240);  slice_scatter_6172 = slice_scatter_6174 = None
        slice_33971: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6175, 1, 8224, 8240)
        slice_33972: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33971, 2, 0, 16)
        slice_scatter_6177: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33971, slice_33972, 2, 0, 16);  slice_33971 = slice_33972 = None
        slice_scatter_6178: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6175, slice_scatter_6177, 1, 8224, 8240);  slice_scatter_6175 = slice_scatter_6177 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_33991: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8240, 8256)
        slice_33992: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_33991, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1033: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_33992, memory_format = torch.contiguous_format);  slice_33992 = None
        view_2070: "f32[32, 16]" = torch.ops.aten.view.default(clone_1033, [32, 16]);  clone_1033 = None
        mm_1030: "f32[32, 8]" = torch.ops.aten.mm.default(view_2070, slice_7)
        view_2071: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1030, [2, 16, 8]);  mm_1030 = None
        slice_33999: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6178, 1, 8240, 8256)
        slice_34000: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_33999, 2, 0, 16)
        add_1032: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34000, view_2071);  slice_34000 = view_2071 = None
        slice_scatter_6180: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_33999, add_1032, 2, 0, 16);  slice_33999 = add_1032 = None
        slice_scatter_6181: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6178, slice_scatter_6180, 1, 8240, 8256);  slice_scatter_6178 = slice_scatter_6180 = None
        slice_34004: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6181, 1, 8240, 8256)
        slice_34005: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34004, 2, 0, 16)
        slice_scatter_6183: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34004, slice_34005, 2, 0, 16);  slice_34004 = slice_34005 = None
        slice_scatter_6184: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6181, slice_scatter_6183, 1, 8240, 8256);  slice_scatter_6181 = slice_scatter_6183 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34025: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_33991, 2, 16, 32);  slice_33991 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1034: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_34025, memory_format = torch.contiguous_format);  slice_34025 = None
        view_2072: "f32[32, 11]" = torch.ops.aten.view.default(clone_1034, [32, 11]);  clone_1034 = None
        mm_1031: "f32[32, 8]" = torch.ops.aten.mm.default(view_2072, slice_37)
        view_2073: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1031, [2, 16, 8]);  mm_1031 = None
        slice_34032: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6184, 1, 8240, 8256)
        slice_34033: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34032, 2, 0, 16)
        add_1033: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34033, view_2073);  slice_34033 = view_2073 = None
        slice_scatter_6186: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34032, add_1033, 2, 0, 16);  slice_34032 = add_1033 = None
        slice_scatter_6187: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6184, slice_scatter_6186, 1, 8240, 8256);  slice_scatter_6184 = slice_scatter_6186 = None
        slice_34037: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6187, 1, 8240, 8256)
        slice_34038: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34037, 2, 0, 16)
        slice_scatter_6189: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34037, slice_34038, 2, 0, 16);  slice_34037 = slice_34038 = None
        slice_scatter_6190: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6187, slice_scatter_6189, 1, 8240, 8256);  slice_scatter_6187 = slice_scatter_6189 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34057: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8256, 8272)
        slice_34058: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_34057, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1035: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_34058, memory_format = torch.contiguous_format);  slice_34058 = None
        view_2074: "f32[32, 16]" = torch.ops.aten.view.default(clone_1035, [32, 16]);  clone_1035 = None
        mm_1032: "f32[32, 8]" = torch.ops.aten.mm.default(view_2074, slice_7)
        view_2075: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1032, [2, 16, 8]);  mm_1032 = None
        slice_34065: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6190, 1, 8256, 8272)
        slice_34066: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34065, 2, 0, 16)
        add_1034: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34066, view_2075);  slice_34066 = view_2075 = None
        slice_scatter_6192: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34065, add_1034, 2, 0, 16);  slice_34065 = add_1034 = None
        slice_scatter_6193: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6190, slice_scatter_6192, 1, 8256, 8272);  slice_scatter_6190 = slice_scatter_6192 = None
        slice_34070: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6193, 1, 8256, 8272)
        slice_34071: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34070, 2, 0, 16)
        slice_scatter_6195: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34070, slice_34071, 2, 0, 16);  slice_34070 = slice_34071 = None
        slice_scatter_6196: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6193, slice_scatter_6195, 1, 8256, 8272);  slice_scatter_6193 = slice_scatter_6195 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34091: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_34057, 2, 16, 32);  slice_34057 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1036: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_34091, memory_format = torch.contiguous_format);  slice_34091 = None
        view_2076: "f32[32, 11]" = torch.ops.aten.view.default(clone_1036, [32, 11]);  clone_1036 = None
        mm_1033: "f32[32, 8]" = torch.ops.aten.mm.default(view_2076, slice_37)
        view_2077: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1033, [2, 16, 8]);  mm_1033 = None
        slice_34098: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6196, 1, 8256, 8272)
        slice_34099: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34098, 2, 0, 16)
        add_1035: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34099, view_2077);  slice_34099 = view_2077 = None
        slice_scatter_6198: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34098, add_1035, 2, 0, 16);  slice_34098 = add_1035 = None
        slice_scatter_6199: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6196, slice_scatter_6198, 1, 8256, 8272);  slice_scatter_6196 = slice_scatter_6198 = None
        slice_34103: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6199, 1, 8256, 8272)
        slice_34104: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34103, 2, 0, 16)
        slice_scatter_6201: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34103, slice_34104, 2, 0, 16);  slice_34103 = slice_34104 = None
        slice_scatter_6202: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6199, slice_scatter_6201, 1, 8256, 8272);  slice_scatter_6199 = slice_scatter_6201 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34123: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8272, 8288)
        slice_34124: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_34123, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1037: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_34124, memory_format = torch.contiguous_format);  slice_34124 = None
        view_2078: "f32[32, 16]" = torch.ops.aten.view.default(clone_1037, [32, 16]);  clone_1037 = None
        mm_1034: "f32[32, 8]" = torch.ops.aten.mm.default(view_2078, slice_7)
        view_2079: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1034, [2, 16, 8]);  mm_1034 = None
        slice_34131: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6202, 1, 8272, 8288)
        slice_34132: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34131, 2, 0, 16)
        add_1036: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34132, view_2079);  slice_34132 = view_2079 = None
        slice_scatter_6204: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34131, add_1036, 2, 0, 16);  slice_34131 = add_1036 = None
        slice_scatter_6205: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6202, slice_scatter_6204, 1, 8272, 8288);  slice_scatter_6202 = slice_scatter_6204 = None
        slice_34136: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6205, 1, 8272, 8288)
        slice_34137: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34136, 2, 0, 16)
        slice_scatter_6207: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34136, slice_34137, 2, 0, 16);  slice_34136 = slice_34137 = None
        slice_scatter_6208: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6205, slice_scatter_6207, 1, 8272, 8288);  slice_scatter_6205 = slice_scatter_6207 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34157: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_34123, 2, 16, 32);  slice_34123 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1038: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_34157, memory_format = torch.contiguous_format);  slice_34157 = None
        view_2080: "f32[32, 11]" = torch.ops.aten.view.default(clone_1038, [32, 11]);  clone_1038 = None
        mm_1035: "f32[32, 8]" = torch.ops.aten.mm.default(view_2080, slice_37)
        view_2081: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1035, [2, 16, 8]);  mm_1035 = None
        slice_34164: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6208, 1, 8272, 8288)
        slice_34165: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34164, 2, 0, 16)
        add_1037: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34165, view_2081);  slice_34165 = view_2081 = None
        slice_scatter_6210: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34164, add_1037, 2, 0, 16);  slice_34164 = add_1037 = None
        slice_scatter_6211: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6208, slice_scatter_6210, 1, 8272, 8288);  slice_scatter_6208 = slice_scatter_6210 = None
        slice_34169: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6211, 1, 8272, 8288)
        slice_34170: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34169, 2, 0, 16)
        slice_scatter_6213: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34169, slice_34170, 2, 0, 16);  slice_34169 = slice_34170 = None
        slice_scatter_6214: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6211, slice_scatter_6213, 1, 8272, 8288);  slice_scatter_6211 = slice_scatter_6213 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34189: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8288, 8304)
        slice_34190: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_34189, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1039: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_34190, memory_format = torch.contiguous_format);  slice_34190 = None
        view_2082: "f32[32, 16]" = torch.ops.aten.view.default(clone_1039, [32, 16]);  clone_1039 = None
        mm_1036: "f32[32, 8]" = torch.ops.aten.mm.default(view_2082, slice_7)
        view_2083: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1036, [2, 16, 8]);  mm_1036 = None
        slice_34197: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6214, 1, 8288, 8304)
        slice_34198: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34197, 2, 0, 16)
        add_1038: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34198, view_2083);  slice_34198 = view_2083 = None
        slice_scatter_6216: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34197, add_1038, 2, 0, 16);  slice_34197 = add_1038 = None
        slice_scatter_6217: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6214, slice_scatter_6216, 1, 8288, 8304);  slice_scatter_6214 = slice_scatter_6216 = None
        slice_34202: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6217, 1, 8288, 8304)
        slice_34203: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34202, 2, 0, 16)
        slice_scatter_6219: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34202, slice_34203, 2, 0, 16);  slice_34202 = slice_34203 = None
        slice_scatter_6220: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6217, slice_scatter_6219, 1, 8288, 8304);  slice_scatter_6217 = slice_scatter_6219 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34223: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_34189, 2, 16, 32);  slice_34189 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1040: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_34223, memory_format = torch.contiguous_format);  slice_34223 = None
        view_2084: "f32[32, 11]" = torch.ops.aten.view.default(clone_1040, [32, 11]);  clone_1040 = None
        mm_1037: "f32[32, 8]" = torch.ops.aten.mm.default(view_2084, slice_37)
        view_2085: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1037, [2, 16, 8]);  mm_1037 = None
        slice_34230: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6220, 1, 8288, 8304)
        slice_34231: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34230, 2, 0, 16)
        add_1039: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34231, view_2085);  slice_34231 = view_2085 = None
        slice_scatter_6222: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34230, add_1039, 2, 0, 16);  slice_34230 = add_1039 = None
        slice_scatter_6223: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6220, slice_scatter_6222, 1, 8288, 8304);  slice_scatter_6220 = slice_scatter_6222 = None
        slice_34235: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6223, 1, 8288, 8304)
        slice_34236: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34235, 2, 0, 16)
        slice_scatter_6225: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34235, slice_34236, 2, 0, 16);  slice_34235 = slice_34236 = None
        slice_scatter_6226: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6223, slice_scatter_6225, 1, 8288, 8304);  slice_scatter_6223 = slice_scatter_6225 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34255: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8304, 8320)
        slice_34256: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_34255, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1041: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_34256, memory_format = torch.contiguous_format);  slice_34256 = None
        view_2086: "f32[32, 16]" = torch.ops.aten.view.default(clone_1041, [32, 16]);  clone_1041 = None
        mm_1038: "f32[32, 8]" = torch.ops.aten.mm.default(view_2086, slice_7)
        view_2087: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1038, [2, 16, 8]);  mm_1038 = None
        slice_34263: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6226, 1, 8304, 8320)
        slice_34264: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34263, 2, 0, 16)
        add_1040: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34264, view_2087);  slice_34264 = view_2087 = None
        slice_scatter_6228: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34263, add_1040, 2, 0, 16);  slice_34263 = add_1040 = None
        slice_scatter_6229: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6226, slice_scatter_6228, 1, 8304, 8320);  slice_scatter_6226 = slice_scatter_6228 = None
        slice_34268: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6229, 1, 8304, 8320)
        slice_34269: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34268, 2, 0, 16)
        slice_scatter_6231: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34268, slice_34269, 2, 0, 16);  slice_34268 = slice_34269 = None
        slice_scatter_6232: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6229, slice_scatter_6231, 1, 8304, 8320);  slice_scatter_6229 = slice_scatter_6231 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34289: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_34255, 2, 16, 32);  slice_34255 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1042: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_34289, memory_format = torch.contiguous_format);  slice_34289 = None
        view_2088: "f32[32, 11]" = torch.ops.aten.view.default(clone_1042, [32, 11]);  clone_1042 = None
        mm_1039: "f32[32, 8]" = torch.ops.aten.mm.default(view_2088, slice_37)
        view_2089: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1039, [2, 16, 8]);  mm_1039 = None
        slice_34296: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6232, 1, 8304, 8320)
        slice_34297: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34296, 2, 0, 16)
        add_1041: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34297, view_2089);  slice_34297 = view_2089 = None
        slice_scatter_6234: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34296, add_1041, 2, 0, 16);  slice_34296 = add_1041 = None
        slice_scatter_6235: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6232, slice_scatter_6234, 1, 8304, 8320);  slice_scatter_6232 = slice_scatter_6234 = None
        slice_34301: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6235, 1, 8304, 8320)
        slice_34302: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34301, 2, 0, 16)
        slice_scatter_6237: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34301, slice_34302, 2, 0, 16);  slice_34301 = slice_34302 = None
        slice_scatter_6238: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6235, slice_scatter_6237, 1, 8304, 8320);  slice_scatter_6235 = slice_scatter_6237 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34321: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8320, 8336)
        slice_34322: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_34321, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1043: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_34322, memory_format = torch.contiguous_format);  slice_34322 = None
        view_2090: "f32[32, 16]" = torch.ops.aten.view.default(clone_1043, [32, 16]);  clone_1043 = None
        mm_1040: "f32[32, 8]" = torch.ops.aten.mm.default(view_2090, slice_7)
        view_2091: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1040, [2, 16, 8]);  mm_1040 = None
        slice_34329: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6238, 1, 8320, 8336)
        slice_34330: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34329, 2, 0, 16)
        add_1042: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34330, view_2091);  slice_34330 = view_2091 = None
        slice_scatter_6240: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34329, add_1042, 2, 0, 16);  slice_34329 = add_1042 = None
        slice_scatter_6241: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6238, slice_scatter_6240, 1, 8320, 8336);  slice_scatter_6238 = slice_scatter_6240 = None
        slice_34334: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6241, 1, 8320, 8336)
        slice_34335: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34334, 2, 0, 16)
        slice_scatter_6243: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34334, slice_34335, 2, 0, 16);  slice_34334 = slice_34335 = None
        slice_scatter_6244: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6241, slice_scatter_6243, 1, 8320, 8336);  slice_scatter_6241 = slice_scatter_6243 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34355: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_34321, 2, 16, 32);  slice_34321 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1044: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_34355, memory_format = torch.contiguous_format);  slice_34355 = None
        view_2092: "f32[32, 11]" = torch.ops.aten.view.default(clone_1044, [32, 11]);  clone_1044 = None
        mm_1041: "f32[32, 8]" = torch.ops.aten.mm.default(view_2092, slice_37)
        view_2093: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1041, [2, 16, 8]);  mm_1041 = None
        slice_34362: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6244, 1, 8320, 8336)
        slice_34363: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34362, 2, 0, 16)
        add_1043: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34363, view_2093);  slice_34363 = view_2093 = None
        slice_scatter_6246: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34362, add_1043, 2, 0, 16);  slice_34362 = add_1043 = None
        slice_scatter_6247: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6244, slice_scatter_6246, 1, 8320, 8336);  slice_scatter_6244 = slice_scatter_6246 = None
        slice_34367: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6247, 1, 8320, 8336)
        slice_34368: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34367, 2, 0, 16)
        slice_scatter_6249: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34367, slice_34368, 2, 0, 16);  slice_34367 = slice_34368 = None
        slice_scatter_6250: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6247, slice_scatter_6249, 1, 8320, 8336);  slice_scatter_6247 = slice_scatter_6249 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34387: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8336, 8352)
        slice_34388: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_34387, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1045: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_34388, memory_format = torch.contiguous_format);  slice_34388 = None
        view_2094: "f32[32, 16]" = torch.ops.aten.view.default(clone_1045, [32, 16]);  clone_1045 = None
        mm_1042: "f32[32, 8]" = torch.ops.aten.mm.default(view_2094, slice_7)
        view_2095: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1042, [2, 16, 8]);  mm_1042 = None
        slice_34395: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6250, 1, 8336, 8352)
        slice_34396: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34395, 2, 0, 16)
        add_1044: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34396, view_2095);  slice_34396 = view_2095 = None
        slice_scatter_6252: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34395, add_1044, 2, 0, 16);  slice_34395 = add_1044 = None
        slice_scatter_6253: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6250, slice_scatter_6252, 1, 8336, 8352);  slice_scatter_6250 = slice_scatter_6252 = None
        slice_34400: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6253, 1, 8336, 8352)
        slice_34401: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34400, 2, 0, 16)
        slice_scatter_6255: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34400, slice_34401, 2, 0, 16);  slice_34400 = slice_34401 = None
        slice_scatter_6256: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6253, slice_scatter_6255, 1, 8336, 8352);  slice_scatter_6253 = slice_scatter_6255 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34421: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_34387, 2, 16, 32);  slice_34387 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1046: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_34421, memory_format = torch.contiguous_format);  slice_34421 = None
        view_2096: "f32[32, 11]" = torch.ops.aten.view.default(clone_1046, [32, 11]);  clone_1046 = None
        mm_1043: "f32[32, 8]" = torch.ops.aten.mm.default(view_2096, slice_37)
        view_2097: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1043, [2, 16, 8]);  mm_1043 = None
        slice_34428: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6256, 1, 8336, 8352)
        slice_34429: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34428, 2, 0, 16)
        add_1045: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34429, view_2097);  slice_34429 = view_2097 = None
        slice_scatter_6258: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34428, add_1045, 2, 0, 16);  slice_34428 = add_1045 = None
        slice_scatter_6259: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6256, slice_scatter_6258, 1, 8336, 8352);  slice_scatter_6256 = slice_scatter_6258 = None
        slice_34433: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6259, 1, 8336, 8352)
        slice_34434: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34433, 2, 0, 16)
        slice_scatter_6261: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34433, slice_34434, 2, 0, 16);  slice_34433 = slice_34434 = None
        slice_scatter_6262: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6259, slice_scatter_6261, 1, 8336, 8352);  slice_scatter_6259 = slice_scatter_6261 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34453: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8352, 8368)
        slice_34454: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_34453, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1047: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_34454, memory_format = torch.contiguous_format);  slice_34454 = None
        view_2098: "f32[32, 16]" = torch.ops.aten.view.default(clone_1047, [32, 16]);  clone_1047 = None
        mm_1044: "f32[32, 8]" = torch.ops.aten.mm.default(view_2098, slice_7)
        view_2099: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1044, [2, 16, 8]);  mm_1044 = None
        slice_34461: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6262, 1, 8352, 8368)
        slice_34462: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34461, 2, 0, 16)
        add_1046: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34462, view_2099);  slice_34462 = view_2099 = None
        slice_scatter_6264: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34461, add_1046, 2, 0, 16);  slice_34461 = add_1046 = None
        slice_scatter_6265: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6262, slice_scatter_6264, 1, 8352, 8368);  slice_scatter_6262 = slice_scatter_6264 = None
        slice_34466: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6265, 1, 8352, 8368)
        slice_34467: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34466, 2, 0, 16)
        slice_scatter_6267: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34466, slice_34467, 2, 0, 16);  slice_34466 = slice_34467 = None
        slice_scatter_6268: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6265, slice_scatter_6267, 1, 8352, 8368);  slice_scatter_6265 = slice_scatter_6267 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34487: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_34453, 2, 16, 32);  slice_34453 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1048: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_34487, memory_format = torch.contiguous_format);  slice_34487 = None
        view_2100: "f32[32, 11]" = torch.ops.aten.view.default(clone_1048, [32, 11]);  clone_1048 = None
        mm_1045: "f32[32, 8]" = torch.ops.aten.mm.default(view_2100, slice_37)
        view_2101: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1045, [2, 16, 8]);  mm_1045 = None
        slice_34494: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6268, 1, 8352, 8368)
        slice_34495: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34494, 2, 0, 16)
        add_1047: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34495, view_2101);  slice_34495 = view_2101 = None
        slice_scatter_6270: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34494, add_1047, 2, 0, 16);  slice_34494 = add_1047 = None
        slice_scatter_6271: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6268, slice_scatter_6270, 1, 8352, 8368);  slice_scatter_6268 = slice_scatter_6270 = None
        slice_34499: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6271, 1, 8352, 8368)
        slice_34500: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34499, 2, 0, 16)
        slice_scatter_6273: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34499, slice_34500, 2, 0, 16);  slice_34499 = slice_34500 = None
        slice_scatter_6274: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6271, slice_scatter_6273, 1, 8352, 8368);  slice_scatter_6271 = slice_scatter_6273 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34519: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8368, 8384)
        slice_34520: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_34519, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1049: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_34520, memory_format = torch.contiguous_format);  slice_34520 = None
        view_2102: "f32[32, 16]" = torch.ops.aten.view.default(clone_1049, [32, 16]);  clone_1049 = None
        mm_1046: "f32[32, 8]" = torch.ops.aten.mm.default(view_2102, slice_7)
        view_2103: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1046, [2, 16, 8]);  mm_1046 = None
        slice_34527: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6274, 1, 8368, 8384)
        slice_34528: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34527, 2, 0, 16)
        add_1048: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34528, view_2103);  slice_34528 = view_2103 = None
        slice_scatter_6276: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34527, add_1048, 2, 0, 16);  slice_34527 = add_1048 = None
        slice_scatter_6277: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6274, slice_scatter_6276, 1, 8368, 8384);  slice_scatter_6274 = slice_scatter_6276 = None
        slice_34532: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6277, 1, 8368, 8384)
        slice_34533: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34532, 2, 0, 16)
        slice_scatter_6279: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34532, slice_34533, 2, 0, 16);  slice_34532 = slice_34533 = None
        slice_scatter_6280: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6277, slice_scatter_6279, 1, 8368, 8384);  slice_scatter_6277 = slice_scatter_6279 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34553: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_34519, 2, 16, 32);  slice_34519 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1050: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_34553, memory_format = torch.contiguous_format);  slice_34553 = None
        view_2104: "f32[32, 11]" = torch.ops.aten.view.default(clone_1050, [32, 11]);  clone_1050 = None
        mm_1047: "f32[32, 8]" = torch.ops.aten.mm.default(view_2104, slice_37)
        view_2105: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1047, [2, 16, 8]);  mm_1047 = None
        slice_34560: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6280, 1, 8368, 8384)
        slice_34561: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34560, 2, 0, 16)
        add_1049: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34561, view_2105);  slice_34561 = view_2105 = None
        slice_scatter_6282: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34560, add_1049, 2, 0, 16);  slice_34560 = add_1049 = None
        slice_scatter_6283: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6280, slice_scatter_6282, 1, 8368, 8384);  slice_scatter_6280 = slice_scatter_6282 = None
        slice_34565: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6283, 1, 8368, 8384)
        slice_34566: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34565, 2, 0, 16)
        slice_scatter_6285: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34565, slice_34566, 2, 0, 16);  slice_34565 = slice_34566 = None
        slice_scatter_6286: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6283, slice_scatter_6285, 1, 8368, 8384);  slice_scatter_6283 = slice_scatter_6285 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34585: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8384, 8400)
        slice_34586: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_34585, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1051: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_34586, memory_format = torch.contiguous_format);  slice_34586 = None
        view_2106: "f32[32, 16]" = torch.ops.aten.view.default(clone_1051, [32, 16]);  clone_1051 = None
        mm_1048: "f32[32, 8]" = torch.ops.aten.mm.default(view_2106, slice_7)
        view_2107: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1048, [2, 16, 8]);  mm_1048 = None
        slice_34593: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6286, 1, 8384, 8400)
        slice_34594: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34593, 2, 0, 16)
        add_1050: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34594, view_2107);  slice_34594 = view_2107 = None
        slice_scatter_6288: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34593, add_1050, 2, 0, 16);  slice_34593 = add_1050 = None
        slice_scatter_6289: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6286, slice_scatter_6288, 1, 8384, 8400);  slice_scatter_6286 = slice_scatter_6288 = None
        slice_34598: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6289, 1, 8384, 8400)
        slice_34599: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34598, 2, 0, 16)
        slice_scatter_6291: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34598, slice_34599, 2, 0, 16);  slice_34598 = slice_34599 = None
        slice_scatter_6292: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6289, slice_scatter_6291, 1, 8384, 8400);  slice_scatter_6289 = slice_scatter_6291 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34619: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_34585, 2, 16, 32);  slice_34585 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1052: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_34619, memory_format = torch.contiguous_format);  slice_34619 = None
        view_2108: "f32[32, 11]" = torch.ops.aten.view.default(clone_1052, [32, 11]);  clone_1052 = None
        mm_1049: "f32[32, 8]" = torch.ops.aten.mm.default(view_2108, slice_37)
        view_2109: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1049, [2, 16, 8]);  mm_1049 = None
        slice_34626: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6292, 1, 8384, 8400)
        slice_34627: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34626, 2, 0, 16)
        add_1051: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34627, view_2109);  slice_34627 = view_2109 = None
        slice_scatter_6294: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34626, add_1051, 2, 0, 16);  slice_34626 = add_1051 = None
        slice_scatter_6295: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6292, slice_scatter_6294, 1, 8384, 8400);  slice_scatter_6292 = slice_scatter_6294 = None
        slice_34631: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6295, 1, 8384, 8400)
        slice_34632: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34631, 2, 0, 16)
        slice_scatter_6297: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34631, slice_34632, 2, 0, 16);  slice_34631 = slice_34632 = None
        slice_scatter_6298: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6295, slice_scatter_6297, 1, 8384, 8400);  slice_scatter_6295 = slice_scatter_6297 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34651: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8400, 8416)
        slice_34652: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_34651, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1053: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_34652, memory_format = torch.contiguous_format);  slice_34652 = None
        view_2110: "f32[32, 16]" = torch.ops.aten.view.default(clone_1053, [32, 16]);  clone_1053 = None
        mm_1050: "f32[32, 8]" = torch.ops.aten.mm.default(view_2110, slice_7)
        view_2111: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1050, [2, 16, 8]);  mm_1050 = None
        slice_34659: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6298, 1, 8400, 8416)
        slice_34660: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34659, 2, 0, 16)
        add_1052: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34660, view_2111);  slice_34660 = view_2111 = None
        slice_scatter_6300: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34659, add_1052, 2, 0, 16);  slice_34659 = add_1052 = None
        slice_scatter_6301: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6298, slice_scatter_6300, 1, 8400, 8416);  slice_scatter_6298 = slice_scatter_6300 = None
        slice_34664: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6301, 1, 8400, 8416)
        slice_34665: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34664, 2, 0, 16)
        slice_scatter_6303: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34664, slice_34665, 2, 0, 16);  slice_34664 = slice_34665 = None
        slice_scatter_6304: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6301, slice_scatter_6303, 1, 8400, 8416);  slice_scatter_6301 = slice_scatter_6303 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34685: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_34651, 2, 16, 32);  slice_34651 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1054: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_34685, memory_format = torch.contiguous_format);  slice_34685 = None
        view_2112: "f32[32, 11]" = torch.ops.aten.view.default(clone_1054, [32, 11]);  clone_1054 = None
        mm_1051: "f32[32, 8]" = torch.ops.aten.mm.default(view_2112, slice_37)
        view_2113: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1051, [2, 16, 8]);  mm_1051 = None
        slice_34692: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6304, 1, 8400, 8416)
        slice_34693: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34692, 2, 0, 16)
        add_1053: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34693, view_2113);  slice_34693 = view_2113 = None
        slice_scatter_6306: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34692, add_1053, 2, 0, 16);  slice_34692 = add_1053 = None
        slice_scatter_6307: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6304, slice_scatter_6306, 1, 8400, 8416);  slice_scatter_6304 = slice_scatter_6306 = None
        slice_34697: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6307, 1, 8400, 8416)
        slice_34698: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34697, 2, 0, 16)
        slice_scatter_6309: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34697, slice_34698, 2, 0, 16);  slice_34697 = slice_34698 = None
        slice_scatter_6310: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6307, slice_scatter_6309, 1, 8400, 8416);  slice_scatter_6307 = slice_scatter_6309 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34717: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8416, 8432)
        slice_34718: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_34717, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1055: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_34718, memory_format = torch.contiguous_format);  slice_34718 = None
        view_2114: "f32[32, 16]" = torch.ops.aten.view.default(clone_1055, [32, 16]);  clone_1055 = None
        mm_1052: "f32[32, 8]" = torch.ops.aten.mm.default(view_2114, slice_7)
        view_2115: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1052, [2, 16, 8]);  mm_1052 = None
        slice_34725: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6310, 1, 8416, 8432)
        slice_34726: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34725, 2, 0, 16)
        add_1054: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34726, view_2115);  slice_34726 = view_2115 = None
        slice_scatter_6312: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34725, add_1054, 2, 0, 16);  slice_34725 = add_1054 = None
        slice_scatter_6313: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6310, slice_scatter_6312, 1, 8416, 8432);  slice_scatter_6310 = slice_scatter_6312 = None
        slice_34730: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6313, 1, 8416, 8432)
        slice_34731: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34730, 2, 0, 16)
        slice_scatter_6315: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34730, slice_34731, 2, 0, 16);  slice_34730 = slice_34731 = None
        slice_scatter_6316: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6313, slice_scatter_6315, 1, 8416, 8432);  slice_scatter_6313 = slice_scatter_6315 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34751: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_34717, 2, 16, 32);  slice_34717 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1056: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_34751, memory_format = torch.contiguous_format);  slice_34751 = None
        view_2116: "f32[32, 11]" = torch.ops.aten.view.default(clone_1056, [32, 11]);  clone_1056 = None
        mm_1053: "f32[32, 8]" = torch.ops.aten.mm.default(view_2116, slice_37)
        view_2117: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1053, [2, 16, 8]);  mm_1053 = None
        slice_34758: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6316, 1, 8416, 8432)
        slice_34759: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34758, 2, 0, 16)
        add_1055: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34759, view_2117);  slice_34759 = view_2117 = None
        slice_scatter_6318: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34758, add_1055, 2, 0, 16);  slice_34758 = add_1055 = None
        slice_scatter_6319: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6316, slice_scatter_6318, 1, 8416, 8432);  slice_scatter_6316 = slice_scatter_6318 = None
        slice_34763: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6319, 1, 8416, 8432)
        slice_34764: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34763, 2, 0, 16)
        slice_scatter_6321: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34763, slice_34764, 2, 0, 16);  slice_34763 = slice_34764 = None
        slice_scatter_6322: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6319, slice_scatter_6321, 1, 8416, 8432);  slice_scatter_6319 = slice_scatter_6321 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34783: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8432, 8448)
        slice_34784: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_34783, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1057: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_34784, memory_format = torch.contiguous_format);  slice_34784 = None
        view_2118: "f32[32, 16]" = torch.ops.aten.view.default(clone_1057, [32, 16]);  clone_1057 = None
        mm_1054: "f32[32, 8]" = torch.ops.aten.mm.default(view_2118, slice_7)
        view_2119: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1054, [2, 16, 8]);  mm_1054 = None
        slice_34791: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6322, 1, 8432, 8448)
        slice_34792: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34791, 2, 0, 16)
        add_1056: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34792, view_2119);  slice_34792 = view_2119 = None
        slice_scatter_6324: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34791, add_1056, 2, 0, 16);  slice_34791 = add_1056 = None
        slice_scatter_6325: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6322, slice_scatter_6324, 1, 8432, 8448);  slice_scatter_6322 = slice_scatter_6324 = None
        slice_34796: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6325, 1, 8432, 8448)
        slice_34797: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34796, 2, 0, 16)
        slice_scatter_6327: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34796, slice_34797, 2, 0, 16);  slice_34796 = slice_34797 = None
        slice_scatter_6328: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6325, slice_scatter_6327, 1, 8432, 8448);  slice_scatter_6325 = slice_scatter_6327 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34817: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_34783, 2, 16, 32);  slice_34783 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1058: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_34817, memory_format = torch.contiguous_format);  slice_34817 = None
        view_2120: "f32[32, 11]" = torch.ops.aten.view.default(clone_1058, [32, 11]);  clone_1058 = None
        mm_1055: "f32[32, 8]" = torch.ops.aten.mm.default(view_2120, slice_37)
        view_2121: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1055, [2, 16, 8]);  mm_1055 = None
        slice_34824: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6328, 1, 8432, 8448)
        slice_34825: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34824, 2, 0, 16)
        add_1057: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34825, view_2121);  slice_34825 = view_2121 = None
        slice_scatter_6330: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34824, add_1057, 2, 0, 16);  slice_34824 = add_1057 = None
        slice_scatter_6331: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6328, slice_scatter_6330, 1, 8432, 8448);  slice_scatter_6328 = slice_scatter_6330 = None
        slice_34829: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6331, 1, 8432, 8448)
        slice_34830: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34829, 2, 0, 16)
        slice_scatter_6333: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34829, slice_34830, 2, 0, 16);  slice_34829 = slice_34830 = None
        slice_scatter_6334: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6331, slice_scatter_6333, 1, 8432, 8448);  slice_scatter_6331 = slice_scatter_6333 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34849: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8448, 8464)
        slice_34850: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_34849, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1059: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_34850, memory_format = torch.contiguous_format);  slice_34850 = None
        view_2122: "f32[32, 16]" = torch.ops.aten.view.default(clone_1059, [32, 16]);  clone_1059 = None
        mm_1056: "f32[32, 8]" = torch.ops.aten.mm.default(view_2122, slice_7)
        view_2123: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1056, [2, 16, 8]);  mm_1056 = None
        slice_34857: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6334, 1, 8448, 8464)
        slice_34858: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34857, 2, 0, 16)
        add_1058: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34858, view_2123);  slice_34858 = view_2123 = None
        slice_scatter_6336: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34857, add_1058, 2, 0, 16);  slice_34857 = add_1058 = None
        slice_scatter_6337: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6334, slice_scatter_6336, 1, 8448, 8464);  slice_scatter_6334 = slice_scatter_6336 = None
        slice_34862: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6337, 1, 8448, 8464)
        slice_34863: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34862, 2, 0, 16)
        slice_scatter_6339: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34862, slice_34863, 2, 0, 16);  slice_34862 = slice_34863 = None
        slice_scatter_6340: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6337, slice_scatter_6339, 1, 8448, 8464);  slice_scatter_6337 = slice_scatter_6339 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34883: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_34849, 2, 16, 32);  slice_34849 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1060: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_34883, memory_format = torch.contiguous_format);  slice_34883 = None
        view_2124: "f32[32, 11]" = torch.ops.aten.view.default(clone_1060, [32, 11]);  clone_1060 = None
        mm_1057: "f32[32, 8]" = torch.ops.aten.mm.default(view_2124, slice_37)
        view_2125: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1057, [2, 16, 8]);  mm_1057 = None
        slice_34890: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6340, 1, 8448, 8464)
        slice_34891: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34890, 2, 0, 16)
        add_1059: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34891, view_2125);  slice_34891 = view_2125 = None
        slice_scatter_6342: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34890, add_1059, 2, 0, 16);  slice_34890 = add_1059 = None
        slice_scatter_6343: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6340, slice_scatter_6342, 1, 8448, 8464);  slice_scatter_6340 = slice_scatter_6342 = None
        slice_34895: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6343, 1, 8448, 8464)
        slice_34896: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34895, 2, 0, 16)
        slice_scatter_6345: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34895, slice_34896, 2, 0, 16);  slice_34895 = slice_34896 = None
        slice_scatter_6346: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6343, slice_scatter_6345, 1, 8448, 8464);  slice_scatter_6343 = slice_scatter_6345 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34915: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8464, 8480)
        slice_34916: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_34915, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1061: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_34916, memory_format = torch.contiguous_format);  slice_34916 = None
        view_2126: "f32[32, 16]" = torch.ops.aten.view.default(clone_1061, [32, 16]);  clone_1061 = None
        mm_1058: "f32[32, 8]" = torch.ops.aten.mm.default(view_2126, slice_7)
        view_2127: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1058, [2, 16, 8]);  mm_1058 = None
        slice_34923: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6346, 1, 8464, 8480)
        slice_34924: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34923, 2, 0, 16)
        add_1060: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34924, view_2127);  slice_34924 = view_2127 = None
        slice_scatter_6348: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34923, add_1060, 2, 0, 16);  slice_34923 = add_1060 = None
        slice_scatter_6349: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6346, slice_scatter_6348, 1, 8464, 8480);  slice_scatter_6346 = slice_scatter_6348 = None
        slice_34928: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6349, 1, 8464, 8480)
        slice_34929: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34928, 2, 0, 16)
        slice_scatter_6351: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34928, slice_34929, 2, 0, 16);  slice_34928 = slice_34929 = None
        slice_scatter_6352: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6349, slice_scatter_6351, 1, 8464, 8480);  slice_scatter_6349 = slice_scatter_6351 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34949: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_34915, 2, 16, 32);  slice_34915 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1062: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_34949, memory_format = torch.contiguous_format);  slice_34949 = None
        view_2128: "f32[32, 11]" = torch.ops.aten.view.default(clone_1062, [32, 11]);  clone_1062 = None
        mm_1059: "f32[32, 8]" = torch.ops.aten.mm.default(view_2128, slice_37)
        view_2129: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1059, [2, 16, 8]);  mm_1059 = None
        slice_34956: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6352, 1, 8464, 8480)
        slice_34957: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34956, 2, 0, 16)
        add_1061: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34957, view_2129);  slice_34957 = view_2129 = None
        slice_scatter_6354: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34956, add_1061, 2, 0, 16);  slice_34956 = add_1061 = None
        slice_scatter_6355: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6352, slice_scatter_6354, 1, 8464, 8480);  slice_scatter_6352 = slice_scatter_6354 = None
        slice_34961: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6355, 1, 8464, 8480)
        slice_34962: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34961, 2, 0, 16)
        slice_scatter_6357: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34961, slice_34962, 2, 0, 16);  slice_34961 = slice_34962 = None
        slice_scatter_6358: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6355, slice_scatter_6357, 1, 8464, 8480);  slice_scatter_6355 = slice_scatter_6357 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_34981: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8480, 8496)
        slice_34982: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_34981, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1063: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_34982, memory_format = torch.contiguous_format);  slice_34982 = None
        view_2130: "f32[32, 16]" = torch.ops.aten.view.default(clone_1063, [32, 16]);  clone_1063 = None
        mm_1060: "f32[32, 8]" = torch.ops.aten.mm.default(view_2130, slice_7)
        view_2131: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1060, [2, 16, 8]);  mm_1060 = None
        slice_34989: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6358, 1, 8480, 8496)
        slice_34990: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34989, 2, 0, 16)
        add_1062: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_34990, view_2131);  slice_34990 = view_2131 = None
        slice_scatter_6360: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34989, add_1062, 2, 0, 16);  slice_34989 = add_1062 = None
        slice_scatter_6361: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6358, slice_scatter_6360, 1, 8480, 8496);  slice_scatter_6358 = slice_scatter_6360 = None
        slice_34994: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6361, 1, 8480, 8496)
        slice_34995: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_34994, 2, 0, 16)
        slice_scatter_6363: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_34994, slice_34995, 2, 0, 16);  slice_34994 = slice_34995 = None
        slice_scatter_6364: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6361, slice_scatter_6363, 1, 8480, 8496);  slice_scatter_6361 = slice_scatter_6363 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35015: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_34981, 2, 16, 32);  slice_34981 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1064: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_35015, memory_format = torch.contiguous_format);  slice_35015 = None
        view_2132: "f32[32, 11]" = torch.ops.aten.view.default(clone_1064, [32, 11]);  clone_1064 = None
        mm_1061: "f32[32, 8]" = torch.ops.aten.mm.default(view_2132, slice_37)
        view_2133: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1061, [2, 16, 8]);  mm_1061 = None
        slice_35022: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6364, 1, 8480, 8496)
        slice_35023: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35022, 2, 0, 16)
        add_1063: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35023, view_2133);  slice_35023 = view_2133 = None
        slice_scatter_6366: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35022, add_1063, 2, 0, 16);  slice_35022 = add_1063 = None
        slice_scatter_6367: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6364, slice_scatter_6366, 1, 8480, 8496);  slice_scatter_6364 = slice_scatter_6366 = None
        slice_35027: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6367, 1, 8480, 8496)
        slice_35028: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35027, 2, 0, 16)
        slice_scatter_6369: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35027, slice_35028, 2, 0, 16);  slice_35027 = slice_35028 = None
        slice_scatter_6370: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6367, slice_scatter_6369, 1, 8480, 8496);  slice_scatter_6367 = slice_scatter_6369 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35047: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8496, 8512)
        slice_35048: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_35047, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1065: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_35048, memory_format = torch.contiguous_format);  slice_35048 = None
        view_2134: "f32[32, 16]" = torch.ops.aten.view.default(clone_1065, [32, 16]);  clone_1065 = None
        mm_1062: "f32[32, 8]" = torch.ops.aten.mm.default(view_2134, slice_7)
        view_2135: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1062, [2, 16, 8]);  mm_1062 = None
        slice_35055: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6370, 1, 8496, 8512)
        slice_35056: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35055, 2, 0, 16)
        add_1064: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35056, view_2135);  slice_35056 = view_2135 = None
        slice_scatter_6372: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35055, add_1064, 2, 0, 16);  slice_35055 = add_1064 = None
        slice_scatter_6373: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6370, slice_scatter_6372, 1, 8496, 8512);  slice_scatter_6370 = slice_scatter_6372 = None
        slice_35060: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6373, 1, 8496, 8512)
        slice_35061: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35060, 2, 0, 16)
        slice_scatter_6375: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35060, slice_35061, 2, 0, 16);  slice_35060 = slice_35061 = None
        slice_scatter_6376: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6373, slice_scatter_6375, 1, 8496, 8512);  slice_scatter_6373 = slice_scatter_6375 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35081: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_35047, 2, 16, 32);  slice_35047 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1066: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_35081, memory_format = torch.contiguous_format);  slice_35081 = None
        view_2136: "f32[32, 11]" = torch.ops.aten.view.default(clone_1066, [32, 11]);  clone_1066 = None
        mm_1063: "f32[32, 8]" = torch.ops.aten.mm.default(view_2136, slice_37)
        view_2137: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1063, [2, 16, 8]);  mm_1063 = None
        slice_35088: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6376, 1, 8496, 8512)
        slice_35089: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35088, 2, 0, 16)
        add_1065: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35089, view_2137);  slice_35089 = view_2137 = None
        slice_scatter_6378: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35088, add_1065, 2, 0, 16);  slice_35088 = add_1065 = None
        slice_scatter_6379: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6376, slice_scatter_6378, 1, 8496, 8512);  slice_scatter_6376 = slice_scatter_6378 = None
        slice_35093: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6379, 1, 8496, 8512)
        slice_35094: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35093, 2, 0, 16)
        slice_scatter_6381: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35093, slice_35094, 2, 0, 16);  slice_35093 = slice_35094 = None
        slice_scatter_6382: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6379, slice_scatter_6381, 1, 8496, 8512);  slice_scatter_6379 = slice_scatter_6381 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35113: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8512, 8528)
        slice_35114: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_35113, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1067: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_35114, memory_format = torch.contiguous_format);  slice_35114 = None
        view_2138: "f32[32, 16]" = torch.ops.aten.view.default(clone_1067, [32, 16]);  clone_1067 = None
        mm_1064: "f32[32, 8]" = torch.ops.aten.mm.default(view_2138, slice_7)
        view_2139: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1064, [2, 16, 8]);  mm_1064 = None
        slice_35121: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6382, 1, 8512, 8528)
        slice_35122: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35121, 2, 0, 16)
        add_1066: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35122, view_2139);  slice_35122 = view_2139 = None
        slice_scatter_6384: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35121, add_1066, 2, 0, 16);  slice_35121 = add_1066 = None
        slice_scatter_6385: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6382, slice_scatter_6384, 1, 8512, 8528);  slice_scatter_6382 = slice_scatter_6384 = None
        slice_35126: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6385, 1, 8512, 8528)
        slice_35127: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35126, 2, 0, 16)
        slice_scatter_6387: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35126, slice_35127, 2, 0, 16);  slice_35126 = slice_35127 = None
        slice_scatter_6388: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6385, slice_scatter_6387, 1, 8512, 8528);  slice_scatter_6385 = slice_scatter_6387 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35147: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_35113, 2, 16, 32);  slice_35113 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1068: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_35147, memory_format = torch.contiguous_format);  slice_35147 = None
        view_2140: "f32[32, 11]" = torch.ops.aten.view.default(clone_1068, [32, 11]);  clone_1068 = None
        mm_1065: "f32[32, 8]" = torch.ops.aten.mm.default(view_2140, slice_37)
        view_2141: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1065, [2, 16, 8]);  mm_1065 = None
        slice_35154: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6388, 1, 8512, 8528)
        slice_35155: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35154, 2, 0, 16)
        add_1067: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35155, view_2141);  slice_35155 = view_2141 = None
        slice_scatter_6390: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35154, add_1067, 2, 0, 16);  slice_35154 = add_1067 = None
        slice_scatter_6391: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6388, slice_scatter_6390, 1, 8512, 8528);  slice_scatter_6388 = slice_scatter_6390 = None
        slice_35159: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6391, 1, 8512, 8528)
        slice_35160: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35159, 2, 0, 16)
        slice_scatter_6393: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35159, slice_35160, 2, 0, 16);  slice_35159 = slice_35160 = None
        slice_scatter_6394: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6391, slice_scatter_6393, 1, 8512, 8528);  slice_scatter_6391 = slice_scatter_6393 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35179: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8528, 8544)
        slice_35180: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_35179, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1069: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_35180, memory_format = torch.contiguous_format);  slice_35180 = None
        view_2142: "f32[32, 16]" = torch.ops.aten.view.default(clone_1069, [32, 16]);  clone_1069 = None
        mm_1066: "f32[32, 8]" = torch.ops.aten.mm.default(view_2142, slice_7)
        view_2143: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1066, [2, 16, 8]);  mm_1066 = None
        slice_35187: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6394, 1, 8528, 8544)
        slice_35188: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35187, 2, 0, 16)
        add_1068: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35188, view_2143);  slice_35188 = view_2143 = None
        slice_scatter_6396: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35187, add_1068, 2, 0, 16);  slice_35187 = add_1068 = None
        slice_scatter_6397: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6394, slice_scatter_6396, 1, 8528, 8544);  slice_scatter_6394 = slice_scatter_6396 = None
        slice_35192: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6397, 1, 8528, 8544)
        slice_35193: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35192, 2, 0, 16)
        slice_scatter_6399: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35192, slice_35193, 2, 0, 16);  slice_35192 = slice_35193 = None
        slice_scatter_6400: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6397, slice_scatter_6399, 1, 8528, 8544);  slice_scatter_6397 = slice_scatter_6399 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35213: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_35179, 2, 16, 32);  slice_35179 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1070: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_35213, memory_format = torch.contiguous_format);  slice_35213 = None
        view_2144: "f32[32, 11]" = torch.ops.aten.view.default(clone_1070, [32, 11]);  clone_1070 = None
        mm_1067: "f32[32, 8]" = torch.ops.aten.mm.default(view_2144, slice_37)
        view_2145: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1067, [2, 16, 8]);  mm_1067 = None
        slice_35220: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6400, 1, 8528, 8544)
        slice_35221: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35220, 2, 0, 16)
        add_1069: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35221, view_2145);  slice_35221 = view_2145 = None
        slice_scatter_6402: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35220, add_1069, 2, 0, 16);  slice_35220 = add_1069 = None
        slice_scatter_6403: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6400, slice_scatter_6402, 1, 8528, 8544);  slice_scatter_6400 = slice_scatter_6402 = None
        slice_35225: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6403, 1, 8528, 8544)
        slice_35226: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35225, 2, 0, 16)
        slice_scatter_6405: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35225, slice_35226, 2, 0, 16);  slice_35225 = slice_35226 = None
        slice_scatter_6406: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6403, slice_scatter_6405, 1, 8528, 8544);  slice_scatter_6403 = slice_scatter_6405 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35245: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8544, 8560)
        slice_35246: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_35245, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1071: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_35246, memory_format = torch.contiguous_format);  slice_35246 = None
        view_2146: "f32[32, 16]" = torch.ops.aten.view.default(clone_1071, [32, 16]);  clone_1071 = None
        mm_1068: "f32[32, 8]" = torch.ops.aten.mm.default(view_2146, slice_7)
        view_2147: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1068, [2, 16, 8]);  mm_1068 = None
        slice_35253: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6406, 1, 8544, 8560)
        slice_35254: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35253, 2, 0, 16)
        add_1070: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35254, view_2147);  slice_35254 = view_2147 = None
        slice_scatter_6408: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35253, add_1070, 2, 0, 16);  slice_35253 = add_1070 = None
        slice_scatter_6409: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6406, slice_scatter_6408, 1, 8544, 8560);  slice_scatter_6406 = slice_scatter_6408 = None
        slice_35258: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6409, 1, 8544, 8560)
        slice_35259: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35258, 2, 0, 16)
        slice_scatter_6411: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35258, slice_35259, 2, 0, 16);  slice_35258 = slice_35259 = None
        slice_scatter_6412: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6409, slice_scatter_6411, 1, 8544, 8560);  slice_scatter_6409 = slice_scatter_6411 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35279: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_35245, 2, 16, 32);  slice_35245 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1072: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_35279, memory_format = torch.contiguous_format);  slice_35279 = None
        view_2148: "f32[32, 11]" = torch.ops.aten.view.default(clone_1072, [32, 11]);  clone_1072 = None
        mm_1069: "f32[32, 8]" = torch.ops.aten.mm.default(view_2148, slice_37)
        view_2149: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1069, [2, 16, 8]);  mm_1069 = None
        slice_35286: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6412, 1, 8544, 8560)
        slice_35287: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35286, 2, 0, 16)
        add_1071: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35287, view_2149);  slice_35287 = view_2149 = None
        slice_scatter_6414: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35286, add_1071, 2, 0, 16);  slice_35286 = add_1071 = None
        slice_scatter_6415: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6412, slice_scatter_6414, 1, 8544, 8560);  slice_scatter_6412 = slice_scatter_6414 = None
        slice_35291: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6415, 1, 8544, 8560)
        slice_35292: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35291, 2, 0, 16)
        slice_scatter_6417: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35291, slice_35292, 2, 0, 16);  slice_35291 = slice_35292 = None
        slice_scatter_6418: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6415, slice_scatter_6417, 1, 8544, 8560);  slice_scatter_6415 = slice_scatter_6417 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35311: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8560, 8576)
        slice_35312: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_35311, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1073: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_35312, memory_format = torch.contiguous_format);  slice_35312 = None
        view_2150: "f32[32, 16]" = torch.ops.aten.view.default(clone_1073, [32, 16]);  clone_1073 = None
        mm_1070: "f32[32, 8]" = torch.ops.aten.mm.default(view_2150, slice_7)
        view_2151: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1070, [2, 16, 8]);  mm_1070 = None
        slice_35319: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6418, 1, 8560, 8576)
        slice_35320: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35319, 2, 0, 16)
        add_1072: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35320, view_2151);  slice_35320 = view_2151 = None
        slice_scatter_6420: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35319, add_1072, 2, 0, 16);  slice_35319 = add_1072 = None
        slice_scatter_6421: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6418, slice_scatter_6420, 1, 8560, 8576);  slice_scatter_6418 = slice_scatter_6420 = None
        slice_35324: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6421, 1, 8560, 8576)
        slice_35325: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35324, 2, 0, 16)
        slice_scatter_6423: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35324, slice_35325, 2, 0, 16);  slice_35324 = slice_35325 = None
        slice_scatter_6424: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6421, slice_scatter_6423, 1, 8560, 8576);  slice_scatter_6421 = slice_scatter_6423 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35345: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_35311, 2, 16, 32);  slice_35311 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1074: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_35345, memory_format = torch.contiguous_format);  slice_35345 = None
        view_2152: "f32[32, 11]" = torch.ops.aten.view.default(clone_1074, [32, 11]);  clone_1074 = None
        mm_1071: "f32[32, 8]" = torch.ops.aten.mm.default(view_2152, slice_37)
        view_2153: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1071, [2, 16, 8]);  mm_1071 = None
        slice_35352: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6424, 1, 8560, 8576)
        slice_35353: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35352, 2, 0, 16)
        add_1073: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35353, view_2153);  slice_35353 = view_2153 = None
        slice_scatter_6426: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35352, add_1073, 2, 0, 16);  slice_35352 = add_1073 = None
        slice_scatter_6427: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6424, slice_scatter_6426, 1, 8560, 8576);  slice_scatter_6424 = slice_scatter_6426 = None
        slice_35357: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6427, 1, 8560, 8576)
        slice_35358: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35357, 2, 0, 16)
        slice_scatter_6429: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35357, slice_35358, 2, 0, 16);  slice_35357 = slice_35358 = None
        slice_scatter_6430: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6427, slice_scatter_6429, 1, 8560, 8576);  slice_scatter_6427 = slice_scatter_6429 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35377: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8576, 8592)
        slice_35378: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_35377, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1075: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_35378, memory_format = torch.contiguous_format);  slice_35378 = None
        view_2154: "f32[32, 16]" = torch.ops.aten.view.default(clone_1075, [32, 16]);  clone_1075 = None
        mm_1072: "f32[32, 8]" = torch.ops.aten.mm.default(view_2154, slice_7)
        view_2155: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1072, [2, 16, 8]);  mm_1072 = None
        slice_35385: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6430, 1, 8576, 8592)
        slice_35386: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35385, 2, 0, 16)
        add_1074: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35386, view_2155);  slice_35386 = view_2155 = None
        slice_scatter_6432: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35385, add_1074, 2, 0, 16);  slice_35385 = add_1074 = None
        slice_scatter_6433: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6430, slice_scatter_6432, 1, 8576, 8592);  slice_scatter_6430 = slice_scatter_6432 = None
        slice_35390: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6433, 1, 8576, 8592)
        slice_35391: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35390, 2, 0, 16)
        slice_scatter_6435: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35390, slice_35391, 2, 0, 16);  slice_35390 = slice_35391 = None
        slice_scatter_6436: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6433, slice_scatter_6435, 1, 8576, 8592);  slice_scatter_6433 = slice_scatter_6435 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35411: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_35377, 2, 16, 32);  slice_35377 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1076: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_35411, memory_format = torch.contiguous_format);  slice_35411 = None
        view_2156: "f32[32, 11]" = torch.ops.aten.view.default(clone_1076, [32, 11]);  clone_1076 = None
        mm_1073: "f32[32, 8]" = torch.ops.aten.mm.default(view_2156, slice_37)
        view_2157: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1073, [2, 16, 8]);  mm_1073 = None
        slice_35418: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6436, 1, 8576, 8592)
        slice_35419: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35418, 2, 0, 16)
        add_1075: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35419, view_2157);  slice_35419 = view_2157 = None
        slice_scatter_6438: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35418, add_1075, 2, 0, 16);  slice_35418 = add_1075 = None
        slice_scatter_6439: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6436, slice_scatter_6438, 1, 8576, 8592);  slice_scatter_6436 = slice_scatter_6438 = None
        slice_35423: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6439, 1, 8576, 8592)
        slice_35424: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35423, 2, 0, 16)
        slice_scatter_6441: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35423, slice_35424, 2, 0, 16);  slice_35423 = slice_35424 = None
        slice_scatter_6442: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6439, slice_scatter_6441, 1, 8576, 8592);  slice_scatter_6439 = slice_scatter_6441 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35443: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8592, 8608)
        slice_35444: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_35443, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1077: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_35444, memory_format = torch.contiguous_format);  slice_35444 = None
        view_2158: "f32[32, 16]" = torch.ops.aten.view.default(clone_1077, [32, 16]);  clone_1077 = None
        mm_1074: "f32[32, 8]" = torch.ops.aten.mm.default(view_2158, slice_7)
        view_2159: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1074, [2, 16, 8]);  mm_1074 = None
        slice_35451: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6442, 1, 8592, 8608)
        slice_35452: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35451, 2, 0, 16)
        add_1076: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35452, view_2159);  slice_35452 = view_2159 = None
        slice_scatter_6444: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35451, add_1076, 2, 0, 16);  slice_35451 = add_1076 = None
        slice_scatter_6445: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6442, slice_scatter_6444, 1, 8592, 8608);  slice_scatter_6442 = slice_scatter_6444 = None
        slice_35456: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6445, 1, 8592, 8608)
        slice_35457: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35456, 2, 0, 16)
        slice_scatter_6447: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35456, slice_35457, 2, 0, 16);  slice_35456 = slice_35457 = None
        slice_scatter_6448: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6445, slice_scatter_6447, 1, 8592, 8608);  slice_scatter_6445 = slice_scatter_6447 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35477: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_35443, 2, 16, 32);  slice_35443 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1078: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_35477, memory_format = torch.contiguous_format);  slice_35477 = None
        view_2160: "f32[32, 11]" = torch.ops.aten.view.default(clone_1078, [32, 11]);  clone_1078 = None
        mm_1075: "f32[32, 8]" = torch.ops.aten.mm.default(view_2160, slice_37)
        view_2161: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1075, [2, 16, 8]);  mm_1075 = None
        slice_35484: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6448, 1, 8592, 8608)
        slice_35485: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35484, 2, 0, 16)
        add_1077: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35485, view_2161);  slice_35485 = view_2161 = None
        slice_scatter_6450: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35484, add_1077, 2, 0, 16);  slice_35484 = add_1077 = None
        slice_scatter_6451: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6448, slice_scatter_6450, 1, 8592, 8608);  slice_scatter_6448 = slice_scatter_6450 = None
        slice_35489: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6451, 1, 8592, 8608)
        slice_35490: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35489, 2, 0, 16)
        slice_scatter_6453: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35489, slice_35490, 2, 0, 16);  slice_35489 = slice_35490 = None
        slice_scatter_6454: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6451, slice_scatter_6453, 1, 8592, 8608);  slice_scatter_6451 = slice_scatter_6453 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35509: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8608, 8624)
        slice_35510: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_35509, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1079: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_35510, memory_format = torch.contiguous_format);  slice_35510 = None
        view_2162: "f32[32, 16]" = torch.ops.aten.view.default(clone_1079, [32, 16]);  clone_1079 = None
        mm_1076: "f32[32, 8]" = torch.ops.aten.mm.default(view_2162, slice_7)
        view_2163: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1076, [2, 16, 8]);  mm_1076 = None
        slice_35517: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6454, 1, 8608, 8624)
        slice_35518: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35517, 2, 0, 16)
        add_1078: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35518, view_2163);  slice_35518 = view_2163 = None
        slice_scatter_6456: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35517, add_1078, 2, 0, 16);  slice_35517 = add_1078 = None
        slice_scatter_6457: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6454, slice_scatter_6456, 1, 8608, 8624);  slice_scatter_6454 = slice_scatter_6456 = None
        slice_35522: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6457, 1, 8608, 8624)
        slice_35523: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35522, 2, 0, 16)
        slice_scatter_6459: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35522, slice_35523, 2, 0, 16);  slice_35522 = slice_35523 = None
        slice_scatter_6460: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6457, slice_scatter_6459, 1, 8608, 8624);  slice_scatter_6457 = slice_scatter_6459 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35543: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_35509, 2, 16, 32);  slice_35509 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1080: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_35543, memory_format = torch.contiguous_format);  slice_35543 = None
        view_2164: "f32[32, 11]" = torch.ops.aten.view.default(clone_1080, [32, 11]);  clone_1080 = None
        mm_1077: "f32[32, 8]" = torch.ops.aten.mm.default(view_2164, slice_37)
        view_2165: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1077, [2, 16, 8]);  mm_1077 = None
        slice_35550: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6460, 1, 8608, 8624)
        slice_35551: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35550, 2, 0, 16)
        add_1079: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35551, view_2165);  slice_35551 = view_2165 = None
        slice_scatter_6462: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35550, add_1079, 2, 0, 16);  slice_35550 = add_1079 = None
        slice_scatter_6463: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6460, slice_scatter_6462, 1, 8608, 8624);  slice_scatter_6460 = slice_scatter_6462 = None
        slice_35555: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6463, 1, 8608, 8624)
        slice_35556: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35555, 2, 0, 16)
        slice_scatter_6465: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35555, slice_35556, 2, 0, 16);  slice_35555 = slice_35556 = None
        slice_scatter_6466: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6463, slice_scatter_6465, 1, 8608, 8624);  slice_scatter_6463 = slice_scatter_6465 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35575: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8624, 8640)
        slice_35576: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_35575, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1081: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_35576, memory_format = torch.contiguous_format);  slice_35576 = None
        view_2166: "f32[32, 16]" = torch.ops.aten.view.default(clone_1081, [32, 16]);  clone_1081 = None
        mm_1078: "f32[32, 8]" = torch.ops.aten.mm.default(view_2166, slice_7)
        view_2167: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1078, [2, 16, 8]);  mm_1078 = None
        slice_35583: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6466, 1, 8624, 8640)
        slice_35584: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35583, 2, 0, 16)
        add_1080: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35584, view_2167);  slice_35584 = view_2167 = None
        slice_scatter_6468: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35583, add_1080, 2, 0, 16);  slice_35583 = add_1080 = None
        slice_scatter_6469: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6466, slice_scatter_6468, 1, 8624, 8640);  slice_scatter_6466 = slice_scatter_6468 = None
        slice_35588: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6469, 1, 8624, 8640)
        slice_35589: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35588, 2, 0, 16)
        slice_scatter_6471: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35588, slice_35589, 2, 0, 16);  slice_35588 = slice_35589 = None
        slice_scatter_6472: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6469, slice_scatter_6471, 1, 8624, 8640);  slice_scatter_6469 = slice_scatter_6471 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35609: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_35575, 2, 16, 32);  slice_35575 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1082: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_35609, memory_format = torch.contiguous_format);  slice_35609 = None
        view_2168: "f32[32, 11]" = torch.ops.aten.view.default(clone_1082, [32, 11]);  clone_1082 = None
        mm_1079: "f32[32, 8]" = torch.ops.aten.mm.default(view_2168, slice_37)
        view_2169: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1079, [2, 16, 8]);  mm_1079 = None
        slice_35616: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6472, 1, 8624, 8640)
        slice_35617: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35616, 2, 0, 16)
        add_1081: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35617, view_2169);  slice_35617 = view_2169 = None
        slice_scatter_6474: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35616, add_1081, 2, 0, 16);  slice_35616 = add_1081 = None
        slice_scatter_6475: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6472, slice_scatter_6474, 1, 8624, 8640);  slice_scatter_6472 = slice_scatter_6474 = None
        slice_35621: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6475, 1, 8624, 8640)
        slice_35622: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35621, 2, 0, 16)
        slice_scatter_6477: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35621, slice_35622, 2, 0, 16);  slice_35621 = slice_35622 = None
        slice_scatter_6478: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6475, slice_scatter_6477, 1, 8624, 8640);  slice_scatter_6475 = slice_scatter_6477 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35641: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8640, 8656)
        slice_35642: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_35641, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1083: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_35642, memory_format = torch.contiguous_format);  slice_35642 = None
        view_2170: "f32[32, 16]" = torch.ops.aten.view.default(clone_1083, [32, 16]);  clone_1083 = None
        mm_1080: "f32[32, 8]" = torch.ops.aten.mm.default(view_2170, slice_7)
        view_2171: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1080, [2, 16, 8]);  mm_1080 = None
        slice_35649: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6478, 1, 8640, 8656)
        slice_35650: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35649, 2, 0, 16)
        add_1082: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35650, view_2171);  slice_35650 = view_2171 = None
        slice_scatter_6480: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35649, add_1082, 2, 0, 16);  slice_35649 = add_1082 = None
        slice_scatter_6481: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6478, slice_scatter_6480, 1, 8640, 8656);  slice_scatter_6478 = slice_scatter_6480 = None
        slice_35654: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6481, 1, 8640, 8656)
        slice_35655: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35654, 2, 0, 16)
        slice_scatter_6483: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35654, slice_35655, 2, 0, 16);  slice_35654 = slice_35655 = None
        slice_scatter_6484: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6481, slice_scatter_6483, 1, 8640, 8656);  slice_scatter_6481 = slice_scatter_6483 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35675: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_35641, 2, 16, 32);  slice_35641 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1084: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_35675, memory_format = torch.contiguous_format);  slice_35675 = None
        view_2172: "f32[32, 11]" = torch.ops.aten.view.default(clone_1084, [32, 11]);  clone_1084 = None
        mm_1081: "f32[32, 8]" = torch.ops.aten.mm.default(view_2172, slice_37)
        view_2173: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1081, [2, 16, 8]);  mm_1081 = None
        slice_35682: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6484, 1, 8640, 8656)
        slice_35683: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35682, 2, 0, 16)
        add_1083: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35683, view_2173);  slice_35683 = view_2173 = None
        slice_scatter_6486: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35682, add_1083, 2, 0, 16);  slice_35682 = add_1083 = None
        slice_scatter_6487: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6484, slice_scatter_6486, 1, 8640, 8656);  slice_scatter_6484 = slice_scatter_6486 = None
        slice_35687: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6487, 1, 8640, 8656)
        slice_35688: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35687, 2, 0, 16)
        slice_scatter_6489: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35687, slice_35688, 2, 0, 16);  slice_35687 = slice_35688 = None
        slice_scatter_6490: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6487, slice_scatter_6489, 1, 8640, 8656);  slice_scatter_6487 = slice_scatter_6489 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35707: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8656, 8672)
        slice_35708: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_35707, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1085: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_35708, memory_format = torch.contiguous_format);  slice_35708 = None
        view_2174: "f32[32, 16]" = torch.ops.aten.view.default(clone_1085, [32, 16]);  clone_1085 = None
        mm_1082: "f32[32, 8]" = torch.ops.aten.mm.default(view_2174, slice_7)
        view_2175: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1082, [2, 16, 8]);  mm_1082 = None
        slice_35715: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6490, 1, 8656, 8672)
        slice_35716: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35715, 2, 0, 16)
        add_1084: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35716, view_2175);  slice_35716 = view_2175 = None
        slice_scatter_6492: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35715, add_1084, 2, 0, 16);  slice_35715 = add_1084 = None
        slice_scatter_6493: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6490, slice_scatter_6492, 1, 8656, 8672);  slice_scatter_6490 = slice_scatter_6492 = None
        slice_35720: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6493, 1, 8656, 8672)
        slice_35721: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35720, 2, 0, 16)
        slice_scatter_6495: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35720, slice_35721, 2, 0, 16);  slice_35720 = slice_35721 = None
        slice_scatter_6496: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6493, slice_scatter_6495, 1, 8656, 8672);  slice_scatter_6493 = slice_scatter_6495 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35741: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_35707, 2, 16, 32);  slice_35707 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1086: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_35741, memory_format = torch.contiguous_format);  slice_35741 = None
        view_2176: "f32[32, 11]" = torch.ops.aten.view.default(clone_1086, [32, 11]);  clone_1086 = None
        mm_1083: "f32[32, 8]" = torch.ops.aten.mm.default(view_2176, slice_37)
        view_2177: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1083, [2, 16, 8]);  mm_1083 = None
        slice_35748: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6496, 1, 8656, 8672)
        slice_35749: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35748, 2, 0, 16)
        add_1085: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35749, view_2177);  slice_35749 = view_2177 = None
        slice_scatter_6498: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35748, add_1085, 2, 0, 16);  slice_35748 = add_1085 = None
        slice_scatter_6499: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6496, slice_scatter_6498, 1, 8656, 8672);  slice_scatter_6496 = slice_scatter_6498 = None
        slice_35753: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6499, 1, 8656, 8672)
        slice_35754: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35753, 2, 0, 16)
        slice_scatter_6501: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35753, slice_35754, 2, 0, 16);  slice_35753 = slice_35754 = None
        slice_scatter_6502: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6499, slice_scatter_6501, 1, 8656, 8672);  slice_scatter_6499 = slice_scatter_6501 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35773: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8672, 8688)
        slice_35774: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_35773, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1087: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_35774, memory_format = torch.contiguous_format);  slice_35774 = None
        view_2178: "f32[32, 16]" = torch.ops.aten.view.default(clone_1087, [32, 16]);  clone_1087 = None
        mm_1084: "f32[32, 8]" = torch.ops.aten.mm.default(view_2178, slice_7)
        view_2179: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1084, [2, 16, 8]);  mm_1084 = None
        slice_35781: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6502, 1, 8672, 8688)
        slice_35782: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35781, 2, 0, 16)
        add_1086: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35782, view_2179);  slice_35782 = view_2179 = None
        slice_scatter_6504: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35781, add_1086, 2, 0, 16);  slice_35781 = add_1086 = None
        slice_scatter_6505: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6502, slice_scatter_6504, 1, 8672, 8688);  slice_scatter_6502 = slice_scatter_6504 = None
        slice_35786: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6505, 1, 8672, 8688)
        slice_35787: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35786, 2, 0, 16)
        slice_scatter_6507: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35786, slice_35787, 2, 0, 16);  slice_35786 = slice_35787 = None
        slice_scatter_6508: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6505, slice_scatter_6507, 1, 8672, 8688);  slice_scatter_6505 = slice_scatter_6507 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35807: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_35773, 2, 16, 32);  slice_35773 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1088: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_35807, memory_format = torch.contiguous_format);  slice_35807 = None
        view_2180: "f32[32, 11]" = torch.ops.aten.view.default(clone_1088, [32, 11]);  clone_1088 = None
        mm_1085: "f32[32, 8]" = torch.ops.aten.mm.default(view_2180, slice_37)
        view_2181: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1085, [2, 16, 8]);  mm_1085 = None
        slice_35814: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6508, 1, 8672, 8688)
        slice_35815: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35814, 2, 0, 16)
        add_1087: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35815, view_2181);  slice_35815 = view_2181 = None
        slice_scatter_6510: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35814, add_1087, 2, 0, 16);  slice_35814 = add_1087 = None
        slice_scatter_6511: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6508, slice_scatter_6510, 1, 8672, 8688);  slice_scatter_6508 = slice_scatter_6510 = None
        slice_35819: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6511, 1, 8672, 8688)
        slice_35820: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35819, 2, 0, 16)
        slice_scatter_6513: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35819, slice_35820, 2, 0, 16);  slice_35819 = slice_35820 = None
        slice_scatter_6514: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6511, slice_scatter_6513, 1, 8672, 8688);  slice_scatter_6511 = slice_scatter_6513 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35839: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8688, 8704)
        slice_35840: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_35839, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1089: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_35840, memory_format = torch.contiguous_format);  slice_35840 = None
        view_2182: "f32[32, 16]" = torch.ops.aten.view.default(clone_1089, [32, 16]);  clone_1089 = None
        mm_1086: "f32[32, 8]" = torch.ops.aten.mm.default(view_2182, slice_7)
        view_2183: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1086, [2, 16, 8]);  mm_1086 = None
        slice_35847: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6514, 1, 8688, 8704)
        slice_35848: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35847, 2, 0, 16)
        add_1088: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35848, view_2183);  slice_35848 = view_2183 = None
        slice_scatter_6516: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35847, add_1088, 2, 0, 16);  slice_35847 = add_1088 = None
        slice_scatter_6517: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6514, slice_scatter_6516, 1, 8688, 8704);  slice_scatter_6514 = slice_scatter_6516 = None
        slice_35852: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6517, 1, 8688, 8704)
        slice_35853: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35852, 2, 0, 16)
        slice_scatter_6519: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35852, slice_35853, 2, 0, 16);  slice_35852 = slice_35853 = None
        slice_scatter_6520: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6517, slice_scatter_6519, 1, 8688, 8704);  slice_scatter_6517 = slice_scatter_6519 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35873: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_35839, 2, 16, 32);  slice_35839 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1090: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_35873, memory_format = torch.contiguous_format);  slice_35873 = None
        view_2184: "f32[32, 11]" = torch.ops.aten.view.default(clone_1090, [32, 11]);  clone_1090 = None
        mm_1087: "f32[32, 8]" = torch.ops.aten.mm.default(view_2184, slice_37)
        view_2185: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1087, [2, 16, 8]);  mm_1087 = None
        slice_35880: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6520, 1, 8688, 8704)
        slice_35881: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35880, 2, 0, 16)
        add_1089: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35881, view_2185);  slice_35881 = view_2185 = None
        slice_scatter_6522: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35880, add_1089, 2, 0, 16);  slice_35880 = add_1089 = None
        slice_scatter_6523: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6520, slice_scatter_6522, 1, 8688, 8704);  slice_scatter_6520 = slice_scatter_6522 = None
        slice_35885: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6523, 1, 8688, 8704)
        slice_35886: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35885, 2, 0, 16)
        slice_scatter_6525: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35885, slice_35886, 2, 0, 16);  slice_35885 = slice_35886 = None
        slice_scatter_6526: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6523, slice_scatter_6525, 1, 8688, 8704);  slice_scatter_6523 = slice_scatter_6525 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35905: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8704, 8720)
        slice_35906: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_35905, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1091: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_35906, memory_format = torch.contiguous_format);  slice_35906 = None
        view_2186: "f32[32, 16]" = torch.ops.aten.view.default(clone_1091, [32, 16]);  clone_1091 = None
        mm_1088: "f32[32, 8]" = torch.ops.aten.mm.default(view_2186, slice_7)
        view_2187: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1088, [2, 16, 8]);  mm_1088 = None
        slice_35913: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6526, 1, 8704, 8720)
        slice_35914: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35913, 2, 0, 16)
        add_1090: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35914, view_2187);  slice_35914 = view_2187 = None
        slice_scatter_6528: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35913, add_1090, 2, 0, 16);  slice_35913 = add_1090 = None
        slice_scatter_6529: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6526, slice_scatter_6528, 1, 8704, 8720);  slice_scatter_6526 = slice_scatter_6528 = None
        slice_35918: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6529, 1, 8704, 8720)
        slice_35919: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35918, 2, 0, 16)
        slice_scatter_6531: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35918, slice_35919, 2, 0, 16);  slice_35918 = slice_35919 = None
        slice_scatter_6532: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6529, slice_scatter_6531, 1, 8704, 8720);  slice_scatter_6529 = slice_scatter_6531 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35939: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_35905, 2, 16, 32);  slice_35905 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1092: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_35939, memory_format = torch.contiguous_format);  slice_35939 = None
        view_2188: "f32[32, 11]" = torch.ops.aten.view.default(clone_1092, [32, 11]);  clone_1092 = None
        mm_1089: "f32[32, 8]" = torch.ops.aten.mm.default(view_2188, slice_37)
        view_2189: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1089, [2, 16, 8]);  mm_1089 = None
        slice_35946: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6532, 1, 8704, 8720)
        slice_35947: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35946, 2, 0, 16)
        add_1091: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35947, view_2189);  slice_35947 = view_2189 = None
        slice_scatter_6534: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35946, add_1091, 2, 0, 16);  slice_35946 = add_1091 = None
        slice_scatter_6535: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6532, slice_scatter_6534, 1, 8704, 8720);  slice_scatter_6532 = slice_scatter_6534 = None
        slice_35951: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6535, 1, 8704, 8720)
        slice_35952: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35951, 2, 0, 16)
        slice_scatter_6537: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35951, slice_35952, 2, 0, 16);  slice_35951 = slice_35952 = None
        slice_scatter_6538: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6535, slice_scatter_6537, 1, 8704, 8720);  slice_scatter_6535 = slice_scatter_6537 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_35971: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8720, 8736)
        slice_35972: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_35971, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1093: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_35972, memory_format = torch.contiguous_format);  slice_35972 = None
        view_2190: "f32[32, 16]" = torch.ops.aten.view.default(clone_1093, [32, 16]);  clone_1093 = None
        mm_1090: "f32[32, 8]" = torch.ops.aten.mm.default(view_2190, slice_7)
        view_2191: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1090, [2, 16, 8]);  mm_1090 = None
        slice_35979: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6538, 1, 8720, 8736)
        slice_35980: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35979, 2, 0, 16)
        add_1092: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_35980, view_2191);  slice_35980 = view_2191 = None
        slice_scatter_6540: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35979, add_1092, 2, 0, 16);  slice_35979 = add_1092 = None
        slice_scatter_6541: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6538, slice_scatter_6540, 1, 8720, 8736);  slice_scatter_6538 = slice_scatter_6540 = None
        slice_35984: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6541, 1, 8720, 8736)
        slice_35985: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_35984, 2, 0, 16)
        slice_scatter_6543: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_35984, slice_35985, 2, 0, 16);  slice_35984 = slice_35985 = None
        slice_scatter_6544: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6541, slice_scatter_6543, 1, 8720, 8736);  slice_scatter_6541 = slice_scatter_6543 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36005: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_35971, 2, 16, 32);  slice_35971 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1094: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_36005, memory_format = torch.contiguous_format);  slice_36005 = None
        view_2192: "f32[32, 11]" = torch.ops.aten.view.default(clone_1094, [32, 11]);  clone_1094 = None
        mm_1091: "f32[32, 8]" = torch.ops.aten.mm.default(view_2192, slice_37)
        view_2193: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1091, [2, 16, 8]);  mm_1091 = None
        slice_36012: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6544, 1, 8720, 8736)
        slice_36013: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36012, 2, 0, 16)
        add_1093: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36013, view_2193);  slice_36013 = view_2193 = None
        slice_scatter_6546: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36012, add_1093, 2, 0, 16);  slice_36012 = add_1093 = None
        slice_scatter_6547: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6544, slice_scatter_6546, 1, 8720, 8736);  slice_scatter_6544 = slice_scatter_6546 = None
        slice_36017: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6547, 1, 8720, 8736)
        slice_36018: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36017, 2, 0, 16)
        slice_scatter_6549: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36017, slice_36018, 2, 0, 16);  slice_36017 = slice_36018 = None
        slice_scatter_6550: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6547, slice_scatter_6549, 1, 8720, 8736);  slice_scatter_6547 = slice_scatter_6549 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36037: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8736, 8752)
        slice_36038: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_36037, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1095: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_36038, memory_format = torch.contiguous_format);  slice_36038 = None
        view_2194: "f32[32, 16]" = torch.ops.aten.view.default(clone_1095, [32, 16]);  clone_1095 = None
        mm_1092: "f32[32, 8]" = torch.ops.aten.mm.default(view_2194, slice_7)
        view_2195: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1092, [2, 16, 8]);  mm_1092 = None
        slice_36045: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6550, 1, 8736, 8752)
        slice_36046: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36045, 2, 0, 16)
        add_1094: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36046, view_2195);  slice_36046 = view_2195 = None
        slice_scatter_6552: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36045, add_1094, 2, 0, 16);  slice_36045 = add_1094 = None
        slice_scatter_6553: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6550, slice_scatter_6552, 1, 8736, 8752);  slice_scatter_6550 = slice_scatter_6552 = None
        slice_36050: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6553, 1, 8736, 8752)
        slice_36051: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36050, 2, 0, 16)
        slice_scatter_6555: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36050, slice_36051, 2, 0, 16);  slice_36050 = slice_36051 = None
        slice_scatter_6556: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6553, slice_scatter_6555, 1, 8736, 8752);  slice_scatter_6553 = slice_scatter_6555 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36071: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_36037, 2, 16, 32);  slice_36037 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1096: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_36071, memory_format = torch.contiguous_format);  slice_36071 = None
        view_2196: "f32[32, 11]" = torch.ops.aten.view.default(clone_1096, [32, 11]);  clone_1096 = None
        mm_1093: "f32[32, 8]" = torch.ops.aten.mm.default(view_2196, slice_37)
        view_2197: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1093, [2, 16, 8]);  mm_1093 = None
        slice_36078: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6556, 1, 8736, 8752)
        slice_36079: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36078, 2, 0, 16)
        add_1095: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36079, view_2197);  slice_36079 = view_2197 = None
        slice_scatter_6558: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36078, add_1095, 2, 0, 16);  slice_36078 = add_1095 = None
        slice_scatter_6559: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6556, slice_scatter_6558, 1, 8736, 8752);  slice_scatter_6556 = slice_scatter_6558 = None
        slice_36083: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6559, 1, 8736, 8752)
        slice_36084: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36083, 2, 0, 16)
        slice_scatter_6561: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36083, slice_36084, 2, 0, 16);  slice_36083 = slice_36084 = None
        slice_scatter_6562: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6559, slice_scatter_6561, 1, 8736, 8752);  slice_scatter_6559 = slice_scatter_6561 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36103: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8752, 8768)
        slice_36104: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_36103, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1097: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_36104, memory_format = torch.contiguous_format);  slice_36104 = None
        view_2198: "f32[32, 16]" = torch.ops.aten.view.default(clone_1097, [32, 16]);  clone_1097 = None
        mm_1094: "f32[32, 8]" = torch.ops.aten.mm.default(view_2198, slice_7)
        view_2199: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1094, [2, 16, 8]);  mm_1094 = None
        slice_36111: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6562, 1, 8752, 8768)
        slice_36112: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36111, 2, 0, 16)
        add_1096: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36112, view_2199);  slice_36112 = view_2199 = None
        slice_scatter_6564: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36111, add_1096, 2, 0, 16);  slice_36111 = add_1096 = None
        slice_scatter_6565: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6562, slice_scatter_6564, 1, 8752, 8768);  slice_scatter_6562 = slice_scatter_6564 = None
        slice_36116: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6565, 1, 8752, 8768)
        slice_36117: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36116, 2, 0, 16)
        slice_scatter_6567: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36116, slice_36117, 2, 0, 16);  slice_36116 = slice_36117 = None
        slice_scatter_6568: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6565, slice_scatter_6567, 1, 8752, 8768);  slice_scatter_6565 = slice_scatter_6567 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36137: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_36103, 2, 16, 32);  slice_36103 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1098: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_36137, memory_format = torch.contiguous_format);  slice_36137 = None
        view_2200: "f32[32, 11]" = torch.ops.aten.view.default(clone_1098, [32, 11]);  clone_1098 = None
        mm_1095: "f32[32, 8]" = torch.ops.aten.mm.default(view_2200, slice_37)
        view_2201: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1095, [2, 16, 8]);  mm_1095 = None
        slice_36144: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6568, 1, 8752, 8768)
        slice_36145: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36144, 2, 0, 16)
        add_1097: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36145, view_2201);  slice_36145 = view_2201 = None
        slice_scatter_6570: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36144, add_1097, 2, 0, 16);  slice_36144 = add_1097 = None
        slice_scatter_6571: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6568, slice_scatter_6570, 1, 8752, 8768);  slice_scatter_6568 = slice_scatter_6570 = None
        slice_36149: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6571, 1, 8752, 8768)
        slice_36150: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36149, 2, 0, 16)
        slice_scatter_6573: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36149, slice_36150, 2, 0, 16);  slice_36149 = slice_36150 = None
        slice_scatter_6574: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6571, slice_scatter_6573, 1, 8752, 8768);  slice_scatter_6571 = slice_scatter_6573 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36169: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8768, 8784)
        slice_36170: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_36169, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1099: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_36170, memory_format = torch.contiguous_format);  slice_36170 = None
        view_2202: "f32[32, 16]" = torch.ops.aten.view.default(clone_1099, [32, 16]);  clone_1099 = None
        mm_1096: "f32[32, 8]" = torch.ops.aten.mm.default(view_2202, slice_7)
        view_2203: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1096, [2, 16, 8]);  mm_1096 = None
        slice_36177: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6574, 1, 8768, 8784)
        slice_36178: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36177, 2, 0, 16)
        add_1098: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36178, view_2203);  slice_36178 = view_2203 = None
        slice_scatter_6576: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36177, add_1098, 2, 0, 16);  slice_36177 = add_1098 = None
        slice_scatter_6577: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6574, slice_scatter_6576, 1, 8768, 8784);  slice_scatter_6574 = slice_scatter_6576 = None
        slice_36182: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6577, 1, 8768, 8784)
        slice_36183: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36182, 2, 0, 16)
        slice_scatter_6579: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36182, slice_36183, 2, 0, 16);  slice_36182 = slice_36183 = None
        slice_scatter_6580: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6577, slice_scatter_6579, 1, 8768, 8784);  slice_scatter_6577 = slice_scatter_6579 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36203: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_36169, 2, 16, 32);  slice_36169 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1100: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_36203, memory_format = torch.contiguous_format);  slice_36203 = None
        view_2204: "f32[32, 11]" = torch.ops.aten.view.default(clone_1100, [32, 11]);  clone_1100 = None
        mm_1097: "f32[32, 8]" = torch.ops.aten.mm.default(view_2204, slice_37)
        view_2205: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1097, [2, 16, 8]);  mm_1097 = None
        slice_36210: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6580, 1, 8768, 8784)
        slice_36211: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36210, 2, 0, 16)
        add_1099: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36211, view_2205);  slice_36211 = view_2205 = None
        slice_scatter_6582: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36210, add_1099, 2, 0, 16);  slice_36210 = add_1099 = None
        slice_scatter_6583: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6580, slice_scatter_6582, 1, 8768, 8784);  slice_scatter_6580 = slice_scatter_6582 = None
        slice_36215: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6583, 1, 8768, 8784)
        slice_36216: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36215, 2, 0, 16)
        slice_scatter_6585: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36215, slice_36216, 2, 0, 16);  slice_36215 = slice_36216 = None
        slice_scatter_6586: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6583, slice_scatter_6585, 1, 8768, 8784);  slice_scatter_6583 = slice_scatter_6585 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36235: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8784, 8800)
        slice_36236: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_36235, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1101: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_36236, memory_format = torch.contiguous_format);  slice_36236 = None
        view_2206: "f32[32, 16]" = torch.ops.aten.view.default(clone_1101, [32, 16]);  clone_1101 = None
        mm_1098: "f32[32, 8]" = torch.ops.aten.mm.default(view_2206, slice_7)
        view_2207: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1098, [2, 16, 8]);  mm_1098 = None
        slice_36243: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6586, 1, 8784, 8800)
        slice_36244: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36243, 2, 0, 16)
        add_1100: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36244, view_2207);  slice_36244 = view_2207 = None
        slice_scatter_6588: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36243, add_1100, 2, 0, 16);  slice_36243 = add_1100 = None
        slice_scatter_6589: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6586, slice_scatter_6588, 1, 8784, 8800);  slice_scatter_6586 = slice_scatter_6588 = None
        slice_36248: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6589, 1, 8784, 8800)
        slice_36249: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36248, 2, 0, 16)
        slice_scatter_6591: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36248, slice_36249, 2, 0, 16);  slice_36248 = slice_36249 = None
        slice_scatter_6592: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6589, slice_scatter_6591, 1, 8784, 8800);  slice_scatter_6589 = slice_scatter_6591 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36269: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_36235, 2, 16, 32);  slice_36235 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1102: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_36269, memory_format = torch.contiguous_format);  slice_36269 = None
        view_2208: "f32[32, 11]" = torch.ops.aten.view.default(clone_1102, [32, 11]);  clone_1102 = None
        mm_1099: "f32[32, 8]" = torch.ops.aten.mm.default(view_2208, slice_37)
        view_2209: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1099, [2, 16, 8]);  mm_1099 = None
        slice_36276: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6592, 1, 8784, 8800)
        slice_36277: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36276, 2, 0, 16)
        add_1101: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36277, view_2209);  slice_36277 = view_2209 = None
        slice_scatter_6594: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36276, add_1101, 2, 0, 16);  slice_36276 = add_1101 = None
        slice_scatter_6595: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6592, slice_scatter_6594, 1, 8784, 8800);  slice_scatter_6592 = slice_scatter_6594 = None
        slice_36281: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6595, 1, 8784, 8800)
        slice_36282: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36281, 2, 0, 16)
        slice_scatter_6597: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36281, slice_36282, 2, 0, 16);  slice_36281 = slice_36282 = None
        slice_scatter_6598: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6595, slice_scatter_6597, 1, 8784, 8800);  slice_scatter_6595 = slice_scatter_6597 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36301: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8800, 8816)
        slice_36302: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_36301, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1103: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_36302, memory_format = torch.contiguous_format);  slice_36302 = None
        view_2210: "f32[32, 16]" = torch.ops.aten.view.default(clone_1103, [32, 16]);  clone_1103 = None
        mm_1100: "f32[32, 8]" = torch.ops.aten.mm.default(view_2210, slice_7)
        view_2211: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1100, [2, 16, 8]);  mm_1100 = None
        slice_36309: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6598, 1, 8800, 8816)
        slice_36310: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36309, 2, 0, 16)
        add_1102: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36310, view_2211);  slice_36310 = view_2211 = None
        slice_scatter_6600: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36309, add_1102, 2, 0, 16);  slice_36309 = add_1102 = None
        slice_scatter_6601: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6598, slice_scatter_6600, 1, 8800, 8816);  slice_scatter_6598 = slice_scatter_6600 = None
        slice_36314: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6601, 1, 8800, 8816)
        slice_36315: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36314, 2, 0, 16)
        slice_scatter_6603: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36314, slice_36315, 2, 0, 16);  slice_36314 = slice_36315 = None
        slice_scatter_6604: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6601, slice_scatter_6603, 1, 8800, 8816);  slice_scatter_6601 = slice_scatter_6603 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36335: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_36301, 2, 16, 32);  slice_36301 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1104: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_36335, memory_format = torch.contiguous_format);  slice_36335 = None
        view_2212: "f32[32, 11]" = torch.ops.aten.view.default(clone_1104, [32, 11]);  clone_1104 = None
        mm_1101: "f32[32, 8]" = torch.ops.aten.mm.default(view_2212, slice_37)
        view_2213: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1101, [2, 16, 8]);  mm_1101 = None
        slice_36342: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6604, 1, 8800, 8816)
        slice_36343: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36342, 2, 0, 16)
        add_1103: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36343, view_2213);  slice_36343 = view_2213 = None
        slice_scatter_6606: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36342, add_1103, 2, 0, 16);  slice_36342 = add_1103 = None
        slice_scatter_6607: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6604, slice_scatter_6606, 1, 8800, 8816);  slice_scatter_6604 = slice_scatter_6606 = None
        slice_36347: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6607, 1, 8800, 8816)
        slice_36348: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36347, 2, 0, 16)
        slice_scatter_6609: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36347, slice_36348, 2, 0, 16);  slice_36347 = slice_36348 = None
        slice_scatter_6610: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6607, slice_scatter_6609, 1, 8800, 8816);  slice_scatter_6607 = slice_scatter_6609 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36367: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8816, 8832)
        slice_36368: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_36367, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1105: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_36368, memory_format = torch.contiguous_format);  slice_36368 = None
        view_2214: "f32[32, 16]" = torch.ops.aten.view.default(clone_1105, [32, 16]);  clone_1105 = None
        mm_1102: "f32[32, 8]" = torch.ops.aten.mm.default(view_2214, slice_7)
        view_2215: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1102, [2, 16, 8]);  mm_1102 = None
        slice_36375: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6610, 1, 8816, 8832)
        slice_36376: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36375, 2, 0, 16)
        add_1104: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36376, view_2215);  slice_36376 = view_2215 = None
        slice_scatter_6612: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36375, add_1104, 2, 0, 16);  slice_36375 = add_1104 = None
        slice_scatter_6613: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6610, slice_scatter_6612, 1, 8816, 8832);  slice_scatter_6610 = slice_scatter_6612 = None
        slice_36380: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6613, 1, 8816, 8832)
        slice_36381: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36380, 2, 0, 16)
        slice_scatter_6615: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36380, slice_36381, 2, 0, 16);  slice_36380 = slice_36381 = None
        slice_scatter_6616: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6613, slice_scatter_6615, 1, 8816, 8832);  slice_scatter_6613 = slice_scatter_6615 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36401: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_36367, 2, 16, 32);  slice_36367 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1106: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_36401, memory_format = torch.contiguous_format);  slice_36401 = None
        view_2216: "f32[32, 11]" = torch.ops.aten.view.default(clone_1106, [32, 11]);  clone_1106 = None
        mm_1103: "f32[32, 8]" = torch.ops.aten.mm.default(view_2216, slice_37)
        view_2217: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1103, [2, 16, 8]);  mm_1103 = None
        slice_36408: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6616, 1, 8816, 8832)
        slice_36409: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36408, 2, 0, 16)
        add_1105: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36409, view_2217);  slice_36409 = view_2217 = None
        slice_scatter_6618: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36408, add_1105, 2, 0, 16);  slice_36408 = add_1105 = None
        slice_scatter_6619: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6616, slice_scatter_6618, 1, 8816, 8832);  slice_scatter_6616 = slice_scatter_6618 = None
        slice_36413: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6619, 1, 8816, 8832)
        slice_36414: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36413, 2, 0, 16)
        slice_scatter_6621: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36413, slice_36414, 2, 0, 16);  slice_36413 = slice_36414 = None
        slice_scatter_6622: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6619, slice_scatter_6621, 1, 8816, 8832);  slice_scatter_6619 = slice_scatter_6621 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36433: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8832, 8848)
        slice_36434: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_36433, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1107: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_36434, memory_format = torch.contiguous_format);  slice_36434 = None
        view_2218: "f32[32, 16]" = torch.ops.aten.view.default(clone_1107, [32, 16]);  clone_1107 = None
        mm_1104: "f32[32, 8]" = torch.ops.aten.mm.default(view_2218, slice_7)
        view_2219: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1104, [2, 16, 8]);  mm_1104 = None
        slice_36441: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6622, 1, 8832, 8848)
        slice_36442: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36441, 2, 0, 16)
        add_1106: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36442, view_2219);  slice_36442 = view_2219 = None
        slice_scatter_6624: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36441, add_1106, 2, 0, 16);  slice_36441 = add_1106 = None
        slice_scatter_6625: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6622, slice_scatter_6624, 1, 8832, 8848);  slice_scatter_6622 = slice_scatter_6624 = None
        slice_36446: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6625, 1, 8832, 8848)
        slice_36447: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36446, 2, 0, 16)
        slice_scatter_6627: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36446, slice_36447, 2, 0, 16);  slice_36446 = slice_36447 = None
        slice_scatter_6628: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6625, slice_scatter_6627, 1, 8832, 8848);  slice_scatter_6625 = slice_scatter_6627 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36467: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_36433, 2, 16, 32);  slice_36433 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1108: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_36467, memory_format = torch.contiguous_format);  slice_36467 = None
        view_2220: "f32[32, 11]" = torch.ops.aten.view.default(clone_1108, [32, 11]);  clone_1108 = None
        mm_1105: "f32[32, 8]" = torch.ops.aten.mm.default(view_2220, slice_37)
        view_2221: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1105, [2, 16, 8]);  mm_1105 = None
        slice_36474: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6628, 1, 8832, 8848)
        slice_36475: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36474, 2, 0, 16)
        add_1107: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36475, view_2221);  slice_36475 = view_2221 = None
        slice_scatter_6630: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36474, add_1107, 2, 0, 16);  slice_36474 = add_1107 = None
        slice_scatter_6631: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6628, slice_scatter_6630, 1, 8832, 8848);  slice_scatter_6628 = slice_scatter_6630 = None
        slice_36479: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6631, 1, 8832, 8848)
        slice_36480: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36479, 2, 0, 16)
        slice_scatter_6633: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36479, slice_36480, 2, 0, 16);  slice_36479 = slice_36480 = None
        slice_scatter_6634: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6631, slice_scatter_6633, 1, 8832, 8848);  slice_scatter_6631 = slice_scatter_6633 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36499: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8848, 8864)
        slice_36500: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_36499, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1109: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_36500, memory_format = torch.contiguous_format);  slice_36500 = None
        view_2222: "f32[32, 16]" = torch.ops.aten.view.default(clone_1109, [32, 16]);  clone_1109 = None
        mm_1106: "f32[32, 8]" = torch.ops.aten.mm.default(view_2222, slice_7)
        view_2223: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1106, [2, 16, 8]);  mm_1106 = None
        slice_36507: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6634, 1, 8848, 8864)
        slice_36508: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36507, 2, 0, 16)
        add_1108: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36508, view_2223);  slice_36508 = view_2223 = None
        slice_scatter_6636: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36507, add_1108, 2, 0, 16);  slice_36507 = add_1108 = None
        slice_scatter_6637: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6634, slice_scatter_6636, 1, 8848, 8864);  slice_scatter_6634 = slice_scatter_6636 = None
        slice_36512: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6637, 1, 8848, 8864)
        slice_36513: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36512, 2, 0, 16)
        slice_scatter_6639: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36512, slice_36513, 2, 0, 16);  slice_36512 = slice_36513 = None
        slice_scatter_6640: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6637, slice_scatter_6639, 1, 8848, 8864);  slice_scatter_6637 = slice_scatter_6639 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36533: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_36499, 2, 16, 32);  slice_36499 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1110: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_36533, memory_format = torch.contiguous_format);  slice_36533 = None
        view_2224: "f32[32, 11]" = torch.ops.aten.view.default(clone_1110, [32, 11]);  clone_1110 = None
        mm_1107: "f32[32, 8]" = torch.ops.aten.mm.default(view_2224, slice_37)
        view_2225: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1107, [2, 16, 8]);  mm_1107 = None
        slice_36540: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6640, 1, 8848, 8864)
        slice_36541: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36540, 2, 0, 16)
        add_1109: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36541, view_2225);  slice_36541 = view_2225 = None
        slice_scatter_6642: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36540, add_1109, 2, 0, 16);  slice_36540 = add_1109 = None
        slice_scatter_6643: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6640, slice_scatter_6642, 1, 8848, 8864);  slice_scatter_6640 = slice_scatter_6642 = None
        slice_36545: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6643, 1, 8848, 8864)
        slice_36546: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36545, 2, 0, 16)
        slice_scatter_6645: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36545, slice_36546, 2, 0, 16);  slice_36545 = slice_36546 = None
        slice_scatter_6646: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6643, slice_scatter_6645, 1, 8848, 8864);  slice_scatter_6643 = slice_scatter_6645 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36565: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8864, 8880)
        slice_36566: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_36565, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1111: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_36566, memory_format = torch.contiguous_format);  slice_36566 = None
        view_2226: "f32[32, 16]" = torch.ops.aten.view.default(clone_1111, [32, 16]);  clone_1111 = None
        mm_1108: "f32[32, 8]" = torch.ops.aten.mm.default(view_2226, slice_7)
        view_2227: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1108, [2, 16, 8]);  mm_1108 = None
        slice_36573: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6646, 1, 8864, 8880)
        slice_36574: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36573, 2, 0, 16)
        add_1110: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36574, view_2227);  slice_36574 = view_2227 = None
        slice_scatter_6648: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36573, add_1110, 2, 0, 16);  slice_36573 = add_1110 = None
        slice_scatter_6649: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6646, slice_scatter_6648, 1, 8864, 8880);  slice_scatter_6646 = slice_scatter_6648 = None
        slice_36578: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6649, 1, 8864, 8880)
        slice_36579: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36578, 2, 0, 16)
        slice_scatter_6651: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36578, slice_36579, 2, 0, 16);  slice_36578 = slice_36579 = None
        slice_scatter_6652: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6649, slice_scatter_6651, 1, 8864, 8880);  slice_scatter_6649 = slice_scatter_6651 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36599: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_36565, 2, 16, 32);  slice_36565 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1112: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_36599, memory_format = torch.contiguous_format);  slice_36599 = None
        view_2228: "f32[32, 11]" = torch.ops.aten.view.default(clone_1112, [32, 11]);  clone_1112 = None
        mm_1109: "f32[32, 8]" = torch.ops.aten.mm.default(view_2228, slice_37)
        view_2229: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1109, [2, 16, 8]);  mm_1109 = None
        slice_36606: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6652, 1, 8864, 8880)
        slice_36607: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36606, 2, 0, 16)
        add_1111: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36607, view_2229);  slice_36607 = view_2229 = None
        slice_scatter_6654: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36606, add_1111, 2, 0, 16);  slice_36606 = add_1111 = None
        slice_scatter_6655: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6652, slice_scatter_6654, 1, 8864, 8880);  slice_scatter_6652 = slice_scatter_6654 = None
        slice_36611: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6655, 1, 8864, 8880)
        slice_36612: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36611, 2, 0, 16)
        slice_scatter_6657: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36611, slice_36612, 2, 0, 16);  slice_36611 = slice_36612 = None
        slice_scatter_6658: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6655, slice_scatter_6657, 1, 8864, 8880);  slice_scatter_6655 = slice_scatter_6657 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36631: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8880, 8896)
        slice_36632: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_36631, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1113: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_36632, memory_format = torch.contiguous_format);  slice_36632 = None
        view_2230: "f32[32, 16]" = torch.ops.aten.view.default(clone_1113, [32, 16]);  clone_1113 = None
        mm_1110: "f32[32, 8]" = torch.ops.aten.mm.default(view_2230, slice_7)
        view_2231: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1110, [2, 16, 8]);  mm_1110 = None
        slice_36639: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6658, 1, 8880, 8896)
        slice_36640: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36639, 2, 0, 16)
        add_1112: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36640, view_2231);  slice_36640 = view_2231 = None
        slice_scatter_6660: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36639, add_1112, 2, 0, 16);  slice_36639 = add_1112 = None
        slice_scatter_6661: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6658, slice_scatter_6660, 1, 8880, 8896);  slice_scatter_6658 = slice_scatter_6660 = None
        slice_36644: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6661, 1, 8880, 8896)
        slice_36645: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36644, 2, 0, 16)
        slice_scatter_6663: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36644, slice_36645, 2, 0, 16);  slice_36644 = slice_36645 = None
        slice_scatter_6664: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6661, slice_scatter_6663, 1, 8880, 8896);  slice_scatter_6661 = slice_scatter_6663 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36665: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_36631, 2, 16, 32);  slice_36631 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1114: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_36665, memory_format = torch.contiguous_format);  slice_36665 = None
        view_2232: "f32[32, 11]" = torch.ops.aten.view.default(clone_1114, [32, 11]);  clone_1114 = None
        mm_1111: "f32[32, 8]" = torch.ops.aten.mm.default(view_2232, slice_37)
        view_2233: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1111, [2, 16, 8]);  mm_1111 = None
        slice_36672: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6664, 1, 8880, 8896)
        slice_36673: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36672, 2, 0, 16)
        add_1113: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36673, view_2233);  slice_36673 = view_2233 = None
        slice_scatter_6666: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36672, add_1113, 2, 0, 16);  slice_36672 = add_1113 = None
        slice_scatter_6667: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6664, slice_scatter_6666, 1, 8880, 8896);  slice_scatter_6664 = slice_scatter_6666 = None
        slice_36677: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6667, 1, 8880, 8896)
        slice_36678: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36677, 2, 0, 16)
        slice_scatter_6669: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36677, slice_36678, 2, 0, 16);  slice_36677 = slice_36678 = None
        slice_scatter_6670: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6667, slice_scatter_6669, 1, 8880, 8896);  slice_scatter_6667 = slice_scatter_6669 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36697: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8896, 8912)
        slice_36698: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_36697, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1115: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_36698, memory_format = torch.contiguous_format);  slice_36698 = None
        view_2234: "f32[32, 16]" = torch.ops.aten.view.default(clone_1115, [32, 16]);  clone_1115 = None
        mm_1112: "f32[32, 8]" = torch.ops.aten.mm.default(view_2234, slice_7)
        view_2235: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1112, [2, 16, 8]);  mm_1112 = None
        slice_36705: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6670, 1, 8896, 8912)
        slice_36706: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36705, 2, 0, 16)
        add_1114: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36706, view_2235);  slice_36706 = view_2235 = None
        slice_scatter_6672: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36705, add_1114, 2, 0, 16);  slice_36705 = add_1114 = None
        slice_scatter_6673: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6670, slice_scatter_6672, 1, 8896, 8912);  slice_scatter_6670 = slice_scatter_6672 = None
        slice_36710: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6673, 1, 8896, 8912)
        slice_36711: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36710, 2, 0, 16)
        slice_scatter_6675: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36710, slice_36711, 2, 0, 16);  slice_36710 = slice_36711 = None
        slice_scatter_6676: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6673, slice_scatter_6675, 1, 8896, 8912);  slice_scatter_6673 = slice_scatter_6675 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36731: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_36697, 2, 16, 32);  slice_36697 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1116: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_36731, memory_format = torch.contiguous_format);  slice_36731 = None
        view_2236: "f32[32, 11]" = torch.ops.aten.view.default(clone_1116, [32, 11]);  clone_1116 = None
        mm_1113: "f32[32, 8]" = torch.ops.aten.mm.default(view_2236, slice_37)
        view_2237: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1113, [2, 16, 8]);  mm_1113 = None
        slice_36738: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6676, 1, 8896, 8912)
        slice_36739: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36738, 2, 0, 16)
        add_1115: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36739, view_2237);  slice_36739 = view_2237 = None
        slice_scatter_6678: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36738, add_1115, 2, 0, 16);  slice_36738 = add_1115 = None
        slice_scatter_6679: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6676, slice_scatter_6678, 1, 8896, 8912);  slice_scatter_6676 = slice_scatter_6678 = None
        slice_36743: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6679, 1, 8896, 8912)
        slice_36744: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36743, 2, 0, 16)
        slice_scatter_6681: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36743, slice_36744, 2, 0, 16);  slice_36743 = slice_36744 = None
        slice_scatter_6682: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6679, slice_scatter_6681, 1, 8896, 8912);  slice_scatter_6679 = slice_scatter_6681 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36763: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8912, 8928)
        slice_36764: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_36763, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1117: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_36764, memory_format = torch.contiguous_format);  slice_36764 = None
        view_2238: "f32[32, 16]" = torch.ops.aten.view.default(clone_1117, [32, 16]);  clone_1117 = None
        mm_1114: "f32[32, 8]" = torch.ops.aten.mm.default(view_2238, slice_7)
        view_2239: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1114, [2, 16, 8]);  mm_1114 = None
        slice_36771: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6682, 1, 8912, 8928)
        slice_36772: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36771, 2, 0, 16)
        add_1116: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36772, view_2239);  slice_36772 = view_2239 = None
        slice_scatter_6684: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36771, add_1116, 2, 0, 16);  slice_36771 = add_1116 = None
        slice_scatter_6685: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6682, slice_scatter_6684, 1, 8912, 8928);  slice_scatter_6682 = slice_scatter_6684 = None
        slice_36776: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6685, 1, 8912, 8928)
        slice_36777: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36776, 2, 0, 16)
        slice_scatter_6687: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36776, slice_36777, 2, 0, 16);  slice_36776 = slice_36777 = None
        slice_scatter_6688: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6685, slice_scatter_6687, 1, 8912, 8928);  slice_scatter_6685 = slice_scatter_6687 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36797: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_36763, 2, 16, 32);  slice_36763 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1118: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_36797, memory_format = torch.contiguous_format);  slice_36797 = None
        view_2240: "f32[32, 11]" = torch.ops.aten.view.default(clone_1118, [32, 11]);  clone_1118 = None
        mm_1115: "f32[32, 8]" = torch.ops.aten.mm.default(view_2240, slice_37)
        view_2241: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1115, [2, 16, 8]);  mm_1115 = None
        slice_36804: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6688, 1, 8912, 8928)
        slice_36805: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36804, 2, 0, 16)
        add_1117: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36805, view_2241);  slice_36805 = view_2241 = None
        slice_scatter_6690: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36804, add_1117, 2, 0, 16);  slice_36804 = add_1117 = None
        slice_scatter_6691: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6688, slice_scatter_6690, 1, 8912, 8928);  slice_scatter_6688 = slice_scatter_6690 = None
        slice_36809: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6691, 1, 8912, 8928)
        slice_36810: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36809, 2, 0, 16)
        slice_scatter_6693: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36809, slice_36810, 2, 0, 16);  slice_36809 = slice_36810 = None
        slice_scatter_6694: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6691, slice_scatter_6693, 1, 8912, 8928);  slice_scatter_6691 = slice_scatter_6693 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36829: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8928, 8944)
        slice_36830: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_36829, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1119: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_36830, memory_format = torch.contiguous_format);  slice_36830 = None
        view_2242: "f32[32, 16]" = torch.ops.aten.view.default(clone_1119, [32, 16]);  clone_1119 = None
        mm_1116: "f32[32, 8]" = torch.ops.aten.mm.default(view_2242, slice_7)
        view_2243: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1116, [2, 16, 8]);  mm_1116 = None
        slice_36837: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6694, 1, 8928, 8944)
        slice_36838: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36837, 2, 0, 16)
        add_1118: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36838, view_2243);  slice_36838 = view_2243 = None
        slice_scatter_6696: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36837, add_1118, 2, 0, 16);  slice_36837 = add_1118 = None
        slice_scatter_6697: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6694, slice_scatter_6696, 1, 8928, 8944);  slice_scatter_6694 = slice_scatter_6696 = None
        slice_36842: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6697, 1, 8928, 8944)
        slice_36843: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36842, 2, 0, 16)
        slice_scatter_6699: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36842, slice_36843, 2, 0, 16);  slice_36842 = slice_36843 = None
        slice_scatter_6700: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6697, slice_scatter_6699, 1, 8928, 8944);  slice_scatter_6697 = slice_scatter_6699 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36863: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_36829, 2, 16, 32);  slice_36829 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1120: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_36863, memory_format = torch.contiguous_format);  slice_36863 = None
        view_2244: "f32[32, 11]" = torch.ops.aten.view.default(clone_1120, [32, 11]);  clone_1120 = None
        mm_1117: "f32[32, 8]" = torch.ops.aten.mm.default(view_2244, slice_37)
        view_2245: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1117, [2, 16, 8]);  mm_1117 = None
        slice_36870: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6700, 1, 8928, 8944)
        slice_36871: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36870, 2, 0, 16)
        add_1119: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36871, view_2245);  slice_36871 = view_2245 = None
        slice_scatter_6702: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36870, add_1119, 2, 0, 16);  slice_36870 = add_1119 = None
        slice_scatter_6703: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6700, slice_scatter_6702, 1, 8928, 8944);  slice_scatter_6700 = slice_scatter_6702 = None
        slice_36875: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6703, 1, 8928, 8944)
        slice_36876: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36875, 2, 0, 16)
        slice_scatter_6705: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36875, slice_36876, 2, 0, 16);  slice_36875 = slice_36876 = None
        slice_scatter_6706: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6703, slice_scatter_6705, 1, 8928, 8944);  slice_scatter_6703 = slice_scatter_6705 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36895: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8944, 8960)
        slice_36896: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_36895, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1121: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_36896, memory_format = torch.contiguous_format);  slice_36896 = None
        view_2246: "f32[32, 16]" = torch.ops.aten.view.default(clone_1121, [32, 16]);  clone_1121 = None
        mm_1118: "f32[32, 8]" = torch.ops.aten.mm.default(view_2246, slice_7)
        view_2247: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1118, [2, 16, 8]);  mm_1118 = None
        slice_36903: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6706, 1, 8944, 8960)
        slice_36904: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36903, 2, 0, 16)
        add_1120: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36904, view_2247);  slice_36904 = view_2247 = None
        slice_scatter_6708: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36903, add_1120, 2, 0, 16);  slice_36903 = add_1120 = None
        slice_scatter_6709: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6706, slice_scatter_6708, 1, 8944, 8960);  slice_scatter_6706 = slice_scatter_6708 = None
        slice_36908: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6709, 1, 8944, 8960)
        slice_36909: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36908, 2, 0, 16)
        slice_scatter_6711: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36908, slice_36909, 2, 0, 16);  slice_36908 = slice_36909 = None
        slice_scatter_6712: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6709, slice_scatter_6711, 1, 8944, 8960);  slice_scatter_6709 = slice_scatter_6711 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36929: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_36895, 2, 16, 32);  slice_36895 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1122: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_36929, memory_format = torch.contiguous_format);  slice_36929 = None
        view_2248: "f32[32, 11]" = torch.ops.aten.view.default(clone_1122, [32, 11]);  clone_1122 = None
        mm_1119: "f32[32, 8]" = torch.ops.aten.mm.default(view_2248, slice_37)
        view_2249: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1119, [2, 16, 8]);  mm_1119 = None
        slice_36936: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6712, 1, 8944, 8960)
        slice_36937: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36936, 2, 0, 16)
        add_1121: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36937, view_2249);  slice_36937 = view_2249 = None
        slice_scatter_6714: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36936, add_1121, 2, 0, 16);  slice_36936 = add_1121 = None
        slice_scatter_6715: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6712, slice_scatter_6714, 1, 8944, 8960);  slice_scatter_6712 = slice_scatter_6714 = None
        slice_36941: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6715, 1, 8944, 8960)
        slice_36942: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36941, 2, 0, 16)
        slice_scatter_6717: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36941, slice_36942, 2, 0, 16);  slice_36941 = slice_36942 = None
        slice_scatter_6718: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6715, slice_scatter_6717, 1, 8944, 8960);  slice_scatter_6715 = slice_scatter_6717 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36961: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8960, 8976)
        slice_36962: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_36961, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1123: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_36962, memory_format = torch.contiguous_format);  slice_36962 = None
        view_2250: "f32[32, 16]" = torch.ops.aten.view.default(clone_1123, [32, 16]);  clone_1123 = None
        mm_1120: "f32[32, 8]" = torch.ops.aten.mm.default(view_2250, slice_7)
        view_2251: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1120, [2, 16, 8]);  mm_1120 = None
        slice_36969: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6718, 1, 8960, 8976)
        slice_36970: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36969, 2, 0, 16)
        add_1122: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_36970, view_2251);  slice_36970 = view_2251 = None
        slice_scatter_6720: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36969, add_1122, 2, 0, 16);  slice_36969 = add_1122 = None
        slice_scatter_6721: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6718, slice_scatter_6720, 1, 8960, 8976);  slice_scatter_6718 = slice_scatter_6720 = None
        slice_36974: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6721, 1, 8960, 8976)
        slice_36975: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_36974, 2, 0, 16)
        slice_scatter_6723: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_36974, slice_36975, 2, 0, 16);  slice_36974 = slice_36975 = None
        slice_scatter_6724: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6721, slice_scatter_6723, 1, 8960, 8976);  slice_scatter_6721 = slice_scatter_6723 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_36995: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_36961, 2, 16, 32);  slice_36961 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1124: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_36995, memory_format = torch.contiguous_format);  slice_36995 = None
        view_2252: "f32[32, 11]" = torch.ops.aten.view.default(clone_1124, [32, 11]);  clone_1124 = None
        mm_1121: "f32[32, 8]" = torch.ops.aten.mm.default(view_2252, slice_37)
        view_2253: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1121, [2, 16, 8]);  mm_1121 = None
        slice_37002: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6724, 1, 8960, 8976)
        slice_37003: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37002, 2, 0, 16)
        add_1123: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37003, view_2253);  slice_37003 = view_2253 = None
        slice_scatter_6726: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37002, add_1123, 2, 0, 16);  slice_37002 = add_1123 = None
        slice_scatter_6727: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6724, slice_scatter_6726, 1, 8960, 8976);  slice_scatter_6724 = slice_scatter_6726 = None
        slice_37007: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6727, 1, 8960, 8976)
        slice_37008: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37007, 2, 0, 16)
        slice_scatter_6729: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37007, slice_37008, 2, 0, 16);  slice_37007 = slice_37008 = None
        slice_scatter_6730: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6727, slice_scatter_6729, 1, 8960, 8976);  slice_scatter_6727 = slice_scatter_6729 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37027: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8976, 8992)
        slice_37028: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_37027, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1125: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_37028, memory_format = torch.contiguous_format);  slice_37028 = None
        view_2254: "f32[32, 16]" = torch.ops.aten.view.default(clone_1125, [32, 16]);  clone_1125 = None
        mm_1122: "f32[32, 8]" = torch.ops.aten.mm.default(view_2254, slice_7)
        view_2255: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1122, [2, 16, 8]);  mm_1122 = None
        slice_37035: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6730, 1, 8976, 8992)
        slice_37036: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37035, 2, 0, 16)
        add_1124: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37036, view_2255);  slice_37036 = view_2255 = None
        slice_scatter_6732: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37035, add_1124, 2, 0, 16);  slice_37035 = add_1124 = None
        slice_scatter_6733: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6730, slice_scatter_6732, 1, 8976, 8992);  slice_scatter_6730 = slice_scatter_6732 = None
        slice_37040: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6733, 1, 8976, 8992)
        slice_37041: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37040, 2, 0, 16)
        slice_scatter_6735: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37040, slice_37041, 2, 0, 16);  slice_37040 = slice_37041 = None
        slice_scatter_6736: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6733, slice_scatter_6735, 1, 8976, 8992);  slice_scatter_6733 = slice_scatter_6735 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37061: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_37027, 2, 16, 32);  slice_37027 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1126: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_37061, memory_format = torch.contiguous_format);  slice_37061 = None
        view_2256: "f32[32, 11]" = torch.ops.aten.view.default(clone_1126, [32, 11]);  clone_1126 = None
        mm_1123: "f32[32, 8]" = torch.ops.aten.mm.default(view_2256, slice_37)
        view_2257: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1123, [2, 16, 8]);  mm_1123 = None
        slice_37068: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6736, 1, 8976, 8992)
        slice_37069: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37068, 2, 0, 16)
        add_1125: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37069, view_2257);  slice_37069 = view_2257 = None
        slice_scatter_6738: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37068, add_1125, 2, 0, 16);  slice_37068 = add_1125 = None
        slice_scatter_6739: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6736, slice_scatter_6738, 1, 8976, 8992);  slice_scatter_6736 = slice_scatter_6738 = None
        slice_37073: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6739, 1, 8976, 8992)
        slice_37074: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37073, 2, 0, 16)
        slice_scatter_6741: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37073, slice_37074, 2, 0, 16);  slice_37073 = slice_37074 = None
        slice_scatter_6742: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6739, slice_scatter_6741, 1, 8976, 8992);  slice_scatter_6739 = slice_scatter_6741 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37093: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 8992, 9008)
        slice_37094: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_37093, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1127: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_37094, memory_format = torch.contiguous_format);  slice_37094 = None
        view_2258: "f32[32, 16]" = torch.ops.aten.view.default(clone_1127, [32, 16]);  clone_1127 = None
        mm_1124: "f32[32, 8]" = torch.ops.aten.mm.default(view_2258, slice_7)
        view_2259: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1124, [2, 16, 8]);  mm_1124 = None
        slice_37101: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6742, 1, 8992, 9008)
        slice_37102: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37101, 2, 0, 16)
        add_1126: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37102, view_2259);  slice_37102 = view_2259 = None
        slice_scatter_6744: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37101, add_1126, 2, 0, 16);  slice_37101 = add_1126 = None
        slice_scatter_6745: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6742, slice_scatter_6744, 1, 8992, 9008);  slice_scatter_6742 = slice_scatter_6744 = None
        slice_37106: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6745, 1, 8992, 9008)
        slice_37107: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37106, 2, 0, 16)
        slice_scatter_6747: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37106, slice_37107, 2, 0, 16);  slice_37106 = slice_37107 = None
        slice_scatter_6748: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6745, slice_scatter_6747, 1, 8992, 9008);  slice_scatter_6745 = slice_scatter_6747 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37127: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_37093, 2, 16, 32);  slice_37093 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1128: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_37127, memory_format = torch.contiguous_format);  slice_37127 = None
        view_2260: "f32[32, 11]" = torch.ops.aten.view.default(clone_1128, [32, 11]);  clone_1128 = None
        mm_1125: "f32[32, 8]" = torch.ops.aten.mm.default(view_2260, slice_37)
        view_2261: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1125, [2, 16, 8]);  mm_1125 = None
        slice_37134: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6748, 1, 8992, 9008)
        slice_37135: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37134, 2, 0, 16)
        add_1127: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37135, view_2261);  slice_37135 = view_2261 = None
        slice_scatter_6750: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37134, add_1127, 2, 0, 16);  slice_37134 = add_1127 = None
        slice_scatter_6751: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6748, slice_scatter_6750, 1, 8992, 9008);  slice_scatter_6748 = slice_scatter_6750 = None
        slice_37139: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6751, 1, 8992, 9008)
        slice_37140: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37139, 2, 0, 16)
        slice_scatter_6753: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37139, slice_37140, 2, 0, 16);  slice_37139 = slice_37140 = None
        slice_scatter_6754: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6751, slice_scatter_6753, 1, 8992, 9008);  slice_scatter_6751 = slice_scatter_6753 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37159: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9008, 9024)
        slice_37160: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_37159, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1129: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_37160, memory_format = torch.contiguous_format);  slice_37160 = None
        view_2262: "f32[32, 16]" = torch.ops.aten.view.default(clone_1129, [32, 16]);  clone_1129 = None
        mm_1126: "f32[32, 8]" = torch.ops.aten.mm.default(view_2262, slice_7)
        view_2263: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1126, [2, 16, 8]);  mm_1126 = None
        slice_37167: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6754, 1, 9008, 9024)
        slice_37168: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37167, 2, 0, 16)
        add_1128: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37168, view_2263);  slice_37168 = view_2263 = None
        slice_scatter_6756: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37167, add_1128, 2, 0, 16);  slice_37167 = add_1128 = None
        slice_scatter_6757: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6754, slice_scatter_6756, 1, 9008, 9024);  slice_scatter_6754 = slice_scatter_6756 = None
        slice_37172: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6757, 1, 9008, 9024)
        slice_37173: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37172, 2, 0, 16)
        slice_scatter_6759: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37172, slice_37173, 2, 0, 16);  slice_37172 = slice_37173 = None
        slice_scatter_6760: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6757, slice_scatter_6759, 1, 9008, 9024);  slice_scatter_6757 = slice_scatter_6759 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37193: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_37159, 2, 16, 32);  slice_37159 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1130: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_37193, memory_format = torch.contiguous_format);  slice_37193 = None
        view_2264: "f32[32, 11]" = torch.ops.aten.view.default(clone_1130, [32, 11]);  clone_1130 = None
        mm_1127: "f32[32, 8]" = torch.ops.aten.mm.default(view_2264, slice_37)
        view_2265: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1127, [2, 16, 8]);  mm_1127 = None
        slice_37200: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6760, 1, 9008, 9024)
        slice_37201: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37200, 2, 0, 16)
        add_1129: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37201, view_2265);  slice_37201 = view_2265 = None
        slice_scatter_6762: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37200, add_1129, 2, 0, 16);  slice_37200 = add_1129 = None
        slice_scatter_6763: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6760, slice_scatter_6762, 1, 9008, 9024);  slice_scatter_6760 = slice_scatter_6762 = None
        slice_37205: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6763, 1, 9008, 9024)
        slice_37206: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37205, 2, 0, 16)
        slice_scatter_6765: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37205, slice_37206, 2, 0, 16);  slice_37205 = slice_37206 = None
        slice_scatter_6766: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6763, slice_scatter_6765, 1, 9008, 9024);  slice_scatter_6763 = slice_scatter_6765 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37225: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9024, 9040)
        slice_37226: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_37225, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1131: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_37226, memory_format = torch.contiguous_format);  slice_37226 = None
        view_2266: "f32[32, 16]" = torch.ops.aten.view.default(clone_1131, [32, 16]);  clone_1131 = None
        mm_1128: "f32[32, 8]" = torch.ops.aten.mm.default(view_2266, slice_7)
        view_2267: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1128, [2, 16, 8]);  mm_1128 = None
        slice_37233: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6766, 1, 9024, 9040)
        slice_37234: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37233, 2, 0, 16)
        add_1130: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37234, view_2267);  slice_37234 = view_2267 = None
        slice_scatter_6768: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37233, add_1130, 2, 0, 16);  slice_37233 = add_1130 = None
        slice_scatter_6769: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6766, slice_scatter_6768, 1, 9024, 9040);  slice_scatter_6766 = slice_scatter_6768 = None
        slice_37238: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6769, 1, 9024, 9040)
        slice_37239: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37238, 2, 0, 16)
        slice_scatter_6771: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37238, slice_37239, 2, 0, 16);  slice_37238 = slice_37239 = None
        slice_scatter_6772: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6769, slice_scatter_6771, 1, 9024, 9040);  slice_scatter_6769 = slice_scatter_6771 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37259: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_37225, 2, 16, 32);  slice_37225 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1132: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_37259, memory_format = torch.contiguous_format);  slice_37259 = None
        view_2268: "f32[32, 11]" = torch.ops.aten.view.default(clone_1132, [32, 11]);  clone_1132 = None
        mm_1129: "f32[32, 8]" = torch.ops.aten.mm.default(view_2268, slice_37)
        view_2269: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1129, [2, 16, 8]);  mm_1129 = None
        slice_37266: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6772, 1, 9024, 9040)
        slice_37267: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37266, 2, 0, 16)
        add_1131: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37267, view_2269);  slice_37267 = view_2269 = None
        slice_scatter_6774: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37266, add_1131, 2, 0, 16);  slice_37266 = add_1131 = None
        slice_scatter_6775: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6772, slice_scatter_6774, 1, 9024, 9040);  slice_scatter_6772 = slice_scatter_6774 = None
        slice_37271: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6775, 1, 9024, 9040)
        slice_37272: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37271, 2, 0, 16)
        slice_scatter_6777: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37271, slice_37272, 2, 0, 16);  slice_37271 = slice_37272 = None
        slice_scatter_6778: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6775, slice_scatter_6777, 1, 9024, 9040);  slice_scatter_6775 = slice_scatter_6777 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37291: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9040, 9056)
        slice_37292: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_37291, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1133: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_37292, memory_format = torch.contiguous_format);  slice_37292 = None
        view_2270: "f32[32, 16]" = torch.ops.aten.view.default(clone_1133, [32, 16]);  clone_1133 = None
        mm_1130: "f32[32, 8]" = torch.ops.aten.mm.default(view_2270, slice_7)
        view_2271: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1130, [2, 16, 8]);  mm_1130 = None
        slice_37299: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6778, 1, 9040, 9056)
        slice_37300: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37299, 2, 0, 16)
        add_1132: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37300, view_2271);  slice_37300 = view_2271 = None
        slice_scatter_6780: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37299, add_1132, 2, 0, 16);  slice_37299 = add_1132 = None
        slice_scatter_6781: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6778, slice_scatter_6780, 1, 9040, 9056);  slice_scatter_6778 = slice_scatter_6780 = None
        slice_37304: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6781, 1, 9040, 9056)
        slice_37305: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37304, 2, 0, 16)
        slice_scatter_6783: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37304, slice_37305, 2, 0, 16);  slice_37304 = slice_37305 = None
        slice_scatter_6784: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6781, slice_scatter_6783, 1, 9040, 9056);  slice_scatter_6781 = slice_scatter_6783 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37325: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_37291, 2, 16, 32);  slice_37291 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1134: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_37325, memory_format = torch.contiguous_format);  slice_37325 = None
        view_2272: "f32[32, 11]" = torch.ops.aten.view.default(clone_1134, [32, 11]);  clone_1134 = None
        mm_1131: "f32[32, 8]" = torch.ops.aten.mm.default(view_2272, slice_37)
        view_2273: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1131, [2, 16, 8]);  mm_1131 = None
        slice_37332: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6784, 1, 9040, 9056)
        slice_37333: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37332, 2, 0, 16)
        add_1133: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37333, view_2273);  slice_37333 = view_2273 = None
        slice_scatter_6786: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37332, add_1133, 2, 0, 16);  slice_37332 = add_1133 = None
        slice_scatter_6787: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6784, slice_scatter_6786, 1, 9040, 9056);  slice_scatter_6784 = slice_scatter_6786 = None
        slice_37337: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6787, 1, 9040, 9056)
        slice_37338: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37337, 2, 0, 16)
        slice_scatter_6789: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37337, slice_37338, 2, 0, 16);  slice_37337 = slice_37338 = None
        slice_scatter_6790: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6787, slice_scatter_6789, 1, 9040, 9056);  slice_scatter_6787 = slice_scatter_6789 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37357: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9056, 9072)
        slice_37358: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_37357, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1135: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_37358, memory_format = torch.contiguous_format);  slice_37358 = None
        view_2274: "f32[32, 16]" = torch.ops.aten.view.default(clone_1135, [32, 16]);  clone_1135 = None
        mm_1132: "f32[32, 8]" = torch.ops.aten.mm.default(view_2274, slice_7)
        view_2275: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1132, [2, 16, 8]);  mm_1132 = None
        slice_37365: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6790, 1, 9056, 9072)
        slice_37366: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37365, 2, 0, 16)
        add_1134: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37366, view_2275);  slice_37366 = view_2275 = None
        slice_scatter_6792: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37365, add_1134, 2, 0, 16);  slice_37365 = add_1134 = None
        slice_scatter_6793: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6790, slice_scatter_6792, 1, 9056, 9072);  slice_scatter_6790 = slice_scatter_6792 = None
        slice_37370: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6793, 1, 9056, 9072)
        slice_37371: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37370, 2, 0, 16)
        slice_scatter_6795: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37370, slice_37371, 2, 0, 16);  slice_37370 = slice_37371 = None
        slice_scatter_6796: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6793, slice_scatter_6795, 1, 9056, 9072);  slice_scatter_6793 = slice_scatter_6795 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37391: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_37357, 2, 16, 32);  slice_37357 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1136: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_37391, memory_format = torch.contiguous_format);  slice_37391 = None
        view_2276: "f32[32, 11]" = torch.ops.aten.view.default(clone_1136, [32, 11]);  clone_1136 = None
        mm_1133: "f32[32, 8]" = torch.ops.aten.mm.default(view_2276, slice_37)
        view_2277: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1133, [2, 16, 8]);  mm_1133 = None
        slice_37398: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6796, 1, 9056, 9072)
        slice_37399: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37398, 2, 0, 16)
        add_1135: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37399, view_2277);  slice_37399 = view_2277 = None
        slice_scatter_6798: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37398, add_1135, 2, 0, 16);  slice_37398 = add_1135 = None
        slice_scatter_6799: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6796, slice_scatter_6798, 1, 9056, 9072);  slice_scatter_6796 = slice_scatter_6798 = None
        slice_37403: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6799, 1, 9056, 9072)
        slice_37404: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37403, 2, 0, 16)
        slice_scatter_6801: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37403, slice_37404, 2, 0, 16);  slice_37403 = slice_37404 = None
        slice_scatter_6802: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6799, slice_scatter_6801, 1, 9056, 9072);  slice_scatter_6799 = slice_scatter_6801 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37423: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9072, 9088)
        slice_37424: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_37423, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1137: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_37424, memory_format = torch.contiguous_format);  slice_37424 = None
        view_2278: "f32[32, 16]" = torch.ops.aten.view.default(clone_1137, [32, 16]);  clone_1137 = None
        mm_1134: "f32[32, 8]" = torch.ops.aten.mm.default(view_2278, slice_7)
        view_2279: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1134, [2, 16, 8]);  mm_1134 = None
        slice_37431: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6802, 1, 9072, 9088)
        slice_37432: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37431, 2, 0, 16)
        add_1136: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37432, view_2279);  slice_37432 = view_2279 = None
        slice_scatter_6804: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37431, add_1136, 2, 0, 16);  slice_37431 = add_1136 = None
        slice_scatter_6805: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6802, slice_scatter_6804, 1, 9072, 9088);  slice_scatter_6802 = slice_scatter_6804 = None
        slice_37436: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6805, 1, 9072, 9088)
        slice_37437: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37436, 2, 0, 16)
        slice_scatter_6807: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37436, slice_37437, 2, 0, 16);  slice_37436 = slice_37437 = None
        slice_scatter_6808: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6805, slice_scatter_6807, 1, 9072, 9088);  slice_scatter_6805 = slice_scatter_6807 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37457: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_37423, 2, 16, 32);  slice_37423 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1138: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_37457, memory_format = torch.contiguous_format);  slice_37457 = None
        view_2280: "f32[32, 11]" = torch.ops.aten.view.default(clone_1138, [32, 11]);  clone_1138 = None
        mm_1135: "f32[32, 8]" = torch.ops.aten.mm.default(view_2280, slice_37)
        view_2281: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1135, [2, 16, 8]);  mm_1135 = None
        slice_37464: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6808, 1, 9072, 9088)
        slice_37465: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37464, 2, 0, 16)
        add_1137: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37465, view_2281);  slice_37465 = view_2281 = None
        slice_scatter_6810: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37464, add_1137, 2, 0, 16);  slice_37464 = add_1137 = None
        slice_scatter_6811: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6808, slice_scatter_6810, 1, 9072, 9088);  slice_scatter_6808 = slice_scatter_6810 = None
        slice_37469: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6811, 1, 9072, 9088)
        slice_37470: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37469, 2, 0, 16)
        slice_scatter_6813: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37469, slice_37470, 2, 0, 16);  slice_37469 = slice_37470 = None
        slice_scatter_6814: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6811, slice_scatter_6813, 1, 9072, 9088);  slice_scatter_6811 = slice_scatter_6813 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37489: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9088, 9104)
        slice_37490: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_37489, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1139: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_37490, memory_format = torch.contiguous_format);  slice_37490 = None
        view_2282: "f32[32, 16]" = torch.ops.aten.view.default(clone_1139, [32, 16]);  clone_1139 = None
        mm_1136: "f32[32, 8]" = torch.ops.aten.mm.default(view_2282, slice_7)
        view_2283: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1136, [2, 16, 8]);  mm_1136 = None
        slice_37497: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6814, 1, 9088, 9104)
        slice_37498: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37497, 2, 0, 16)
        add_1138: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37498, view_2283);  slice_37498 = view_2283 = None
        slice_scatter_6816: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37497, add_1138, 2, 0, 16);  slice_37497 = add_1138 = None
        slice_scatter_6817: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6814, slice_scatter_6816, 1, 9088, 9104);  slice_scatter_6814 = slice_scatter_6816 = None
        slice_37502: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6817, 1, 9088, 9104)
        slice_37503: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37502, 2, 0, 16)
        slice_scatter_6819: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37502, slice_37503, 2, 0, 16);  slice_37502 = slice_37503 = None
        slice_scatter_6820: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6817, slice_scatter_6819, 1, 9088, 9104);  slice_scatter_6817 = slice_scatter_6819 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37523: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_37489, 2, 16, 32);  slice_37489 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1140: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_37523, memory_format = torch.contiguous_format);  slice_37523 = None
        view_2284: "f32[32, 11]" = torch.ops.aten.view.default(clone_1140, [32, 11]);  clone_1140 = None
        mm_1137: "f32[32, 8]" = torch.ops.aten.mm.default(view_2284, slice_37)
        view_2285: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1137, [2, 16, 8]);  mm_1137 = None
        slice_37530: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6820, 1, 9088, 9104)
        slice_37531: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37530, 2, 0, 16)
        add_1139: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37531, view_2285);  slice_37531 = view_2285 = None
        slice_scatter_6822: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37530, add_1139, 2, 0, 16);  slice_37530 = add_1139 = None
        slice_scatter_6823: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6820, slice_scatter_6822, 1, 9088, 9104);  slice_scatter_6820 = slice_scatter_6822 = None
        slice_37535: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6823, 1, 9088, 9104)
        slice_37536: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37535, 2, 0, 16)
        slice_scatter_6825: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37535, slice_37536, 2, 0, 16);  slice_37535 = slice_37536 = None
        slice_scatter_6826: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6823, slice_scatter_6825, 1, 9088, 9104);  slice_scatter_6823 = slice_scatter_6825 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37555: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9104, 9120)
        slice_37556: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_37555, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1141: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_37556, memory_format = torch.contiguous_format);  slice_37556 = None
        view_2286: "f32[32, 16]" = torch.ops.aten.view.default(clone_1141, [32, 16]);  clone_1141 = None
        mm_1138: "f32[32, 8]" = torch.ops.aten.mm.default(view_2286, slice_7)
        view_2287: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1138, [2, 16, 8]);  mm_1138 = None
        slice_37563: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6826, 1, 9104, 9120)
        slice_37564: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37563, 2, 0, 16)
        add_1140: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37564, view_2287);  slice_37564 = view_2287 = None
        slice_scatter_6828: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37563, add_1140, 2, 0, 16);  slice_37563 = add_1140 = None
        slice_scatter_6829: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6826, slice_scatter_6828, 1, 9104, 9120);  slice_scatter_6826 = slice_scatter_6828 = None
        slice_37568: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6829, 1, 9104, 9120)
        slice_37569: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37568, 2, 0, 16)
        slice_scatter_6831: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37568, slice_37569, 2, 0, 16);  slice_37568 = slice_37569 = None
        slice_scatter_6832: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6829, slice_scatter_6831, 1, 9104, 9120);  slice_scatter_6829 = slice_scatter_6831 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37589: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_37555, 2, 16, 32);  slice_37555 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1142: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_37589, memory_format = torch.contiguous_format);  slice_37589 = None
        view_2288: "f32[32, 11]" = torch.ops.aten.view.default(clone_1142, [32, 11]);  clone_1142 = None
        mm_1139: "f32[32, 8]" = torch.ops.aten.mm.default(view_2288, slice_37)
        view_2289: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1139, [2, 16, 8]);  mm_1139 = None
        slice_37596: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6832, 1, 9104, 9120)
        slice_37597: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37596, 2, 0, 16)
        add_1141: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37597, view_2289);  slice_37597 = view_2289 = None
        slice_scatter_6834: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37596, add_1141, 2, 0, 16);  slice_37596 = add_1141 = None
        slice_scatter_6835: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6832, slice_scatter_6834, 1, 9104, 9120);  slice_scatter_6832 = slice_scatter_6834 = None
        slice_37601: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6835, 1, 9104, 9120)
        slice_37602: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37601, 2, 0, 16)
        slice_scatter_6837: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37601, slice_37602, 2, 0, 16);  slice_37601 = slice_37602 = None
        slice_scatter_6838: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6835, slice_scatter_6837, 1, 9104, 9120);  slice_scatter_6835 = slice_scatter_6837 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37621: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9120, 9136)
        slice_37622: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_37621, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1143: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_37622, memory_format = torch.contiguous_format);  slice_37622 = None
        view_2290: "f32[32, 16]" = torch.ops.aten.view.default(clone_1143, [32, 16]);  clone_1143 = None
        mm_1140: "f32[32, 8]" = torch.ops.aten.mm.default(view_2290, slice_7)
        view_2291: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1140, [2, 16, 8]);  mm_1140 = None
        slice_37629: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6838, 1, 9120, 9136)
        slice_37630: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37629, 2, 0, 16)
        add_1142: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37630, view_2291);  slice_37630 = view_2291 = None
        slice_scatter_6840: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37629, add_1142, 2, 0, 16);  slice_37629 = add_1142 = None
        slice_scatter_6841: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6838, slice_scatter_6840, 1, 9120, 9136);  slice_scatter_6838 = slice_scatter_6840 = None
        slice_37634: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6841, 1, 9120, 9136)
        slice_37635: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37634, 2, 0, 16)
        slice_scatter_6843: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37634, slice_37635, 2, 0, 16);  slice_37634 = slice_37635 = None
        slice_scatter_6844: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6841, slice_scatter_6843, 1, 9120, 9136);  slice_scatter_6841 = slice_scatter_6843 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37655: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_37621, 2, 16, 32);  slice_37621 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1144: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_37655, memory_format = torch.contiguous_format);  slice_37655 = None
        view_2292: "f32[32, 11]" = torch.ops.aten.view.default(clone_1144, [32, 11]);  clone_1144 = None
        mm_1141: "f32[32, 8]" = torch.ops.aten.mm.default(view_2292, slice_37)
        view_2293: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1141, [2, 16, 8]);  mm_1141 = None
        slice_37662: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6844, 1, 9120, 9136)
        slice_37663: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37662, 2, 0, 16)
        add_1143: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37663, view_2293);  slice_37663 = view_2293 = None
        slice_scatter_6846: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37662, add_1143, 2, 0, 16);  slice_37662 = add_1143 = None
        slice_scatter_6847: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6844, slice_scatter_6846, 1, 9120, 9136);  slice_scatter_6844 = slice_scatter_6846 = None
        slice_37667: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6847, 1, 9120, 9136)
        slice_37668: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37667, 2, 0, 16)
        slice_scatter_6849: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37667, slice_37668, 2, 0, 16);  slice_37667 = slice_37668 = None
        slice_scatter_6850: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6847, slice_scatter_6849, 1, 9120, 9136);  slice_scatter_6847 = slice_scatter_6849 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37687: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9136, 9152)
        slice_37688: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_37687, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1145: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_37688, memory_format = torch.contiguous_format);  slice_37688 = None
        view_2294: "f32[32, 16]" = torch.ops.aten.view.default(clone_1145, [32, 16]);  clone_1145 = None
        mm_1142: "f32[32, 8]" = torch.ops.aten.mm.default(view_2294, slice_7)
        view_2295: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1142, [2, 16, 8]);  mm_1142 = None
        slice_37695: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6850, 1, 9136, 9152)
        slice_37696: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37695, 2, 0, 16)
        add_1144: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37696, view_2295);  slice_37696 = view_2295 = None
        slice_scatter_6852: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37695, add_1144, 2, 0, 16);  slice_37695 = add_1144 = None
        slice_scatter_6853: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6850, slice_scatter_6852, 1, 9136, 9152);  slice_scatter_6850 = slice_scatter_6852 = None
        slice_37700: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6853, 1, 9136, 9152)
        slice_37701: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37700, 2, 0, 16)
        slice_scatter_6855: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37700, slice_37701, 2, 0, 16);  slice_37700 = slice_37701 = None
        slice_scatter_6856: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6853, slice_scatter_6855, 1, 9136, 9152);  slice_scatter_6853 = slice_scatter_6855 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37721: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_37687, 2, 16, 32);  slice_37687 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1146: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_37721, memory_format = torch.contiguous_format);  slice_37721 = None
        view_2296: "f32[32, 11]" = torch.ops.aten.view.default(clone_1146, [32, 11]);  clone_1146 = None
        mm_1143: "f32[32, 8]" = torch.ops.aten.mm.default(view_2296, slice_37)
        view_2297: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1143, [2, 16, 8]);  mm_1143 = None
        slice_37728: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6856, 1, 9136, 9152)
        slice_37729: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37728, 2, 0, 16)
        add_1145: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37729, view_2297);  slice_37729 = view_2297 = None
        slice_scatter_6858: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37728, add_1145, 2, 0, 16);  slice_37728 = add_1145 = None
        slice_scatter_6859: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6856, slice_scatter_6858, 1, 9136, 9152);  slice_scatter_6856 = slice_scatter_6858 = None
        slice_37733: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6859, 1, 9136, 9152)
        slice_37734: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37733, 2, 0, 16)
        slice_scatter_6861: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37733, slice_37734, 2, 0, 16);  slice_37733 = slice_37734 = None
        slice_scatter_6862: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6859, slice_scatter_6861, 1, 9136, 9152);  slice_scatter_6859 = slice_scatter_6861 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37753: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9152, 9168)
        slice_37754: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_37753, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1147: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_37754, memory_format = torch.contiguous_format);  slice_37754 = None
        view_2298: "f32[32, 16]" = torch.ops.aten.view.default(clone_1147, [32, 16]);  clone_1147 = None
        mm_1144: "f32[32, 8]" = torch.ops.aten.mm.default(view_2298, slice_7)
        view_2299: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1144, [2, 16, 8]);  mm_1144 = None
        slice_37761: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6862, 1, 9152, 9168)
        slice_37762: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37761, 2, 0, 16)
        add_1146: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37762, view_2299);  slice_37762 = view_2299 = None
        slice_scatter_6864: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37761, add_1146, 2, 0, 16);  slice_37761 = add_1146 = None
        slice_scatter_6865: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6862, slice_scatter_6864, 1, 9152, 9168);  slice_scatter_6862 = slice_scatter_6864 = None
        slice_37766: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6865, 1, 9152, 9168)
        slice_37767: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37766, 2, 0, 16)
        slice_scatter_6867: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37766, slice_37767, 2, 0, 16);  slice_37766 = slice_37767 = None
        slice_scatter_6868: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6865, slice_scatter_6867, 1, 9152, 9168);  slice_scatter_6865 = slice_scatter_6867 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37787: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_37753, 2, 16, 32);  slice_37753 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1148: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_37787, memory_format = torch.contiguous_format);  slice_37787 = None
        view_2300: "f32[32, 11]" = torch.ops.aten.view.default(clone_1148, [32, 11]);  clone_1148 = None
        mm_1145: "f32[32, 8]" = torch.ops.aten.mm.default(view_2300, slice_37)
        view_2301: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1145, [2, 16, 8]);  mm_1145 = None
        slice_37794: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6868, 1, 9152, 9168)
        slice_37795: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37794, 2, 0, 16)
        add_1147: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37795, view_2301);  slice_37795 = view_2301 = None
        slice_scatter_6870: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37794, add_1147, 2, 0, 16);  slice_37794 = add_1147 = None
        slice_scatter_6871: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6868, slice_scatter_6870, 1, 9152, 9168);  slice_scatter_6868 = slice_scatter_6870 = None
        slice_37799: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6871, 1, 9152, 9168)
        slice_37800: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37799, 2, 0, 16)
        slice_scatter_6873: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37799, slice_37800, 2, 0, 16);  slice_37799 = slice_37800 = None
        slice_scatter_6874: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6871, slice_scatter_6873, 1, 9152, 9168);  slice_scatter_6871 = slice_scatter_6873 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37819: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9168, 9184)
        slice_37820: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_37819, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1149: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_37820, memory_format = torch.contiguous_format);  slice_37820 = None
        view_2302: "f32[32, 16]" = torch.ops.aten.view.default(clone_1149, [32, 16]);  clone_1149 = None
        mm_1146: "f32[32, 8]" = torch.ops.aten.mm.default(view_2302, slice_7)
        view_2303: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1146, [2, 16, 8]);  mm_1146 = None
        slice_37827: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6874, 1, 9168, 9184)
        slice_37828: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37827, 2, 0, 16)
        add_1148: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37828, view_2303);  slice_37828 = view_2303 = None
        slice_scatter_6876: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37827, add_1148, 2, 0, 16);  slice_37827 = add_1148 = None
        slice_scatter_6877: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6874, slice_scatter_6876, 1, 9168, 9184);  slice_scatter_6874 = slice_scatter_6876 = None
        slice_37832: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6877, 1, 9168, 9184)
        slice_37833: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37832, 2, 0, 16)
        slice_scatter_6879: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37832, slice_37833, 2, 0, 16);  slice_37832 = slice_37833 = None
        slice_scatter_6880: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6877, slice_scatter_6879, 1, 9168, 9184);  slice_scatter_6877 = slice_scatter_6879 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37853: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_37819, 2, 16, 32);  slice_37819 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1150: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_37853, memory_format = torch.contiguous_format);  slice_37853 = None
        view_2304: "f32[32, 11]" = torch.ops.aten.view.default(clone_1150, [32, 11]);  clone_1150 = None
        mm_1147: "f32[32, 8]" = torch.ops.aten.mm.default(view_2304, slice_37)
        view_2305: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1147, [2, 16, 8]);  mm_1147 = None
        slice_37860: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6880, 1, 9168, 9184)
        slice_37861: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37860, 2, 0, 16)
        add_1149: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37861, view_2305);  slice_37861 = view_2305 = None
        slice_scatter_6882: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37860, add_1149, 2, 0, 16);  slice_37860 = add_1149 = None
        slice_scatter_6883: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6880, slice_scatter_6882, 1, 9168, 9184);  slice_scatter_6880 = slice_scatter_6882 = None
        slice_37865: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6883, 1, 9168, 9184)
        slice_37866: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37865, 2, 0, 16)
        slice_scatter_6885: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37865, slice_37866, 2, 0, 16);  slice_37865 = slice_37866 = None
        slice_scatter_6886: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6883, slice_scatter_6885, 1, 9168, 9184);  slice_scatter_6883 = slice_scatter_6885 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37885: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9184, 9200)
        slice_37886: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_37885, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1151: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_37886, memory_format = torch.contiguous_format);  slice_37886 = None
        view_2306: "f32[32, 16]" = torch.ops.aten.view.default(clone_1151, [32, 16]);  clone_1151 = None
        mm_1148: "f32[32, 8]" = torch.ops.aten.mm.default(view_2306, slice_7)
        view_2307: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1148, [2, 16, 8]);  mm_1148 = None
        slice_37893: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6886, 1, 9184, 9200)
        slice_37894: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37893, 2, 0, 16)
        add_1150: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37894, view_2307);  slice_37894 = view_2307 = None
        slice_scatter_6888: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37893, add_1150, 2, 0, 16);  slice_37893 = add_1150 = None
        slice_scatter_6889: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6886, slice_scatter_6888, 1, 9184, 9200);  slice_scatter_6886 = slice_scatter_6888 = None
        slice_37898: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6889, 1, 9184, 9200)
        slice_37899: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37898, 2, 0, 16)
        slice_scatter_6891: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37898, slice_37899, 2, 0, 16);  slice_37898 = slice_37899 = None
        slice_scatter_6892: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6889, slice_scatter_6891, 1, 9184, 9200);  slice_scatter_6889 = slice_scatter_6891 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37919: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_37885, 2, 16, 32);  slice_37885 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1152: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_37919, memory_format = torch.contiguous_format);  slice_37919 = None
        view_2308: "f32[32, 11]" = torch.ops.aten.view.default(clone_1152, [32, 11]);  clone_1152 = None
        mm_1149: "f32[32, 8]" = torch.ops.aten.mm.default(view_2308, slice_37)
        view_2309: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1149, [2, 16, 8]);  mm_1149 = None
        slice_37926: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6892, 1, 9184, 9200)
        slice_37927: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37926, 2, 0, 16)
        add_1151: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37927, view_2309);  slice_37927 = view_2309 = None
        slice_scatter_6894: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37926, add_1151, 2, 0, 16);  slice_37926 = add_1151 = None
        slice_scatter_6895: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6892, slice_scatter_6894, 1, 9184, 9200);  slice_scatter_6892 = slice_scatter_6894 = None
        slice_37931: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6895, 1, 9184, 9200)
        slice_37932: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37931, 2, 0, 16)
        slice_scatter_6897: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37931, slice_37932, 2, 0, 16);  slice_37931 = slice_37932 = None
        slice_scatter_6898: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6895, slice_scatter_6897, 1, 9184, 9200);  slice_scatter_6895 = slice_scatter_6897 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37951: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9200, 9216)
        slice_37952: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_37951, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1153: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_37952, memory_format = torch.contiguous_format);  slice_37952 = None
        view_2310: "f32[32, 16]" = torch.ops.aten.view.default(clone_1153, [32, 16]);  clone_1153 = None
        mm_1150: "f32[32, 8]" = torch.ops.aten.mm.default(view_2310, slice_7)
        view_2311: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1150, [2, 16, 8]);  mm_1150 = None
        slice_37959: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6898, 1, 9200, 9216)
        slice_37960: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37959, 2, 0, 16)
        add_1152: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37960, view_2311);  slice_37960 = view_2311 = None
        slice_scatter_6900: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37959, add_1152, 2, 0, 16);  slice_37959 = add_1152 = None
        slice_scatter_6901: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6898, slice_scatter_6900, 1, 9200, 9216);  slice_scatter_6898 = slice_scatter_6900 = None
        slice_37964: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6901, 1, 9200, 9216)
        slice_37965: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37964, 2, 0, 16)
        slice_scatter_6903: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37964, slice_37965, 2, 0, 16);  slice_37964 = slice_37965 = None
        slice_scatter_6904: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6901, slice_scatter_6903, 1, 9200, 9216);  slice_scatter_6901 = slice_scatter_6903 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_37985: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_37951, 2, 16, 32);  slice_37951 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1154: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_37985, memory_format = torch.contiguous_format);  slice_37985 = None
        view_2312: "f32[32, 11]" = torch.ops.aten.view.default(clone_1154, [32, 11]);  clone_1154 = None
        mm_1151: "f32[32, 8]" = torch.ops.aten.mm.default(view_2312, slice_37)
        view_2313: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1151, [2, 16, 8]);  mm_1151 = None
        slice_37992: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6904, 1, 9200, 9216)
        slice_37993: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37992, 2, 0, 16)
        add_1153: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_37993, view_2313);  slice_37993 = view_2313 = None
        slice_scatter_6906: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37992, add_1153, 2, 0, 16);  slice_37992 = add_1153 = None
        slice_scatter_6907: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6904, slice_scatter_6906, 1, 9200, 9216);  slice_scatter_6904 = slice_scatter_6906 = None
        slice_37997: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6907, 1, 9200, 9216)
        slice_37998: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_37997, 2, 0, 16)
        slice_scatter_6909: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_37997, slice_37998, 2, 0, 16);  slice_37997 = slice_37998 = None
        slice_scatter_6910: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6907, slice_scatter_6909, 1, 9200, 9216);  slice_scatter_6907 = slice_scatter_6909 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38017: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9216, 9232)
        slice_38018: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_38017, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1155: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_38018, memory_format = torch.contiguous_format);  slice_38018 = None
        view_2314: "f32[32, 16]" = torch.ops.aten.view.default(clone_1155, [32, 16]);  clone_1155 = None
        mm_1152: "f32[32, 8]" = torch.ops.aten.mm.default(view_2314, slice_7)
        view_2315: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1152, [2, 16, 8]);  mm_1152 = None
        slice_38025: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6910, 1, 9216, 9232)
        slice_38026: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38025, 2, 0, 16)
        add_1154: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38026, view_2315);  slice_38026 = view_2315 = None
        slice_scatter_6912: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38025, add_1154, 2, 0, 16);  slice_38025 = add_1154 = None
        slice_scatter_6913: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6910, slice_scatter_6912, 1, 9216, 9232);  slice_scatter_6910 = slice_scatter_6912 = None
        slice_38030: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6913, 1, 9216, 9232)
        slice_38031: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38030, 2, 0, 16)
        slice_scatter_6915: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38030, slice_38031, 2, 0, 16);  slice_38030 = slice_38031 = None
        slice_scatter_6916: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6913, slice_scatter_6915, 1, 9216, 9232);  slice_scatter_6913 = slice_scatter_6915 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38051: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_38017, 2, 16, 32);  slice_38017 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1156: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_38051, memory_format = torch.contiguous_format);  slice_38051 = None
        view_2316: "f32[32, 11]" = torch.ops.aten.view.default(clone_1156, [32, 11]);  clone_1156 = None
        mm_1153: "f32[32, 8]" = torch.ops.aten.mm.default(view_2316, slice_37)
        view_2317: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1153, [2, 16, 8]);  mm_1153 = None
        slice_38058: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6916, 1, 9216, 9232)
        slice_38059: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38058, 2, 0, 16)
        add_1155: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38059, view_2317);  slice_38059 = view_2317 = None
        slice_scatter_6918: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38058, add_1155, 2, 0, 16);  slice_38058 = add_1155 = None
        slice_scatter_6919: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6916, slice_scatter_6918, 1, 9216, 9232);  slice_scatter_6916 = slice_scatter_6918 = None
        slice_38063: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6919, 1, 9216, 9232)
        slice_38064: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38063, 2, 0, 16)
        slice_scatter_6921: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38063, slice_38064, 2, 0, 16);  slice_38063 = slice_38064 = None
        slice_scatter_6922: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6919, slice_scatter_6921, 1, 9216, 9232);  slice_scatter_6919 = slice_scatter_6921 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38083: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9232, 9248)
        slice_38084: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_38083, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1157: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_38084, memory_format = torch.contiguous_format);  slice_38084 = None
        view_2318: "f32[32, 16]" = torch.ops.aten.view.default(clone_1157, [32, 16]);  clone_1157 = None
        mm_1154: "f32[32, 8]" = torch.ops.aten.mm.default(view_2318, slice_7)
        view_2319: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1154, [2, 16, 8]);  mm_1154 = None
        slice_38091: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6922, 1, 9232, 9248)
        slice_38092: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38091, 2, 0, 16)
        add_1156: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38092, view_2319);  slice_38092 = view_2319 = None
        slice_scatter_6924: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38091, add_1156, 2, 0, 16);  slice_38091 = add_1156 = None
        slice_scatter_6925: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6922, slice_scatter_6924, 1, 9232, 9248);  slice_scatter_6922 = slice_scatter_6924 = None
        slice_38096: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6925, 1, 9232, 9248)
        slice_38097: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38096, 2, 0, 16)
        slice_scatter_6927: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38096, slice_38097, 2, 0, 16);  slice_38096 = slice_38097 = None
        slice_scatter_6928: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6925, slice_scatter_6927, 1, 9232, 9248);  slice_scatter_6925 = slice_scatter_6927 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38117: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_38083, 2, 16, 32);  slice_38083 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1158: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_38117, memory_format = torch.contiguous_format);  slice_38117 = None
        view_2320: "f32[32, 11]" = torch.ops.aten.view.default(clone_1158, [32, 11]);  clone_1158 = None
        mm_1155: "f32[32, 8]" = torch.ops.aten.mm.default(view_2320, slice_37)
        view_2321: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1155, [2, 16, 8]);  mm_1155 = None
        slice_38124: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6928, 1, 9232, 9248)
        slice_38125: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38124, 2, 0, 16)
        add_1157: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38125, view_2321);  slice_38125 = view_2321 = None
        slice_scatter_6930: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38124, add_1157, 2, 0, 16);  slice_38124 = add_1157 = None
        slice_scatter_6931: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6928, slice_scatter_6930, 1, 9232, 9248);  slice_scatter_6928 = slice_scatter_6930 = None
        slice_38129: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6931, 1, 9232, 9248)
        slice_38130: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38129, 2, 0, 16)
        slice_scatter_6933: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38129, slice_38130, 2, 0, 16);  slice_38129 = slice_38130 = None
        slice_scatter_6934: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6931, slice_scatter_6933, 1, 9232, 9248);  slice_scatter_6931 = slice_scatter_6933 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38149: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9248, 9264)
        slice_38150: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_38149, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1159: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_38150, memory_format = torch.contiguous_format);  slice_38150 = None
        view_2322: "f32[32, 16]" = torch.ops.aten.view.default(clone_1159, [32, 16]);  clone_1159 = None
        mm_1156: "f32[32, 8]" = torch.ops.aten.mm.default(view_2322, slice_7)
        view_2323: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1156, [2, 16, 8]);  mm_1156 = None
        slice_38157: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6934, 1, 9248, 9264)
        slice_38158: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38157, 2, 0, 16)
        add_1158: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38158, view_2323);  slice_38158 = view_2323 = None
        slice_scatter_6936: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38157, add_1158, 2, 0, 16);  slice_38157 = add_1158 = None
        slice_scatter_6937: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6934, slice_scatter_6936, 1, 9248, 9264);  slice_scatter_6934 = slice_scatter_6936 = None
        slice_38162: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6937, 1, 9248, 9264)
        slice_38163: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38162, 2, 0, 16)
        slice_scatter_6939: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38162, slice_38163, 2, 0, 16);  slice_38162 = slice_38163 = None
        slice_scatter_6940: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6937, slice_scatter_6939, 1, 9248, 9264);  slice_scatter_6937 = slice_scatter_6939 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38183: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_38149, 2, 16, 32);  slice_38149 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1160: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_38183, memory_format = torch.contiguous_format);  slice_38183 = None
        view_2324: "f32[32, 11]" = torch.ops.aten.view.default(clone_1160, [32, 11]);  clone_1160 = None
        mm_1157: "f32[32, 8]" = torch.ops.aten.mm.default(view_2324, slice_37)
        view_2325: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1157, [2, 16, 8]);  mm_1157 = None
        slice_38190: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6940, 1, 9248, 9264)
        slice_38191: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38190, 2, 0, 16)
        add_1159: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38191, view_2325);  slice_38191 = view_2325 = None
        slice_scatter_6942: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38190, add_1159, 2, 0, 16);  slice_38190 = add_1159 = None
        slice_scatter_6943: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6940, slice_scatter_6942, 1, 9248, 9264);  slice_scatter_6940 = slice_scatter_6942 = None
        slice_38195: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6943, 1, 9248, 9264)
        slice_38196: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38195, 2, 0, 16)
        slice_scatter_6945: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38195, slice_38196, 2, 0, 16);  slice_38195 = slice_38196 = None
        slice_scatter_6946: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6943, slice_scatter_6945, 1, 9248, 9264);  slice_scatter_6943 = slice_scatter_6945 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38215: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9264, 9280)
        slice_38216: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_38215, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1161: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_38216, memory_format = torch.contiguous_format);  slice_38216 = None
        view_2326: "f32[32, 16]" = torch.ops.aten.view.default(clone_1161, [32, 16]);  clone_1161 = None
        mm_1158: "f32[32, 8]" = torch.ops.aten.mm.default(view_2326, slice_7)
        view_2327: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1158, [2, 16, 8]);  mm_1158 = None
        slice_38223: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6946, 1, 9264, 9280)
        slice_38224: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38223, 2, 0, 16)
        add_1160: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38224, view_2327);  slice_38224 = view_2327 = None
        slice_scatter_6948: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38223, add_1160, 2, 0, 16);  slice_38223 = add_1160 = None
        slice_scatter_6949: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6946, slice_scatter_6948, 1, 9264, 9280);  slice_scatter_6946 = slice_scatter_6948 = None
        slice_38228: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6949, 1, 9264, 9280)
        slice_38229: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38228, 2, 0, 16)
        slice_scatter_6951: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38228, slice_38229, 2, 0, 16);  slice_38228 = slice_38229 = None
        slice_scatter_6952: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6949, slice_scatter_6951, 1, 9264, 9280);  slice_scatter_6949 = slice_scatter_6951 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38249: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_38215, 2, 16, 32);  slice_38215 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1162: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_38249, memory_format = torch.contiguous_format);  slice_38249 = None
        view_2328: "f32[32, 11]" = torch.ops.aten.view.default(clone_1162, [32, 11]);  clone_1162 = None
        mm_1159: "f32[32, 8]" = torch.ops.aten.mm.default(view_2328, slice_37)
        view_2329: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1159, [2, 16, 8]);  mm_1159 = None
        slice_38256: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6952, 1, 9264, 9280)
        slice_38257: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38256, 2, 0, 16)
        add_1161: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38257, view_2329);  slice_38257 = view_2329 = None
        slice_scatter_6954: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38256, add_1161, 2, 0, 16);  slice_38256 = add_1161 = None
        slice_scatter_6955: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6952, slice_scatter_6954, 1, 9264, 9280);  slice_scatter_6952 = slice_scatter_6954 = None
        slice_38261: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6955, 1, 9264, 9280)
        slice_38262: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38261, 2, 0, 16)
        slice_scatter_6957: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38261, slice_38262, 2, 0, 16);  slice_38261 = slice_38262 = None
        slice_scatter_6958: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6955, slice_scatter_6957, 1, 9264, 9280);  slice_scatter_6955 = slice_scatter_6957 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38281: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9280, 9296)
        slice_38282: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_38281, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1163: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_38282, memory_format = torch.contiguous_format);  slice_38282 = None
        view_2330: "f32[32, 16]" = torch.ops.aten.view.default(clone_1163, [32, 16]);  clone_1163 = None
        mm_1160: "f32[32, 8]" = torch.ops.aten.mm.default(view_2330, slice_7)
        view_2331: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1160, [2, 16, 8]);  mm_1160 = None
        slice_38289: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6958, 1, 9280, 9296)
        slice_38290: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38289, 2, 0, 16)
        add_1162: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38290, view_2331);  slice_38290 = view_2331 = None
        slice_scatter_6960: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38289, add_1162, 2, 0, 16);  slice_38289 = add_1162 = None
        slice_scatter_6961: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6958, slice_scatter_6960, 1, 9280, 9296);  slice_scatter_6958 = slice_scatter_6960 = None
        slice_38294: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6961, 1, 9280, 9296)
        slice_38295: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38294, 2, 0, 16)
        slice_scatter_6963: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38294, slice_38295, 2, 0, 16);  slice_38294 = slice_38295 = None
        slice_scatter_6964: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6961, slice_scatter_6963, 1, 9280, 9296);  slice_scatter_6961 = slice_scatter_6963 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38315: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_38281, 2, 16, 32);  slice_38281 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1164: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_38315, memory_format = torch.contiguous_format);  slice_38315 = None
        view_2332: "f32[32, 11]" = torch.ops.aten.view.default(clone_1164, [32, 11]);  clone_1164 = None
        mm_1161: "f32[32, 8]" = torch.ops.aten.mm.default(view_2332, slice_37)
        view_2333: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1161, [2, 16, 8]);  mm_1161 = None
        slice_38322: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6964, 1, 9280, 9296)
        slice_38323: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38322, 2, 0, 16)
        add_1163: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38323, view_2333);  slice_38323 = view_2333 = None
        slice_scatter_6966: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38322, add_1163, 2, 0, 16);  slice_38322 = add_1163 = None
        slice_scatter_6967: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6964, slice_scatter_6966, 1, 9280, 9296);  slice_scatter_6964 = slice_scatter_6966 = None
        slice_38327: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6967, 1, 9280, 9296)
        slice_38328: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38327, 2, 0, 16)
        slice_scatter_6969: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38327, slice_38328, 2, 0, 16);  slice_38327 = slice_38328 = None
        slice_scatter_6970: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6967, slice_scatter_6969, 1, 9280, 9296);  slice_scatter_6967 = slice_scatter_6969 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38347: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9296, 9312)
        slice_38348: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_38347, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1165: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_38348, memory_format = torch.contiguous_format);  slice_38348 = None
        view_2334: "f32[32, 16]" = torch.ops.aten.view.default(clone_1165, [32, 16]);  clone_1165 = None
        mm_1162: "f32[32, 8]" = torch.ops.aten.mm.default(view_2334, slice_7)
        view_2335: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1162, [2, 16, 8]);  mm_1162 = None
        slice_38355: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6970, 1, 9296, 9312)
        slice_38356: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38355, 2, 0, 16)
        add_1164: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38356, view_2335);  slice_38356 = view_2335 = None
        slice_scatter_6972: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38355, add_1164, 2, 0, 16);  slice_38355 = add_1164 = None
        slice_scatter_6973: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6970, slice_scatter_6972, 1, 9296, 9312);  slice_scatter_6970 = slice_scatter_6972 = None
        slice_38360: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6973, 1, 9296, 9312)
        slice_38361: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38360, 2, 0, 16)
        slice_scatter_6975: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38360, slice_38361, 2, 0, 16);  slice_38360 = slice_38361 = None
        slice_scatter_6976: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6973, slice_scatter_6975, 1, 9296, 9312);  slice_scatter_6973 = slice_scatter_6975 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38381: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_38347, 2, 16, 32);  slice_38347 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1166: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_38381, memory_format = torch.contiguous_format);  slice_38381 = None
        view_2336: "f32[32, 11]" = torch.ops.aten.view.default(clone_1166, [32, 11]);  clone_1166 = None
        mm_1163: "f32[32, 8]" = torch.ops.aten.mm.default(view_2336, slice_37)
        view_2337: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1163, [2, 16, 8]);  mm_1163 = None
        slice_38388: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6976, 1, 9296, 9312)
        slice_38389: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38388, 2, 0, 16)
        add_1165: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38389, view_2337);  slice_38389 = view_2337 = None
        slice_scatter_6978: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38388, add_1165, 2, 0, 16);  slice_38388 = add_1165 = None
        slice_scatter_6979: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6976, slice_scatter_6978, 1, 9296, 9312);  slice_scatter_6976 = slice_scatter_6978 = None
        slice_38393: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6979, 1, 9296, 9312)
        slice_38394: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38393, 2, 0, 16)
        slice_scatter_6981: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38393, slice_38394, 2, 0, 16);  slice_38393 = slice_38394 = None
        slice_scatter_6982: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6979, slice_scatter_6981, 1, 9296, 9312);  slice_scatter_6979 = slice_scatter_6981 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38413: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9312, 9328)
        slice_38414: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_38413, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1167: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_38414, memory_format = torch.contiguous_format);  slice_38414 = None
        view_2338: "f32[32, 16]" = torch.ops.aten.view.default(clone_1167, [32, 16]);  clone_1167 = None
        mm_1164: "f32[32, 8]" = torch.ops.aten.mm.default(view_2338, slice_7)
        view_2339: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1164, [2, 16, 8]);  mm_1164 = None
        slice_38421: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6982, 1, 9312, 9328)
        slice_38422: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38421, 2, 0, 16)
        add_1166: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38422, view_2339);  slice_38422 = view_2339 = None
        slice_scatter_6984: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38421, add_1166, 2, 0, 16);  slice_38421 = add_1166 = None
        slice_scatter_6985: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6982, slice_scatter_6984, 1, 9312, 9328);  slice_scatter_6982 = slice_scatter_6984 = None
        slice_38426: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6985, 1, 9312, 9328)
        slice_38427: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38426, 2, 0, 16)
        slice_scatter_6987: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38426, slice_38427, 2, 0, 16);  slice_38426 = slice_38427 = None
        slice_scatter_6988: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6985, slice_scatter_6987, 1, 9312, 9328);  slice_scatter_6985 = slice_scatter_6987 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38447: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_38413, 2, 16, 32);  slice_38413 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1168: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_38447, memory_format = torch.contiguous_format);  slice_38447 = None
        view_2340: "f32[32, 11]" = torch.ops.aten.view.default(clone_1168, [32, 11]);  clone_1168 = None
        mm_1165: "f32[32, 8]" = torch.ops.aten.mm.default(view_2340, slice_37)
        view_2341: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1165, [2, 16, 8]);  mm_1165 = None
        slice_38454: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6988, 1, 9312, 9328)
        slice_38455: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38454, 2, 0, 16)
        add_1167: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38455, view_2341);  slice_38455 = view_2341 = None
        slice_scatter_6990: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38454, add_1167, 2, 0, 16);  slice_38454 = add_1167 = None
        slice_scatter_6991: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6988, slice_scatter_6990, 1, 9312, 9328);  slice_scatter_6988 = slice_scatter_6990 = None
        slice_38459: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6991, 1, 9312, 9328)
        slice_38460: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38459, 2, 0, 16)
        slice_scatter_6993: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38459, slice_38460, 2, 0, 16);  slice_38459 = slice_38460 = None
        slice_scatter_6994: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6991, slice_scatter_6993, 1, 9312, 9328);  slice_scatter_6991 = slice_scatter_6993 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38479: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9328, 9344)
        slice_38480: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_38479, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1169: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_38480, memory_format = torch.contiguous_format);  slice_38480 = None
        view_2342: "f32[32, 16]" = torch.ops.aten.view.default(clone_1169, [32, 16]);  clone_1169 = None
        mm_1166: "f32[32, 8]" = torch.ops.aten.mm.default(view_2342, slice_7)
        view_2343: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1166, [2, 16, 8]);  mm_1166 = None
        slice_38487: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6994, 1, 9328, 9344)
        slice_38488: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38487, 2, 0, 16)
        add_1168: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38488, view_2343);  slice_38488 = view_2343 = None
        slice_scatter_6996: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38487, add_1168, 2, 0, 16);  slice_38487 = add_1168 = None
        slice_scatter_6997: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6994, slice_scatter_6996, 1, 9328, 9344);  slice_scatter_6994 = slice_scatter_6996 = None
        slice_38492: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_6997, 1, 9328, 9344)
        slice_38493: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38492, 2, 0, 16)
        slice_scatter_6999: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38492, slice_38493, 2, 0, 16);  slice_38492 = slice_38493 = None
        slice_scatter_7000: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_6997, slice_scatter_6999, 1, 9328, 9344);  slice_scatter_6997 = slice_scatter_6999 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38513: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_38479, 2, 16, 32);  slice_38479 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1170: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_38513, memory_format = torch.contiguous_format);  slice_38513 = None
        view_2344: "f32[32, 11]" = torch.ops.aten.view.default(clone_1170, [32, 11]);  clone_1170 = None
        mm_1167: "f32[32, 8]" = torch.ops.aten.mm.default(view_2344, slice_37)
        view_2345: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1167, [2, 16, 8]);  mm_1167 = None
        slice_38520: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7000, 1, 9328, 9344)
        slice_38521: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38520, 2, 0, 16)
        add_1169: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38521, view_2345);  slice_38521 = view_2345 = None
        slice_scatter_7002: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38520, add_1169, 2, 0, 16);  slice_38520 = add_1169 = None
        slice_scatter_7003: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7000, slice_scatter_7002, 1, 9328, 9344);  slice_scatter_7000 = slice_scatter_7002 = None
        slice_38525: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7003, 1, 9328, 9344)
        slice_38526: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38525, 2, 0, 16)
        slice_scatter_7005: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38525, slice_38526, 2, 0, 16);  slice_38525 = slice_38526 = None
        slice_scatter_7006: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7003, slice_scatter_7005, 1, 9328, 9344);  slice_scatter_7003 = slice_scatter_7005 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38545: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9344, 9360)
        slice_38546: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_38545, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1171: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_38546, memory_format = torch.contiguous_format);  slice_38546 = None
        view_2346: "f32[32, 16]" = torch.ops.aten.view.default(clone_1171, [32, 16]);  clone_1171 = None
        mm_1168: "f32[32, 8]" = torch.ops.aten.mm.default(view_2346, slice_7)
        view_2347: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1168, [2, 16, 8]);  mm_1168 = None
        slice_38553: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7006, 1, 9344, 9360)
        slice_38554: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38553, 2, 0, 16)
        add_1170: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38554, view_2347);  slice_38554 = view_2347 = None
        slice_scatter_7008: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38553, add_1170, 2, 0, 16);  slice_38553 = add_1170 = None
        slice_scatter_7009: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7006, slice_scatter_7008, 1, 9344, 9360);  slice_scatter_7006 = slice_scatter_7008 = None
        slice_38558: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7009, 1, 9344, 9360)
        slice_38559: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38558, 2, 0, 16)
        slice_scatter_7011: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38558, slice_38559, 2, 0, 16);  slice_38558 = slice_38559 = None
        slice_scatter_7012: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7009, slice_scatter_7011, 1, 9344, 9360);  slice_scatter_7009 = slice_scatter_7011 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38579: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_38545, 2, 16, 32);  slice_38545 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1172: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_38579, memory_format = torch.contiguous_format);  slice_38579 = None
        view_2348: "f32[32, 11]" = torch.ops.aten.view.default(clone_1172, [32, 11]);  clone_1172 = None
        mm_1169: "f32[32, 8]" = torch.ops.aten.mm.default(view_2348, slice_37)
        view_2349: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1169, [2, 16, 8]);  mm_1169 = None
        slice_38586: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7012, 1, 9344, 9360)
        slice_38587: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38586, 2, 0, 16)
        add_1171: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38587, view_2349);  slice_38587 = view_2349 = None
        slice_scatter_7014: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38586, add_1171, 2, 0, 16);  slice_38586 = add_1171 = None
        slice_scatter_7015: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7012, slice_scatter_7014, 1, 9344, 9360);  slice_scatter_7012 = slice_scatter_7014 = None
        slice_38591: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7015, 1, 9344, 9360)
        slice_38592: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38591, 2, 0, 16)
        slice_scatter_7017: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38591, slice_38592, 2, 0, 16);  slice_38591 = slice_38592 = None
        slice_scatter_7018: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7015, slice_scatter_7017, 1, 9344, 9360);  slice_scatter_7015 = slice_scatter_7017 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38611: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9360, 9376)
        slice_38612: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_38611, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1173: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_38612, memory_format = torch.contiguous_format);  slice_38612 = None
        view_2350: "f32[32, 16]" = torch.ops.aten.view.default(clone_1173, [32, 16]);  clone_1173 = None
        mm_1170: "f32[32, 8]" = torch.ops.aten.mm.default(view_2350, slice_7)
        view_2351: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1170, [2, 16, 8]);  mm_1170 = None
        slice_38619: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7018, 1, 9360, 9376)
        slice_38620: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38619, 2, 0, 16)
        add_1172: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38620, view_2351);  slice_38620 = view_2351 = None
        slice_scatter_7020: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38619, add_1172, 2, 0, 16);  slice_38619 = add_1172 = None
        slice_scatter_7021: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7018, slice_scatter_7020, 1, 9360, 9376);  slice_scatter_7018 = slice_scatter_7020 = None
        slice_38624: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7021, 1, 9360, 9376)
        slice_38625: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38624, 2, 0, 16)
        slice_scatter_7023: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38624, slice_38625, 2, 0, 16);  slice_38624 = slice_38625 = None
        slice_scatter_7024: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7021, slice_scatter_7023, 1, 9360, 9376);  slice_scatter_7021 = slice_scatter_7023 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38645: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_38611, 2, 16, 32);  slice_38611 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1174: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_38645, memory_format = torch.contiguous_format);  slice_38645 = None
        view_2352: "f32[32, 11]" = torch.ops.aten.view.default(clone_1174, [32, 11]);  clone_1174 = None
        mm_1171: "f32[32, 8]" = torch.ops.aten.mm.default(view_2352, slice_37)
        view_2353: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1171, [2, 16, 8]);  mm_1171 = None
        slice_38652: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7024, 1, 9360, 9376)
        slice_38653: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38652, 2, 0, 16)
        add_1173: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38653, view_2353);  slice_38653 = view_2353 = None
        slice_scatter_7026: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38652, add_1173, 2, 0, 16);  slice_38652 = add_1173 = None
        slice_scatter_7027: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7024, slice_scatter_7026, 1, 9360, 9376);  slice_scatter_7024 = slice_scatter_7026 = None
        slice_38657: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7027, 1, 9360, 9376)
        slice_38658: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38657, 2, 0, 16)
        slice_scatter_7029: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38657, slice_38658, 2, 0, 16);  slice_38657 = slice_38658 = None
        slice_scatter_7030: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7027, slice_scatter_7029, 1, 9360, 9376);  slice_scatter_7027 = slice_scatter_7029 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38677: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9376, 9392)
        slice_38678: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_38677, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1175: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_38678, memory_format = torch.contiguous_format);  slice_38678 = None
        view_2354: "f32[32, 16]" = torch.ops.aten.view.default(clone_1175, [32, 16]);  clone_1175 = None
        mm_1172: "f32[32, 8]" = torch.ops.aten.mm.default(view_2354, slice_7)
        view_2355: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1172, [2, 16, 8]);  mm_1172 = None
        slice_38685: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7030, 1, 9376, 9392)
        slice_38686: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38685, 2, 0, 16)
        add_1174: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38686, view_2355);  slice_38686 = view_2355 = None
        slice_scatter_7032: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38685, add_1174, 2, 0, 16);  slice_38685 = add_1174 = None
        slice_scatter_7033: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7030, slice_scatter_7032, 1, 9376, 9392);  slice_scatter_7030 = slice_scatter_7032 = None
        slice_38690: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7033, 1, 9376, 9392)
        slice_38691: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38690, 2, 0, 16)
        slice_scatter_7035: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38690, slice_38691, 2, 0, 16);  slice_38690 = slice_38691 = None
        slice_scatter_7036: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7033, slice_scatter_7035, 1, 9376, 9392);  slice_scatter_7033 = slice_scatter_7035 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38711: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_38677, 2, 16, 32);  slice_38677 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1176: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_38711, memory_format = torch.contiguous_format);  slice_38711 = None
        view_2356: "f32[32, 11]" = torch.ops.aten.view.default(clone_1176, [32, 11]);  clone_1176 = None
        mm_1173: "f32[32, 8]" = torch.ops.aten.mm.default(view_2356, slice_37)
        view_2357: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1173, [2, 16, 8]);  mm_1173 = None
        slice_38718: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7036, 1, 9376, 9392)
        slice_38719: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38718, 2, 0, 16)
        add_1175: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38719, view_2357);  slice_38719 = view_2357 = None
        slice_scatter_7038: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38718, add_1175, 2, 0, 16);  slice_38718 = add_1175 = None
        slice_scatter_7039: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7036, slice_scatter_7038, 1, 9376, 9392);  slice_scatter_7036 = slice_scatter_7038 = None
        slice_38723: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7039, 1, 9376, 9392)
        slice_38724: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38723, 2, 0, 16)
        slice_scatter_7041: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38723, slice_38724, 2, 0, 16);  slice_38723 = slice_38724 = None
        slice_scatter_7042: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7039, slice_scatter_7041, 1, 9376, 9392);  slice_scatter_7039 = slice_scatter_7041 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38743: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9392, 9408)
        slice_38744: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_38743, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1177: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_38744, memory_format = torch.contiguous_format);  slice_38744 = None
        view_2358: "f32[32, 16]" = torch.ops.aten.view.default(clone_1177, [32, 16]);  clone_1177 = None
        mm_1174: "f32[32, 8]" = torch.ops.aten.mm.default(view_2358, slice_7)
        view_2359: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1174, [2, 16, 8]);  mm_1174 = None
        slice_38751: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7042, 1, 9392, 9408)
        slice_38752: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38751, 2, 0, 16)
        add_1176: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38752, view_2359);  slice_38752 = view_2359 = None
        slice_scatter_7044: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38751, add_1176, 2, 0, 16);  slice_38751 = add_1176 = None
        slice_scatter_7045: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7042, slice_scatter_7044, 1, 9392, 9408);  slice_scatter_7042 = slice_scatter_7044 = None
        slice_38756: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7045, 1, 9392, 9408)
        slice_38757: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38756, 2, 0, 16)
        slice_scatter_7047: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38756, slice_38757, 2, 0, 16);  slice_38756 = slice_38757 = None
        slice_scatter_7048: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7045, slice_scatter_7047, 1, 9392, 9408);  slice_scatter_7045 = slice_scatter_7047 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38777: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_38743, 2, 16, 32);  slice_38743 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1178: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_38777, memory_format = torch.contiguous_format);  slice_38777 = None
        view_2360: "f32[32, 11]" = torch.ops.aten.view.default(clone_1178, [32, 11]);  clone_1178 = None
        mm_1175: "f32[32, 8]" = torch.ops.aten.mm.default(view_2360, slice_37)
        view_2361: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1175, [2, 16, 8]);  mm_1175 = None
        slice_38784: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7048, 1, 9392, 9408)
        slice_38785: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38784, 2, 0, 16)
        add_1177: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38785, view_2361);  slice_38785 = view_2361 = None
        slice_scatter_7050: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38784, add_1177, 2, 0, 16);  slice_38784 = add_1177 = None
        slice_scatter_7051: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7048, slice_scatter_7050, 1, 9392, 9408);  slice_scatter_7048 = slice_scatter_7050 = None
        slice_38789: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7051, 1, 9392, 9408)
        slice_38790: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38789, 2, 0, 16)
        slice_scatter_7053: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38789, slice_38790, 2, 0, 16);  slice_38789 = slice_38790 = None
        slice_scatter_7054: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7051, slice_scatter_7053, 1, 9392, 9408);  slice_scatter_7051 = slice_scatter_7053 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38809: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9408, 9424)
        slice_38810: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_38809, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1179: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_38810, memory_format = torch.contiguous_format);  slice_38810 = None
        view_2362: "f32[32, 16]" = torch.ops.aten.view.default(clone_1179, [32, 16]);  clone_1179 = None
        mm_1176: "f32[32, 8]" = torch.ops.aten.mm.default(view_2362, slice_7)
        view_2363: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1176, [2, 16, 8]);  mm_1176 = None
        slice_38817: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7054, 1, 9408, 9424)
        slice_38818: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38817, 2, 0, 16)
        add_1178: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38818, view_2363);  slice_38818 = view_2363 = None
        slice_scatter_7056: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38817, add_1178, 2, 0, 16);  slice_38817 = add_1178 = None
        slice_scatter_7057: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7054, slice_scatter_7056, 1, 9408, 9424);  slice_scatter_7054 = slice_scatter_7056 = None
        slice_38822: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7057, 1, 9408, 9424)
        slice_38823: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38822, 2, 0, 16)
        slice_scatter_7059: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38822, slice_38823, 2, 0, 16);  slice_38822 = slice_38823 = None
        slice_scatter_7060: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7057, slice_scatter_7059, 1, 9408, 9424);  slice_scatter_7057 = slice_scatter_7059 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38843: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_38809, 2, 16, 32);  slice_38809 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1180: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_38843, memory_format = torch.contiguous_format);  slice_38843 = None
        view_2364: "f32[32, 11]" = torch.ops.aten.view.default(clone_1180, [32, 11]);  clone_1180 = None
        mm_1177: "f32[32, 8]" = torch.ops.aten.mm.default(view_2364, slice_37)
        view_2365: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1177, [2, 16, 8]);  mm_1177 = None
        slice_38850: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7060, 1, 9408, 9424)
        slice_38851: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38850, 2, 0, 16)
        add_1179: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38851, view_2365);  slice_38851 = view_2365 = None
        slice_scatter_7062: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38850, add_1179, 2, 0, 16);  slice_38850 = add_1179 = None
        slice_scatter_7063: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7060, slice_scatter_7062, 1, 9408, 9424);  slice_scatter_7060 = slice_scatter_7062 = None
        slice_38855: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7063, 1, 9408, 9424)
        slice_38856: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38855, 2, 0, 16)
        slice_scatter_7065: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38855, slice_38856, 2, 0, 16);  slice_38855 = slice_38856 = None
        slice_scatter_7066: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7063, slice_scatter_7065, 1, 9408, 9424);  slice_scatter_7063 = slice_scatter_7065 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38875: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9424, 9440)
        slice_38876: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_38875, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1181: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_38876, memory_format = torch.contiguous_format);  slice_38876 = None
        view_2366: "f32[32, 16]" = torch.ops.aten.view.default(clone_1181, [32, 16]);  clone_1181 = None
        mm_1178: "f32[32, 8]" = torch.ops.aten.mm.default(view_2366, slice_7)
        view_2367: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1178, [2, 16, 8]);  mm_1178 = None
        slice_38883: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7066, 1, 9424, 9440)
        slice_38884: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38883, 2, 0, 16)
        add_1180: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38884, view_2367);  slice_38884 = view_2367 = None
        slice_scatter_7068: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38883, add_1180, 2, 0, 16);  slice_38883 = add_1180 = None
        slice_scatter_7069: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7066, slice_scatter_7068, 1, 9424, 9440);  slice_scatter_7066 = slice_scatter_7068 = None
        slice_38888: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7069, 1, 9424, 9440)
        slice_38889: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38888, 2, 0, 16)
        slice_scatter_7071: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38888, slice_38889, 2, 0, 16);  slice_38888 = slice_38889 = None
        slice_scatter_7072: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7069, slice_scatter_7071, 1, 9424, 9440);  slice_scatter_7069 = slice_scatter_7071 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38909: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_38875, 2, 16, 32);  slice_38875 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1182: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_38909, memory_format = torch.contiguous_format);  slice_38909 = None
        view_2368: "f32[32, 11]" = torch.ops.aten.view.default(clone_1182, [32, 11]);  clone_1182 = None
        mm_1179: "f32[32, 8]" = torch.ops.aten.mm.default(view_2368, slice_37)
        view_2369: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1179, [2, 16, 8]);  mm_1179 = None
        slice_38916: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7072, 1, 9424, 9440)
        slice_38917: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38916, 2, 0, 16)
        add_1181: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38917, view_2369);  slice_38917 = view_2369 = None
        slice_scatter_7074: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38916, add_1181, 2, 0, 16);  slice_38916 = add_1181 = None
        slice_scatter_7075: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7072, slice_scatter_7074, 1, 9424, 9440);  slice_scatter_7072 = slice_scatter_7074 = None
        slice_38921: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7075, 1, 9424, 9440)
        slice_38922: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38921, 2, 0, 16)
        slice_scatter_7077: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38921, slice_38922, 2, 0, 16);  slice_38921 = slice_38922 = None
        slice_scatter_7078: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7075, slice_scatter_7077, 1, 9424, 9440);  slice_scatter_7075 = slice_scatter_7077 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38941: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9440, 9456)
        slice_38942: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_38941, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1183: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_38942, memory_format = torch.contiguous_format);  slice_38942 = None
        view_2370: "f32[32, 16]" = torch.ops.aten.view.default(clone_1183, [32, 16]);  clone_1183 = None
        mm_1180: "f32[32, 8]" = torch.ops.aten.mm.default(view_2370, slice_7)
        view_2371: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1180, [2, 16, 8]);  mm_1180 = None
        slice_38949: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7078, 1, 9440, 9456)
        slice_38950: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38949, 2, 0, 16)
        add_1182: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38950, view_2371);  slice_38950 = view_2371 = None
        slice_scatter_7080: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38949, add_1182, 2, 0, 16);  slice_38949 = add_1182 = None
        slice_scatter_7081: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7078, slice_scatter_7080, 1, 9440, 9456);  slice_scatter_7078 = slice_scatter_7080 = None
        slice_38954: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7081, 1, 9440, 9456)
        slice_38955: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38954, 2, 0, 16)
        slice_scatter_7083: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38954, slice_38955, 2, 0, 16);  slice_38954 = slice_38955 = None
        slice_scatter_7084: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7081, slice_scatter_7083, 1, 9440, 9456);  slice_scatter_7081 = slice_scatter_7083 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_38975: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_38941, 2, 16, 32);  slice_38941 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1184: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_38975, memory_format = torch.contiguous_format);  slice_38975 = None
        view_2372: "f32[32, 11]" = torch.ops.aten.view.default(clone_1184, [32, 11]);  clone_1184 = None
        mm_1181: "f32[32, 8]" = torch.ops.aten.mm.default(view_2372, slice_37)
        view_2373: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1181, [2, 16, 8]);  mm_1181 = None
        slice_38982: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7084, 1, 9440, 9456)
        slice_38983: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38982, 2, 0, 16)
        add_1183: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_38983, view_2373);  slice_38983 = view_2373 = None
        slice_scatter_7086: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38982, add_1183, 2, 0, 16);  slice_38982 = add_1183 = None
        slice_scatter_7087: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7084, slice_scatter_7086, 1, 9440, 9456);  slice_scatter_7084 = slice_scatter_7086 = None
        slice_38987: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7087, 1, 9440, 9456)
        slice_38988: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_38987, 2, 0, 16)
        slice_scatter_7089: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_38987, slice_38988, 2, 0, 16);  slice_38987 = slice_38988 = None
        slice_scatter_7090: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7087, slice_scatter_7089, 1, 9440, 9456);  slice_scatter_7087 = slice_scatter_7089 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39007: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9456, 9472)
        slice_39008: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_39007, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1185: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_39008, memory_format = torch.contiguous_format);  slice_39008 = None
        view_2374: "f32[32, 16]" = torch.ops.aten.view.default(clone_1185, [32, 16]);  clone_1185 = None
        mm_1182: "f32[32, 8]" = torch.ops.aten.mm.default(view_2374, slice_7)
        view_2375: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1182, [2, 16, 8]);  mm_1182 = None
        slice_39015: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7090, 1, 9456, 9472)
        slice_39016: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39015, 2, 0, 16)
        add_1184: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39016, view_2375);  slice_39016 = view_2375 = None
        slice_scatter_7092: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39015, add_1184, 2, 0, 16);  slice_39015 = add_1184 = None
        slice_scatter_7093: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7090, slice_scatter_7092, 1, 9456, 9472);  slice_scatter_7090 = slice_scatter_7092 = None
        slice_39020: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7093, 1, 9456, 9472)
        slice_39021: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39020, 2, 0, 16)
        slice_scatter_7095: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39020, slice_39021, 2, 0, 16);  slice_39020 = slice_39021 = None
        slice_scatter_7096: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7093, slice_scatter_7095, 1, 9456, 9472);  slice_scatter_7093 = slice_scatter_7095 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39041: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_39007, 2, 16, 32);  slice_39007 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1186: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_39041, memory_format = torch.contiguous_format);  slice_39041 = None
        view_2376: "f32[32, 11]" = torch.ops.aten.view.default(clone_1186, [32, 11]);  clone_1186 = None
        mm_1183: "f32[32, 8]" = torch.ops.aten.mm.default(view_2376, slice_37)
        view_2377: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1183, [2, 16, 8]);  mm_1183 = None
        slice_39048: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7096, 1, 9456, 9472)
        slice_39049: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39048, 2, 0, 16)
        add_1185: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39049, view_2377);  slice_39049 = view_2377 = None
        slice_scatter_7098: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39048, add_1185, 2, 0, 16);  slice_39048 = add_1185 = None
        slice_scatter_7099: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7096, slice_scatter_7098, 1, 9456, 9472);  slice_scatter_7096 = slice_scatter_7098 = None
        slice_39053: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7099, 1, 9456, 9472)
        slice_39054: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39053, 2, 0, 16)
        slice_scatter_7101: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39053, slice_39054, 2, 0, 16);  slice_39053 = slice_39054 = None
        slice_scatter_7102: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7099, slice_scatter_7101, 1, 9456, 9472);  slice_scatter_7099 = slice_scatter_7101 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39073: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9472, 9488)
        slice_39074: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_39073, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1187: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_39074, memory_format = torch.contiguous_format);  slice_39074 = None
        view_2378: "f32[32, 16]" = torch.ops.aten.view.default(clone_1187, [32, 16]);  clone_1187 = None
        mm_1184: "f32[32, 8]" = torch.ops.aten.mm.default(view_2378, slice_7)
        view_2379: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1184, [2, 16, 8]);  mm_1184 = None
        slice_39081: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7102, 1, 9472, 9488)
        slice_39082: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39081, 2, 0, 16)
        add_1186: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39082, view_2379);  slice_39082 = view_2379 = None
        slice_scatter_7104: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39081, add_1186, 2, 0, 16);  slice_39081 = add_1186 = None
        slice_scatter_7105: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7102, slice_scatter_7104, 1, 9472, 9488);  slice_scatter_7102 = slice_scatter_7104 = None
        slice_39086: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7105, 1, 9472, 9488)
        slice_39087: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39086, 2, 0, 16)
        slice_scatter_7107: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39086, slice_39087, 2, 0, 16);  slice_39086 = slice_39087 = None
        slice_scatter_7108: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7105, slice_scatter_7107, 1, 9472, 9488);  slice_scatter_7105 = slice_scatter_7107 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39107: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_39073, 2, 16, 32);  slice_39073 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1188: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_39107, memory_format = torch.contiguous_format);  slice_39107 = None
        view_2380: "f32[32, 11]" = torch.ops.aten.view.default(clone_1188, [32, 11]);  clone_1188 = None
        mm_1185: "f32[32, 8]" = torch.ops.aten.mm.default(view_2380, slice_37)
        view_2381: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1185, [2, 16, 8]);  mm_1185 = None
        slice_39114: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7108, 1, 9472, 9488)
        slice_39115: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39114, 2, 0, 16)
        add_1187: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39115, view_2381);  slice_39115 = view_2381 = None
        slice_scatter_7110: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39114, add_1187, 2, 0, 16);  slice_39114 = add_1187 = None
        slice_scatter_7111: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7108, slice_scatter_7110, 1, 9472, 9488);  slice_scatter_7108 = slice_scatter_7110 = None
        slice_39119: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7111, 1, 9472, 9488)
        slice_39120: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39119, 2, 0, 16)
        slice_scatter_7113: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39119, slice_39120, 2, 0, 16);  slice_39119 = slice_39120 = None
        slice_scatter_7114: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7111, slice_scatter_7113, 1, 9472, 9488);  slice_scatter_7111 = slice_scatter_7113 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39139: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9488, 9504)
        slice_39140: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_39139, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1189: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_39140, memory_format = torch.contiguous_format);  slice_39140 = None
        view_2382: "f32[32, 16]" = torch.ops.aten.view.default(clone_1189, [32, 16]);  clone_1189 = None
        mm_1186: "f32[32, 8]" = torch.ops.aten.mm.default(view_2382, slice_7)
        view_2383: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1186, [2, 16, 8]);  mm_1186 = None
        slice_39147: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7114, 1, 9488, 9504)
        slice_39148: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39147, 2, 0, 16)
        add_1188: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39148, view_2383);  slice_39148 = view_2383 = None
        slice_scatter_7116: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39147, add_1188, 2, 0, 16);  slice_39147 = add_1188 = None
        slice_scatter_7117: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7114, slice_scatter_7116, 1, 9488, 9504);  slice_scatter_7114 = slice_scatter_7116 = None
        slice_39152: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7117, 1, 9488, 9504)
        slice_39153: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39152, 2, 0, 16)
        slice_scatter_7119: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39152, slice_39153, 2, 0, 16);  slice_39152 = slice_39153 = None
        slice_scatter_7120: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7117, slice_scatter_7119, 1, 9488, 9504);  slice_scatter_7117 = slice_scatter_7119 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39173: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_39139, 2, 16, 32);  slice_39139 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1190: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_39173, memory_format = torch.contiguous_format);  slice_39173 = None
        view_2384: "f32[32, 11]" = torch.ops.aten.view.default(clone_1190, [32, 11]);  clone_1190 = None
        mm_1187: "f32[32, 8]" = torch.ops.aten.mm.default(view_2384, slice_37)
        view_2385: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1187, [2, 16, 8]);  mm_1187 = None
        slice_39180: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7120, 1, 9488, 9504)
        slice_39181: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39180, 2, 0, 16)
        add_1189: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39181, view_2385);  slice_39181 = view_2385 = None
        slice_scatter_7122: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39180, add_1189, 2, 0, 16);  slice_39180 = add_1189 = None
        slice_scatter_7123: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7120, slice_scatter_7122, 1, 9488, 9504);  slice_scatter_7120 = slice_scatter_7122 = None
        slice_39185: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7123, 1, 9488, 9504)
        slice_39186: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39185, 2, 0, 16)
        slice_scatter_7125: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39185, slice_39186, 2, 0, 16);  slice_39185 = slice_39186 = None
        slice_scatter_7126: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7123, slice_scatter_7125, 1, 9488, 9504);  slice_scatter_7123 = slice_scatter_7125 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39205: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9504, 9520)
        slice_39206: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_39205, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1191: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_39206, memory_format = torch.contiguous_format);  slice_39206 = None
        view_2386: "f32[32, 16]" = torch.ops.aten.view.default(clone_1191, [32, 16]);  clone_1191 = None
        mm_1188: "f32[32, 8]" = torch.ops.aten.mm.default(view_2386, slice_7)
        view_2387: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1188, [2, 16, 8]);  mm_1188 = None
        slice_39213: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7126, 1, 9504, 9520)
        slice_39214: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39213, 2, 0, 16)
        add_1190: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39214, view_2387);  slice_39214 = view_2387 = None
        slice_scatter_7128: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39213, add_1190, 2, 0, 16);  slice_39213 = add_1190 = None
        slice_scatter_7129: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7126, slice_scatter_7128, 1, 9504, 9520);  slice_scatter_7126 = slice_scatter_7128 = None
        slice_39218: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7129, 1, 9504, 9520)
        slice_39219: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39218, 2, 0, 16)
        slice_scatter_7131: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39218, slice_39219, 2, 0, 16);  slice_39218 = slice_39219 = None
        slice_scatter_7132: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7129, slice_scatter_7131, 1, 9504, 9520);  slice_scatter_7129 = slice_scatter_7131 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39239: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_39205, 2, 16, 32);  slice_39205 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1192: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_39239, memory_format = torch.contiguous_format);  slice_39239 = None
        view_2388: "f32[32, 11]" = torch.ops.aten.view.default(clone_1192, [32, 11]);  clone_1192 = None
        mm_1189: "f32[32, 8]" = torch.ops.aten.mm.default(view_2388, slice_37)
        view_2389: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1189, [2, 16, 8]);  mm_1189 = None
        slice_39246: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7132, 1, 9504, 9520)
        slice_39247: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39246, 2, 0, 16)
        add_1191: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39247, view_2389);  slice_39247 = view_2389 = None
        slice_scatter_7134: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39246, add_1191, 2, 0, 16);  slice_39246 = add_1191 = None
        slice_scatter_7135: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7132, slice_scatter_7134, 1, 9504, 9520);  slice_scatter_7132 = slice_scatter_7134 = None
        slice_39251: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7135, 1, 9504, 9520)
        slice_39252: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39251, 2, 0, 16)
        slice_scatter_7137: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39251, slice_39252, 2, 0, 16);  slice_39251 = slice_39252 = None
        slice_scatter_7138: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7135, slice_scatter_7137, 1, 9504, 9520);  slice_scatter_7135 = slice_scatter_7137 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39271: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9520, 9536)
        slice_39272: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_39271, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1193: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_39272, memory_format = torch.contiguous_format);  slice_39272 = None
        view_2390: "f32[32, 16]" = torch.ops.aten.view.default(clone_1193, [32, 16]);  clone_1193 = None
        mm_1190: "f32[32, 8]" = torch.ops.aten.mm.default(view_2390, slice_7)
        view_2391: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1190, [2, 16, 8]);  mm_1190 = None
        slice_39279: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7138, 1, 9520, 9536)
        slice_39280: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39279, 2, 0, 16)
        add_1192: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39280, view_2391);  slice_39280 = view_2391 = None
        slice_scatter_7140: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39279, add_1192, 2, 0, 16);  slice_39279 = add_1192 = None
        slice_scatter_7141: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7138, slice_scatter_7140, 1, 9520, 9536);  slice_scatter_7138 = slice_scatter_7140 = None
        slice_39284: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7141, 1, 9520, 9536)
        slice_39285: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39284, 2, 0, 16)
        slice_scatter_7143: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39284, slice_39285, 2, 0, 16);  slice_39284 = slice_39285 = None
        slice_scatter_7144: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7141, slice_scatter_7143, 1, 9520, 9536);  slice_scatter_7141 = slice_scatter_7143 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39305: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_39271, 2, 16, 32);  slice_39271 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1194: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_39305, memory_format = torch.contiguous_format);  slice_39305 = None
        view_2392: "f32[32, 11]" = torch.ops.aten.view.default(clone_1194, [32, 11]);  clone_1194 = None
        mm_1191: "f32[32, 8]" = torch.ops.aten.mm.default(view_2392, slice_37)
        view_2393: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1191, [2, 16, 8]);  mm_1191 = None
        slice_39312: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7144, 1, 9520, 9536)
        slice_39313: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39312, 2, 0, 16)
        add_1193: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39313, view_2393);  slice_39313 = view_2393 = None
        slice_scatter_7146: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39312, add_1193, 2, 0, 16);  slice_39312 = add_1193 = None
        slice_scatter_7147: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7144, slice_scatter_7146, 1, 9520, 9536);  slice_scatter_7144 = slice_scatter_7146 = None
        slice_39317: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7147, 1, 9520, 9536)
        slice_39318: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39317, 2, 0, 16)
        slice_scatter_7149: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39317, slice_39318, 2, 0, 16);  slice_39317 = slice_39318 = None
        slice_scatter_7150: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7147, slice_scatter_7149, 1, 9520, 9536);  slice_scatter_7147 = slice_scatter_7149 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39337: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9536, 9552)
        slice_39338: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_39337, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1195: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_39338, memory_format = torch.contiguous_format);  slice_39338 = None
        view_2394: "f32[32, 16]" = torch.ops.aten.view.default(clone_1195, [32, 16]);  clone_1195 = None
        mm_1192: "f32[32, 8]" = torch.ops.aten.mm.default(view_2394, slice_7)
        view_2395: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1192, [2, 16, 8]);  mm_1192 = None
        slice_39345: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7150, 1, 9536, 9552)
        slice_39346: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39345, 2, 0, 16)
        add_1194: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39346, view_2395);  slice_39346 = view_2395 = None
        slice_scatter_7152: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39345, add_1194, 2, 0, 16);  slice_39345 = add_1194 = None
        slice_scatter_7153: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7150, slice_scatter_7152, 1, 9536, 9552);  slice_scatter_7150 = slice_scatter_7152 = None
        slice_39350: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7153, 1, 9536, 9552)
        slice_39351: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39350, 2, 0, 16)
        slice_scatter_7155: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39350, slice_39351, 2, 0, 16);  slice_39350 = slice_39351 = None
        slice_scatter_7156: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7153, slice_scatter_7155, 1, 9536, 9552);  slice_scatter_7153 = slice_scatter_7155 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39371: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_39337, 2, 16, 32);  slice_39337 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1196: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_39371, memory_format = torch.contiguous_format);  slice_39371 = None
        view_2396: "f32[32, 11]" = torch.ops.aten.view.default(clone_1196, [32, 11]);  clone_1196 = None
        mm_1193: "f32[32, 8]" = torch.ops.aten.mm.default(view_2396, slice_37)
        view_2397: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1193, [2, 16, 8]);  mm_1193 = None
        slice_39378: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7156, 1, 9536, 9552)
        slice_39379: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39378, 2, 0, 16)
        add_1195: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39379, view_2397);  slice_39379 = view_2397 = None
        slice_scatter_7158: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39378, add_1195, 2, 0, 16);  slice_39378 = add_1195 = None
        slice_scatter_7159: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7156, slice_scatter_7158, 1, 9536, 9552);  slice_scatter_7156 = slice_scatter_7158 = None
        slice_39383: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7159, 1, 9536, 9552)
        slice_39384: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39383, 2, 0, 16)
        slice_scatter_7161: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39383, slice_39384, 2, 0, 16);  slice_39383 = slice_39384 = None
        slice_scatter_7162: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7159, slice_scatter_7161, 1, 9536, 9552);  slice_scatter_7159 = slice_scatter_7161 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39403: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9552, 9568)
        slice_39404: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_39403, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1197: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_39404, memory_format = torch.contiguous_format);  slice_39404 = None
        view_2398: "f32[32, 16]" = torch.ops.aten.view.default(clone_1197, [32, 16]);  clone_1197 = None
        mm_1194: "f32[32, 8]" = torch.ops.aten.mm.default(view_2398, slice_7)
        view_2399: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1194, [2, 16, 8]);  mm_1194 = None
        slice_39411: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7162, 1, 9552, 9568)
        slice_39412: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39411, 2, 0, 16)
        add_1196: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39412, view_2399);  slice_39412 = view_2399 = None
        slice_scatter_7164: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39411, add_1196, 2, 0, 16);  slice_39411 = add_1196 = None
        slice_scatter_7165: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7162, slice_scatter_7164, 1, 9552, 9568);  slice_scatter_7162 = slice_scatter_7164 = None
        slice_39416: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7165, 1, 9552, 9568)
        slice_39417: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39416, 2, 0, 16)
        slice_scatter_7167: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39416, slice_39417, 2, 0, 16);  slice_39416 = slice_39417 = None
        slice_scatter_7168: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7165, slice_scatter_7167, 1, 9552, 9568);  slice_scatter_7165 = slice_scatter_7167 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39437: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_39403, 2, 16, 32);  slice_39403 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1198: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_39437, memory_format = torch.contiguous_format);  slice_39437 = None
        view_2400: "f32[32, 11]" = torch.ops.aten.view.default(clone_1198, [32, 11]);  clone_1198 = None
        mm_1195: "f32[32, 8]" = torch.ops.aten.mm.default(view_2400, slice_37)
        view_2401: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1195, [2, 16, 8]);  mm_1195 = None
        slice_39444: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7168, 1, 9552, 9568)
        slice_39445: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39444, 2, 0, 16)
        add_1197: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39445, view_2401);  slice_39445 = view_2401 = None
        slice_scatter_7170: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39444, add_1197, 2, 0, 16);  slice_39444 = add_1197 = None
        slice_scatter_7171: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7168, slice_scatter_7170, 1, 9552, 9568);  slice_scatter_7168 = slice_scatter_7170 = None
        slice_39449: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7171, 1, 9552, 9568)
        slice_39450: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39449, 2, 0, 16)
        slice_scatter_7173: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39449, slice_39450, 2, 0, 16);  slice_39449 = slice_39450 = None
        slice_scatter_7174: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7171, slice_scatter_7173, 1, 9552, 9568);  slice_scatter_7171 = slice_scatter_7173 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39469: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9568, 9584)
        slice_39470: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_39469, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1199: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_39470, memory_format = torch.contiguous_format);  slice_39470 = None
        view_2402: "f32[32, 16]" = torch.ops.aten.view.default(clone_1199, [32, 16]);  clone_1199 = None
        mm_1196: "f32[32, 8]" = torch.ops.aten.mm.default(view_2402, slice_7)
        view_2403: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1196, [2, 16, 8]);  mm_1196 = None
        slice_39477: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7174, 1, 9568, 9584)
        slice_39478: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39477, 2, 0, 16)
        add_1198: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39478, view_2403);  slice_39478 = view_2403 = None
        slice_scatter_7176: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39477, add_1198, 2, 0, 16);  slice_39477 = add_1198 = None
        slice_scatter_7177: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7174, slice_scatter_7176, 1, 9568, 9584);  slice_scatter_7174 = slice_scatter_7176 = None
        slice_39482: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7177, 1, 9568, 9584)
        slice_39483: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39482, 2, 0, 16)
        slice_scatter_7179: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39482, slice_39483, 2, 0, 16);  slice_39482 = slice_39483 = None
        slice_scatter_7180: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7177, slice_scatter_7179, 1, 9568, 9584);  slice_scatter_7177 = slice_scatter_7179 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39503: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_39469, 2, 16, 32);  slice_39469 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1200: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_39503, memory_format = torch.contiguous_format);  slice_39503 = None
        view_2404: "f32[32, 11]" = torch.ops.aten.view.default(clone_1200, [32, 11]);  clone_1200 = None
        mm_1197: "f32[32, 8]" = torch.ops.aten.mm.default(view_2404, slice_37)
        view_2405: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1197, [2, 16, 8]);  mm_1197 = None
        slice_39510: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7180, 1, 9568, 9584)
        slice_39511: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39510, 2, 0, 16)
        add_1199: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39511, view_2405);  slice_39511 = view_2405 = None
        slice_scatter_7182: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39510, add_1199, 2, 0, 16);  slice_39510 = add_1199 = None
        slice_scatter_7183: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7180, slice_scatter_7182, 1, 9568, 9584);  slice_scatter_7180 = slice_scatter_7182 = None
        slice_39515: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7183, 1, 9568, 9584)
        slice_39516: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39515, 2, 0, 16)
        slice_scatter_7185: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39515, slice_39516, 2, 0, 16);  slice_39515 = slice_39516 = None
        slice_scatter_7186: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7183, slice_scatter_7185, 1, 9568, 9584);  slice_scatter_7183 = slice_scatter_7185 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39535: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9584, 9600)
        slice_39536: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_39535, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1201: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_39536, memory_format = torch.contiguous_format);  slice_39536 = None
        view_2406: "f32[32, 16]" = torch.ops.aten.view.default(clone_1201, [32, 16]);  clone_1201 = None
        mm_1198: "f32[32, 8]" = torch.ops.aten.mm.default(view_2406, slice_7)
        view_2407: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1198, [2, 16, 8]);  mm_1198 = None
        slice_39543: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7186, 1, 9584, 9600)
        slice_39544: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39543, 2, 0, 16)
        add_1200: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39544, view_2407);  slice_39544 = view_2407 = None
        slice_scatter_7188: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39543, add_1200, 2, 0, 16);  slice_39543 = add_1200 = None
        slice_scatter_7189: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7186, slice_scatter_7188, 1, 9584, 9600);  slice_scatter_7186 = slice_scatter_7188 = None
        slice_39548: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7189, 1, 9584, 9600)
        slice_39549: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39548, 2, 0, 16)
        slice_scatter_7191: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39548, slice_39549, 2, 0, 16);  slice_39548 = slice_39549 = None
        slice_scatter_7192: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7189, slice_scatter_7191, 1, 9584, 9600);  slice_scatter_7189 = slice_scatter_7191 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39569: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_39535, 2, 16, 32);  slice_39535 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1202: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_39569, memory_format = torch.contiguous_format);  slice_39569 = None
        view_2408: "f32[32, 11]" = torch.ops.aten.view.default(clone_1202, [32, 11]);  clone_1202 = None
        mm_1199: "f32[32, 8]" = torch.ops.aten.mm.default(view_2408, slice_37)
        view_2409: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1199, [2, 16, 8]);  mm_1199 = None
        slice_39576: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7192, 1, 9584, 9600)
        slice_39577: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39576, 2, 0, 16)
        add_1201: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39577, view_2409);  slice_39577 = view_2409 = None
        slice_scatter_7194: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39576, add_1201, 2, 0, 16);  slice_39576 = add_1201 = None
        slice_scatter_7195: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7192, slice_scatter_7194, 1, 9584, 9600);  slice_scatter_7192 = slice_scatter_7194 = None
        slice_39581: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7195, 1, 9584, 9600)
        slice_39582: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39581, 2, 0, 16)
        slice_scatter_7197: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39581, slice_39582, 2, 0, 16);  slice_39581 = slice_39582 = None
        slice_scatter_7198: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7195, slice_scatter_7197, 1, 9584, 9600);  slice_scatter_7195 = slice_scatter_7197 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39601: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9600, 9616)
        slice_39602: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_39601, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1203: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_39602, memory_format = torch.contiguous_format);  slice_39602 = None
        view_2410: "f32[32, 16]" = torch.ops.aten.view.default(clone_1203, [32, 16]);  clone_1203 = None
        mm_1200: "f32[32, 8]" = torch.ops.aten.mm.default(view_2410, slice_7)
        view_2411: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1200, [2, 16, 8]);  mm_1200 = None
        slice_39609: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7198, 1, 9600, 9616)
        slice_39610: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39609, 2, 0, 16)
        add_1202: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39610, view_2411);  slice_39610 = view_2411 = None
        slice_scatter_7200: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39609, add_1202, 2, 0, 16);  slice_39609 = add_1202 = None
        slice_scatter_7201: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7198, slice_scatter_7200, 1, 9600, 9616);  slice_scatter_7198 = slice_scatter_7200 = None
        slice_39614: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7201, 1, 9600, 9616)
        slice_39615: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39614, 2, 0, 16)
        slice_scatter_7203: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39614, slice_39615, 2, 0, 16);  slice_39614 = slice_39615 = None
        slice_scatter_7204: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7201, slice_scatter_7203, 1, 9600, 9616);  slice_scatter_7201 = slice_scatter_7203 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39635: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_39601, 2, 16, 32);  slice_39601 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1204: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_39635, memory_format = torch.contiguous_format);  slice_39635 = None
        view_2412: "f32[32, 11]" = torch.ops.aten.view.default(clone_1204, [32, 11]);  clone_1204 = None
        mm_1201: "f32[32, 8]" = torch.ops.aten.mm.default(view_2412, slice_37)
        view_2413: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1201, [2, 16, 8]);  mm_1201 = None
        slice_39642: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7204, 1, 9600, 9616)
        slice_39643: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39642, 2, 0, 16)
        add_1203: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39643, view_2413);  slice_39643 = view_2413 = None
        slice_scatter_7206: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39642, add_1203, 2, 0, 16);  slice_39642 = add_1203 = None
        slice_scatter_7207: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7204, slice_scatter_7206, 1, 9600, 9616);  slice_scatter_7204 = slice_scatter_7206 = None
        slice_39647: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7207, 1, 9600, 9616)
        slice_39648: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39647, 2, 0, 16)
        slice_scatter_7209: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39647, slice_39648, 2, 0, 16);  slice_39647 = slice_39648 = None
        slice_scatter_7210: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7207, slice_scatter_7209, 1, 9600, 9616);  slice_scatter_7207 = slice_scatter_7209 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39667: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9616, 9632)
        slice_39668: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_39667, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1205: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_39668, memory_format = torch.contiguous_format);  slice_39668 = None
        view_2414: "f32[32, 16]" = torch.ops.aten.view.default(clone_1205, [32, 16]);  clone_1205 = None
        mm_1202: "f32[32, 8]" = torch.ops.aten.mm.default(view_2414, slice_7)
        view_2415: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1202, [2, 16, 8]);  mm_1202 = None
        slice_39675: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7210, 1, 9616, 9632)
        slice_39676: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39675, 2, 0, 16)
        add_1204: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39676, view_2415);  slice_39676 = view_2415 = None
        slice_scatter_7212: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39675, add_1204, 2, 0, 16);  slice_39675 = add_1204 = None
        slice_scatter_7213: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7210, slice_scatter_7212, 1, 9616, 9632);  slice_scatter_7210 = slice_scatter_7212 = None
        slice_39680: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7213, 1, 9616, 9632)
        slice_39681: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39680, 2, 0, 16)
        slice_scatter_7215: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39680, slice_39681, 2, 0, 16);  slice_39680 = slice_39681 = None
        slice_scatter_7216: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7213, slice_scatter_7215, 1, 9616, 9632);  slice_scatter_7213 = slice_scatter_7215 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39701: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_39667, 2, 16, 32);  slice_39667 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1206: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_39701, memory_format = torch.contiguous_format);  slice_39701 = None
        view_2416: "f32[32, 11]" = torch.ops.aten.view.default(clone_1206, [32, 11]);  clone_1206 = None
        mm_1203: "f32[32, 8]" = torch.ops.aten.mm.default(view_2416, slice_37)
        view_2417: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1203, [2, 16, 8]);  mm_1203 = None
        slice_39708: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7216, 1, 9616, 9632)
        slice_39709: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39708, 2, 0, 16)
        add_1205: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39709, view_2417);  slice_39709 = view_2417 = None
        slice_scatter_7218: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39708, add_1205, 2, 0, 16);  slice_39708 = add_1205 = None
        slice_scatter_7219: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7216, slice_scatter_7218, 1, 9616, 9632);  slice_scatter_7216 = slice_scatter_7218 = None
        slice_39713: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7219, 1, 9616, 9632)
        slice_39714: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39713, 2, 0, 16)
        slice_scatter_7221: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39713, slice_39714, 2, 0, 16);  slice_39713 = slice_39714 = None
        slice_scatter_7222: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7219, slice_scatter_7221, 1, 9616, 9632);  slice_scatter_7219 = slice_scatter_7221 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39733: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9632, 9648)
        slice_39734: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_39733, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1207: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_39734, memory_format = torch.contiguous_format);  slice_39734 = None
        view_2418: "f32[32, 16]" = torch.ops.aten.view.default(clone_1207, [32, 16]);  clone_1207 = None
        mm_1204: "f32[32, 8]" = torch.ops.aten.mm.default(view_2418, slice_7)
        view_2419: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1204, [2, 16, 8]);  mm_1204 = None
        slice_39741: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7222, 1, 9632, 9648)
        slice_39742: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39741, 2, 0, 16)
        add_1206: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39742, view_2419);  slice_39742 = view_2419 = None
        slice_scatter_7224: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39741, add_1206, 2, 0, 16);  slice_39741 = add_1206 = None
        slice_scatter_7225: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7222, slice_scatter_7224, 1, 9632, 9648);  slice_scatter_7222 = slice_scatter_7224 = None
        slice_39746: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7225, 1, 9632, 9648)
        slice_39747: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39746, 2, 0, 16)
        slice_scatter_7227: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39746, slice_39747, 2, 0, 16);  slice_39746 = slice_39747 = None
        slice_scatter_7228: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7225, slice_scatter_7227, 1, 9632, 9648);  slice_scatter_7225 = slice_scatter_7227 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39767: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_39733, 2, 16, 32);  slice_39733 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1208: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_39767, memory_format = torch.contiguous_format);  slice_39767 = None
        view_2420: "f32[32, 11]" = torch.ops.aten.view.default(clone_1208, [32, 11]);  clone_1208 = None
        mm_1205: "f32[32, 8]" = torch.ops.aten.mm.default(view_2420, slice_37)
        view_2421: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1205, [2, 16, 8]);  mm_1205 = None
        slice_39774: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7228, 1, 9632, 9648)
        slice_39775: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39774, 2, 0, 16)
        add_1207: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39775, view_2421);  slice_39775 = view_2421 = None
        slice_scatter_7230: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39774, add_1207, 2, 0, 16);  slice_39774 = add_1207 = None
        slice_scatter_7231: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7228, slice_scatter_7230, 1, 9632, 9648);  slice_scatter_7228 = slice_scatter_7230 = None
        slice_39779: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7231, 1, 9632, 9648)
        slice_39780: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39779, 2, 0, 16)
        slice_scatter_7233: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39779, slice_39780, 2, 0, 16);  slice_39779 = slice_39780 = None
        slice_scatter_7234: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7231, slice_scatter_7233, 1, 9632, 9648);  slice_scatter_7231 = slice_scatter_7233 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39799: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9648, 9664)
        slice_39800: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_39799, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1209: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_39800, memory_format = torch.contiguous_format);  slice_39800 = None
        view_2422: "f32[32, 16]" = torch.ops.aten.view.default(clone_1209, [32, 16]);  clone_1209 = None
        mm_1206: "f32[32, 8]" = torch.ops.aten.mm.default(view_2422, slice_7)
        view_2423: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1206, [2, 16, 8]);  mm_1206 = None
        slice_39807: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7234, 1, 9648, 9664)
        slice_39808: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39807, 2, 0, 16)
        add_1208: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39808, view_2423);  slice_39808 = view_2423 = None
        slice_scatter_7236: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39807, add_1208, 2, 0, 16);  slice_39807 = add_1208 = None
        slice_scatter_7237: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7234, slice_scatter_7236, 1, 9648, 9664);  slice_scatter_7234 = slice_scatter_7236 = None
        slice_39812: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7237, 1, 9648, 9664)
        slice_39813: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39812, 2, 0, 16)
        slice_scatter_7239: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39812, slice_39813, 2, 0, 16);  slice_39812 = slice_39813 = None
        slice_scatter_7240: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7237, slice_scatter_7239, 1, 9648, 9664);  slice_scatter_7237 = slice_scatter_7239 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39833: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_39799, 2, 16, 32);  slice_39799 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1210: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_39833, memory_format = torch.contiguous_format);  slice_39833 = None
        view_2424: "f32[32, 11]" = torch.ops.aten.view.default(clone_1210, [32, 11]);  clone_1210 = None
        mm_1207: "f32[32, 8]" = torch.ops.aten.mm.default(view_2424, slice_37)
        view_2425: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1207, [2, 16, 8]);  mm_1207 = None
        slice_39840: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7240, 1, 9648, 9664)
        slice_39841: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39840, 2, 0, 16)
        add_1209: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39841, view_2425);  slice_39841 = view_2425 = None
        slice_scatter_7242: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39840, add_1209, 2, 0, 16);  slice_39840 = add_1209 = None
        slice_scatter_7243: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7240, slice_scatter_7242, 1, 9648, 9664);  slice_scatter_7240 = slice_scatter_7242 = None
        slice_39845: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7243, 1, 9648, 9664)
        slice_39846: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39845, 2, 0, 16)
        slice_scatter_7245: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39845, slice_39846, 2, 0, 16);  slice_39845 = slice_39846 = None
        slice_scatter_7246: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7243, slice_scatter_7245, 1, 9648, 9664);  slice_scatter_7243 = slice_scatter_7245 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39865: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9664, 9680)
        slice_39866: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_39865, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1211: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_39866, memory_format = torch.contiguous_format);  slice_39866 = None
        view_2426: "f32[32, 16]" = torch.ops.aten.view.default(clone_1211, [32, 16]);  clone_1211 = None
        mm_1208: "f32[32, 8]" = torch.ops.aten.mm.default(view_2426, slice_7)
        view_2427: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1208, [2, 16, 8]);  mm_1208 = None
        slice_39873: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7246, 1, 9664, 9680)
        slice_39874: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39873, 2, 0, 16)
        add_1210: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39874, view_2427);  slice_39874 = view_2427 = None
        slice_scatter_7248: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39873, add_1210, 2, 0, 16);  slice_39873 = add_1210 = None
        slice_scatter_7249: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7246, slice_scatter_7248, 1, 9664, 9680);  slice_scatter_7246 = slice_scatter_7248 = None
        slice_39878: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7249, 1, 9664, 9680)
        slice_39879: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39878, 2, 0, 16)
        slice_scatter_7251: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39878, slice_39879, 2, 0, 16);  slice_39878 = slice_39879 = None
        slice_scatter_7252: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7249, slice_scatter_7251, 1, 9664, 9680);  slice_scatter_7249 = slice_scatter_7251 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39899: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_39865, 2, 16, 32);  slice_39865 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1212: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_39899, memory_format = torch.contiguous_format);  slice_39899 = None
        view_2428: "f32[32, 11]" = torch.ops.aten.view.default(clone_1212, [32, 11]);  clone_1212 = None
        mm_1209: "f32[32, 8]" = torch.ops.aten.mm.default(view_2428, slice_37)
        view_2429: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1209, [2, 16, 8]);  mm_1209 = None
        slice_39906: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7252, 1, 9664, 9680)
        slice_39907: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39906, 2, 0, 16)
        add_1211: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39907, view_2429);  slice_39907 = view_2429 = None
        slice_scatter_7254: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39906, add_1211, 2, 0, 16);  slice_39906 = add_1211 = None
        slice_scatter_7255: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7252, slice_scatter_7254, 1, 9664, 9680);  slice_scatter_7252 = slice_scatter_7254 = None
        slice_39911: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7255, 1, 9664, 9680)
        slice_39912: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39911, 2, 0, 16)
        slice_scatter_7257: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39911, slice_39912, 2, 0, 16);  slice_39911 = slice_39912 = None
        slice_scatter_7258: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7255, slice_scatter_7257, 1, 9664, 9680);  slice_scatter_7255 = slice_scatter_7257 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39931: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9680, 9696)
        slice_39932: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_39931, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1213: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_39932, memory_format = torch.contiguous_format);  slice_39932 = None
        view_2430: "f32[32, 16]" = torch.ops.aten.view.default(clone_1213, [32, 16]);  clone_1213 = None
        mm_1210: "f32[32, 8]" = torch.ops.aten.mm.default(view_2430, slice_7)
        view_2431: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1210, [2, 16, 8]);  mm_1210 = None
        slice_39939: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7258, 1, 9680, 9696)
        slice_39940: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39939, 2, 0, 16)
        add_1212: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39940, view_2431);  slice_39940 = view_2431 = None
        slice_scatter_7260: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39939, add_1212, 2, 0, 16);  slice_39939 = add_1212 = None
        slice_scatter_7261: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7258, slice_scatter_7260, 1, 9680, 9696);  slice_scatter_7258 = slice_scatter_7260 = None
        slice_39944: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7261, 1, 9680, 9696)
        slice_39945: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39944, 2, 0, 16)
        slice_scatter_7263: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39944, slice_39945, 2, 0, 16);  slice_39944 = slice_39945 = None
        slice_scatter_7264: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7261, slice_scatter_7263, 1, 9680, 9696);  slice_scatter_7261 = slice_scatter_7263 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39965: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_39931, 2, 16, 32);  slice_39931 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1214: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_39965, memory_format = torch.contiguous_format);  slice_39965 = None
        view_2432: "f32[32, 11]" = torch.ops.aten.view.default(clone_1214, [32, 11]);  clone_1214 = None
        mm_1211: "f32[32, 8]" = torch.ops.aten.mm.default(view_2432, slice_37)
        view_2433: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1211, [2, 16, 8]);  mm_1211 = None
        slice_39972: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7264, 1, 9680, 9696)
        slice_39973: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39972, 2, 0, 16)
        add_1213: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_39973, view_2433);  slice_39973 = view_2433 = None
        slice_scatter_7266: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39972, add_1213, 2, 0, 16);  slice_39972 = add_1213 = None
        slice_scatter_7267: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7264, slice_scatter_7266, 1, 9680, 9696);  slice_scatter_7264 = slice_scatter_7266 = None
        slice_39977: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7267, 1, 9680, 9696)
        slice_39978: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_39977, 2, 0, 16)
        slice_scatter_7269: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_39977, slice_39978, 2, 0, 16);  slice_39977 = slice_39978 = None
        slice_scatter_7270: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7267, slice_scatter_7269, 1, 9680, 9696);  slice_scatter_7267 = slice_scatter_7269 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_39997: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9696, 9712)
        slice_39998: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_39997, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1215: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_39998, memory_format = torch.contiguous_format);  slice_39998 = None
        view_2434: "f32[32, 16]" = torch.ops.aten.view.default(clone_1215, [32, 16]);  clone_1215 = None
        mm_1212: "f32[32, 8]" = torch.ops.aten.mm.default(view_2434, slice_7)
        view_2435: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1212, [2, 16, 8]);  mm_1212 = None
        slice_40005: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7270, 1, 9696, 9712)
        slice_40006: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40005, 2, 0, 16)
        add_1214: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40006, view_2435);  slice_40006 = view_2435 = None
        slice_scatter_7272: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40005, add_1214, 2, 0, 16);  slice_40005 = add_1214 = None
        slice_scatter_7273: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7270, slice_scatter_7272, 1, 9696, 9712);  slice_scatter_7270 = slice_scatter_7272 = None
        slice_40010: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7273, 1, 9696, 9712)
        slice_40011: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40010, 2, 0, 16)
        slice_scatter_7275: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40010, slice_40011, 2, 0, 16);  slice_40010 = slice_40011 = None
        slice_scatter_7276: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7273, slice_scatter_7275, 1, 9696, 9712);  slice_scatter_7273 = slice_scatter_7275 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40031: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_39997, 2, 16, 32);  slice_39997 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1216: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_40031, memory_format = torch.contiguous_format);  slice_40031 = None
        view_2436: "f32[32, 11]" = torch.ops.aten.view.default(clone_1216, [32, 11]);  clone_1216 = None
        mm_1213: "f32[32, 8]" = torch.ops.aten.mm.default(view_2436, slice_37)
        view_2437: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1213, [2, 16, 8]);  mm_1213 = None
        slice_40038: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7276, 1, 9696, 9712)
        slice_40039: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40038, 2, 0, 16)
        add_1215: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40039, view_2437);  slice_40039 = view_2437 = None
        slice_scatter_7278: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40038, add_1215, 2, 0, 16);  slice_40038 = add_1215 = None
        slice_scatter_7279: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7276, slice_scatter_7278, 1, 9696, 9712);  slice_scatter_7276 = slice_scatter_7278 = None
        slice_40043: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7279, 1, 9696, 9712)
        slice_40044: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40043, 2, 0, 16)
        slice_scatter_7281: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40043, slice_40044, 2, 0, 16);  slice_40043 = slice_40044 = None
        slice_scatter_7282: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7279, slice_scatter_7281, 1, 9696, 9712);  slice_scatter_7279 = slice_scatter_7281 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40063: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9712, 9728)
        slice_40064: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_40063, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1217: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_40064, memory_format = torch.contiguous_format);  slice_40064 = None
        view_2438: "f32[32, 16]" = torch.ops.aten.view.default(clone_1217, [32, 16]);  clone_1217 = None
        mm_1214: "f32[32, 8]" = torch.ops.aten.mm.default(view_2438, slice_7)
        view_2439: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1214, [2, 16, 8]);  mm_1214 = None
        slice_40071: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7282, 1, 9712, 9728)
        slice_40072: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40071, 2, 0, 16)
        add_1216: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40072, view_2439);  slice_40072 = view_2439 = None
        slice_scatter_7284: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40071, add_1216, 2, 0, 16);  slice_40071 = add_1216 = None
        slice_scatter_7285: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7282, slice_scatter_7284, 1, 9712, 9728);  slice_scatter_7282 = slice_scatter_7284 = None
        slice_40076: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7285, 1, 9712, 9728)
        slice_40077: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40076, 2, 0, 16)
        slice_scatter_7287: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40076, slice_40077, 2, 0, 16);  slice_40076 = slice_40077 = None
        slice_scatter_7288: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7285, slice_scatter_7287, 1, 9712, 9728);  slice_scatter_7285 = slice_scatter_7287 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40097: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_40063, 2, 16, 32);  slice_40063 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1218: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_40097, memory_format = torch.contiguous_format);  slice_40097 = None
        view_2440: "f32[32, 11]" = torch.ops.aten.view.default(clone_1218, [32, 11]);  clone_1218 = None
        mm_1215: "f32[32, 8]" = torch.ops.aten.mm.default(view_2440, slice_37)
        view_2441: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1215, [2, 16, 8]);  mm_1215 = None
        slice_40104: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7288, 1, 9712, 9728)
        slice_40105: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40104, 2, 0, 16)
        add_1217: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40105, view_2441);  slice_40105 = view_2441 = None
        slice_scatter_7290: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40104, add_1217, 2, 0, 16);  slice_40104 = add_1217 = None
        slice_scatter_7291: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7288, slice_scatter_7290, 1, 9712, 9728);  slice_scatter_7288 = slice_scatter_7290 = None
        slice_40109: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7291, 1, 9712, 9728)
        slice_40110: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40109, 2, 0, 16)
        slice_scatter_7293: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40109, slice_40110, 2, 0, 16);  slice_40109 = slice_40110 = None
        slice_scatter_7294: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7291, slice_scatter_7293, 1, 9712, 9728);  slice_scatter_7291 = slice_scatter_7293 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40129: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9728, 9744)
        slice_40130: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_40129, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1219: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_40130, memory_format = torch.contiguous_format);  slice_40130 = None
        view_2442: "f32[32, 16]" = torch.ops.aten.view.default(clone_1219, [32, 16]);  clone_1219 = None
        mm_1216: "f32[32, 8]" = torch.ops.aten.mm.default(view_2442, slice_7)
        view_2443: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1216, [2, 16, 8]);  mm_1216 = None
        slice_40137: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7294, 1, 9728, 9744)
        slice_40138: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40137, 2, 0, 16)
        add_1218: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40138, view_2443);  slice_40138 = view_2443 = None
        slice_scatter_7296: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40137, add_1218, 2, 0, 16);  slice_40137 = add_1218 = None
        slice_scatter_7297: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7294, slice_scatter_7296, 1, 9728, 9744);  slice_scatter_7294 = slice_scatter_7296 = None
        slice_40142: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7297, 1, 9728, 9744)
        slice_40143: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40142, 2, 0, 16)
        slice_scatter_7299: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40142, slice_40143, 2, 0, 16);  slice_40142 = slice_40143 = None
        slice_scatter_7300: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7297, slice_scatter_7299, 1, 9728, 9744);  slice_scatter_7297 = slice_scatter_7299 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40163: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_40129, 2, 16, 32);  slice_40129 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1220: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_40163, memory_format = torch.contiguous_format);  slice_40163 = None
        view_2444: "f32[32, 11]" = torch.ops.aten.view.default(clone_1220, [32, 11]);  clone_1220 = None
        mm_1217: "f32[32, 8]" = torch.ops.aten.mm.default(view_2444, slice_37)
        view_2445: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1217, [2, 16, 8]);  mm_1217 = None
        slice_40170: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7300, 1, 9728, 9744)
        slice_40171: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40170, 2, 0, 16)
        add_1219: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40171, view_2445);  slice_40171 = view_2445 = None
        slice_scatter_7302: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40170, add_1219, 2, 0, 16);  slice_40170 = add_1219 = None
        slice_scatter_7303: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7300, slice_scatter_7302, 1, 9728, 9744);  slice_scatter_7300 = slice_scatter_7302 = None
        slice_40175: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7303, 1, 9728, 9744)
        slice_40176: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40175, 2, 0, 16)
        slice_scatter_7305: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40175, slice_40176, 2, 0, 16);  slice_40175 = slice_40176 = None
        slice_scatter_7306: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7303, slice_scatter_7305, 1, 9728, 9744);  slice_scatter_7303 = slice_scatter_7305 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40195: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9744, 9760)
        slice_40196: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_40195, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1221: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_40196, memory_format = torch.contiguous_format);  slice_40196 = None
        view_2446: "f32[32, 16]" = torch.ops.aten.view.default(clone_1221, [32, 16]);  clone_1221 = None
        mm_1218: "f32[32, 8]" = torch.ops.aten.mm.default(view_2446, slice_7)
        view_2447: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1218, [2, 16, 8]);  mm_1218 = None
        slice_40203: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7306, 1, 9744, 9760)
        slice_40204: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40203, 2, 0, 16)
        add_1220: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40204, view_2447);  slice_40204 = view_2447 = None
        slice_scatter_7308: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40203, add_1220, 2, 0, 16);  slice_40203 = add_1220 = None
        slice_scatter_7309: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7306, slice_scatter_7308, 1, 9744, 9760);  slice_scatter_7306 = slice_scatter_7308 = None
        slice_40208: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7309, 1, 9744, 9760)
        slice_40209: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40208, 2, 0, 16)
        slice_scatter_7311: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40208, slice_40209, 2, 0, 16);  slice_40208 = slice_40209 = None
        slice_scatter_7312: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7309, slice_scatter_7311, 1, 9744, 9760);  slice_scatter_7309 = slice_scatter_7311 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40229: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_40195, 2, 16, 32);  slice_40195 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1222: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_40229, memory_format = torch.contiguous_format);  slice_40229 = None
        view_2448: "f32[32, 11]" = torch.ops.aten.view.default(clone_1222, [32, 11]);  clone_1222 = None
        mm_1219: "f32[32, 8]" = torch.ops.aten.mm.default(view_2448, slice_37)
        view_2449: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1219, [2, 16, 8]);  mm_1219 = None
        slice_40236: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7312, 1, 9744, 9760)
        slice_40237: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40236, 2, 0, 16)
        add_1221: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40237, view_2449);  slice_40237 = view_2449 = None
        slice_scatter_7314: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40236, add_1221, 2, 0, 16);  slice_40236 = add_1221 = None
        slice_scatter_7315: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7312, slice_scatter_7314, 1, 9744, 9760);  slice_scatter_7312 = slice_scatter_7314 = None
        slice_40241: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7315, 1, 9744, 9760)
        slice_40242: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40241, 2, 0, 16)
        slice_scatter_7317: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40241, slice_40242, 2, 0, 16);  slice_40241 = slice_40242 = None
        slice_scatter_7318: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7315, slice_scatter_7317, 1, 9744, 9760);  slice_scatter_7315 = slice_scatter_7317 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40261: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9760, 9776)
        slice_40262: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_40261, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1223: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_40262, memory_format = torch.contiguous_format);  slice_40262 = None
        view_2450: "f32[32, 16]" = torch.ops.aten.view.default(clone_1223, [32, 16]);  clone_1223 = None
        mm_1220: "f32[32, 8]" = torch.ops.aten.mm.default(view_2450, slice_7)
        view_2451: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1220, [2, 16, 8]);  mm_1220 = None
        slice_40269: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7318, 1, 9760, 9776)
        slice_40270: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40269, 2, 0, 16)
        add_1222: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40270, view_2451);  slice_40270 = view_2451 = None
        slice_scatter_7320: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40269, add_1222, 2, 0, 16);  slice_40269 = add_1222 = None
        slice_scatter_7321: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7318, slice_scatter_7320, 1, 9760, 9776);  slice_scatter_7318 = slice_scatter_7320 = None
        slice_40274: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7321, 1, 9760, 9776)
        slice_40275: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40274, 2, 0, 16)
        slice_scatter_7323: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40274, slice_40275, 2, 0, 16);  slice_40274 = slice_40275 = None
        slice_scatter_7324: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7321, slice_scatter_7323, 1, 9760, 9776);  slice_scatter_7321 = slice_scatter_7323 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40295: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_40261, 2, 16, 32);  slice_40261 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1224: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_40295, memory_format = torch.contiguous_format);  slice_40295 = None
        view_2452: "f32[32, 11]" = torch.ops.aten.view.default(clone_1224, [32, 11]);  clone_1224 = None
        mm_1221: "f32[32, 8]" = torch.ops.aten.mm.default(view_2452, slice_37)
        view_2453: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1221, [2, 16, 8]);  mm_1221 = None
        slice_40302: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7324, 1, 9760, 9776)
        slice_40303: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40302, 2, 0, 16)
        add_1223: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40303, view_2453);  slice_40303 = view_2453 = None
        slice_scatter_7326: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40302, add_1223, 2, 0, 16);  slice_40302 = add_1223 = None
        slice_scatter_7327: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7324, slice_scatter_7326, 1, 9760, 9776);  slice_scatter_7324 = slice_scatter_7326 = None
        slice_40307: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7327, 1, 9760, 9776)
        slice_40308: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40307, 2, 0, 16)
        slice_scatter_7329: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40307, slice_40308, 2, 0, 16);  slice_40307 = slice_40308 = None
        slice_scatter_7330: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7327, slice_scatter_7329, 1, 9760, 9776);  slice_scatter_7327 = slice_scatter_7329 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40327: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9776, 9792)
        slice_40328: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_40327, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1225: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_40328, memory_format = torch.contiguous_format);  slice_40328 = None
        view_2454: "f32[32, 16]" = torch.ops.aten.view.default(clone_1225, [32, 16]);  clone_1225 = None
        mm_1222: "f32[32, 8]" = torch.ops.aten.mm.default(view_2454, slice_7)
        view_2455: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1222, [2, 16, 8]);  mm_1222 = None
        slice_40335: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7330, 1, 9776, 9792)
        slice_40336: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40335, 2, 0, 16)
        add_1224: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40336, view_2455);  slice_40336 = view_2455 = None
        slice_scatter_7332: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40335, add_1224, 2, 0, 16);  slice_40335 = add_1224 = None
        slice_scatter_7333: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7330, slice_scatter_7332, 1, 9776, 9792);  slice_scatter_7330 = slice_scatter_7332 = None
        slice_40340: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7333, 1, 9776, 9792)
        slice_40341: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40340, 2, 0, 16)
        slice_scatter_7335: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40340, slice_40341, 2, 0, 16);  slice_40340 = slice_40341 = None
        slice_scatter_7336: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7333, slice_scatter_7335, 1, 9776, 9792);  slice_scatter_7333 = slice_scatter_7335 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40361: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_40327, 2, 16, 32);  slice_40327 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1226: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_40361, memory_format = torch.contiguous_format);  slice_40361 = None
        view_2456: "f32[32, 11]" = torch.ops.aten.view.default(clone_1226, [32, 11]);  clone_1226 = None
        mm_1223: "f32[32, 8]" = torch.ops.aten.mm.default(view_2456, slice_37)
        view_2457: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1223, [2, 16, 8]);  mm_1223 = None
        slice_40368: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7336, 1, 9776, 9792)
        slice_40369: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40368, 2, 0, 16)
        add_1225: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40369, view_2457);  slice_40369 = view_2457 = None
        slice_scatter_7338: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40368, add_1225, 2, 0, 16);  slice_40368 = add_1225 = None
        slice_scatter_7339: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7336, slice_scatter_7338, 1, 9776, 9792);  slice_scatter_7336 = slice_scatter_7338 = None
        slice_40373: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7339, 1, 9776, 9792)
        slice_40374: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40373, 2, 0, 16)
        slice_scatter_7341: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40373, slice_40374, 2, 0, 16);  slice_40373 = slice_40374 = None
        slice_scatter_7342: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7339, slice_scatter_7341, 1, 9776, 9792);  slice_scatter_7339 = slice_scatter_7341 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40393: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9792, 9808)
        slice_40394: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_40393, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1227: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_40394, memory_format = torch.contiguous_format);  slice_40394 = None
        view_2458: "f32[32, 16]" = torch.ops.aten.view.default(clone_1227, [32, 16]);  clone_1227 = None
        mm_1224: "f32[32, 8]" = torch.ops.aten.mm.default(view_2458, slice_7)
        view_2459: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1224, [2, 16, 8]);  mm_1224 = None
        slice_40401: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7342, 1, 9792, 9808)
        slice_40402: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40401, 2, 0, 16)
        add_1226: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40402, view_2459);  slice_40402 = view_2459 = None
        slice_scatter_7344: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40401, add_1226, 2, 0, 16);  slice_40401 = add_1226 = None
        slice_scatter_7345: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7342, slice_scatter_7344, 1, 9792, 9808);  slice_scatter_7342 = slice_scatter_7344 = None
        slice_40406: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7345, 1, 9792, 9808)
        slice_40407: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40406, 2, 0, 16)
        slice_scatter_7347: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40406, slice_40407, 2, 0, 16);  slice_40406 = slice_40407 = None
        slice_scatter_7348: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7345, slice_scatter_7347, 1, 9792, 9808);  slice_scatter_7345 = slice_scatter_7347 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40427: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_40393, 2, 16, 32);  slice_40393 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1228: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_40427, memory_format = torch.contiguous_format);  slice_40427 = None
        view_2460: "f32[32, 11]" = torch.ops.aten.view.default(clone_1228, [32, 11]);  clone_1228 = None
        mm_1225: "f32[32, 8]" = torch.ops.aten.mm.default(view_2460, slice_37)
        view_2461: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1225, [2, 16, 8]);  mm_1225 = None
        slice_40434: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7348, 1, 9792, 9808)
        slice_40435: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40434, 2, 0, 16)
        add_1227: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40435, view_2461);  slice_40435 = view_2461 = None
        slice_scatter_7350: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40434, add_1227, 2, 0, 16);  slice_40434 = add_1227 = None
        slice_scatter_7351: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7348, slice_scatter_7350, 1, 9792, 9808);  slice_scatter_7348 = slice_scatter_7350 = None
        slice_40439: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7351, 1, 9792, 9808)
        slice_40440: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40439, 2, 0, 16)
        slice_scatter_7353: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40439, slice_40440, 2, 0, 16);  slice_40439 = slice_40440 = None
        slice_scatter_7354: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7351, slice_scatter_7353, 1, 9792, 9808);  slice_scatter_7351 = slice_scatter_7353 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40459: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9808, 9824)
        slice_40460: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_40459, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1229: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_40460, memory_format = torch.contiguous_format);  slice_40460 = None
        view_2462: "f32[32, 16]" = torch.ops.aten.view.default(clone_1229, [32, 16]);  clone_1229 = None
        mm_1226: "f32[32, 8]" = torch.ops.aten.mm.default(view_2462, slice_7)
        view_2463: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1226, [2, 16, 8]);  mm_1226 = None
        slice_40467: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7354, 1, 9808, 9824)
        slice_40468: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40467, 2, 0, 16)
        add_1228: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40468, view_2463);  slice_40468 = view_2463 = None
        slice_scatter_7356: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40467, add_1228, 2, 0, 16);  slice_40467 = add_1228 = None
        slice_scatter_7357: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7354, slice_scatter_7356, 1, 9808, 9824);  slice_scatter_7354 = slice_scatter_7356 = None
        slice_40472: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7357, 1, 9808, 9824)
        slice_40473: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40472, 2, 0, 16)
        slice_scatter_7359: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40472, slice_40473, 2, 0, 16);  slice_40472 = slice_40473 = None
        slice_scatter_7360: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7357, slice_scatter_7359, 1, 9808, 9824);  slice_scatter_7357 = slice_scatter_7359 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40493: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_40459, 2, 16, 32);  slice_40459 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1230: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_40493, memory_format = torch.contiguous_format);  slice_40493 = None
        view_2464: "f32[32, 11]" = torch.ops.aten.view.default(clone_1230, [32, 11]);  clone_1230 = None
        mm_1227: "f32[32, 8]" = torch.ops.aten.mm.default(view_2464, slice_37)
        view_2465: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1227, [2, 16, 8]);  mm_1227 = None
        slice_40500: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7360, 1, 9808, 9824)
        slice_40501: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40500, 2, 0, 16)
        add_1229: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40501, view_2465);  slice_40501 = view_2465 = None
        slice_scatter_7362: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40500, add_1229, 2, 0, 16);  slice_40500 = add_1229 = None
        slice_scatter_7363: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7360, slice_scatter_7362, 1, 9808, 9824);  slice_scatter_7360 = slice_scatter_7362 = None
        slice_40505: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7363, 1, 9808, 9824)
        slice_40506: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40505, 2, 0, 16)
        slice_scatter_7365: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40505, slice_40506, 2, 0, 16);  slice_40505 = slice_40506 = None
        slice_scatter_7366: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7363, slice_scatter_7365, 1, 9808, 9824);  slice_scatter_7363 = slice_scatter_7365 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40525: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9824, 9840)
        slice_40526: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_40525, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1231: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_40526, memory_format = torch.contiguous_format);  slice_40526 = None
        view_2466: "f32[32, 16]" = torch.ops.aten.view.default(clone_1231, [32, 16]);  clone_1231 = None
        mm_1228: "f32[32, 8]" = torch.ops.aten.mm.default(view_2466, slice_7)
        view_2467: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1228, [2, 16, 8]);  mm_1228 = None
        slice_40533: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7366, 1, 9824, 9840)
        slice_40534: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40533, 2, 0, 16)
        add_1230: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40534, view_2467);  slice_40534 = view_2467 = None
        slice_scatter_7368: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40533, add_1230, 2, 0, 16);  slice_40533 = add_1230 = None
        slice_scatter_7369: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7366, slice_scatter_7368, 1, 9824, 9840);  slice_scatter_7366 = slice_scatter_7368 = None
        slice_40538: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7369, 1, 9824, 9840)
        slice_40539: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40538, 2, 0, 16)
        slice_scatter_7371: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40538, slice_40539, 2, 0, 16);  slice_40538 = slice_40539 = None
        slice_scatter_7372: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7369, slice_scatter_7371, 1, 9824, 9840);  slice_scatter_7369 = slice_scatter_7371 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40559: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_40525, 2, 16, 32);  slice_40525 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1232: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_40559, memory_format = torch.contiguous_format);  slice_40559 = None
        view_2468: "f32[32, 11]" = torch.ops.aten.view.default(clone_1232, [32, 11]);  clone_1232 = None
        mm_1229: "f32[32, 8]" = torch.ops.aten.mm.default(view_2468, slice_37)
        view_2469: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1229, [2, 16, 8]);  mm_1229 = None
        slice_40566: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7372, 1, 9824, 9840)
        slice_40567: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40566, 2, 0, 16)
        add_1231: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40567, view_2469);  slice_40567 = view_2469 = None
        slice_scatter_7374: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40566, add_1231, 2, 0, 16);  slice_40566 = add_1231 = None
        slice_scatter_7375: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7372, slice_scatter_7374, 1, 9824, 9840);  slice_scatter_7372 = slice_scatter_7374 = None
        slice_40571: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7375, 1, 9824, 9840)
        slice_40572: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40571, 2, 0, 16)
        slice_scatter_7377: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40571, slice_40572, 2, 0, 16);  slice_40571 = slice_40572 = None
        slice_scatter_7378: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7375, slice_scatter_7377, 1, 9824, 9840);  slice_scatter_7375 = slice_scatter_7377 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40591: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9840, 9856)
        slice_40592: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_40591, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1233: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_40592, memory_format = torch.contiguous_format);  slice_40592 = None
        view_2470: "f32[32, 16]" = torch.ops.aten.view.default(clone_1233, [32, 16]);  clone_1233 = None
        mm_1230: "f32[32, 8]" = torch.ops.aten.mm.default(view_2470, slice_7)
        view_2471: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1230, [2, 16, 8]);  mm_1230 = None
        slice_40599: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7378, 1, 9840, 9856)
        slice_40600: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40599, 2, 0, 16)
        add_1232: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40600, view_2471);  slice_40600 = view_2471 = None
        slice_scatter_7380: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40599, add_1232, 2, 0, 16);  slice_40599 = add_1232 = None
        slice_scatter_7381: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7378, slice_scatter_7380, 1, 9840, 9856);  slice_scatter_7378 = slice_scatter_7380 = None
        slice_40604: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7381, 1, 9840, 9856)
        slice_40605: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40604, 2, 0, 16)
        slice_scatter_7383: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40604, slice_40605, 2, 0, 16);  slice_40604 = slice_40605 = None
        slice_scatter_7384: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7381, slice_scatter_7383, 1, 9840, 9856);  slice_scatter_7381 = slice_scatter_7383 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40625: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_40591, 2, 16, 32);  slice_40591 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1234: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_40625, memory_format = torch.contiguous_format);  slice_40625 = None
        view_2472: "f32[32, 11]" = torch.ops.aten.view.default(clone_1234, [32, 11]);  clone_1234 = None
        mm_1231: "f32[32, 8]" = torch.ops.aten.mm.default(view_2472, slice_37)
        view_2473: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1231, [2, 16, 8]);  mm_1231 = None
        slice_40632: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7384, 1, 9840, 9856)
        slice_40633: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40632, 2, 0, 16)
        add_1233: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40633, view_2473);  slice_40633 = view_2473 = None
        slice_scatter_7386: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40632, add_1233, 2, 0, 16);  slice_40632 = add_1233 = None
        slice_scatter_7387: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7384, slice_scatter_7386, 1, 9840, 9856);  slice_scatter_7384 = slice_scatter_7386 = None
        slice_40637: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7387, 1, 9840, 9856)
        slice_40638: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40637, 2, 0, 16)
        slice_scatter_7389: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40637, slice_40638, 2, 0, 16);  slice_40637 = slice_40638 = None
        slice_scatter_7390: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7387, slice_scatter_7389, 1, 9840, 9856);  slice_scatter_7387 = slice_scatter_7389 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40657: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9856, 9872)
        slice_40658: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_40657, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1235: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_40658, memory_format = torch.contiguous_format);  slice_40658 = None
        view_2474: "f32[32, 16]" = torch.ops.aten.view.default(clone_1235, [32, 16]);  clone_1235 = None
        mm_1232: "f32[32, 8]" = torch.ops.aten.mm.default(view_2474, slice_7)
        view_2475: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1232, [2, 16, 8]);  mm_1232 = None
        slice_40665: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7390, 1, 9856, 9872)
        slice_40666: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40665, 2, 0, 16)
        add_1234: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40666, view_2475);  slice_40666 = view_2475 = None
        slice_scatter_7392: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40665, add_1234, 2, 0, 16);  slice_40665 = add_1234 = None
        slice_scatter_7393: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7390, slice_scatter_7392, 1, 9856, 9872);  slice_scatter_7390 = slice_scatter_7392 = None
        slice_40670: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7393, 1, 9856, 9872)
        slice_40671: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40670, 2, 0, 16)
        slice_scatter_7395: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40670, slice_40671, 2, 0, 16);  slice_40670 = slice_40671 = None
        slice_scatter_7396: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7393, slice_scatter_7395, 1, 9856, 9872);  slice_scatter_7393 = slice_scatter_7395 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40691: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_40657, 2, 16, 32);  slice_40657 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1236: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_40691, memory_format = torch.contiguous_format);  slice_40691 = None
        view_2476: "f32[32, 11]" = torch.ops.aten.view.default(clone_1236, [32, 11]);  clone_1236 = None
        mm_1233: "f32[32, 8]" = torch.ops.aten.mm.default(view_2476, slice_37)
        view_2477: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1233, [2, 16, 8]);  mm_1233 = None
        slice_40698: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7396, 1, 9856, 9872)
        slice_40699: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40698, 2, 0, 16)
        add_1235: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40699, view_2477);  slice_40699 = view_2477 = None
        slice_scatter_7398: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40698, add_1235, 2, 0, 16);  slice_40698 = add_1235 = None
        slice_scatter_7399: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7396, slice_scatter_7398, 1, 9856, 9872);  slice_scatter_7396 = slice_scatter_7398 = None
        slice_40703: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7399, 1, 9856, 9872)
        slice_40704: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40703, 2, 0, 16)
        slice_scatter_7401: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40703, slice_40704, 2, 0, 16);  slice_40703 = slice_40704 = None
        slice_scatter_7402: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7399, slice_scatter_7401, 1, 9856, 9872);  slice_scatter_7399 = slice_scatter_7401 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40723: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9872, 9888)
        slice_40724: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_40723, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1237: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_40724, memory_format = torch.contiguous_format);  slice_40724 = None
        view_2478: "f32[32, 16]" = torch.ops.aten.view.default(clone_1237, [32, 16]);  clone_1237 = None
        mm_1234: "f32[32, 8]" = torch.ops.aten.mm.default(view_2478, slice_7)
        view_2479: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1234, [2, 16, 8]);  mm_1234 = None
        slice_40731: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7402, 1, 9872, 9888)
        slice_40732: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40731, 2, 0, 16)
        add_1236: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40732, view_2479);  slice_40732 = view_2479 = None
        slice_scatter_7404: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40731, add_1236, 2, 0, 16);  slice_40731 = add_1236 = None
        slice_scatter_7405: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7402, slice_scatter_7404, 1, 9872, 9888);  slice_scatter_7402 = slice_scatter_7404 = None
        slice_40736: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7405, 1, 9872, 9888)
        slice_40737: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40736, 2, 0, 16)
        slice_scatter_7407: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40736, slice_40737, 2, 0, 16);  slice_40736 = slice_40737 = None
        slice_scatter_7408: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7405, slice_scatter_7407, 1, 9872, 9888);  slice_scatter_7405 = slice_scatter_7407 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40757: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_40723, 2, 16, 32);  slice_40723 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1238: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_40757, memory_format = torch.contiguous_format);  slice_40757 = None
        view_2480: "f32[32, 11]" = torch.ops.aten.view.default(clone_1238, [32, 11]);  clone_1238 = None
        mm_1235: "f32[32, 8]" = torch.ops.aten.mm.default(view_2480, slice_37)
        view_2481: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1235, [2, 16, 8]);  mm_1235 = None
        slice_40764: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7408, 1, 9872, 9888)
        slice_40765: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40764, 2, 0, 16)
        add_1237: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40765, view_2481);  slice_40765 = view_2481 = None
        slice_scatter_7410: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40764, add_1237, 2, 0, 16);  slice_40764 = add_1237 = None
        slice_scatter_7411: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7408, slice_scatter_7410, 1, 9872, 9888);  slice_scatter_7408 = slice_scatter_7410 = None
        slice_40769: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7411, 1, 9872, 9888)
        slice_40770: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40769, 2, 0, 16)
        slice_scatter_7413: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40769, slice_40770, 2, 0, 16);  slice_40769 = slice_40770 = None
        slice_scatter_7414: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7411, slice_scatter_7413, 1, 9872, 9888);  slice_scatter_7411 = slice_scatter_7413 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40789: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9888, 9904)
        slice_40790: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_40789, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1239: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_40790, memory_format = torch.contiguous_format);  slice_40790 = None
        view_2482: "f32[32, 16]" = torch.ops.aten.view.default(clone_1239, [32, 16]);  clone_1239 = None
        mm_1236: "f32[32, 8]" = torch.ops.aten.mm.default(view_2482, slice_7)
        view_2483: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1236, [2, 16, 8]);  mm_1236 = None
        slice_40797: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7414, 1, 9888, 9904)
        slice_40798: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40797, 2, 0, 16)
        add_1238: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40798, view_2483);  slice_40798 = view_2483 = None
        slice_scatter_7416: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40797, add_1238, 2, 0, 16);  slice_40797 = add_1238 = None
        slice_scatter_7417: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7414, slice_scatter_7416, 1, 9888, 9904);  slice_scatter_7414 = slice_scatter_7416 = None
        slice_40802: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7417, 1, 9888, 9904)
        slice_40803: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40802, 2, 0, 16)
        slice_scatter_7419: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40802, slice_40803, 2, 0, 16);  slice_40802 = slice_40803 = None
        slice_scatter_7420: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7417, slice_scatter_7419, 1, 9888, 9904);  slice_scatter_7417 = slice_scatter_7419 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40823: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_40789, 2, 16, 32);  slice_40789 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1240: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_40823, memory_format = torch.contiguous_format);  slice_40823 = None
        view_2484: "f32[32, 11]" = torch.ops.aten.view.default(clone_1240, [32, 11]);  clone_1240 = None
        mm_1237: "f32[32, 8]" = torch.ops.aten.mm.default(view_2484, slice_37)
        view_2485: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1237, [2, 16, 8]);  mm_1237 = None
        slice_40830: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7420, 1, 9888, 9904)
        slice_40831: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40830, 2, 0, 16)
        add_1239: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40831, view_2485);  slice_40831 = view_2485 = None
        slice_scatter_7422: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40830, add_1239, 2, 0, 16);  slice_40830 = add_1239 = None
        slice_scatter_7423: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7420, slice_scatter_7422, 1, 9888, 9904);  slice_scatter_7420 = slice_scatter_7422 = None
        slice_40835: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7423, 1, 9888, 9904)
        slice_40836: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40835, 2, 0, 16)
        slice_scatter_7425: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40835, slice_40836, 2, 0, 16);  slice_40835 = slice_40836 = None
        slice_scatter_7426: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7423, slice_scatter_7425, 1, 9888, 9904);  slice_scatter_7423 = slice_scatter_7425 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40855: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9904, 9920)
        slice_40856: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_40855, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1241: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_40856, memory_format = torch.contiguous_format);  slice_40856 = None
        view_2486: "f32[32, 16]" = torch.ops.aten.view.default(clone_1241, [32, 16]);  clone_1241 = None
        mm_1238: "f32[32, 8]" = torch.ops.aten.mm.default(view_2486, slice_7)
        view_2487: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1238, [2, 16, 8]);  mm_1238 = None
        slice_40863: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7426, 1, 9904, 9920)
        slice_40864: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40863, 2, 0, 16)
        add_1240: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40864, view_2487);  slice_40864 = view_2487 = None
        slice_scatter_7428: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40863, add_1240, 2, 0, 16);  slice_40863 = add_1240 = None
        slice_scatter_7429: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7426, slice_scatter_7428, 1, 9904, 9920);  slice_scatter_7426 = slice_scatter_7428 = None
        slice_40868: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7429, 1, 9904, 9920)
        slice_40869: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40868, 2, 0, 16)
        slice_scatter_7431: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40868, slice_40869, 2, 0, 16);  slice_40868 = slice_40869 = None
        slice_scatter_7432: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7429, slice_scatter_7431, 1, 9904, 9920);  slice_scatter_7429 = slice_scatter_7431 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40889: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_40855, 2, 16, 32);  slice_40855 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1242: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_40889, memory_format = torch.contiguous_format);  slice_40889 = None
        view_2488: "f32[32, 11]" = torch.ops.aten.view.default(clone_1242, [32, 11]);  clone_1242 = None
        mm_1239: "f32[32, 8]" = torch.ops.aten.mm.default(view_2488, slice_37)
        view_2489: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1239, [2, 16, 8]);  mm_1239 = None
        slice_40896: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7432, 1, 9904, 9920)
        slice_40897: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40896, 2, 0, 16)
        add_1241: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40897, view_2489);  slice_40897 = view_2489 = None
        slice_scatter_7434: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40896, add_1241, 2, 0, 16);  slice_40896 = add_1241 = None
        slice_scatter_7435: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7432, slice_scatter_7434, 1, 9904, 9920);  slice_scatter_7432 = slice_scatter_7434 = None
        slice_40901: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7435, 1, 9904, 9920)
        slice_40902: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40901, 2, 0, 16)
        slice_scatter_7437: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40901, slice_40902, 2, 0, 16);  slice_40901 = slice_40902 = None
        slice_scatter_7438: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7435, slice_scatter_7437, 1, 9904, 9920);  slice_scatter_7435 = slice_scatter_7437 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40921: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9920, 9936)
        slice_40922: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_40921, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1243: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_40922, memory_format = torch.contiguous_format);  slice_40922 = None
        view_2490: "f32[32, 16]" = torch.ops.aten.view.default(clone_1243, [32, 16]);  clone_1243 = None
        mm_1240: "f32[32, 8]" = torch.ops.aten.mm.default(view_2490, slice_7)
        view_2491: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1240, [2, 16, 8]);  mm_1240 = None
        slice_40929: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7438, 1, 9920, 9936)
        slice_40930: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40929, 2, 0, 16)
        add_1242: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40930, view_2491);  slice_40930 = view_2491 = None
        slice_scatter_7440: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40929, add_1242, 2, 0, 16);  slice_40929 = add_1242 = None
        slice_scatter_7441: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7438, slice_scatter_7440, 1, 9920, 9936);  slice_scatter_7438 = slice_scatter_7440 = None
        slice_40934: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7441, 1, 9920, 9936)
        slice_40935: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40934, 2, 0, 16)
        slice_scatter_7443: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40934, slice_40935, 2, 0, 16);  slice_40934 = slice_40935 = None
        slice_scatter_7444: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7441, slice_scatter_7443, 1, 9920, 9936);  slice_scatter_7441 = slice_scatter_7443 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40955: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_40921, 2, 16, 32);  slice_40921 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1244: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_40955, memory_format = torch.contiguous_format);  slice_40955 = None
        view_2492: "f32[32, 11]" = torch.ops.aten.view.default(clone_1244, [32, 11]);  clone_1244 = None
        mm_1241: "f32[32, 8]" = torch.ops.aten.mm.default(view_2492, slice_37)
        view_2493: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1241, [2, 16, 8]);  mm_1241 = None
        slice_40962: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7444, 1, 9920, 9936)
        slice_40963: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40962, 2, 0, 16)
        add_1243: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40963, view_2493);  slice_40963 = view_2493 = None
        slice_scatter_7446: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40962, add_1243, 2, 0, 16);  slice_40962 = add_1243 = None
        slice_scatter_7447: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7444, slice_scatter_7446, 1, 9920, 9936);  slice_scatter_7444 = slice_scatter_7446 = None
        slice_40967: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7447, 1, 9920, 9936)
        slice_40968: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40967, 2, 0, 16)
        slice_scatter_7449: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40967, slice_40968, 2, 0, 16);  slice_40967 = slice_40968 = None
        slice_scatter_7450: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7447, slice_scatter_7449, 1, 9920, 9936);  slice_scatter_7447 = slice_scatter_7449 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_40987: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9936, 9952)
        slice_40988: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_40987, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1245: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_40988, memory_format = torch.contiguous_format);  slice_40988 = None
        view_2494: "f32[32, 16]" = torch.ops.aten.view.default(clone_1245, [32, 16]);  clone_1245 = None
        mm_1242: "f32[32, 8]" = torch.ops.aten.mm.default(view_2494, slice_7)
        view_2495: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1242, [2, 16, 8]);  mm_1242 = None
        slice_40995: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7450, 1, 9936, 9952)
        slice_40996: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_40995, 2, 0, 16)
        add_1244: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_40996, view_2495);  slice_40996 = view_2495 = None
        slice_scatter_7452: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_40995, add_1244, 2, 0, 16);  slice_40995 = add_1244 = None
        slice_scatter_7453: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7450, slice_scatter_7452, 1, 9936, 9952);  slice_scatter_7450 = slice_scatter_7452 = None
        slice_41000: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7453, 1, 9936, 9952)
        slice_41001: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_41000, 2, 0, 16)
        slice_scatter_7455: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_41000, slice_41001, 2, 0, 16);  slice_41000 = slice_41001 = None
        slice_scatter_7456: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7453, slice_scatter_7455, 1, 9936, 9952);  slice_scatter_7453 = slice_scatter_7455 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_41021: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_40987, 2, 16, 32);  slice_40987 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1246: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_41021, memory_format = torch.contiguous_format);  slice_41021 = None
        view_2496: "f32[32, 11]" = torch.ops.aten.view.default(clone_1246, [32, 11]);  clone_1246 = None
        mm_1243: "f32[32, 8]" = torch.ops.aten.mm.default(view_2496, slice_37)
        view_2497: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1243, [2, 16, 8]);  mm_1243 = None
        slice_41028: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7456, 1, 9936, 9952)
        slice_41029: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_41028, 2, 0, 16)
        add_1245: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_41029, view_2497);  slice_41029 = view_2497 = None
        slice_scatter_7458: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_41028, add_1245, 2, 0, 16);  slice_41028 = add_1245 = None
        slice_scatter_7459: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7456, slice_scatter_7458, 1, 9936, 9952);  slice_scatter_7456 = slice_scatter_7458 = None
        slice_41033: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7459, 1, 9936, 9952)
        slice_41034: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_41033, 2, 0, 16)
        slice_scatter_7461: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_41033, slice_41034, 2, 0, 16);  slice_41033 = slice_41034 = None
        slice_scatter_7462: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7459, slice_scatter_7461, 1, 9936, 9952);  slice_scatter_7459 = slice_scatter_7461 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_41053: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9952, 9968)
        slice_41054: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_41053, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1247: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_41054, memory_format = torch.contiguous_format);  slice_41054 = None
        view_2498: "f32[32, 16]" = torch.ops.aten.view.default(clone_1247, [32, 16]);  clone_1247 = None
        mm_1244: "f32[32, 8]" = torch.ops.aten.mm.default(view_2498, slice_7)
        view_2499: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1244, [2, 16, 8]);  mm_1244 = None
        slice_41061: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7462, 1, 9952, 9968)
        slice_41062: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_41061, 2, 0, 16)
        add_1246: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_41062, view_2499);  slice_41062 = view_2499 = None
        slice_scatter_7464: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_41061, add_1246, 2, 0, 16);  slice_41061 = add_1246 = None
        slice_scatter_7465: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7462, slice_scatter_7464, 1, 9952, 9968);  slice_scatter_7462 = slice_scatter_7464 = None
        slice_41066: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7465, 1, 9952, 9968)
        slice_41067: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_41066, 2, 0, 16)
        slice_scatter_7467: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_41066, slice_41067, 2, 0, 16);  slice_41066 = slice_41067 = None
        slice_scatter_7468: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7465, slice_scatter_7467, 1, 9952, 9968);  slice_scatter_7465 = slice_scatter_7467 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_41087: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_41053, 2, 16, 32);  slice_41053 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1248: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_41087, memory_format = torch.contiguous_format);  slice_41087 = None
        view_2500: "f32[32, 11]" = torch.ops.aten.view.default(clone_1248, [32, 11]);  clone_1248 = None
        mm_1245: "f32[32, 8]" = torch.ops.aten.mm.default(view_2500, slice_37)
        view_2501: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1245, [2, 16, 8]);  mm_1245 = None
        slice_41094: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7468, 1, 9952, 9968)
        slice_41095: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_41094, 2, 0, 16)
        add_1247: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_41095, view_2501);  slice_41095 = view_2501 = None
        slice_scatter_7470: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_41094, add_1247, 2, 0, 16);  slice_41094 = add_1247 = None
        slice_scatter_7471: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7468, slice_scatter_7470, 1, 9952, 9968);  slice_scatter_7468 = slice_scatter_7470 = None
        slice_41099: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7471, 1, 9952, 9968)
        slice_41100: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_41099, 2, 0, 16)
        slice_scatter_7473: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_41099, slice_41100, 2, 0, 16);  slice_41099 = slice_41100 = None
        slice_scatter_7474: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7471, slice_scatter_7473, 1, 9952, 9968);  slice_scatter_7471 = slice_scatter_7473 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_41119: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9968, 9984)
        slice_41120: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_41119, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1249: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_41120, memory_format = torch.contiguous_format);  slice_41120 = None
        view_2502: "f32[32, 16]" = torch.ops.aten.view.default(clone_1249, [32, 16]);  clone_1249 = None
        mm_1246: "f32[32, 8]" = torch.ops.aten.mm.default(view_2502, slice_7)
        view_2503: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1246, [2, 16, 8]);  mm_1246 = None
        slice_41127: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7474, 1, 9968, 9984)
        slice_41128: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_41127, 2, 0, 16)
        add_1248: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_41128, view_2503);  slice_41128 = view_2503 = None
        slice_scatter_7476: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_41127, add_1248, 2, 0, 16);  slice_41127 = add_1248 = None
        slice_scatter_7477: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7474, slice_scatter_7476, 1, 9968, 9984);  slice_scatter_7474 = slice_scatter_7476 = None
        slice_41132: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7477, 1, 9968, 9984)
        slice_41133: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_41132, 2, 0, 16)
        slice_scatter_7479: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_41132, slice_41133, 2, 0, 16);  slice_41132 = slice_41133 = None
        slice_scatter_7480: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7477, slice_scatter_7479, 1, 9968, 9984);  slice_scatter_7477 = slice_scatter_7479 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_41153: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_41119, 2, 16, 32);  slice_41119 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1250: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_41153, memory_format = torch.contiguous_format);  slice_41153 = None
        view_2504: "f32[32, 11]" = torch.ops.aten.view.default(clone_1250, [32, 11]);  clone_1250 = None
        mm_1247: "f32[32, 8]" = torch.ops.aten.mm.default(view_2504, slice_37)
        view_2505: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1247, [2, 16, 8]);  mm_1247 = None
        slice_41160: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7480, 1, 9968, 9984)
        slice_41161: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_41160, 2, 0, 16)
        add_1249: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_41161, view_2505);  slice_41161 = view_2505 = None
        slice_scatter_7482: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_41160, add_1249, 2, 0, 16);  slice_41160 = add_1249 = None
        slice_scatter_7483: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7480, slice_scatter_7482, 1, 9968, 9984);  slice_scatter_7480 = slice_scatter_7482 = None
        slice_41165: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7483, 1, 9968, 9984)
        slice_41166: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_41165, 2, 0, 16)
        slice_scatter_7485: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_41165, slice_41166, 2, 0, 16);  slice_41165 = slice_41166 = None
        slice_scatter_7486: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7483, slice_scatter_7485, 1, 9968, 9984);  slice_scatter_7483 = slice_scatter_7485 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_41185: "f32[2, 16, 27]" = torch.ops.aten.slice.Tensor(view_8, 1, 9984, 10000);  view_8 = None
        slice_41186: "f32[2, 16, 16]" = torch.ops.aten.slice.Tensor(slice_41185, 2, 0, 16)
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1251: "f32[2, 16, 16]" = torch.ops.aten.clone.default(slice_41186, memory_format = torch.contiguous_format);  slice_41186 = None
        view_2506: "f32[32, 16]" = torch.ops.aten.view.default(clone_1251, [32, 16]);  clone_1251 = None
        mm_1248: "f32[32, 8]" = torch.ops.aten.mm.default(view_2506, slice_7);  slice_7 = None
        view_2507: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1248, [2, 16, 8]);  mm_1248 = None
        slice_41193: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7486, 1, 9984, 10000)
        slice_41194: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_41193, 2, 0, 16)
        add_1250: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_41194, view_2507);  slice_41194 = view_2507 = None
        slice_scatter_7488: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_41193, add_1250, 2, 0, 16);  slice_41193 = add_1250 = None
        slice_scatter_7489: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7486, slice_scatter_7488, 1, 9984, 10000);  slice_scatter_7486 = slice_scatter_7488 = None
        slice_41198: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7489, 1, 9984, 10000)
        slice_41199: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_41198, 2, 0, 16)
        slice_scatter_7491: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_41198, slice_41199, 2, 0, 16);  slice_41198 = slice_41199 = None
        slice_scatter_7492: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7489, slice_scatter_7491, 1, 9984, 10000);  slice_scatter_7489 = slice_scatter_7491 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:77 in conv2d_manual, code: cols_tile = cols[:, i:i+TILE_SIZE, k:k+TILE_SIZE]  # (N, TILE_SIZE, TILE_SIZE)
        slice_41219: "f32[2, 16, 11]" = torch.ops.aten.slice.Tensor(slice_41185, 2, 16, 32);  slice_41185 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        clone_1252: "f32[2, 16, 11]" = torch.ops.aten.clone.default(slice_41219, memory_format = torch.contiguous_format);  slice_41219 = None
        view_2508: "f32[32, 11]" = torch.ops.aten.view.default(clone_1252, [32, 11]);  clone_1252 = None
        mm_1249: "f32[32, 8]" = torch.ops.aten.mm.default(view_2508, slice_37);  slice_37 = None
        view_2509: "f32[2, 16, 8]" = torch.ops.aten.view.default(mm_1249, [2, 16, 8]);  mm_1249 = None
        slice_41226: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7492, 1, 9984, 10000)
        slice_41227: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_41226, 2, 0, 16)
        add_1251: "f32[2, 16, 8]" = torch.ops.aten.add.Tensor(slice_41227, view_2509);  slice_41227 = view_2509 = None
        slice_scatter_7494: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_41226, add_1251, 2, 0, 16);  slice_41226 = add_1251 = None
        slice_scatter_7495: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7492, slice_scatter_7494, 1, 9984, 10000);  slice_scatter_7492 = slice_scatter_7494 = None
        slice_41231: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_scatter_7495, 1, 9984, 10000)
        slice_41232: "f32[2, 16, 8]" = torch.ops.aten.slice.Tensor(slice_41231, 2, 0, 16)
        slice_scatter_7497: "f32[2, 16, 8]" = torch.ops.aten.slice_scatter.default(slice_41231, slice_41232, 2, 0, 16);  slice_41231 = slice_41232 = None
        slice_scatter_7498: "f32[2, 10000, 8]" = torch.ops.aten.slice_scatter.default(slice_scatter_7495, slice_scatter_7497, 1, 9984, 10000);  slice_scatter_7495 = slice_scatter_7497 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:81 in conv2d_manual, code: out = out + self.bias # (N, out_h*out_w, C_out) + (C_out,) > (N, out_h*out_w, C_out)
        add_1252: "f32[2, 10000, 8]" = torch.ops.aten.add.Tensor(slice_scatter_7498, primals_3);  slice_scatter_7498 = primals_3 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:83 in conv2d_manual, code: out = out.permute(0, 2, 1).contiguous().view(N, C_out, int(self.out_h), int(self.out_w)) # (N, C_out, out_h, out_w)
        permute_2: "f32[2, 8, 10000]" = torch.ops.aten.permute.default(add_1252, [0, 2, 1]);  add_1252 = None
        clone_1253: "f32[2, 8, 10000]" = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None
        view_2510: "f32[2, 8, 100, 100]" = torch.ops.aten.view.default(clone_1253, [2, 8, 100, 100]);  clone_1253 = None
        
         # File: /home/ec2-user/ML_compilers/mp2/gpu/myconv.py:79 in conv2d_manual, code: out[:, i:i+TILE_SIZE, j:j+TILE_SIZE] += torch.matmul(cols_tile, weight_tile) # (N, TILE_SIZE, TILE_SIZE)
        permute_4: "f32[11, 32]" = torch.ops.aten.permute.default(view_2508, [1, 0]);  view_2508 = None
        permute_5: "f32[16, 32]" = torch.ops.aten.permute.default(view_2506, [1, 0]);  view_2506 = None
        permute_6: "f32[11, 32]" = torch.ops.aten.permute.default(view_2504, [1, 0]);  view_2504 = None
        permute_7: "f32[16, 32]" = torch.ops.aten.permute.default(view_2502, [1, 0]);  view_2502 = None
        permute_8: "f32[11, 32]" = torch.ops.aten.permute.default(view_2500, [1, 0]);  view_2500 = None
        permute_9: "f32[16, 32]" = torch.ops.aten.permute.default(view_2498, [1, 0]);  view_2498 = None
        permute_10: "f32[11, 32]" = torch.ops.aten.permute.default(view_2496, [1, 0]);  view_2496 = None
        permute_11: "f32[16, 32]" = torch.ops.aten.permute.default(view_2494, [1, 0]);  view_2494 = None
        permute_12: "f32[11, 32]" = torch.ops.aten.permute.default(view_2492, [1, 0]);  view_2492 = None
        permute_13: "f32[16, 32]" = torch.ops.aten.permute.default(view_2490, [1, 0]);  view_2490 = None
        permute_14: "f32[11, 32]" = torch.ops.aten.permute.default(view_2488, [1, 0]);  view_2488 = None
        permute_15: "f32[16, 32]" = torch.ops.aten.permute.default(view_2486, [1, 0]);  view_2486 = None
        permute_16: "f32[11, 32]" = torch.ops.aten.permute.default(view_2484, [1, 0]);  view_2484 = None
        permute_17: "f32[16, 32]" = torch.ops.aten.permute.default(view_2482, [1, 0]);  view_2482 = None
        permute_18: "f32[11, 32]" = torch.ops.aten.permute.default(view_2480, [1, 0]);  view_2480 = None
        permute_19: "f32[16, 32]" = torch.ops.aten.permute.default(view_2478, [1, 0]);  view_2478 = None
        permute_20: "f32[11, 32]" = torch.ops.aten.permute.default(view_2476, [1, 0]);  view_2476 = None
        permute_21: "f32[16, 32]" = torch.ops.aten.permute.default(view_2474, [1, 0]);  view_2474 = None
        permute_22: "f32[11, 32]" = torch.ops.aten.permute.default(view_2472, [1, 0]);  view_2472 = None
        permute_23: "f32[16, 32]" = torch.ops.aten.permute.default(view_2470, [1, 0]);  view_2470 = None
        permute_24: "f32[11, 32]" = torch.ops.aten.permute.default(view_2468, [1, 0]);  view_2468 = None
        permute_25: "f32[16, 32]" = torch.ops.aten.permute.default(view_2466, [1, 0]);  view_2466 = None
        permute_26: "f32[11, 32]" = torch.ops.aten.permute.default(view_2464, [1, 0]);  view_2464 = None
        permute_27: "f32[16, 32]" = torch.ops.aten.permute.default(view_2462, [1, 0]);  view_2462 = None
        permute_28: "f32[11, 32]" = torch.ops.aten.permute.default(view_2460, [1, 0]);  view_2460 = None
        permute_29: "f32[16, 32]" = torch.ops.aten.permute.default(view_2458, [1, 0]);  view_2458 = None
        permute_30: "f32[11, 32]" = torch.ops.aten.permute.default(view_2456, [1, 0]);  view_2456 = None
        permute_31: "f32[16, 32]" = torch.ops.aten.permute.default(view_2454, [1, 0]);  view_2454 = None
        permute_32: "f32[11, 32]" = torch.ops.aten.permute.default(view_2452, [1, 0]);  view_2452 = None
        permute_33: "f32[16, 32]" = torch.ops.aten.permute.default(view_2450, [1, 0]);  view_2450 = None
        permute_34: "f32[11, 32]" = torch.ops.aten.permute.default(view_2448, [1, 0]);  view_2448 = None
        permute_35: "f32[16, 32]" = torch.ops.aten.permute.default(view_2446, [1, 0]);  view_2446 = None
        permute_36: "f32[11, 32]" = torch.ops.aten.permute.default(view_2444, [1, 0]);  view_2444 = None
        permute_37: "f32[16, 32]" = torch.ops.aten.permute.default(view_2442, [1, 0]);  view_2442 = None
        permute_38: "f32[11, 32]" = torch.ops.aten.permute.default(view_2440, [1, 0]);  view_2440 = None
        permute_39: "f32[16, 32]" = torch.ops.aten.permute.default(view_2438, [1, 0]);  view_2438 = None
        permute_40: "f32[11, 32]" = torch.ops.aten.permute.default(view_2436, [1, 0]);  view_2436 = None
        permute_41: "f32[16, 32]" = torch.ops.aten.permute.default(view_2434, [1, 0]);  view_2434 = None
        permute_42: "f32[11, 32]" = torch.ops.aten.permute.default(view_2432, [1, 0]);  view_2432 = None
        permute_43: "f32[16, 32]" = torch.ops.aten.permute.default(view_2430, [1, 0]);  view_2430 = None
        permute_44: "f32[11, 32]" = torch.ops.aten.permute.default(view_2428, [1, 0]);  view_2428 = None
        permute_45: "f32[16, 32]" = torch.ops.aten.permute.default(view_2426, [1, 0]);  view_2426 = None
        permute_46: "f32[11, 32]" = torch.ops.aten.permute.default(view_2424, [1, 0]);  view_2424 = None
        permute_47: "f32[16, 32]" = torch.ops.aten.permute.default(view_2422, [1, 0]);  view_2422 = None
        permute_48: "f32[11, 32]" = torch.ops.aten.permute.default(view_2420, [1, 0]);  view_2420 = None
        permute_49: "f32[16, 32]" = torch.ops.aten.permute.default(view_2418, [1, 0]);  view_2418 = None
        permute_50: "f32[11, 32]" = torch.ops.aten.permute.default(view_2416, [1, 0]);  view_2416 = None
        permute_51: "f32[16, 32]" = torch.ops.aten.permute.default(view_2414, [1, 0]);  view_2414 = None
        permute_52: "f32[11, 32]" = torch.ops.aten.permute.default(view_2412, [1, 0]);  view_2412 = None
        permute_53: "f32[16, 32]" = torch.ops.aten.permute.default(view_2410, [1, 0]);  view_2410 = None
        permute_54: "f32[11, 32]" = torch.ops.aten.permute.default(view_2408, [1, 0]);  view_2408 = None
        permute_55: "f32[16, 32]" = torch.ops.aten.permute.default(view_2406, [1, 0]);  view_2406 = None
        permute_56: "f32[11, 32]" = torch.ops.aten.permute.default(view_2404, [1, 0]);  view_2404 = None
        permute_57: "f32[16, 32]" = torch.ops.aten.permute.default(view_2402, [1, 0]);  view_2402 = None
        permute_58: "f32[11, 32]" = torch.ops.aten.permute.default(view_2400, [1, 0]);  view_2400 = None
        permute_59: "f32[16, 32]" = torch.ops.aten.permute.default(view_2398, [1, 0]);  view_2398 = None
        permute_60: "f32[11, 32]" = torch.ops.aten.permute.default(view_2396, [1, 0]);  view_2396 = None
        permute_61: "f32[16, 32]" = torch.ops.aten.permute.default(view_2394, [1, 0]);  view_2394 = None
        permute_62: "f32[11, 32]" = torch.ops.aten.permute.default(view_2392, [1, 0]);  view_2392 = None
        permute_63: "f32[16, 32]" = torch.ops.aten.permute.default(view_2390, [1, 0]);  view_2390 = None
        permute_64: "f32[11, 32]" = torch.ops.aten.permute.default(view_2388, [1, 0]);  view_2388 = None
        permute_65: "f32[16, 32]" = torch.ops.aten.permute.default(view_2386, [1, 0]);  view_2386 = None
        permute_66: "f32[11, 32]" = torch.ops.aten.permute.default(view_2384, [1, 0]);  view_2384 = None
        permute_67: "f32[16, 32]" = torch.ops.aten.permute.default(view_2382, [1, 0]);  view_2382 = None
        permute_68: "f32[11, 32]" = torch.ops.aten.permute.default(view_2380, [1, 0]);  view_2380 = None
        permute_69: "f32[16, 32]" = torch.ops.aten.permute.default(view_2378, [1, 0]);  view_2378 = None
        permute_70: "f32[11, 32]" = torch.ops.aten.permute.default(view_2376, [1, 0]);  view_2376 = None
        permute_71: "f32[16, 32]" = torch.ops.aten.permute.default(view_2374, [1, 0]);  view_2374 = None
        permute_72: "f32[11, 32]" = torch.ops.aten.permute.default(view_2372, [1, 0]);  view_2372 = None
        permute_73: "f32[16, 32]" = torch.ops.aten.permute.default(view_2370, [1, 0]);  view_2370 = None
        permute_74: "f32[11, 32]" = torch.ops.aten.permute.default(view_2368, [1, 0]);  view_2368 = None
        permute_75: "f32[16, 32]" = torch.ops.aten.permute.default(view_2366, [1, 0]);  view_2366 = None
        permute_76: "f32[11, 32]" = torch.ops.aten.permute.default(view_2364, [1, 0]);  view_2364 = None
        permute_77: "f32[16, 32]" = torch.ops.aten.permute.default(view_2362, [1, 0]);  view_2362 = None
        permute_78: "f32[11, 32]" = torch.ops.aten.permute.default(view_2360, [1, 0]);  view_2360 = None
        permute_79: "f32[16, 32]" = torch.ops.aten.permute.default(view_2358, [1, 0]);  view_2358 = None
        permute_80: "f32[11, 32]" = torch.ops.aten.permute.default(view_2356, [1, 0]);  view_2356 = None
        permute_81: "f32[16, 32]" = torch.ops.aten.permute.default(view_2354, [1, 0]);  view_2354 = None
        permute_82: "f32[11, 32]" = torch.ops.aten.permute.default(view_2352, [1, 0]);  view_2352 = None
        permute_83: "f32[16, 32]" = torch.ops.aten.permute.default(view_2350, [1, 0]);  view_2350 = None
        permute_84: "f32[11, 32]" = torch.ops.aten.permute.default(view_2348, [1, 0]);  view_2348 = None
        permute_85: "f32[16, 32]" = torch.ops.aten.permute.default(view_2346, [1, 0]);  view_2346 = None
        permute_86: "f32[11, 32]" = torch.ops.aten.permute.default(view_2344, [1, 0]);  view_2344 = None
        permute_87: "f32[16, 32]" = torch.ops.aten.permute.default(view_2342, [1, 0]);  view_2342 = None
        permute_88: "f32[11, 32]" = torch.ops.aten.permute.default(view_2340, [1, 0]);  view_2340 = None
        permute_89: "f32[16, 32]" = torch.ops.aten.permute.default(view_2338, [1, 0]);  view_2338 = None
        permute_90: "f32[11, 32]" = torch.ops.aten.permute.default(view_2336, [1, 0]);  view_2336 = None
        permute_91: "f32[16, 32]" = torch.ops.aten.permute.default(view_2334, [1, 0]);  view_2334 = None
        permute_92: "f32[11, 32]" = torch.ops.aten.permute.default(view_2332, [1, 0]);  view_2332 = None
        permute_93: "f32[16, 32]" = torch.ops.aten.permute.default(view_2330, [1, 0]);  view_2330 = None
        permute_94: "f32[11, 32]" = torch.ops.aten.permute.default(view_2328, [1, 0]);  view_2328 = None
        permute_95: "f32[16, 32]" = torch.ops.aten.permute.default(view_2326, [1, 0]);  view_2326 = None
        permute_96: "f32[11, 32]" = torch.ops.aten.permute.default(view_2324, [1, 0]);  view_2324 = None
        permute_97: "f32[16, 32]" = torch.ops.aten.permute.default(view_2322, [1, 0]);  view_2322 = None
        permute_98: "f32[11, 32]" = torch.ops.aten.permute.default(view_2320, [1, 0]);  view_2320 = None
        permute_99: "f32[16, 32]" = torch.ops.aten.permute.default(view_2318, [1, 0]);  view_2318 = None
        permute_100: "f32[11, 32]" = torch.ops.aten.permute.default(view_2316, [1, 0]);  view_2316 = None
        permute_101: "f32[16, 32]" = torch.ops.aten.permute.default(view_2314, [1, 0]);  view_2314 = None
        permute_102: "f32[11, 32]" = torch.ops.aten.permute.default(view_2312, [1, 0]);  view_2312 = None
        permute_103: "f32[16, 32]" = torch.ops.aten.permute.default(view_2310, [1, 0]);  view_2310 = None
        permute_104: "f32[11, 32]" = torch.ops.aten.permute.default(view_2308, [1, 0]);  view_2308 = None
        permute_105: "f32[16, 32]" = torch.ops.aten.permute.default(view_2306, [1, 0]);  view_2306 = None
        permute_106: "f32[11, 32]" = torch.ops.aten.permute.default(view_2304, [1, 0]);  view_2304 = None
        permute_107: "f32[16, 32]" = torch.ops.aten.permute.default(view_2302, [1, 0]);  view_2302 = None
        permute_108: "f32[11, 32]" = torch.ops.aten.permute.default(view_2300, [1, 0]);  view_2300 = None
        permute_109: "f32[16, 32]" = torch.ops.aten.permute.default(view_2298, [1, 0]);  view_2298 = None
        permute_110: "f32[11, 32]" = torch.ops.aten.permute.default(view_2296, [1, 0]);  view_2296 = None
        permute_111: "f32[16, 32]" = torch.ops.aten.permute.default(view_2294, [1, 0]);  view_2294 = None
        permute_112: "f32[11, 32]" = torch.ops.aten.permute.default(view_2292, [1, 0]);  view_2292 = None
        permute_113: "f32[16, 32]" = torch.ops.aten.permute.default(view_2290, [1, 0]);  view_2290 = None
        permute_114: "f32[11, 32]" = torch.ops.aten.permute.default(view_2288, [1, 0]);  view_2288 = None
        permute_115: "f32[16, 32]" = torch.ops.aten.permute.default(view_2286, [1, 0]);  view_2286 = None
        permute_116: "f32[11, 32]" = torch.ops.aten.permute.default(view_2284, [1, 0]);  view_2284 = None
        permute_117: "f32[16, 32]" = torch.ops.aten.permute.default(view_2282, [1, 0]);  view_2282 = None
        permute_118: "f32[11, 32]" = torch.ops.aten.permute.default(view_2280, [1, 0]);  view_2280 = None
        permute_119: "f32[16, 32]" = torch.ops.aten.permute.default(view_2278, [1, 0]);  view_2278 = None
        permute_120: "f32[11, 32]" = torch.ops.aten.permute.default(view_2276, [1, 0]);  view_2276 = None
        permute_121: "f32[16, 32]" = torch.ops.aten.permute.default(view_2274, [1, 0]);  view_2274 = None
        permute_122: "f32[11, 32]" = torch.ops.aten.permute.default(view_2272, [1, 0]);  view_2272 = None
        permute_123: "f32[16, 32]" = torch.ops.aten.permute.default(view_2270, [1, 0]);  view_2270 = None
        permute_124: "f32[11, 32]" = torch.ops.aten.permute.default(view_2268, [1, 0]);  view_2268 = None
        permute_125: "f32[16, 32]" = torch.ops.aten.permute.default(view_2266, [1, 0]);  view_2266 = None
        permute_126: "f32[11, 32]" = torch.ops.aten.permute.default(view_2264, [1, 0]);  view_2264 = None
        permute_127: "f32[16, 32]" = torch.ops.aten.permute.default(view_2262, [1, 0]);  view_2262 = None
        permute_128: "f32[11, 32]" = torch.ops.aten.permute.default(view_2260, [1, 0]);  view_2260 = None
        permute_129: "f32[16, 32]" = torch.ops.aten.permute.default(view_2258, [1, 0]);  view_2258 = None
        permute_130: "f32[11, 32]" = torch.ops.aten.permute.default(view_2256, [1, 0]);  view_2256 = None
        permute_131: "f32[16, 32]" = torch.ops.aten.permute.default(view_2254, [1, 0]);  view_2254 = None
        permute_132: "f32[11, 32]" = torch.ops.aten.permute.default(view_2252, [1, 0]);  view_2252 = None
        permute_133: "f32[16, 32]" = torch.ops.aten.permute.default(view_2250, [1, 0]);  view_2250 = None
        permute_134: "f32[11, 32]" = torch.ops.aten.permute.default(view_2248, [1, 0]);  view_2248 = None
        permute_135: "f32[16, 32]" = torch.ops.aten.permute.default(view_2246, [1, 0]);  view_2246 = None
        permute_136: "f32[11, 32]" = torch.ops.aten.permute.default(view_2244, [1, 0]);  view_2244 = None
        permute_137: "f32[16, 32]" = torch.ops.aten.permute.default(view_2242, [1, 0]);  view_2242 = None
        permute_138: "f32[11, 32]" = torch.ops.aten.permute.default(view_2240, [1, 0]);  view_2240 = None
        permute_139: "f32[16, 32]" = torch.ops.aten.permute.default(view_2238, [1, 0]);  view_2238 = None
        permute_140: "f32[11, 32]" = torch.ops.aten.permute.default(view_2236, [1, 0]);  view_2236 = None
        permute_141: "f32[16, 32]" = torch.ops.aten.permute.default(view_2234, [1, 0]);  view_2234 = None
        permute_142: "f32[11, 32]" = torch.ops.aten.permute.default(view_2232, [1, 0]);  view_2232 = None
        permute_143: "f32[16, 32]" = torch.ops.aten.permute.default(view_2230, [1, 0]);  view_2230 = None
        permute_144: "f32[11, 32]" = torch.ops.aten.permute.default(view_2228, [1, 0]);  view_2228 = None
        permute_145: "f32[16, 32]" = torch.ops.aten.permute.default(view_2226, [1, 0]);  view_2226 = None
        permute_146: "f32[11, 32]" = torch.ops.aten.permute.default(view_2224, [1, 0]);  view_2224 = None
        permute_147: "f32[16, 32]" = torch.ops.aten.permute.default(view_2222, [1, 0]);  view_2222 = None
        permute_148: "f32[11, 32]" = torch.ops.aten.permute.default(view_2220, [1, 0]);  view_2220 = None
        permute_149: "f32[16, 32]" = torch.ops.aten.permute.default(view_2218, [1, 0]);  view_2218 = None
        permute_150: "f32[11, 32]" = torch.ops.aten.permute.default(view_2216, [1, 0]);  view_2216 = None
        permute_151: "f32[16, 32]" = torch.ops.aten.permute.default(view_2214, [1, 0]);  view_2214 = None
        permute_152: "f32[11, 32]" = torch.ops.aten.permute.default(view_2212, [1, 0]);  view_2212 = None
        permute_153: "f32[16, 32]" = torch.ops.aten.permute.default(view_2210, [1, 0]);  view_2210 = None
        permute_154: "f32[11, 32]" = torch.ops.aten.permute.default(view_2208, [1, 0]);  view_2208 = None
        permute_155: "f32[16, 32]" = torch.ops.aten.permute.default(view_2206, [1, 0]);  view_2206 = None
        permute_156: "f32[11, 32]" = torch.ops.aten.permute.default(view_2204, [1, 0]);  view_2204 = None
        permute_157: "f32[16, 32]" = torch.ops.aten.permute.default(view_2202, [1, 0]);  view_2202 = None
        permute_158: "f32[11, 32]" = torch.ops.aten.permute.default(view_2200, [1, 0]);  view_2200 = None
        permute_159: "f32[16, 32]" = torch.ops.aten.permute.default(view_2198, [1, 0]);  view_2198 = None
        permute_160: "f32[11, 32]" = torch.ops.aten.permute.default(view_2196, [1, 0]);  view_2196 = None
        permute_161: "f32[16, 32]" = torch.ops.aten.permute.default(view_2194, [1, 0]);  view_2194 = None
        permute_162: "f32[11, 32]" = torch.ops.aten.permute.default(view_2192, [1, 0]);  view_2192 = None
        permute_163: "f32[16, 32]" = torch.ops.aten.permute.default(view_2190, [1, 0]);  view_2190 = None
        permute_164: "f32[11, 32]" = torch.ops.aten.permute.default(view_2188, [1, 0]);  view_2188 = None
        permute_165: "f32[16, 32]" = torch.ops.aten.permute.default(view_2186, [1, 0]);  view_2186 = None
        permute_166: "f32[11, 32]" = torch.ops.aten.permute.default(view_2184, [1, 0]);  view_2184 = None
        permute_167: "f32[16, 32]" = torch.ops.aten.permute.default(view_2182, [1, 0]);  view_2182 = None
        permute_168: "f32[11, 32]" = torch.ops.aten.permute.default(view_2180, [1, 0]);  view_2180 = None
        permute_169: "f32[16, 32]" = torch.ops.aten.permute.default(view_2178, [1, 0]);  view_2178 = None
        permute_170: "f32[11, 32]" = torch.ops.aten.permute.default(view_2176, [1, 0]);  view_2176 = None
        permute_171: "f32[16, 32]" = torch.ops.aten.permute.default(view_2174, [1, 0]);  view_2174 = None
        permute_172: "f32[11, 32]" = torch.ops.aten.permute.default(view_2172, [1, 0]);  view_2172 = None
        permute_173: "f32[16, 32]" = torch.ops.aten.permute.default(view_2170, [1, 0]);  view_2170 = None
        permute_174: "f32[11, 32]" = torch.ops.aten.permute.default(view_2168, [1, 0]);  view_2168 = None
        permute_175: "f32[16, 32]" = torch.ops.aten.permute.default(view_2166, [1, 0]);  view_2166 = None
        permute_176: "f32[11, 32]" = torch.ops.aten.permute.default(view_2164, [1, 0]);  view_2164 = None
        permute_177: "f32[16, 32]" = torch.ops.aten.permute.default(view_2162, [1, 0]);  view_2162 = None
        permute_178: "f32[11, 32]" = torch.ops.aten.permute.default(view_2160, [1, 0]);  view_2160 = None
        permute_179: "f32[16, 32]" = torch.ops.aten.permute.default(view_2158, [1, 0]);  view_2158 = None
        permute_180: "f32[11, 32]" = torch.ops.aten.permute.default(view_2156, [1, 0]);  view_2156 = None
        permute_181: "f32[16, 32]" = torch.ops.aten.permute.default(view_2154, [1, 0]);  view_2154 = None
        permute_182: "f32[11, 32]" = torch.ops.aten.permute.default(view_2152, [1, 0]);  view_2152 = None
        permute_183: "f32[16, 32]" = torch.ops.aten.permute.default(view_2150, [1, 0]);  view_2150 = None
        permute_184: "f32[11, 32]" = torch.ops.aten.permute.default(view_2148, [1, 0]);  view_2148 = None
        permute_185: "f32[16, 32]" = torch.ops.aten.permute.default(view_2146, [1, 0]);  view_2146 = None
        permute_186: "f32[11, 32]" = torch.ops.aten.permute.default(view_2144, [1, 0]);  view_2144 = None
        permute_187: "f32[16, 32]" = torch.ops.aten.permute.default(view_2142, [1, 0]);  view_2142 = None
        permute_188: "f32[11, 32]" = torch.ops.aten.permute.default(view_2140, [1, 0]);  view_2140 = None
        permute_189: "f32[16, 32]" = torch.ops.aten.permute.default(view_2138, [1, 0]);  view_2138 = None
        permute_190: "f32[11, 32]" = torch.ops.aten.permute.default(view_2136, [1, 0]);  view_2136 = None
        permute_191: "f32[16, 32]" = torch.ops.aten.permute.default(view_2134, [1, 0]);  view_2134 = None
        permute_192: "f32[11, 32]" = torch.ops.aten.permute.default(view_2132, [1, 0]);  view_2132 = None
        permute_193: "f32[16, 32]" = torch.ops.aten.permute.default(view_2130, [1, 0]);  view_2130 = None
        permute_194: "f32[11, 32]" = torch.ops.aten.permute.default(view_2128, [1, 0]);  view_2128 = None
        permute_195: "f32[16, 32]" = torch.ops.aten.permute.default(view_2126, [1, 0]);  view_2126 = None
        permute_196: "f32[11, 32]" = torch.ops.aten.permute.default(view_2124, [1, 0]);  view_2124 = None
        permute_197: "f32[16, 32]" = torch.ops.aten.permute.default(view_2122, [1, 0]);  view_2122 = None
        permute_198: "f32[11, 32]" = torch.ops.aten.permute.default(view_2120, [1, 0]);  view_2120 = None
        permute_199: "f32[16, 32]" = torch.ops.aten.permute.default(view_2118, [1, 0]);  view_2118 = None
        permute_200: "f32[11, 32]" = torch.ops.aten.permute.default(view_2116, [1, 0]);  view_2116 = None
        permute_201: "f32[16, 32]" = torch.ops.aten.permute.default(view_2114, [1, 0]);  view_2114 = None
        permute_202: "f32[11, 32]" = torch.ops.aten.permute.default(view_2112, [1, 0]);  view_2112 = None
        permute_203: "f32[16, 32]" = torch.ops.aten.permute.default(view_2110, [1, 0]);  view_2110 = None
        permute_204: "f32[11, 32]" = torch.ops.aten.permute.default(view_2108, [1, 0]);  view_2108 = None
        permute_205: "f32[16, 32]" = torch.ops.aten.permute.default(view_2106, [1, 0]);  view_2106 = None
        permute_206: "f32[11, 32]" = torch.ops.aten.permute.default(view_2104, [1, 0]);  view_2104 = None
        permute_207: "f32[16, 32]" = torch.ops.aten.permute.default(view_2102, [1, 0]);  view_2102 = None
        permute_208: "f32[11, 32]" = torch.ops.aten.permute.default(view_2100, [1, 0]);  view_2100 = None
        permute_209: "f32[16, 32]" = torch.ops.aten.permute.default(view_2098, [1, 0]);  view_2098 = None
        permute_210: "f32[11, 32]" = torch.ops.aten.permute.default(view_2096, [1, 0]);  view_2096 = None
        permute_211: "f32[16, 32]" = torch.ops.aten.permute.default(view_2094, [1, 0]);  view_2094 = None
        permute_212: "f32[11, 32]" = torch.ops.aten.permute.default(view_2092, [1, 0]);  view_2092 = None
        permute_213: "f32[16, 32]" = torch.ops.aten.permute.default(view_2090, [1, 0]);  view_2090 = None
        permute_214: "f32[11, 32]" = torch.ops.aten.permute.default(view_2088, [1, 0]);  view_2088 = None
        permute_215: "f32[16, 32]" = torch.ops.aten.permute.default(view_2086, [1, 0]);  view_2086 = None
        permute_216: "f32[11, 32]" = torch.ops.aten.permute.default(view_2084, [1, 0]);  view_2084 = None
        permute_217: "f32[16, 32]" = torch.ops.aten.permute.default(view_2082, [1, 0]);  view_2082 = None
        permute_218: "f32[11, 32]" = torch.ops.aten.permute.default(view_2080, [1, 0]);  view_2080 = None
        permute_219: "f32[16, 32]" = torch.ops.aten.permute.default(view_2078, [1, 0]);  view_2078 = None
        permute_220: "f32[11, 32]" = torch.ops.aten.permute.default(view_2076, [1, 0]);  view_2076 = None
        permute_221: "f32[16, 32]" = torch.ops.aten.permute.default(view_2074, [1, 0]);  view_2074 = None
        permute_222: "f32[11, 32]" = torch.ops.aten.permute.default(view_2072, [1, 0]);  view_2072 = None
        permute_223: "f32[16, 32]" = torch.ops.aten.permute.default(view_2070, [1, 0]);  view_2070 = None
        permute_224: "f32[11, 32]" = torch.ops.aten.permute.default(view_2068, [1, 0]);  view_2068 = None
        permute_225: "f32[16, 32]" = torch.ops.aten.permute.default(view_2066, [1, 0]);  view_2066 = None
        permute_226: "f32[11, 32]" = torch.ops.aten.permute.default(view_2064, [1, 0]);  view_2064 = None
        permute_227: "f32[16, 32]" = torch.ops.aten.permute.default(view_2062, [1, 0]);  view_2062 = None
        permute_228: "f32[11, 32]" = torch.ops.aten.permute.default(view_2060, [1, 0]);  view_2060 = None
        permute_229: "f32[16, 32]" = torch.ops.aten.permute.default(view_2058, [1, 0]);  view_2058 = None
        permute_230: "f32[11, 32]" = torch.ops.aten.permute.default(view_2056, [1, 0]);  view_2056 = None
        permute_231: "f32[16, 32]" = torch.ops.aten.permute.default(view_2054, [1, 0]);  view_2054 = None
        permute_232: "f32[11, 32]" = torch.ops.aten.permute.default(view_2052, [1, 0]);  view_2052 = None
        permute_233: "f32[16, 32]" = torch.ops.aten.permute.default(view_2050, [1, 0]);  view_2050 = None
        permute_234: "f32[11, 32]" = torch.ops.aten.permute.default(view_2048, [1, 0]);  view_2048 = None
        permute_235: "f32[16, 32]" = torch.ops.aten.permute.default(view_2046, [1, 0]);  view_2046 = None
        permute_236: "f32[11, 32]" = torch.ops.aten.permute.default(view_2044, [1, 0]);  view_2044 = None
        permute_237: "f32[16, 32]" = torch.ops.aten.permute.default(view_2042, [1, 0]);  view_2042 = None
        permute_238: "f32[11, 32]" = torch.ops.aten.permute.default(view_2040, [1, 0]);  view_2040 = None
        permute_239: "f32[16, 32]" = torch.ops.aten.permute.default(view_2038, [1, 0]);  view_2038 = None
        permute_240: "f32[11, 32]" = torch.ops.aten.permute.default(view_2036, [1, 0]);  view_2036 = None
        permute_241: "f32[16, 32]" = torch.ops.aten.permute.default(view_2034, [1, 0]);  view_2034 = None
        permute_242: "f32[11, 32]" = torch.ops.aten.permute.default(view_2032, [1, 0]);  view_2032 = None
        permute_243: "f32[16, 32]" = torch.ops.aten.permute.default(view_2030, [1, 0]);  view_2030 = None
        permute_244: "f32[11, 32]" = torch.ops.aten.permute.default(view_2028, [1, 0]);  view_2028 = None
        permute_245: "f32[16, 32]" = torch.ops.aten.permute.default(view_2026, [1, 0]);  view_2026 = None
        permute_246: "f32[11, 32]" = torch.ops.aten.permute.default(view_2024, [1, 0]);  view_2024 = None
        permute_247: "f32[16, 32]" = torch.ops.aten.permute.default(view_2022, [1, 0]);  view_2022 = None
        permute_248: "f32[11, 32]" = torch.ops.aten.permute.default(view_2020, [1, 0]);  view_2020 = None
        permute_249: "f32[16, 32]" = torch.ops.aten.permute.default(view_2018, [1, 0]);  view_2018 = None
        permute_250: "f32[11, 32]" = torch.ops.aten.permute.default(view_2016, [1, 0]);  view_2016 = None
        permute_251: "f32[16, 32]" = torch.ops.aten.permute.default(view_2014, [1, 0]);  view_2014 = None
        permute_252: "f32[11, 32]" = torch.ops.aten.permute.default(view_2012, [1, 0]);  view_2012 = None
        permute_253: "f32[16, 32]" = torch.ops.aten.permute.default(view_2010, [1, 0]);  view_2010 = None
        permute_254: "f32[11, 32]" = torch.ops.aten.permute.default(view_2008, [1, 0]);  view_2008 = None
        permute_255: "f32[16, 32]" = torch.ops.aten.permute.default(view_2006, [1, 0]);  view_2006 = None
        permute_256: "f32[11, 32]" = torch.ops.aten.permute.default(view_2004, [1, 0]);  view_2004 = None
        permute_257: "f32[16, 32]" = torch.ops.aten.permute.default(view_2002, [1, 0]);  view_2002 = None
        permute_258: "f32[11, 32]" = torch.ops.aten.permute.default(view_2000, [1, 0]);  view_2000 = None
        permute_259: "f32[16, 32]" = torch.ops.aten.permute.default(view_1998, [1, 0]);  view_1998 = None
        permute_260: "f32[11, 32]" = torch.ops.aten.permute.default(view_1996, [1, 0]);  view_1996 = None
        permute_261: "f32[16, 32]" = torch.ops.aten.permute.default(view_1994, [1, 0]);  view_1994 = None
        permute_262: "f32[11, 32]" = torch.ops.aten.permute.default(view_1992, [1, 0]);  view_1992 = None
        permute_263: "f32[16, 32]" = torch.ops.aten.permute.default(view_1990, [1, 0]);  view_1990 = None
        permute_264: "f32[11, 32]" = torch.ops.aten.permute.default(view_1988, [1, 0]);  view_1988 = None
        permute_265: "f32[16, 32]" = torch.ops.aten.permute.default(view_1986, [1, 0]);  view_1986 = None
        permute_266: "f32[11, 32]" = torch.ops.aten.permute.default(view_1984, [1, 0]);  view_1984 = None
        permute_267: "f32[16, 32]" = torch.ops.aten.permute.default(view_1982, [1, 0]);  view_1982 = None
        permute_268: "f32[11, 32]" = torch.ops.aten.permute.default(view_1980, [1, 0]);  view_1980 = None
        permute_269: "f32[16, 32]" = torch.ops.aten.permute.default(view_1978, [1, 0]);  view_1978 = None
        permute_270: "f32[11, 32]" = torch.ops.aten.permute.default(view_1976, [1, 0]);  view_1976 = None
        permute_271: "f32[16, 32]" = torch.ops.aten.permute.default(view_1974, [1, 0]);  view_1974 = None
        permute_272: "f32[11, 32]" = torch.ops.aten.permute.default(view_1972, [1, 0]);  view_1972 = None
        permute_273: "f32[16, 32]" = torch.ops.aten.permute.default(view_1970, [1, 0]);  view_1970 = None
        permute_274: "f32[11, 32]" = torch.ops.aten.permute.default(view_1968, [1, 0]);  view_1968 = None
        permute_275: "f32[16, 32]" = torch.ops.aten.permute.default(view_1966, [1, 0]);  view_1966 = None
        permute_276: "f32[11, 32]" = torch.ops.aten.permute.default(view_1964, [1, 0]);  view_1964 = None
        permute_277: "f32[16, 32]" = torch.ops.aten.permute.default(view_1962, [1, 0]);  view_1962 = None
        permute_278: "f32[11, 32]" = torch.ops.aten.permute.default(view_1960, [1, 0]);  view_1960 = None
        permute_279: "f32[16, 32]" = torch.ops.aten.permute.default(view_1958, [1, 0]);  view_1958 = None
        permute_280: "f32[11, 32]" = torch.ops.aten.permute.default(view_1956, [1, 0]);  view_1956 = None
        permute_281: "f32[16, 32]" = torch.ops.aten.permute.default(view_1954, [1, 0]);  view_1954 = None
        permute_282: "f32[11, 32]" = torch.ops.aten.permute.default(view_1952, [1, 0]);  view_1952 = None
        permute_283: "f32[16, 32]" = torch.ops.aten.permute.default(view_1950, [1, 0]);  view_1950 = None
        permute_284: "f32[11, 32]" = torch.ops.aten.permute.default(view_1948, [1, 0]);  view_1948 = None
        permute_285: "f32[16, 32]" = torch.ops.aten.permute.default(view_1946, [1, 0]);  view_1946 = None
        permute_286: "f32[11, 32]" = torch.ops.aten.permute.default(view_1944, [1, 0]);  view_1944 = None
        permute_287: "f32[16, 32]" = torch.ops.aten.permute.default(view_1942, [1, 0]);  view_1942 = None
        permute_288: "f32[11, 32]" = torch.ops.aten.permute.default(view_1940, [1, 0]);  view_1940 = None
        permute_289: "f32[16, 32]" = torch.ops.aten.permute.default(view_1938, [1, 0]);  view_1938 = None
        permute_290: "f32[11, 32]" = torch.ops.aten.permute.default(view_1936, [1, 0]);  view_1936 = None
        permute_291: "f32[16, 32]" = torch.ops.aten.permute.default(view_1934, [1, 0]);  view_1934 = None
        permute_292: "f32[11, 32]" = torch.ops.aten.permute.default(view_1932, [1, 0]);  view_1932 = None
        permute_293: "f32[16, 32]" = torch.ops.aten.permute.default(view_1930, [1, 0]);  view_1930 = None
        permute_294: "f32[11, 32]" = torch.ops.aten.permute.default(view_1928, [1, 0]);  view_1928 = None
        permute_295: "f32[16, 32]" = torch.ops.aten.permute.default(view_1926, [1, 0]);  view_1926 = None
        permute_296: "f32[11, 32]" = torch.ops.aten.permute.default(view_1924, [1, 0]);  view_1924 = None
        permute_297: "f32[16, 32]" = torch.ops.aten.permute.default(view_1922, [1, 0]);  view_1922 = None
        permute_298: "f32[11, 32]" = torch.ops.aten.permute.default(view_1920, [1, 0]);  view_1920 = None
        permute_299: "f32[16, 32]" = torch.ops.aten.permute.default(view_1918, [1, 0]);  view_1918 = None
        permute_300: "f32[11, 32]" = torch.ops.aten.permute.default(view_1916, [1, 0]);  view_1916 = None
        permute_301: "f32[16, 32]" = torch.ops.aten.permute.default(view_1914, [1, 0]);  view_1914 = None
        permute_302: "f32[11, 32]" = torch.ops.aten.permute.default(view_1912, [1, 0]);  view_1912 = None
        permute_303: "f32[16, 32]" = torch.ops.aten.permute.default(view_1910, [1, 0]);  view_1910 = None
        permute_304: "f32[11, 32]" = torch.ops.aten.permute.default(view_1908, [1, 0]);  view_1908 = None
        permute_305: "f32[16, 32]" = torch.ops.aten.permute.default(view_1906, [1, 0]);  view_1906 = None
        permute_306: "f32[11, 32]" = torch.ops.aten.permute.default(view_1904, [1, 0]);  view_1904 = None
        permute_307: "f32[16, 32]" = torch.ops.aten.permute.default(view_1902, [1, 0]);  view_1902 = None
        permute_308: "f32[11, 32]" = torch.ops.aten.permute.default(view_1900, [1, 0]);  view_1900 = None
        permute_309: "f32[16, 32]" = torch.ops.aten.permute.default(view_1898, [1, 0]);  view_1898 = None
        permute_310: "f32[11, 32]" = torch.ops.aten.permute.default(view_1896, [1, 0]);  view_1896 = None
        permute_311: "f32[16, 32]" = torch.ops.aten.permute.default(view_1894, [1, 0]);  view_1894 = None
        permute_312: "f32[11, 32]" = torch.ops.aten.permute.default(view_1892, [1, 0]);  view_1892 = None
        permute_313: "f32[16, 32]" = torch.ops.aten.permute.default(view_1890, [1, 0]);  view_1890 = None
        permute_314: "f32[11, 32]" = torch.ops.aten.permute.default(view_1888, [1, 0]);  view_1888 = None
        permute_315: "f32[16, 32]" = torch.ops.aten.permute.default(view_1886, [1, 0]);  view_1886 = None
        permute_316: "f32[11, 32]" = torch.ops.aten.permute.default(view_1884, [1, 0]);  view_1884 = None
        permute_317: "f32[16, 32]" = torch.ops.aten.permute.default(view_1882, [1, 0]);  view_1882 = None
        permute_318: "f32[11, 32]" = torch.ops.aten.permute.default(view_1880, [1, 0]);  view_1880 = None
        permute_319: "f32[16, 32]" = torch.ops.aten.permute.default(view_1878, [1, 0]);  view_1878 = None
        permute_320: "f32[11, 32]" = torch.ops.aten.permute.default(view_1876, [1, 0]);  view_1876 = None
        permute_321: "f32[16, 32]" = torch.ops.aten.permute.default(view_1874, [1, 0]);  view_1874 = None
        permute_322: "f32[11, 32]" = torch.ops.aten.permute.default(view_1872, [1, 0]);  view_1872 = None
        permute_323: "f32[16, 32]" = torch.ops.aten.permute.default(view_1870, [1, 0]);  view_1870 = None
        permute_324: "f32[11, 32]" = torch.ops.aten.permute.default(view_1868, [1, 0]);  view_1868 = None
        permute_325: "f32[16, 32]" = torch.ops.aten.permute.default(view_1866, [1, 0]);  view_1866 = None
        permute_326: "f32[11, 32]" = torch.ops.aten.permute.default(view_1864, [1, 0]);  view_1864 = None
        permute_327: "f32[16, 32]" = torch.ops.aten.permute.default(view_1862, [1, 0]);  view_1862 = None
        permute_328: "f32[11, 32]" = torch.ops.aten.permute.default(view_1860, [1, 0]);  view_1860 = None
        permute_329: "f32[16, 32]" = torch.ops.aten.permute.default(view_1858, [1, 0]);  view_1858 = None
        permute_330: "f32[11, 32]" = torch.ops.aten.permute.default(view_1856, [1, 0]);  view_1856 = None
        permute_331: "f32[16, 32]" = torch.ops.aten.permute.default(view_1854, [1, 0]);  view_1854 = None
        permute_332: "f32[11, 32]" = torch.ops.aten.permute.default(view_1852, [1, 0]);  view_1852 = None
        permute_333: "f32[16, 32]" = torch.ops.aten.permute.default(view_1850, [1, 0]);  view_1850 = None
        permute_334: "f32[11, 32]" = torch.ops.aten.permute.default(view_1848, [1, 0]);  view_1848 = None
        permute_335: "f32[16, 32]" = torch.ops.aten.permute.default(view_1846, [1, 0]);  view_1846 = None
        permute_336: "f32[11, 32]" = torch.ops.aten.permute.default(view_1844, [1, 0]);  view_1844 = None
        permute_337: "f32[16, 32]" = torch.ops.aten.permute.default(view_1842, [1, 0]);  view_1842 = None
        permute_338: "f32[11, 32]" = torch.ops.aten.permute.default(view_1840, [1, 0]);  view_1840 = None
        permute_339: "f32[16, 32]" = torch.ops.aten.permute.default(view_1838, [1, 0]);  view_1838 = None
        permute_340: "f32[11, 32]" = torch.ops.aten.permute.default(view_1836, [1, 0]);  view_1836 = None
        permute_341: "f32[16, 32]" = torch.ops.aten.permute.default(view_1834, [1, 0]);  view_1834 = None
        permute_342: "f32[11, 32]" = torch.ops.aten.permute.default(view_1832, [1, 0]);  view_1832 = None
        permute_343: "f32[16, 32]" = torch.ops.aten.permute.default(view_1830, [1, 0]);  view_1830 = None
        permute_344: "f32[11, 32]" = torch.ops.aten.permute.default(view_1828, [1, 0]);  view_1828 = None
        permute_345: "f32[16, 32]" = torch.ops.aten.permute.default(view_1826, [1, 0]);  view_1826 = None
        permute_346: "f32[11, 32]" = torch.ops.aten.permute.default(view_1824, [1, 0]);  view_1824 = None
        permute_347: "f32[16, 32]" = torch.ops.aten.permute.default(view_1822, [1, 0]);  view_1822 = None
        permute_348: "f32[11, 32]" = torch.ops.aten.permute.default(view_1820, [1, 0]);  view_1820 = None
        permute_349: "f32[16, 32]" = torch.ops.aten.permute.default(view_1818, [1, 0]);  view_1818 = None
        permute_350: "f32[11, 32]" = torch.ops.aten.permute.default(view_1816, [1, 0]);  view_1816 = None
        permute_351: "f32[16, 32]" = torch.ops.aten.permute.default(view_1814, [1, 0]);  view_1814 = None
        permute_352: "f32[11, 32]" = torch.ops.aten.permute.default(view_1812, [1, 0]);  view_1812 = None
        permute_353: "f32[16, 32]" = torch.ops.aten.permute.default(view_1810, [1, 0]);  view_1810 = None
        permute_354: "f32[11, 32]" = torch.ops.aten.permute.default(view_1808, [1, 0]);  view_1808 = None
        permute_355: "f32[16, 32]" = torch.ops.aten.permute.default(view_1806, [1, 0]);  view_1806 = None
        permute_356: "f32[11, 32]" = torch.ops.aten.permute.default(view_1804, [1, 0]);  view_1804 = None
        permute_357: "f32[16, 32]" = torch.ops.aten.permute.default(view_1802, [1, 0]);  view_1802 = None
        permute_358: "f32[11, 32]" = torch.ops.aten.permute.default(view_1800, [1, 0]);  view_1800 = None
        permute_359: "f32[16, 32]" = torch.ops.aten.permute.default(view_1798, [1, 0]);  view_1798 = None
        permute_360: "f32[11, 32]" = torch.ops.aten.permute.default(view_1796, [1, 0]);  view_1796 = None
        permute_361: "f32[16, 32]" = torch.ops.aten.permute.default(view_1794, [1, 0]);  view_1794 = None
        permute_362: "f32[11, 32]" = torch.ops.aten.permute.default(view_1792, [1, 0]);  view_1792 = None
        permute_363: "f32[16, 32]" = torch.ops.aten.permute.default(view_1790, [1, 0]);  view_1790 = None
        permute_364: "f32[11, 32]" = torch.ops.aten.permute.default(view_1788, [1, 0]);  view_1788 = None
        permute_365: "f32[16, 32]" = torch.ops.aten.permute.default(view_1786, [1, 0]);  view_1786 = None
        permute_366: "f32[11, 32]" = torch.ops.aten.permute.default(view_1784, [1, 0]);  view_1784 = None
        permute_367: "f32[16, 32]" = torch.ops.aten.permute.default(view_1782, [1, 0]);  view_1782 = None
        permute_368: "f32[11, 32]" = torch.ops.aten.permute.default(view_1780, [1, 0]);  view_1780 = None
        permute_369: "f32[16, 32]" = torch.ops.aten.permute.default(view_1778, [1, 0]);  view_1778 = None
        permute_370: "f32[11, 32]" = torch.ops.aten.permute.default(view_1776, [1, 0]);  view_1776 = None
        permute_371: "f32[16, 32]" = torch.ops.aten.permute.default(view_1774, [1, 0]);  view_1774 = None
        permute_372: "f32[11, 32]" = torch.ops.aten.permute.default(view_1772, [1, 0]);  view_1772 = None
        permute_373: "f32[16, 32]" = torch.ops.aten.permute.default(view_1770, [1, 0]);  view_1770 = None
        permute_374: "f32[11, 32]" = torch.ops.aten.permute.default(view_1768, [1, 0]);  view_1768 = None
        permute_375: "f32[16, 32]" = torch.ops.aten.permute.default(view_1766, [1, 0]);  view_1766 = None
        permute_376: "f32[11, 32]" = torch.ops.aten.permute.default(view_1764, [1, 0]);  view_1764 = None
        permute_377: "f32[16, 32]" = torch.ops.aten.permute.default(view_1762, [1, 0]);  view_1762 = None
        permute_378: "f32[11, 32]" = torch.ops.aten.permute.default(view_1760, [1, 0]);  view_1760 = None
        permute_379: "f32[16, 32]" = torch.ops.aten.permute.default(view_1758, [1, 0]);  view_1758 = None
        permute_380: "f32[11, 32]" = torch.ops.aten.permute.default(view_1756, [1, 0]);  view_1756 = None
        permute_381: "f32[16, 32]" = torch.ops.aten.permute.default(view_1754, [1, 0]);  view_1754 = None
        permute_382: "f32[11, 32]" = torch.ops.aten.permute.default(view_1752, [1, 0]);  view_1752 = None
        permute_383: "f32[16, 32]" = torch.ops.aten.permute.default(view_1750, [1, 0]);  view_1750 = None
        permute_384: "f32[11, 32]" = torch.ops.aten.permute.default(view_1748, [1, 0]);  view_1748 = None
        permute_385: "f32[16, 32]" = torch.ops.aten.permute.default(view_1746, [1, 0]);  view_1746 = None
        permute_386: "f32[11, 32]" = torch.ops.aten.permute.default(view_1744, [1, 0]);  view_1744 = None
        permute_387: "f32[16, 32]" = torch.ops.aten.permute.default(view_1742, [1, 0]);  view_1742 = None
        permute_388: "f32[11, 32]" = torch.ops.aten.permute.default(view_1740, [1, 0]);  view_1740 = None
        permute_389: "f32[16, 32]" = torch.ops.aten.permute.default(view_1738, [1, 0]);  view_1738 = None
        permute_390: "f32[11, 32]" = torch.ops.aten.permute.default(view_1736, [1, 0]);  view_1736 = None
        permute_391: "f32[16, 32]" = torch.ops.aten.permute.default(view_1734, [1, 0]);  view_1734 = None
        permute_392: "f32[11, 32]" = torch.ops.aten.permute.default(view_1732, [1, 0]);  view_1732 = None
        permute_393: "f32[16, 32]" = torch.ops.aten.permute.default(view_1730, [1, 0]);  view_1730 = None
        permute_394: "f32[11, 32]" = torch.ops.aten.permute.default(view_1728, [1, 0]);  view_1728 = None
        permute_395: "f32[16, 32]" = torch.ops.aten.permute.default(view_1726, [1, 0]);  view_1726 = None
        permute_396: "f32[11, 32]" = torch.ops.aten.permute.default(view_1724, [1, 0]);  view_1724 = None
        permute_397: "f32[16, 32]" = torch.ops.aten.permute.default(view_1722, [1, 0]);  view_1722 = None
        permute_398: "f32[11, 32]" = torch.ops.aten.permute.default(view_1720, [1, 0]);  view_1720 = None
        permute_399: "f32[16, 32]" = torch.ops.aten.permute.default(view_1718, [1, 0]);  view_1718 = None
        permute_400: "f32[11, 32]" = torch.ops.aten.permute.default(view_1716, [1, 0]);  view_1716 = None
        permute_401: "f32[16, 32]" = torch.ops.aten.permute.default(view_1714, [1, 0]);  view_1714 = None
        permute_402: "f32[11, 32]" = torch.ops.aten.permute.default(view_1712, [1, 0]);  view_1712 = None
        permute_403: "f32[16, 32]" = torch.ops.aten.permute.default(view_1710, [1, 0]);  view_1710 = None
        permute_404: "f32[11, 32]" = torch.ops.aten.permute.default(view_1708, [1, 0]);  view_1708 = None
        permute_405: "f32[16, 32]" = torch.ops.aten.permute.default(view_1706, [1, 0]);  view_1706 = None
        permute_406: "f32[11, 32]" = torch.ops.aten.permute.default(view_1704, [1, 0]);  view_1704 = None
        permute_407: "f32[16, 32]" = torch.ops.aten.permute.default(view_1702, [1, 0]);  view_1702 = None
        permute_408: "f32[11, 32]" = torch.ops.aten.permute.default(view_1700, [1, 0]);  view_1700 = None
        permute_409: "f32[16, 32]" = torch.ops.aten.permute.default(view_1698, [1, 0]);  view_1698 = None
        permute_410: "f32[11, 32]" = torch.ops.aten.permute.default(view_1696, [1, 0]);  view_1696 = None
        permute_411: "f32[16, 32]" = torch.ops.aten.permute.default(view_1694, [1, 0]);  view_1694 = None
        permute_412: "f32[11, 32]" = torch.ops.aten.permute.default(view_1692, [1, 0]);  view_1692 = None
        permute_413: "f32[16, 32]" = torch.ops.aten.permute.default(view_1690, [1, 0]);  view_1690 = None
        permute_414: "f32[11, 32]" = torch.ops.aten.permute.default(view_1688, [1, 0]);  view_1688 = None
        permute_415: "f32[16, 32]" = torch.ops.aten.permute.default(view_1686, [1, 0]);  view_1686 = None
        permute_416: "f32[11, 32]" = torch.ops.aten.permute.default(view_1684, [1, 0]);  view_1684 = None
        permute_417: "f32[16, 32]" = torch.ops.aten.permute.default(view_1682, [1, 0]);  view_1682 = None
        permute_418: "f32[11, 32]" = torch.ops.aten.permute.default(view_1680, [1, 0]);  view_1680 = None
        permute_419: "f32[16, 32]" = torch.ops.aten.permute.default(view_1678, [1, 0]);  view_1678 = None
        permute_420: "f32[11, 32]" = torch.ops.aten.permute.default(view_1676, [1, 0]);  view_1676 = None
        permute_421: "f32[16, 32]" = torch.ops.aten.permute.default(view_1674, [1, 0]);  view_1674 = None
        permute_422: "f32[11, 32]" = torch.ops.aten.permute.default(view_1672, [1, 0]);  view_1672 = None
        permute_423: "f32[16, 32]" = torch.ops.aten.permute.default(view_1670, [1, 0]);  view_1670 = None
        permute_424: "f32[11, 32]" = torch.ops.aten.permute.default(view_1668, [1, 0]);  view_1668 = None
        permute_425: "f32[16, 32]" = torch.ops.aten.permute.default(view_1666, [1, 0]);  view_1666 = None
        permute_426: "f32[11, 32]" = torch.ops.aten.permute.default(view_1664, [1, 0]);  view_1664 = None
        permute_427: "f32[16, 32]" = torch.ops.aten.permute.default(view_1662, [1, 0]);  view_1662 = None
        permute_428: "f32[11, 32]" = torch.ops.aten.permute.default(view_1660, [1, 0]);  view_1660 = None
        permute_429: "f32[16, 32]" = torch.ops.aten.permute.default(view_1658, [1, 0]);  view_1658 = None
        permute_430: "f32[11, 32]" = torch.ops.aten.permute.default(view_1656, [1, 0]);  view_1656 = None
        permute_431: "f32[16, 32]" = torch.ops.aten.permute.default(view_1654, [1, 0]);  view_1654 = None
        permute_432: "f32[11, 32]" = torch.ops.aten.permute.default(view_1652, [1, 0]);  view_1652 = None
        permute_433: "f32[16, 32]" = torch.ops.aten.permute.default(view_1650, [1, 0]);  view_1650 = None
        permute_434: "f32[11, 32]" = torch.ops.aten.permute.default(view_1648, [1, 0]);  view_1648 = None
        permute_435: "f32[16, 32]" = torch.ops.aten.permute.default(view_1646, [1, 0]);  view_1646 = None
        permute_436: "f32[11, 32]" = torch.ops.aten.permute.default(view_1644, [1, 0]);  view_1644 = None
        permute_437: "f32[16, 32]" = torch.ops.aten.permute.default(view_1642, [1, 0]);  view_1642 = None
        permute_438: "f32[11, 32]" = torch.ops.aten.permute.default(view_1640, [1, 0]);  view_1640 = None
        permute_439: "f32[16, 32]" = torch.ops.aten.permute.default(view_1638, [1, 0]);  view_1638 = None
        permute_440: "f32[11, 32]" = torch.ops.aten.permute.default(view_1636, [1, 0]);  view_1636 = None
        permute_441: "f32[16, 32]" = torch.ops.aten.permute.default(view_1634, [1, 0]);  view_1634 = None
        permute_442: "f32[11, 32]" = torch.ops.aten.permute.default(view_1632, [1, 0]);  view_1632 = None
        permute_443: "f32[16, 32]" = torch.ops.aten.permute.default(view_1630, [1, 0]);  view_1630 = None
        permute_444: "f32[11, 32]" = torch.ops.aten.permute.default(view_1628, [1, 0]);  view_1628 = None
        permute_445: "f32[16, 32]" = torch.ops.aten.permute.default(view_1626, [1, 0]);  view_1626 = None
        permute_446: "f32[11, 32]" = torch.ops.aten.permute.default(view_1624, [1, 0]);  view_1624 = None
        permute_447: "f32[16, 32]" = torch.ops.aten.permute.default(view_1622, [1, 0]);  view_1622 = None
        permute_448: "f32[11, 32]" = torch.ops.aten.permute.default(view_1620, [1, 0]);  view_1620 = None
        permute_449: "f32[16, 32]" = torch.ops.aten.permute.default(view_1618, [1, 0]);  view_1618 = None
        permute_450: "f32[11, 32]" = torch.ops.aten.permute.default(view_1616, [1, 0]);  view_1616 = None
        permute_451: "f32[16, 32]" = torch.ops.aten.permute.default(view_1614, [1, 0]);  view_1614 = None
        permute_452: "f32[11, 32]" = torch.ops.aten.permute.default(view_1612, [1, 0]);  view_1612 = None
        permute_453: "f32[16, 32]" = torch.ops.aten.permute.default(view_1610, [1, 0]);  view_1610 = None
        permute_454: "f32[11, 32]" = torch.ops.aten.permute.default(view_1608, [1, 0]);  view_1608 = None
        permute_455: "f32[16, 32]" = torch.ops.aten.permute.default(view_1606, [1, 0]);  view_1606 = None
        permute_456: "f32[11, 32]" = torch.ops.aten.permute.default(view_1604, [1, 0]);  view_1604 = None
        permute_457: "f32[16, 32]" = torch.ops.aten.permute.default(view_1602, [1, 0]);  view_1602 = None
        permute_458: "f32[11, 32]" = torch.ops.aten.permute.default(view_1600, [1, 0]);  view_1600 = None
        permute_459: "f32[16, 32]" = torch.ops.aten.permute.default(view_1598, [1, 0]);  view_1598 = None
        permute_460: "f32[11, 32]" = torch.ops.aten.permute.default(view_1596, [1, 0]);  view_1596 = None
        permute_461: "f32[16, 32]" = torch.ops.aten.permute.default(view_1594, [1, 0]);  view_1594 = None
        permute_462: "f32[11, 32]" = torch.ops.aten.permute.default(view_1592, [1, 0]);  view_1592 = None
        permute_463: "f32[16, 32]" = torch.ops.aten.permute.default(view_1590, [1, 0]);  view_1590 = None
        permute_464: "f32[11, 32]" = torch.ops.aten.permute.default(view_1588, [1, 0]);  view_1588 = None
        permute_465: "f32[16, 32]" = torch.ops.aten.permute.default(view_1586, [1, 0]);  view_1586 = None
        permute_466: "f32[11, 32]" = torch.ops.aten.permute.default(view_1584, [1, 0]);  view_1584 = None
        permute_467: "f32[16, 32]" = torch.ops.aten.permute.default(view_1582, [1, 0]);  view_1582 = None
        permute_468: "f32[11, 32]" = torch.ops.aten.permute.default(view_1580, [1, 0]);  view_1580 = None
        permute_469: "f32[16, 32]" = torch.ops.aten.permute.default(view_1578, [1, 0]);  view_1578 = None
        permute_470: "f32[11, 32]" = torch.ops.aten.permute.default(view_1576, [1, 0]);  view_1576 = None
        permute_471: "f32[16, 32]" = torch.ops.aten.permute.default(view_1574, [1, 0]);  view_1574 = None
        permute_472: "f32[11, 32]" = torch.ops.aten.permute.default(view_1572, [1, 0]);  view_1572 = None
        permute_473: "f32[16, 32]" = torch.ops.aten.permute.default(view_1570, [1, 0]);  view_1570 = None
        permute_474: "f32[11, 32]" = torch.ops.aten.permute.default(view_1568, [1, 0]);  view_1568 = None
        permute_475: "f32[16, 32]" = torch.ops.aten.permute.default(view_1566, [1, 0]);  view_1566 = None
        permute_476: "f32[11, 32]" = torch.ops.aten.permute.default(view_1564, [1, 0]);  view_1564 = None
        permute_477: "f32[16, 32]" = torch.ops.aten.permute.default(view_1562, [1, 0]);  view_1562 = None
        permute_478: "f32[11, 32]" = torch.ops.aten.permute.default(view_1560, [1, 0]);  view_1560 = None
        permute_479: "f32[16, 32]" = torch.ops.aten.permute.default(view_1558, [1, 0]);  view_1558 = None
        permute_480: "f32[11, 32]" = torch.ops.aten.permute.default(view_1556, [1, 0]);  view_1556 = None
        permute_481: "f32[16, 32]" = torch.ops.aten.permute.default(view_1554, [1, 0]);  view_1554 = None
        permute_482: "f32[11, 32]" = torch.ops.aten.permute.default(view_1552, [1, 0]);  view_1552 = None
        permute_483: "f32[16, 32]" = torch.ops.aten.permute.default(view_1550, [1, 0]);  view_1550 = None
        permute_484: "f32[11, 32]" = torch.ops.aten.permute.default(view_1548, [1, 0]);  view_1548 = None
        permute_485: "f32[16, 32]" = torch.ops.aten.permute.default(view_1546, [1, 0]);  view_1546 = None
        permute_486: "f32[11, 32]" = torch.ops.aten.permute.default(view_1544, [1, 0]);  view_1544 = None
        permute_487: "f32[16, 32]" = torch.ops.aten.permute.default(view_1542, [1, 0]);  view_1542 = None
        permute_488: "f32[11, 32]" = torch.ops.aten.permute.default(view_1540, [1, 0]);  view_1540 = None
        permute_489: "f32[16, 32]" = torch.ops.aten.permute.default(view_1538, [1, 0]);  view_1538 = None
        permute_490: "f32[11, 32]" = torch.ops.aten.permute.default(view_1536, [1, 0]);  view_1536 = None
        permute_491: "f32[16, 32]" = torch.ops.aten.permute.default(view_1534, [1, 0]);  view_1534 = None
        permute_492: "f32[11, 32]" = torch.ops.aten.permute.default(view_1532, [1, 0]);  view_1532 = None
        permute_493: "f32[16, 32]" = torch.ops.aten.permute.default(view_1530, [1, 0]);  view_1530 = None
        permute_494: "f32[11, 32]" = torch.ops.aten.permute.default(view_1528, [1, 0]);  view_1528 = None
        permute_495: "f32[16, 32]" = torch.ops.aten.permute.default(view_1526, [1, 0]);  view_1526 = None
        permute_496: "f32[11, 32]" = torch.ops.aten.permute.default(view_1524, [1, 0]);  view_1524 = None
        permute_497: "f32[16, 32]" = torch.ops.aten.permute.default(view_1522, [1, 0]);  view_1522 = None
        permute_498: "f32[11, 32]" = torch.ops.aten.permute.default(view_1520, [1, 0]);  view_1520 = None
        permute_499: "f32[16, 32]" = torch.ops.aten.permute.default(view_1518, [1, 0]);  view_1518 = None
        permute_500: "f32[11, 32]" = torch.ops.aten.permute.default(view_1516, [1, 0]);  view_1516 = None
        permute_501: "f32[16, 32]" = torch.ops.aten.permute.default(view_1514, [1, 0]);  view_1514 = None
        permute_502: "f32[11, 32]" = torch.ops.aten.permute.default(view_1512, [1, 0]);  view_1512 = None
        permute_503: "f32[16, 32]" = torch.ops.aten.permute.default(view_1510, [1, 0]);  view_1510 = None
        permute_504: "f32[11, 32]" = torch.ops.aten.permute.default(view_1508, [1, 0]);  view_1508 = None
        permute_505: "f32[16, 32]" = torch.ops.aten.permute.default(view_1506, [1, 0]);  view_1506 = None
        permute_506: "f32[11, 32]" = torch.ops.aten.permute.default(view_1504, [1, 0]);  view_1504 = None
        permute_507: "f32[16, 32]" = torch.ops.aten.permute.default(view_1502, [1, 0]);  view_1502 = None
        permute_508: "f32[11, 32]" = torch.ops.aten.permute.default(view_1500, [1, 0]);  view_1500 = None
        permute_509: "f32[16, 32]" = torch.ops.aten.permute.default(view_1498, [1, 0]);  view_1498 = None
        permute_510: "f32[11, 32]" = torch.ops.aten.permute.default(view_1496, [1, 0]);  view_1496 = None
        permute_511: "f32[16, 32]" = torch.ops.aten.permute.default(view_1494, [1, 0]);  view_1494 = None
        permute_512: "f32[11, 32]" = torch.ops.aten.permute.default(view_1492, [1, 0]);  view_1492 = None
        permute_513: "f32[16, 32]" = torch.ops.aten.permute.default(view_1490, [1, 0]);  view_1490 = None
        permute_514: "f32[11, 32]" = torch.ops.aten.permute.default(view_1488, [1, 0]);  view_1488 = None
        permute_515: "f32[16, 32]" = torch.ops.aten.permute.default(view_1486, [1, 0]);  view_1486 = None
        permute_516: "f32[11, 32]" = torch.ops.aten.permute.default(view_1484, [1, 0]);  view_1484 = None
        permute_517: "f32[16, 32]" = torch.ops.aten.permute.default(view_1482, [1, 0]);  view_1482 = None
        permute_518: "f32[11, 32]" = torch.ops.aten.permute.default(view_1480, [1, 0]);  view_1480 = None
        permute_519: "f32[16, 32]" = torch.ops.aten.permute.default(view_1478, [1, 0]);  view_1478 = None
        permute_520: "f32[11, 32]" = torch.ops.aten.permute.default(view_1476, [1, 0]);  view_1476 = None
        permute_521: "f32[16, 32]" = torch.ops.aten.permute.default(view_1474, [1, 0]);  view_1474 = None
        permute_522: "f32[11, 32]" = torch.ops.aten.permute.default(view_1472, [1, 0]);  view_1472 = None
        permute_523: "f32[16, 32]" = torch.ops.aten.permute.default(view_1470, [1, 0]);  view_1470 = None
        permute_524: "f32[11, 32]" = torch.ops.aten.permute.default(view_1468, [1, 0]);  view_1468 = None
        permute_525: "f32[16, 32]" = torch.ops.aten.permute.default(view_1466, [1, 0]);  view_1466 = None
        permute_526: "f32[11, 32]" = torch.ops.aten.permute.default(view_1464, [1, 0]);  view_1464 = None
        permute_527: "f32[16, 32]" = torch.ops.aten.permute.default(view_1462, [1, 0]);  view_1462 = None
        permute_528: "f32[11, 32]" = torch.ops.aten.permute.default(view_1460, [1, 0]);  view_1460 = None
        permute_529: "f32[16, 32]" = torch.ops.aten.permute.default(view_1458, [1, 0]);  view_1458 = None
        permute_530: "f32[11, 32]" = torch.ops.aten.permute.default(view_1456, [1, 0]);  view_1456 = None
        permute_531: "f32[16, 32]" = torch.ops.aten.permute.default(view_1454, [1, 0]);  view_1454 = None
        permute_532: "f32[11, 32]" = torch.ops.aten.permute.default(view_1452, [1, 0]);  view_1452 = None
        permute_533: "f32[16, 32]" = torch.ops.aten.permute.default(view_1450, [1, 0]);  view_1450 = None
        permute_534: "f32[11, 32]" = torch.ops.aten.permute.default(view_1448, [1, 0]);  view_1448 = None
        permute_535: "f32[16, 32]" = torch.ops.aten.permute.default(view_1446, [1, 0]);  view_1446 = None
        permute_536: "f32[11, 32]" = torch.ops.aten.permute.default(view_1444, [1, 0]);  view_1444 = None
        permute_537: "f32[16, 32]" = torch.ops.aten.permute.default(view_1442, [1, 0]);  view_1442 = None
        permute_538: "f32[11, 32]" = torch.ops.aten.permute.default(view_1440, [1, 0]);  view_1440 = None
        permute_539: "f32[16, 32]" = torch.ops.aten.permute.default(view_1438, [1, 0]);  view_1438 = None
        permute_540: "f32[11, 32]" = torch.ops.aten.permute.default(view_1436, [1, 0]);  view_1436 = None
        permute_541: "f32[16, 32]" = torch.ops.aten.permute.default(view_1434, [1, 0]);  view_1434 = None
        permute_542: "f32[11, 32]" = torch.ops.aten.permute.default(view_1432, [1, 0]);  view_1432 = None
        permute_543: "f32[16, 32]" = torch.ops.aten.permute.default(view_1430, [1, 0]);  view_1430 = None
        permute_544: "f32[11, 32]" = torch.ops.aten.permute.default(view_1428, [1, 0]);  view_1428 = None
        permute_545: "f32[16, 32]" = torch.ops.aten.permute.default(view_1426, [1, 0]);  view_1426 = None
        permute_546: "f32[11, 32]" = torch.ops.aten.permute.default(view_1424, [1, 0]);  view_1424 = None
        permute_547: "f32[16, 32]" = torch.ops.aten.permute.default(view_1422, [1, 0]);  view_1422 = None
        permute_548: "f32[11, 32]" = torch.ops.aten.permute.default(view_1420, [1, 0]);  view_1420 = None
        permute_549: "f32[16, 32]" = torch.ops.aten.permute.default(view_1418, [1, 0]);  view_1418 = None
        permute_550: "f32[11, 32]" = torch.ops.aten.permute.default(view_1416, [1, 0]);  view_1416 = None
        permute_551: "f32[16, 32]" = torch.ops.aten.permute.default(view_1414, [1, 0]);  view_1414 = None
        permute_552: "f32[11, 32]" = torch.ops.aten.permute.default(view_1412, [1, 0]);  view_1412 = None
        permute_553: "f32[16, 32]" = torch.ops.aten.permute.default(view_1410, [1, 0]);  view_1410 = None
        permute_554: "f32[11, 32]" = torch.ops.aten.permute.default(view_1408, [1, 0]);  view_1408 = None
        permute_555: "f32[16, 32]" = torch.ops.aten.permute.default(view_1406, [1, 0]);  view_1406 = None
        permute_556: "f32[11, 32]" = torch.ops.aten.permute.default(view_1404, [1, 0]);  view_1404 = None
        permute_557: "f32[16, 32]" = torch.ops.aten.permute.default(view_1402, [1, 0]);  view_1402 = None
        permute_558: "f32[11, 32]" = torch.ops.aten.permute.default(view_1400, [1, 0]);  view_1400 = None
        permute_559: "f32[16, 32]" = torch.ops.aten.permute.default(view_1398, [1, 0]);  view_1398 = None
        permute_560: "f32[11, 32]" = torch.ops.aten.permute.default(view_1396, [1, 0]);  view_1396 = None
        permute_561: "f32[16, 32]" = torch.ops.aten.permute.default(view_1394, [1, 0]);  view_1394 = None
        permute_562: "f32[11, 32]" = torch.ops.aten.permute.default(view_1392, [1, 0]);  view_1392 = None
        permute_563: "f32[16, 32]" = torch.ops.aten.permute.default(view_1390, [1, 0]);  view_1390 = None
        permute_564: "f32[11, 32]" = torch.ops.aten.permute.default(view_1388, [1, 0]);  view_1388 = None
        permute_565: "f32[16, 32]" = torch.ops.aten.permute.default(view_1386, [1, 0]);  view_1386 = None
        permute_566: "f32[11, 32]" = torch.ops.aten.permute.default(view_1384, [1, 0]);  view_1384 = None
        permute_567: "f32[16, 32]" = torch.ops.aten.permute.default(view_1382, [1, 0]);  view_1382 = None
        permute_568: "f32[11, 32]" = torch.ops.aten.permute.default(view_1380, [1, 0]);  view_1380 = None
        permute_569: "f32[16, 32]" = torch.ops.aten.permute.default(view_1378, [1, 0]);  view_1378 = None
        permute_570: "f32[11, 32]" = torch.ops.aten.permute.default(view_1376, [1, 0]);  view_1376 = None
        permute_571: "f32[16, 32]" = torch.ops.aten.permute.default(view_1374, [1, 0]);  view_1374 = None
        permute_572: "f32[11, 32]" = torch.ops.aten.permute.default(view_1372, [1, 0]);  view_1372 = None
        permute_573: "f32[16, 32]" = torch.ops.aten.permute.default(view_1370, [1, 0]);  view_1370 = None
        permute_574: "f32[11, 32]" = torch.ops.aten.permute.default(view_1368, [1, 0]);  view_1368 = None
        permute_575: "f32[16, 32]" = torch.ops.aten.permute.default(view_1366, [1, 0]);  view_1366 = None
        permute_576: "f32[11, 32]" = torch.ops.aten.permute.default(view_1364, [1, 0]);  view_1364 = None
        permute_577: "f32[16, 32]" = torch.ops.aten.permute.default(view_1362, [1, 0]);  view_1362 = None
        permute_578: "f32[11, 32]" = torch.ops.aten.permute.default(view_1360, [1, 0]);  view_1360 = None
        permute_579: "f32[16, 32]" = torch.ops.aten.permute.default(view_1358, [1, 0]);  view_1358 = None
        permute_580: "f32[11, 32]" = torch.ops.aten.permute.default(view_1356, [1, 0]);  view_1356 = None
        permute_581: "f32[16, 32]" = torch.ops.aten.permute.default(view_1354, [1, 0]);  view_1354 = None
        permute_582: "f32[11, 32]" = torch.ops.aten.permute.default(view_1352, [1, 0]);  view_1352 = None
        permute_583: "f32[16, 32]" = torch.ops.aten.permute.default(view_1350, [1, 0]);  view_1350 = None
        permute_584: "f32[11, 32]" = torch.ops.aten.permute.default(view_1348, [1, 0]);  view_1348 = None
        permute_585: "f32[16, 32]" = torch.ops.aten.permute.default(view_1346, [1, 0]);  view_1346 = None
        permute_586: "f32[11, 32]" = torch.ops.aten.permute.default(view_1344, [1, 0]);  view_1344 = None
        permute_587: "f32[16, 32]" = torch.ops.aten.permute.default(view_1342, [1, 0]);  view_1342 = None
        permute_588: "f32[11, 32]" = torch.ops.aten.permute.default(view_1340, [1, 0]);  view_1340 = None
        permute_589: "f32[16, 32]" = torch.ops.aten.permute.default(view_1338, [1, 0]);  view_1338 = None
        permute_590: "f32[11, 32]" = torch.ops.aten.permute.default(view_1336, [1, 0]);  view_1336 = None
        permute_591: "f32[16, 32]" = torch.ops.aten.permute.default(view_1334, [1, 0]);  view_1334 = None
        permute_592: "f32[11, 32]" = torch.ops.aten.permute.default(view_1332, [1, 0]);  view_1332 = None
        permute_593: "f32[16, 32]" = torch.ops.aten.permute.default(view_1330, [1, 0]);  view_1330 = None
        permute_594: "f32[11, 32]" = torch.ops.aten.permute.default(view_1328, [1, 0]);  view_1328 = None
        permute_595: "f32[16, 32]" = torch.ops.aten.permute.default(view_1326, [1, 0]);  view_1326 = None
        permute_596: "f32[11, 32]" = torch.ops.aten.permute.default(view_1324, [1, 0]);  view_1324 = None
        permute_597: "f32[16, 32]" = torch.ops.aten.permute.default(view_1322, [1, 0]);  view_1322 = None
        permute_598: "f32[11, 32]" = torch.ops.aten.permute.default(view_1320, [1, 0]);  view_1320 = None
        permute_599: "f32[16, 32]" = torch.ops.aten.permute.default(view_1318, [1, 0]);  view_1318 = None
        permute_600: "f32[11, 32]" = torch.ops.aten.permute.default(view_1316, [1, 0]);  view_1316 = None
        permute_601: "f32[16, 32]" = torch.ops.aten.permute.default(view_1314, [1, 0]);  view_1314 = None
        permute_602: "f32[11, 32]" = torch.ops.aten.permute.default(view_1312, [1, 0]);  view_1312 = None
        permute_603: "f32[16, 32]" = torch.ops.aten.permute.default(view_1310, [1, 0]);  view_1310 = None
        permute_604: "f32[11, 32]" = torch.ops.aten.permute.default(view_1308, [1, 0]);  view_1308 = None
        permute_605: "f32[16, 32]" = torch.ops.aten.permute.default(view_1306, [1, 0]);  view_1306 = None
        permute_606: "f32[11, 32]" = torch.ops.aten.permute.default(view_1304, [1, 0]);  view_1304 = None
        permute_607: "f32[16, 32]" = torch.ops.aten.permute.default(view_1302, [1, 0]);  view_1302 = None
        permute_608: "f32[11, 32]" = torch.ops.aten.permute.default(view_1300, [1, 0]);  view_1300 = None
        permute_609: "f32[16, 32]" = torch.ops.aten.permute.default(view_1298, [1, 0]);  view_1298 = None
        permute_610: "f32[11, 32]" = torch.ops.aten.permute.default(view_1296, [1, 0]);  view_1296 = None
        permute_611: "f32[16, 32]" = torch.ops.aten.permute.default(view_1294, [1, 0]);  view_1294 = None
        permute_612: "f32[11, 32]" = torch.ops.aten.permute.default(view_1292, [1, 0]);  view_1292 = None
        permute_613: "f32[16, 32]" = torch.ops.aten.permute.default(view_1290, [1, 0]);  view_1290 = None
        permute_614: "f32[11, 32]" = torch.ops.aten.permute.default(view_1288, [1, 0]);  view_1288 = None
        permute_615: "f32[16, 32]" = torch.ops.aten.permute.default(view_1286, [1, 0]);  view_1286 = None
        permute_616: "f32[11, 32]" = torch.ops.aten.permute.default(view_1284, [1, 0]);  view_1284 = None
        permute_617: "f32[16, 32]" = torch.ops.aten.permute.default(view_1282, [1, 0]);  view_1282 = None
        permute_618: "f32[11, 32]" = torch.ops.aten.permute.default(view_1280, [1, 0]);  view_1280 = None
        permute_619: "f32[16, 32]" = torch.ops.aten.permute.default(view_1278, [1, 0]);  view_1278 = None
        permute_620: "f32[11, 32]" = torch.ops.aten.permute.default(view_1276, [1, 0]);  view_1276 = None
        permute_621: "f32[16, 32]" = torch.ops.aten.permute.default(view_1274, [1, 0]);  view_1274 = None
        permute_622: "f32[11, 32]" = torch.ops.aten.permute.default(view_1272, [1, 0]);  view_1272 = None
        permute_623: "f32[16, 32]" = torch.ops.aten.permute.default(view_1270, [1, 0]);  view_1270 = None
        permute_624: "f32[11, 32]" = torch.ops.aten.permute.default(view_1268, [1, 0]);  view_1268 = None
        permute_625: "f32[16, 32]" = torch.ops.aten.permute.default(view_1266, [1, 0]);  view_1266 = None
        permute_626: "f32[11, 32]" = torch.ops.aten.permute.default(view_1264, [1, 0]);  view_1264 = None
        permute_627: "f32[16, 32]" = torch.ops.aten.permute.default(view_1262, [1, 0]);  view_1262 = None
        permute_628: "f32[11, 32]" = torch.ops.aten.permute.default(view_1260, [1, 0]);  view_1260 = None
        permute_629: "f32[16, 32]" = torch.ops.aten.permute.default(view_1258, [1, 0]);  view_1258 = None
        permute_630: "f32[11, 32]" = torch.ops.aten.permute.default(view_1256, [1, 0]);  view_1256 = None
        permute_631: "f32[16, 32]" = torch.ops.aten.permute.default(view_1254, [1, 0]);  view_1254 = None
        permute_632: "f32[11, 32]" = torch.ops.aten.permute.default(view_1252, [1, 0]);  view_1252 = None
        permute_633: "f32[16, 32]" = torch.ops.aten.permute.default(view_1250, [1, 0]);  view_1250 = None
        permute_634: "f32[11, 32]" = torch.ops.aten.permute.default(view_1248, [1, 0]);  view_1248 = None
        permute_635: "f32[16, 32]" = torch.ops.aten.permute.default(view_1246, [1, 0]);  view_1246 = None
        permute_636: "f32[11, 32]" = torch.ops.aten.permute.default(view_1244, [1, 0]);  view_1244 = None
        permute_637: "f32[16, 32]" = torch.ops.aten.permute.default(view_1242, [1, 0]);  view_1242 = None
        permute_638: "f32[11, 32]" = torch.ops.aten.permute.default(view_1240, [1, 0]);  view_1240 = None
        permute_639: "f32[16, 32]" = torch.ops.aten.permute.default(view_1238, [1, 0]);  view_1238 = None
        permute_640: "f32[11, 32]" = torch.ops.aten.permute.default(view_1236, [1, 0]);  view_1236 = None
        permute_641: "f32[16, 32]" = torch.ops.aten.permute.default(view_1234, [1, 0]);  view_1234 = None
        permute_642: "f32[11, 32]" = torch.ops.aten.permute.default(view_1232, [1, 0]);  view_1232 = None
        permute_643: "f32[16, 32]" = torch.ops.aten.permute.default(view_1230, [1, 0]);  view_1230 = None
        permute_644: "f32[11, 32]" = torch.ops.aten.permute.default(view_1228, [1, 0]);  view_1228 = None
        permute_645: "f32[16, 32]" = torch.ops.aten.permute.default(view_1226, [1, 0]);  view_1226 = None
        permute_646: "f32[11, 32]" = torch.ops.aten.permute.default(view_1224, [1, 0]);  view_1224 = None
        permute_647: "f32[16, 32]" = torch.ops.aten.permute.default(view_1222, [1, 0]);  view_1222 = None
        permute_648: "f32[11, 32]" = torch.ops.aten.permute.default(view_1220, [1, 0]);  view_1220 = None
        permute_649: "f32[16, 32]" = torch.ops.aten.permute.default(view_1218, [1, 0]);  view_1218 = None
        permute_650: "f32[11, 32]" = torch.ops.aten.permute.default(view_1216, [1, 0]);  view_1216 = None
        permute_651: "f32[16, 32]" = torch.ops.aten.permute.default(view_1214, [1, 0]);  view_1214 = None
        permute_652: "f32[11, 32]" = torch.ops.aten.permute.default(view_1212, [1, 0]);  view_1212 = None
        permute_653: "f32[16, 32]" = torch.ops.aten.permute.default(view_1210, [1, 0]);  view_1210 = None
        permute_654: "f32[11, 32]" = torch.ops.aten.permute.default(view_1208, [1, 0]);  view_1208 = None
        permute_655: "f32[16, 32]" = torch.ops.aten.permute.default(view_1206, [1, 0]);  view_1206 = None
        permute_656: "f32[11, 32]" = torch.ops.aten.permute.default(view_1204, [1, 0]);  view_1204 = None
        permute_657: "f32[16, 32]" = torch.ops.aten.permute.default(view_1202, [1, 0]);  view_1202 = None
        permute_658: "f32[11, 32]" = torch.ops.aten.permute.default(view_1200, [1, 0]);  view_1200 = None
        permute_659: "f32[16, 32]" = torch.ops.aten.permute.default(view_1198, [1, 0]);  view_1198 = None
        permute_660: "f32[11, 32]" = torch.ops.aten.permute.default(view_1196, [1, 0]);  view_1196 = None
        permute_661: "f32[16, 32]" = torch.ops.aten.permute.default(view_1194, [1, 0]);  view_1194 = None
        permute_662: "f32[11, 32]" = torch.ops.aten.permute.default(view_1192, [1, 0]);  view_1192 = None
        permute_663: "f32[16, 32]" = torch.ops.aten.permute.default(view_1190, [1, 0]);  view_1190 = None
        permute_664: "f32[11, 32]" = torch.ops.aten.permute.default(view_1188, [1, 0]);  view_1188 = None
        permute_665: "f32[16, 32]" = torch.ops.aten.permute.default(view_1186, [1, 0]);  view_1186 = None
        permute_666: "f32[11, 32]" = torch.ops.aten.permute.default(view_1184, [1, 0]);  view_1184 = None
        permute_667: "f32[16, 32]" = torch.ops.aten.permute.default(view_1182, [1, 0]);  view_1182 = None
        permute_668: "f32[11, 32]" = torch.ops.aten.permute.default(view_1180, [1, 0]);  view_1180 = None
        permute_669: "f32[16, 32]" = torch.ops.aten.permute.default(view_1178, [1, 0]);  view_1178 = None
        permute_670: "f32[11, 32]" = torch.ops.aten.permute.default(view_1176, [1, 0]);  view_1176 = None
        permute_671: "f32[16, 32]" = torch.ops.aten.permute.default(view_1174, [1, 0]);  view_1174 = None
        permute_672: "f32[11, 32]" = torch.ops.aten.permute.default(view_1172, [1, 0]);  view_1172 = None
        permute_673: "f32[16, 32]" = torch.ops.aten.permute.default(view_1170, [1, 0]);  view_1170 = None
        permute_674: "f32[11, 32]" = torch.ops.aten.permute.default(view_1168, [1, 0]);  view_1168 = None
        permute_675: "f32[16, 32]" = torch.ops.aten.permute.default(view_1166, [1, 0]);  view_1166 = None
        permute_676: "f32[11, 32]" = torch.ops.aten.permute.default(view_1164, [1, 0]);  view_1164 = None
        permute_677: "f32[16, 32]" = torch.ops.aten.permute.default(view_1162, [1, 0]);  view_1162 = None
        permute_678: "f32[11, 32]" = torch.ops.aten.permute.default(view_1160, [1, 0]);  view_1160 = None
        permute_679: "f32[16, 32]" = torch.ops.aten.permute.default(view_1158, [1, 0]);  view_1158 = None
        permute_680: "f32[11, 32]" = torch.ops.aten.permute.default(view_1156, [1, 0]);  view_1156 = None
        permute_681: "f32[16, 32]" = torch.ops.aten.permute.default(view_1154, [1, 0]);  view_1154 = None
        permute_682: "f32[11, 32]" = torch.ops.aten.permute.default(view_1152, [1, 0]);  view_1152 = None
        permute_683: "f32[16, 32]" = torch.ops.aten.permute.default(view_1150, [1, 0]);  view_1150 = None
        permute_684: "f32[11, 32]" = torch.ops.aten.permute.default(view_1148, [1, 0]);  view_1148 = None
        permute_685: "f32[16, 32]" = torch.ops.aten.permute.default(view_1146, [1, 0]);  view_1146 = None
        permute_686: "f32[11, 32]" = torch.ops.aten.permute.default(view_1144, [1, 0]);  view_1144 = None
        permute_687: "f32[16, 32]" = torch.ops.aten.permute.default(view_1142, [1, 0]);  view_1142 = None
        permute_688: "f32[11, 32]" = torch.ops.aten.permute.default(view_1140, [1, 0]);  view_1140 = None
        permute_689: "f32[16, 32]" = torch.ops.aten.permute.default(view_1138, [1, 0]);  view_1138 = None
        permute_690: "f32[11, 32]" = torch.ops.aten.permute.default(view_1136, [1, 0]);  view_1136 = None
        permute_691: "f32[16, 32]" = torch.ops.aten.permute.default(view_1134, [1, 0]);  view_1134 = None
        permute_692: "f32[11, 32]" = torch.ops.aten.permute.default(view_1132, [1, 0]);  view_1132 = None
        permute_693: "f32[16, 32]" = torch.ops.aten.permute.default(view_1130, [1, 0]);  view_1130 = None
        permute_694: "f32[11, 32]" = torch.ops.aten.permute.default(view_1128, [1, 0]);  view_1128 = None
        permute_695: "f32[16, 32]" = torch.ops.aten.permute.default(view_1126, [1, 0]);  view_1126 = None
        permute_696: "f32[11, 32]" = torch.ops.aten.permute.default(view_1124, [1, 0]);  view_1124 = None
        permute_697: "f32[16, 32]" = torch.ops.aten.permute.default(view_1122, [1, 0]);  view_1122 = None
        permute_698: "f32[11, 32]" = torch.ops.aten.permute.default(view_1120, [1, 0]);  view_1120 = None
        permute_699: "f32[16, 32]" = torch.ops.aten.permute.default(view_1118, [1, 0]);  view_1118 = None
        permute_700: "f32[11, 32]" = torch.ops.aten.permute.default(view_1116, [1, 0]);  view_1116 = None
        permute_701: "f32[16, 32]" = torch.ops.aten.permute.default(view_1114, [1, 0]);  view_1114 = None
        permute_702: "f32[11, 32]" = torch.ops.aten.permute.default(view_1112, [1, 0]);  view_1112 = None
        permute_703: "f32[16, 32]" = torch.ops.aten.permute.default(view_1110, [1, 0]);  view_1110 = None
        permute_704: "f32[11, 32]" = torch.ops.aten.permute.default(view_1108, [1, 0]);  view_1108 = None
        permute_705: "f32[16, 32]" = torch.ops.aten.permute.default(view_1106, [1, 0]);  view_1106 = None
        permute_706: "f32[11, 32]" = torch.ops.aten.permute.default(view_1104, [1, 0]);  view_1104 = None
        permute_707: "f32[16, 32]" = torch.ops.aten.permute.default(view_1102, [1, 0]);  view_1102 = None
        permute_708: "f32[11, 32]" = torch.ops.aten.permute.default(view_1100, [1, 0]);  view_1100 = None
        permute_709: "f32[16, 32]" = torch.ops.aten.permute.default(view_1098, [1, 0]);  view_1098 = None
        permute_710: "f32[11, 32]" = torch.ops.aten.permute.default(view_1096, [1, 0]);  view_1096 = None
        permute_711: "f32[16, 32]" = torch.ops.aten.permute.default(view_1094, [1, 0]);  view_1094 = None
        permute_712: "f32[11, 32]" = torch.ops.aten.permute.default(view_1092, [1, 0]);  view_1092 = None
        permute_713: "f32[16, 32]" = torch.ops.aten.permute.default(view_1090, [1, 0]);  view_1090 = None
        permute_714: "f32[11, 32]" = torch.ops.aten.permute.default(view_1088, [1, 0]);  view_1088 = None
        permute_715: "f32[16, 32]" = torch.ops.aten.permute.default(view_1086, [1, 0]);  view_1086 = None
        permute_716: "f32[11, 32]" = torch.ops.aten.permute.default(view_1084, [1, 0]);  view_1084 = None
        permute_717: "f32[16, 32]" = torch.ops.aten.permute.default(view_1082, [1, 0]);  view_1082 = None
        permute_718: "f32[11, 32]" = torch.ops.aten.permute.default(view_1080, [1, 0]);  view_1080 = None
        permute_719: "f32[16, 32]" = torch.ops.aten.permute.default(view_1078, [1, 0]);  view_1078 = None
        permute_720: "f32[11, 32]" = torch.ops.aten.permute.default(view_1076, [1, 0]);  view_1076 = None
        permute_721: "f32[16, 32]" = torch.ops.aten.permute.default(view_1074, [1, 0]);  view_1074 = None
        permute_722: "f32[11, 32]" = torch.ops.aten.permute.default(view_1072, [1, 0]);  view_1072 = None
        permute_723: "f32[16, 32]" = torch.ops.aten.permute.default(view_1070, [1, 0]);  view_1070 = None
        permute_724: "f32[11, 32]" = torch.ops.aten.permute.default(view_1068, [1, 0]);  view_1068 = None
        permute_725: "f32[16, 32]" = torch.ops.aten.permute.default(view_1066, [1, 0]);  view_1066 = None
        permute_726: "f32[11, 32]" = torch.ops.aten.permute.default(view_1064, [1, 0]);  view_1064 = None
        permute_727: "f32[16, 32]" = torch.ops.aten.permute.default(view_1062, [1, 0]);  view_1062 = None
        permute_728: "f32[11, 32]" = torch.ops.aten.permute.default(view_1060, [1, 0]);  view_1060 = None
        permute_729: "f32[16, 32]" = torch.ops.aten.permute.default(view_1058, [1, 0]);  view_1058 = None
        permute_730: "f32[11, 32]" = torch.ops.aten.permute.default(view_1056, [1, 0]);  view_1056 = None
        permute_731: "f32[16, 32]" = torch.ops.aten.permute.default(view_1054, [1, 0]);  view_1054 = None
        permute_732: "f32[11, 32]" = torch.ops.aten.permute.default(view_1052, [1, 0]);  view_1052 = None
        permute_733: "f32[16, 32]" = torch.ops.aten.permute.default(view_1050, [1, 0]);  view_1050 = None
        permute_734: "f32[11, 32]" = torch.ops.aten.permute.default(view_1048, [1, 0]);  view_1048 = None
        permute_735: "f32[16, 32]" = torch.ops.aten.permute.default(view_1046, [1, 0]);  view_1046 = None
        permute_736: "f32[11, 32]" = torch.ops.aten.permute.default(view_1044, [1, 0]);  view_1044 = None
        permute_737: "f32[16, 32]" = torch.ops.aten.permute.default(view_1042, [1, 0]);  view_1042 = None
        permute_738: "f32[11, 32]" = torch.ops.aten.permute.default(view_1040, [1, 0]);  view_1040 = None
        permute_739: "f32[16, 32]" = torch.ops.aten.permute.default(view_1038, [1, 0]);  view_1038 = None
        permute_740: "f32[11, 32]" = torch.ops.aten.permute.default(view_1036, [1, 0]);  view_1036 = None
        permute_741: "f32[16, 32]" = torch.ops.aten.permute.default(view_1034, [1, 0]);  view_1034 = None
        permute_742: "f32[11, 32]" = torch.ops.aten.permute.default(view_1032, [1, 0]);  view_1032 = None
        permute_743: "f32[16, 32]" = torch.ops.aten.permute.default(view_1030, [1, 0]);  view_1030 = None
        permute_744: "f32[11, 32]" = torch.ops.aten.permute.default(view_1028, [1, 0]);  view_1028 = None
        permute_745: "f32[16, 32]" = torch.ops.aten.permute.default(view_1026, [1, 0]);  view_1026 = None
        permute_746: "f32[11, 32]" = torch.ops.aten.permute.default(view_1024, [1, 0]);  view_1024 = None
        permute_747: "f32[16, 32]" = torch.ops.aten.permute.default(view_1022, [1, 0]);  view_1022 = None
        permute_748: "f32[11, 32]" = torch.ops.aten.permute.default(view_1020, [1, 0]);  view_1020 = None
        permute_749: "f32[16, 32]" = torch.ops.aten.permute.default(view_1018, [1, 0]);  view_1018 = None
        permute_750: "f32[11, 32]" = torch.ops.aten.permute.default(view_1016, [1, 0]);  view_1016 = None
        permute_751: "f32[16, 32]" = torch.ops.aten.permute.default(view_1014, [1, 0]);  view_1014 = None
        permute_752: "f32[11, 32]" = torch.ops.aten.permute.default(view_1012, [1, 0]);  view_1012 = None
        permute_753: "f32[16, 32]" = torch.ops.aten.permute.default(view_1010, [1, 0]);  view_1010 = None
        permute_754: "f32[11, 32]" = torch.ops.aten.permute.default(view_1008, [1, 0]);  view_1008 = None
        permute_755: "f32[16, 32]" = torch.ops.aten.permute.default(view_1006, [1, 0]);  view_1006 = None
        permute_756: "f32[11, 32]" = torch.ops.aten.permute.default(view_1004, [1, 0]);  view_1004 = None
        permute_757: "f32[16, 32]" = torch.ops.aten.permute.default(view_1002, [1, 0]);  view_1002 = None
        permute_758: "f32[11, 32]" = torch.ops.aten.permute.default(view_1000, [1, 0]);  view_1000 = None
        permute_759: "f32[16, 32]" = torch.ops.aten.permute.default(view_998, [1, 0]);  view_998 = None
        permute_760: "f32[11, 32]" = torch.ops.aten.permute.default(view_996, [1, 0]);  view_996 = None
        permute_761: "f32[16, 32]" = torch.ops.aten.permute.default(view_994, [1, 0]);  view_994 = None
        permute_762: "f32[11, 32]" = torch.ops.aten.permute.default(view_992, [1, 0]);  view_992 = None
        permute_763: "f32[16, 32]" = torch.ops.aten.permute.default(view_990, [1, 0]);  view_990 = None
        permute_764: "f32[11, 32]" = torch.ops.aten.permute.default(view_988, [1, 0]);  view_988 = None
        permute_765: "f32[16, 32]" = torch.ops.aten.permute.default(view_986, [1, 0]);  view_986 = None
        permute_766: "f32[11, 32]" = torch.ops.aten.permute.default(view_984, [1, 0]);  view_984 = None
        permute_767: "f32[16, 32]" = torch.ops.aten.permute.default(view_982, [1, 0]);  view_982 = None
        permute_768: "f32[11, 32]" = torch.ops.aten.permute.default(view_980, [1, 0]);  view_980 = None
        permute_769: "f32[16, 32]" = torch.ops.aten.permute.default(view_978, [1, 0]);  view_978 = None
        permute_770: "f32[11, 32]" = torch.ops.aten.permute.default(view_976, [1, 0]);  view_976 = None
        permute_771: "f32[16, 32]" = torch.ops.aten.permute.default(view_974, [1, 0]);  view_974 = None
        permute_772: "f32[11, 32]" = torch.ops.aten.permute.default(view_972, [1, 0]);  view_972 = None
        permute_773: "f32[16, 32]" = torch.ops.aten.permute.default(view_970, [1, 0]);  view_970 = None
        permute_774: "f32[11, 32]" = torch.ops.aten.permute.default(view_968, [1, 0]);  view_968 = None
        permute_775: "f32[16, 32]" = torch.ops.aten.permute.default(view_966, [1, 0]);  view_966 = None
        permute_776: "f32[11, 32]" = torch.ops.aten.permute.default(view_964, [1, 0]);  view_964 = None
        permute_777: "f32[16, 32]" = torch.ops.aten.permute.default(view_962, [1, 0]);  view_962 = None
        permute_778: "f32[11, 32]" = torch.ops.aten.permute.default(view_960, [1, 0]);  view_960 = None
        permute_779: "f32[16, 32]" = torch.ops.aten.permute.default(view_958, [1, 0]);  view_958 = None
        permute_780: "f32[11, 32]" = torch.ops.aten.permute.default(view_956, [1, 0]);  view_956 = None
        permute_781: "f32[16, 32]" = torch.ops.aten.permute.default(view_954, [1, 0]);  view_954 = None
        permute_782: "f32[11, 32]" = torch.ops.aten.permute.default(view_952, [1, 0]);  view_952 = None
        permute_783: "f32[16, 32]" = torch.ops.aten.permute.default(view_950, [1, 0]);  view_950 = None
        permute_784: "f32[11, 32]" = torch.ops.aten.permute.default(view_948, [1, 0]);  view_948 = None
        permute_785: "f32[16, 32]" = torch.ops.aten.permute.default(view_946, [1, 0]);  view_946 = None
        permute_786: "f32[11, 32]" = torch.ops.aten.permute.default(view_944, [1, 0]);  view_944 = None
        permute_787: "f32[16, 32]" = torch.ops.aten.permute.default(view_942, [1, 0]);  view_942 = None
        permute_788: "f32[11, 32]" = torch.ops.aten.permute.default(view_940, [1, 0]);  view_940 = None
        permute_789: "f32[16, 32]" = torch.ops.aten.permute.default(view_938, [1, 0]);  view_938 = None
        permute_790: "f32[11, 32]" = torch.ops.aten.permute.default(view_936, [1, 0]);  view_936 = None
        permute_791: "f32[16, 32]" = torch.ops.aten.permute.default(view_934, [1, 0]);  view_934 = None
        permute_792: "f32[11, 32]" = torch.ops.aten.permute.default(view_932, [1, 0]);  view_932 = None
        permute_793: "f32[16, 32]" = torch.ops.aten.permute.default(view_930, [1, 0]);  view_930 = None
        permute_794: "f32[11, 32]" = torch.ops.aten.permute.default(view_928, [1, 0]);  view_928 = None
        permute_795: "f32[16, 32]" = torch.ops.aten.permute.default(view_926, [1, 0]);  view_926 = None
        permute_796: "f32[11, 32]" = torch.ops.aten.permute.default(view_924, [1, 0]);  view_924 = None
        permute_797: "f32[16, 32]" = torch.ops.aten.permute.default(view_922, [1, 0]);  view_922 = None
        permute_798: "f32[11, 32]" = torch.ops.aten.permute.default(view_920, [1, 0]);  view_920 = None
        permute_799: "f32[16, 32]" = torch.ops.aten.permute.default(view_918, [1, 0]);  view_918 = None
        permute_800: "f32[11, 32]" = torch.ops.aten.permute.default(view_916, [1, 0]);  view_916 = None
        permute_801: "f32[16, 32]" = torch.ops.aten.permute.default(view_914, [1, 0]);  view_914 = None
        permute_802: "f32[11, 32]" = torch.ops.aten.permute.default(view_912, [1, 0]);  view_912 = None
        permute_803: "f32[16, 32]" = torch.ops.aten.permute.default(view_910, [1, 0]);  view_910 = None
        permute_804: "f32[11, 32]" = torch.ops.aten.permute.default(view_908, [1, 0]);  view_908 = None
        permute_805: "f32[16, 32]" = torch.ops.aten.permute.default(view_906, [1, 0]);  view_906 = None
        permute_806: "f32[11, 32]" = torch.ops.aten.permute.default(view_904, [1, 0]);  view_904 = None
        permute_807: "f32[16, 32]" = torch.ops.aten.permute.default(view_902, [1, 0]);  view_902 = None
        permute_808: "f32[11, 32]" = torch.ops.aten.permute.default(view_900, [1, 0]);  view_900 = None
        permute_809: "f32[16, 32]" = torch.ops.aten.permute.default(view_898, [1, 0]);  view_898 = None
        permute_810: "f32[11, 32]" = torch.ops.aten.permute.default(view_896, [1, 0]);  view_896 = None
        permute_811: "f32[16, 32]" = torch.ops.aten.permute.default(view_894, [1, 0]);  view_894 = None
        permute_812: "f32[11, 32]" = torch.ops.aten.permute.default(view_892, [1, 0]);  view_892 = None
        permute_813: "f32[16, 32]" = torch.ops.aten.permute.default(view_890, [1, 0]);  view_890 = None
        permute_814: "f32[11, 32]" = torch.ops.aten.permute.default(view_888, [1, 0]);  view_888 = None
        permute_815: "f32[16, 32]" = torch.ops.aten.permute.default(view_886, [1, 0]);  view_886 = None
        permute_816: "f32[11, 32]" = torch.ops.aten.permute.default(view_884, [1, 0]);  view_884 = None
        permute_817: "f32[16, 32]" = torch.ops.aten.permute.default(view_882, [1, 0]);  view_882 = None
        permute_818: "f32[11, 32]" = torch.ops.aten.permute.default(view_880, [1, 0]);  view_880 = None
        permute_819: "f32[16, 32]" = torch.ops.aten.permute.default(view_878, [1, 0]);  view_878 = None
        permute_820: "f32[11, 32]" = torch.ops.aten.permute.default(view_876, [1, 0]);  view_876 = None
        permute_821: "f32[16, 32]" = torch.ops.aten.permute.default(view_874, [1, 0]);  view_874 = None
        permute_822: "f32[11, 32]" = torch.ops.aten.permute.default(view_872, [1, 0]);  view_872 = None
        permute_823: "f32[16, 32]" = torch.ops.aten.permute.default(view_870, [1, 0]);  view_870 = None
        permute_824: "f32[11, 32]" = torch.ops.aten.permute.default(view_868, [1, 0]);  view_868 = None
        permute_825: "f32[16, 32]" = torch.ops.aten.permute.default(view_866, [1, 0]);  view_866 = None
        permute_826: "f32[11, 32]" = torch.ops.aten.permute.default(view_864, [1, 0]);  view_864 = None
        permute_827: "f32[16, 32]" = torch.ops.aten.permute.default(view_862, [1, 0]);  view_862 = None
        permute_828: "f32[11, 32]" = torch.ops.aten.permute.default(view_860, [1, 0]);  view_860 = None
        permute_829: "f32[16, 32]" = torch.ops.aten.permute.default(view_858, [1, 0]);  view_858 = None
        permute_830: "f32[11, 32]" = torch.ops.aten.permute.default(view_856, [1, 0]);  view_856 = None
        permute_831: "f32[16, 32]" = torch.ops.aten.permute.default(view_854, [1, 0]);  view_854 = None
        permute_832: "f32[11, 32]" = torch.ops.aten.permute.default(view_852, [1, 0]);  view_852 = None
        permute_833: "f32[16, 32]" = torch.ops.aten.permute.default(view_850, [1, 0]);  view_850 = None
        permute_834: "f32[11, 32]" = torch.ops.aten.permute.default(view_848, [1, 0]);  view_848 = None
        permute_835: "f32[16, 32]" = torch.ops.aten.permute.default(view_846, [1, 0]);  view_846 = None
        permute_836: "f32[11, 32]" = torch.ops.aten.permute.default(view_844, [1, 0]);  view_844 = None
        permute_837: "f32[16, 32]" = torch.ops.aten.permute.default(view_842, [1, 0]);  view_842 = None
        permute_838: "f32[11, 32]" = torch.ops.aten.permute.default(view_840, [1, 0]);  view_840 = None
        permute_839: "f32[16, 32]" = torch.ops.aten.permute.default(view_838, [1, 0]);  view_838 = None
        permute_840: "f32[11, 32]" = torch.ops.aten.permute.default(view_836, [1, 0]);  view_836 = None
        permute_841: "f32[16, 32]" = torch.ops.aten.permute.default(view_834, [1, 0]);  view_834 = None
        permute_842: "f32[11, 32]" = torch.ops.aten.permute.default(view_832, [1, 0]);  view_832 = None
        permute_843: "f32[16, 32]" = torch.ops.aten.permute.default(view_830, [1, 0]);  view_830 = None
        permute_844: "f32[11, 32]" = torch.ops.aten.permute.default(view_828, [1, 0]);  view_828 = None
        permute_845: "f32[16, 32]" = torch.ops.aten.permute.default(view_826, [1, 0]);  view_826 = None
        permute_846: "f32[11, 32]" = torch.ops.aten.permute.default(view_824, [1, 0]);  view_824 = None
        permute_847: "f32[16, 32]" = torch.ops.aten.permute.default(view_822, [1, 0]);  view_822 = None
        permute_848: "f32[11, 32]" = torch.ops.aten.permute.default(view_820, [1, 0]);  view_820 = None
        permute_849: "f32[16, 32]" = torch.ops.aten.permute.default(view_818, [1, 0]);  view_818 = None
        permute_850: "f32[11, 32]" = torch.ops.aten.permute.default(view_816, [1, 0]);  view_816 = None
        permute_851: "f32[16, 32]" = torch.ops.aten.permute.default(view_814, [1, 0]);  view_814 = None
        permute_852: "f32[11, 32]" = torch.ops.aten.permute.default(view_812, [1, 0]);  view_812 = None
        permute_853: "f32[16, 32]" = torch.ops.aten.permute.default(view_810, [1, 0]);  view_810 = None
        permute_854: "f32[11, 32]" = torch.ops.aten.permute.default(view_808, [1, 0]);  view_808 = None
        permute_855: "f32[16, 32]" = torch.ops.aten.permute.default(view_806, [1, 0]);  view_806 = None
        permute_856: "f32[11, 32]" = torch.ops.aten.permute.default(view_804, [1, 0]);  view_804 = None
        permute_857: "f32[16, 32]" = torch.ops.aten.permute.default(view_802, [1, 0]);  view_802 = None
        permute_858: "f32[11, 32]" = torch.ops.aten.permute.default(view_800, [1, 0]);  view_800 = None
        permute_859: "f32[16, 32]" = torch.ops.aten.permute.default(view_798, [1, 0]);  view_798 = None
        permute_860: "f32[11, 32]" = torch.ops.aten.permute.default(view_796, [1, 0]);  view_796 = None
        permute_861: "f32[16, 32]" = torch.ops.aten.permute.default(view_794, [1, 0]);  view_794 = None
        permute_862: "f32[11, 32]" = torch.ops.aten.permute.default(view_792, [1, 0]);  view_792 = None
        permute_863: "f32[16, 32]" = torch.ops.aten.permute.default(view_790, [1, 0]);  view_790 = None
        permute_864: "f32[11, 32]" = torch.ops.aten.permute.default(view_788, [1, 0]);  view_788 = None
        permute_865: "f32[16, 32]" = torch.ops.aten.permute.default(view_786, [1, 0]);  view_786 = None
        permute_866: "f32[11, 32]" = torch.ops.aten.permute.default(view_784, [1, 0]);  view_784 = None
        permute_867: "f32[16, 32]" = torch.ops.aten.permute.default(view_782, [1, 0]);  view_782 = None
        permute_868: "f32[11, 32]" = torch.ops.aten.permute.default(view_780, [1, 0]);  view_780 = None
        permute_869: "f32[16, 32]" = torch.ops.aten.permute.default(view_778, [1, 0]);  view_778 = None
        permute_870: "f32[11, 32]" = torch.ops.aten.permute.default(view_776, [1, 0]);  view_776 = None
        permute_871: "f32[16, 32]" = torch.ops.aten.permute.default(view_774, [1, 0]);  view_774 = None
        permute_872: "f32[11, 32]" = torch.ops.aten.permute.default(view_772, [1, 0]);  view_772 = None
        permute_873: "f32[16, 32]" = torch.ops.aten.permute.default(view_770, [1, 0]);  view_770 = None
        permute_874: "f32[11, 32]" = torch.ops.aten.permute.default(view_768, [1, 0]);  view_768 = None
        permute_875: "f32[16, 32]" = torch.ops.aten.permute.default(view_766, [1, 0]);  view_766 = None
        permute_876: "f32[11, 32]" = torch.ops.aten.permute.default(view_764, [1, 0]);  view_764 = None
        permute_877: "f32[16, 32]" = torch.ops.aten.permute.default(view_762, [1, 0]);  view_762 = None
        permute_878: "f32[11, 32]" = torch.ops.aten.permute.default(view_760, [1, 0]);  view_760 = None
        permute_879: "f32[16, 32]" = torch.ops.aten.permute.default(view_758, [1, 0]);  view_758 = None
        permute_880: "f32[11, 32]" = torch.ops.aten.permute.default(view_756, [1, 0]);  view_756 = None
        permute_881: "f32[16, 32]" = torch.ops.aten.permute.default(view_754, [1, 0]);  view_754 = None
        permute_882: "f32[11, 32]" = torch.ops.aten.permute.default(view_752, [1, 0]);  view_752 = None
        permute_883: "f32[16, 32]" = torch.ops.aten.permute.default(view_750, [1, 0]);  view_750 = None
        permute_884: "f32[11, 32]" = torch.ops.aten.permute.default(view_748, [1, 0]);  view_748 = None
        permute_885: "f32[16, 32]" = torch.ops.aten.permute.default(view_746, [1, 0]);  view_746 = None
        permute_886: "f32[11, 32]" = torch.ops.aten.permute.default(view_744, [1, 0]);  view_744 = None
        permute_887: "f32[16, 32]" = torch.ops.aten.permute.default(view_742, [1, 0]);  view_742 = None
        permute_888: "f32[11, 32]" = torch.ops.aten.permute.default(view_740, [1, 0]);  view_740 = None
        permute_889: "f32[16, 32]" = torch.ops.aten.permute.default(view_738, [1, 0]);  view_738 = None
        permute_890: "f32[11, 32]" = torch.ops.aten.permute.default(view_736, [1, 0]);  view_736 = None
        permute_891: "f32[16, 32]" = torch.ops.aten.permute.default(view_734, [1, 0]);  view_734 = None
        permute_892: "f32[11, 32]" = torch.ops.aten.permute.default(view_732, [1, 0]);  view_732 = None
        permute_893: "f32[16, 32]" = torch.ops.aten.permute.default(view_730, [1, 0]);  view_730 = None
        permute_894: "f32[11, 32]" = torch.ops.aten.permute.default(view_728, [1, 0]);  view_728 = None
        permute_895: "f32[16, 32]" = torch.ops.aten.permute.default(view_726, [1, 0]);  view_726 = None
        permute_896: "f32[11, 32]" = torch.ops.aten.permute.default(view_724, [1, 0]);  view_724 = None
        permute_897: "f32[16, 32]" = torch.ops.aten.permute.default(view_722, [1, 0]);  view_722 = None
        permute_898: "f32[11, 32]" = torch.ops.aten.permute.default(view_720, [1, 0]);  view_720 = None
        permute_899: "f32[16, 32]" = torch.ops.aten.permute.default(view_718, [1, 0]);  view_718 = None
        permute_900: "f32[11, 32]" = torch.ops.aten.permute.default(view_716, [1, 0]);  view_716 = None
        permute_901: "f32[16, 32]" = torch.ops.aten.permute.default(view_714, [1, 0]);  view_714 = None
        permute_902: "f32[11, 32]" = torch.ops.aten.permute.default(view_712, [1, 0]);  view_712 = None
        permute_903: "f32[16, 32]" = torch.ops.aten.permute.default(view_710, [1, 0]);  view_710 = None
        permute_904: "f32[11, 32]" = torch.ops.aten.permute.default(view_708, [1, 0]);  view_708 = None
        permute_905: "f32[16, 32]" = torch.ops.aten.permute.default(view_706, [1, 0]);  view_706 = None
        permute_906: "f32[11, 32]" = torch.ops.aten.permute.default(view_704, [1, 0]);  view_704 = None
        permute_907: "f32[16, 32]" = torch.ops.aten.permute.default(view_702, [1, 0]);  view_702 = None
        permute_908: "f32[11, 32]" = torch.ops.aten.permute.default(view_700, [1, 0]);  view_700 = None
        permute_909: "f32[16, 32]" = torch.ops.aten.permute.default(view_698, [1, 0]);  view_698 = None
        permute_910: "f32[11, 32]" = torch.ops.aten.permute.default(view_696, [1, 0]);  view_696 = None
        permute_911: "f32[16, 32]" = torch.ops.aten.permute.default(view_694, [1, 0]);  view_694 = None
        permute_912: "f32[11, 32]" = torch.ops.aten.permute.default(view_692, [1, 0]);  view_692 = None
        permute_913: "f32[16, 32]" = torch.ops.aten.permute.default(view_690, [1, 0]);  view_690 = None
        permute_914: "f32[11, 32]" = torch.ops.aten.permute.default(view_688, [1, 0]);  view_688 = None
        permute_915: "f32[16, 32]" = torch.ops.aten.permute.default(view_686, [1, 0]);  view_686 = None
        permute_916: "f32[11, 32]" = torch.ops.aten.permute.default(view_684, [1, 0]);  view_684 = None
        permute_917: "f32[16, 32]" = torch.ops.aten.permute.default(view_682, [1, 0]);  view_682 = None
        permute_918: "f32[11, 32]" = torch.ops.aten.permute.default(view_680, [1, 0]);  view_680 = None
        permute_919: "f32[16, 32]" = torch.ops.aten.permute.default(view_678, [1, 0]);  view_678 = None
        permute_920: "f32[11, 32]" = torch.ops.aten.permute.default(view_676, [1, 0]);  view_676 = None
        permute_921: "f32[16, 32]" = torch.ops.aten.permute.default(view_674, [1, 0]);  view_674 = None
        permute_922: "f32[11, 32]" = torch.ops.aten.permute.default(view_672, [1, 0]);  view_672 = None
        permute_923: "f32[16, 32]" = torch.ops.aten.permute.default(view_670, [1, 0]);  view_670 = None
        permute_924: "f32[11, 32]" = torch.ops.aten.permute.default(view_668, [1, 0]);  view_668 = None
        permute_925: "f32[16, 32]" = torch.ops.aten.permute.default(view_666, [1, 0]);  view_666 = None
        permute_926: "f32[11, 32]" = torch.ops.aten.permute.default(view_664, [1, 0]);  view_664 = None
        permute_927: "f32[16, 32]" = torch.ops.aten.permute.default(view_662, [1, 0]);  view_662 = None
        permute_928: "f32[11, 32]" = torch.ops.aten.permute.default(view_660, [1, 0]);  view_660 = None
        permute_929: "f32[16, 32]" = torch.ops.aten.permute.default(view_658, [1, 0]);  view_658 = None
        permute_930: "f32[11, 32]" = torch.ops.aten.permute.default(view_656, [1, 0]);  view_656 = None
        permute_931: "f32[16, 32]" = torch.ops.aten.permute.default(view_654, [1, 0]);  view_654 = None
        permute_932: "f32[11, 32]" = torch.ops.aten.permute.default(view_652, [1, 0]);  view_652 = None
        permute_933: "f32[16, 32]" = torch.ops.aten.permute.default(view_650, [1, 0]);  view_650 = None
        permute_934: "f32[11, 32]" = torch.ops.aten.permute.default(view_648, [1, 0]);  view_648 = None
        permute_935: "f32[16, 32]" = torch.ops.aten.permute.default(view_646, [1, 0]);  view_646 = None
        permute_936: "f32[11, 32]" = torch.ops.aten.permute.default(view_644, [1, 0]);  view_644 = None
        permute_937: "f32[16, 32]" = torch.ops.aten.permute.default(view_642, [1, 0]);  view_642 = None
        permute_938: "f32[11, 32]" = torch.ops.aten.permute.default(view_640, [1, 0]);  view_640 = None
        permute_939: "f32[16, 32]" = torch.ops.aten.permute.default(view_638, [1, 0]);  view_638 = None
        permute_940: "f32[11, 32]" = torch.ops.aten.permute.default(view_636, [1, 0]);  view_636 = None
        permute_941: "f32[16, 32]" = torch.ops.aten.permute.default(view_634, [1, 0]);  view_634 = None
        permute_942: "f32[11, 32]" = torch.ops.aten.permute.default(view_632, [1, 0]);  view_632 = None
        permute_943: "f32[16, 32]" = torch.ops.aten.permute.default(view_630, [1, 0]);  view_630 = None
        permute_944: "f32[11, 32]" = torch.ops.aten.permute.default(view_628, [1, 0]);  view_628 = None
        permute_945: "f32[16, 32]" = torch.ops.aten.permute.default(view_626, [1, 0]);  view_626 = None
        permute_946: "f32[11, 32]" = torch.ops.aten.permute.default(view_624, [1, 0]);  view_624 = None
        permute_947: "f32[16, 32]" = torch.ops.aten.permute.default(view_622, [1, 0]);  view_622 = None
        permute_948: "f32[11, 32]" = torch.ops.aten.permute.default(view_620, [1, 0]);  view_620 = None
        permute_949: "f32[16, 32]" = torch.ops.aten.permute.default(view_618, [1, 0]);  view_618 = None
        permute_950: "f32[11, 32]" = torch.ops.aten.permute.default(view_616, [1, 0]);  view_616 = None
        permute_951: "f32[16, 32]" = torch.ops.aten.permute.default(view_614, [1, 0]);  view_614 = None
        permute_952: "f32[11, 32]" = torch.ops.aten.permute.default(view_612, [1, 0]);  view_612 = None
        permute_953: "f32[16, 32]" = torch.ops.aten.permute.default(view_610, [1, 0]);  view_610 = None
        permute_954: "f32[11, 32]" = torch.ops.aten.permute.default(view_608, [1, 0]);  view_608 = None
        permute_955: "f32[16, 32]" = torch.ops.aten.permute.default(view_606, [1, 0]);  view_606 = None
        permute_956: "f32[11, 32]" = torch.ops.aten.permute.default(view_604, [1, 0]);  view_604 = None
        permute_957: "f32[16, 32]" = torch.ops.aten.permute.default(view_602, [1, 0]);  view_602 = None
        permute_958: "f32[11, 32]" = torch.ops.aten.permute.default(view_600, [1, 0]);  view_600 = None
        permute_959: "f32[16, 32]" = torch.ops.aten.permute.default(view_598, [1, 0]);  view_598 = None
        permute_960: "f32[11, 32]" = torch.ops.aten.permute.default(view_596, [1, 0]);  view_596 = None
        permute_961: "f32[16, 32]" = torch.ops.aten.permute.default(view_594, [1, 0]);  view_594 = None
        permute_962: "f32[11, 32]" = torch.ops.aten.permute.default(view_592, [1, 0]);  view_592 = None
        permute_963: "f32[16, 32]" = torch.ops.aten.permute.default(view_590, [1, 0]);  view_590 = None
        permute_964: "f32[11, 32]" = torch.ops.aten.permute.default(view_588, [1, 0]);  view_588 = None
        permute_965: "f32[16, 32]" = torch.ops.aten.permute.default(view_586, [1, 0]);  view_586 = None
        permute_966: "f32[11, 32]" = torch.ops.aten.permute.default(view_584, [1, 0]);  view_584 = None
        permute_967: "f32[16, 32]" = torch.ops.aten.permute.default(view_582, [1, 0]);  view_582 = None
        permute_968: "f32[11, 32]" = torch.ops.aten.permute.default(view_580, [1, 0]);  view_580 = None
        permute_969: "f32[16, 32]" = torch.ops.aten.permute.default(view_578, [1, 0]);  view_578 = None
        permute_970: "f32[11, 32]" = torch.ops.aten.permute.default(view_576, [1, 0]);  view_576 = None
        permute_971: "f32[16, 32]" = torch.ops.aten.permute.default(view_574, [1, 0]);  view_574 = None
        permute_972: "f32[11, 32]" = torch.ops.aten.permute.default(view_572, [1, 0]);  view_572 = None
        permute_973: "f32[16, 32]" = torch.ops.aten.permute.default(view_570, [1, 0]);  view_570 = None
        permute_974: "f32[11, 32]" = torch.ops.aten.permute.default(view_568, [1, 0]);  view_568 = None
        permute_975: "f32[16, 32]" = torch.ops.aten.permute.default(view_566, [1, 0]);  view_566 = None
        permute_976: "f32[11, 32]" = torch.ops.aten.permute.default(view_564, [1, 0]);  view_564 = None
        permute_977: "f32[16, 32]" = torch.ops.aten.permute.default(view_562, [1, 0]);  view_562 = None
        permute_978: "f32[11, 32]" = torch.ops.aten.permute.default(view_560, [1, 0]);  view_560 = None
        permute_979: "f32[16, 32]" = torch.ops.aten.permute.default(view_558, [1, 0]);  view_558 = None
        permute_980: "f32[11, 32]" = torch.ops.aten.permute.default(view_556, [1, 0]);  view_556 = None
        permute_981: "f32[16, 32]" = torch.ops.aten.permute.default(view_554, [1, 0]);  view_554 = None
        permute_982: "f32[11, 32]" = torch.ops.aten.permute.default(view_552, [1, 0]);  view_552 = None
        permute_983: "f32[16, 32]" = torch.ops.aten.permute.default(view_550, [1, 0]);  view_550 = None
        permute_984: "f32[11, 32]" = torch.ops.aten.permute.default(view_548, [1, 0]);  view_548 = None
        permute_985: "f32[16, 32]" = torch.ops.aten.permute.default(view_546, [1, 0]);  view_546 = None
        permute_986: "f32[11, 32]" = torch.ops.aten.permute.default(view_544, [1, 0]);  view_544 = None
        permute_987: "f32[16, 32]" = torch.ops.aten.permute.default(view_542, [1, 0]);  view_542 = None
        permute_988: "f32[11, 32]" = torch.ops.aten.permute.default(view_540, [1, 0]);  view_540 = None
        permute_989: "f32[16, 32]" = torch.ops.aten.permute.default(view_538, [1, 0]);  view_538 = None
        permute_990: "f32[11, 32]" = torch.ops.aten.permute.default(view_536, [1, 0]);  view_536 = None
        permute_991: "f32[16, 32]" = torch.ops.aten.permute.default(view_534, [1, 0]);  view_534 = None
        permute_992: "f32[11, 32]" = torch.ops.aten.permute.default(view_532, [1, 0]);  view_532 = None
        permute_993: "f32[16, 32]" = torch.ops.aten.permute.default(view_530, [1, 0]);  view_530 = None
        permute_994: "f32[11, 32]" = torch.ops.aten.permute.default(view_528, [1, 0]);  view_528 = None
        permute_995: "f32[16, 32]" = torch.ops.aten.permute.default(view_526, [1, 0]);  view_526 = None
        permute_996: "f32[11, 32]" = torch.ops.aten.permute.default(view_524, [1, 0]);  view_524 = None
        permute_997: "f32[16, 32]" = torch.ops.aten.permute.default(view_522, [1, 0]);  view_522 = None
        permute_998: "f32[11, 32]" = torch.ops.aten.permute.default(view_520, [1, 0]);  view_520 = None
        permute_999: "f32[16, 32]" = torch.ops.aten.permute.default(view_518, [1, 0]);  view_518 = None
        permute_1000: "f32[11, 32]" = torch.ops.aten.permute.default(view_516, [1, 0]);  view_516 = None
        permute_1001: "f32[16, 32]" = torch.ops.aten.permute.default(view_514, [1, 0]);  view_514 = None
        permute_1002: "f32[11, 32]" = torch.ops.aten.permute.default(view_512, [1, 0]);  view_512 = None
        permute_1003: "f32[16, 32]" = torch.ops.aten.permute.default(view_510, [1, 0]);  view_510 = None
        permute_1004: "f32[11, 32]" = torch.ops.aten.permute.default(view_508, [1, 0]);  view_508 = None
        permute_1005: "f32[16, 32]" = torch.ops.aten.permute.default(view_506, [1, 0]);  view_506 = None
        permute_1006: "f32[11, 32]" = torch.ops.aten.permute.default(view_504, [1, 0]);  view_504 = None
        permute_1007: "f32[16, 32]" = torch.ops.aten.permute.default(view_502, [1, 0]);  view_502 = None
        permute_1008: "f32[11, 32]" = torch.ops.aten.permute.default(view_500, [1, 0]);  view_500 = None
        permute_1009: "f32[16, 32]" = torch.ops.aten.permute.default(view_498, [1, 0]);  view_498 = None
        permute_1010: "f32[11, 32]" = torch.ops.aten.permute.default(view_496, [1, 0]);  view_496 = None
        permute_1011: "f32[16, 32]" = torch.ops.aten.permute.default(view_494, [1, 0]);  view_494 = None
        permute_1012: "f32[11, 32]" = torch.ops.aten.permute.default(view_492, [1, 0]);  view_492 = None
        permute_1013: "f32[16, 32]" = torch.ops.aten.permute.default(view_490, [1, 0]);  view_490 = None
        permute_1014: "f32[11, 32]" = torch.ops.aten.permute.default(view_488, [1, 0]);  view_488 = None
        permute_1015: "f32[16, 32]" = torch.ops.aten.permute.default(view_486, [1, 0]);  view_486 = None
        permute_1016: "f32[11, 32]" = torch.ops.aten.permute.default(view_484, [1, 0]);  view_484 = None
        permute_1017: "f32[16, 32]" = torch.ops.aten.permute.default(view_482, [1, 0]);  view_482 = None
        permute_1018: "f32[11, 32]" = torch.ops.aten.permute.default(view_480, [1, 0]);  view_480 = None
        permute_1019: "f32[16, 32]" = torch.ops.aten.permute.default(view_478, [1, 0]);  view_478 = None
        permute_1020: "f32[11, 32]" = torch.ops.aten.permute.default(view_476, [1, 0]);  view_476 = None
        permute_1021: "f32[16, 32]" = torch.ops.aten.permute.default(view_474, [1, 0]);  view_474 = None
        permute_1022: "f32[11, 32]" = torch.ops.aten.permute.default(view_472, [1, 0]);  view_472 = None
        permute_1023: "f32[16, 32]" = torch.ops.aten.permute.default(view_470, [1, 0]);  view_470 = None
        permute_1024: "f32[11, 32]" = torch.ops.aten.permute.default(view_468, [1, 0]);  view_468 = None
        permute_1025: "f32[16, 32]" = torch.ops.aten.permute.default(view_466, [1, 0]);  view_466 = None
        permute_1026: "f32[11, 32]" = torch.ops.aten.permute.default(view_464, [1, 0]);  view_464 = None
        permute_1027: "f32[16, 32]" = torch.ops.aten.permute.default(view_462, [1, 0]);  view_462 = None
        permute_1028: "f32[11, 32]" = torch.ops.aten.permute.default(view_460, [1, 0]);  view_460 = None
        permute_1029: "f32[16, 32]" = torch.ops.aten.permute.default(view_458, [1, 0]);  view_458 = None
        permute_1030: "f32[11, 32]" = torch.ops.aten.permute.default(view_456, [1, 0]);  view_456 = None
        permute_1031: "f32[16, 32]" = torch.ops.aten.permute.default(view_454, [1, 0]);  view_454 = None
        permute_1032: "f32[11, 32]" = torch.ops.aten.permute.default(view_452, [1, 0]);  view_452 = None
        permute_1033: "f32[16, 32]" = torch.ops.aten.permute.default(view_450, [1, 0]);  view_450 = None
        permute_1034: "f32[11, 32]" = torch.ops.aten.permute.default(view_448, [1, 0]);  view_448 = None
        permute_1035: "f32[16, 32]" = torch.ops.aten.permute.default(view_446, [1, 0]);  view_446 = None
        permute_1036: "f32[11, 32]" = torch.ops.aten.permute.default(view_444, [1, 0]);  view_444 = None
        permute_1037: "f32[16, 32]" = torch.ops.aten.permute.default(view_442, [1, 0]);  view_442 = None
        permute_1038: "f32[11, 32]" = torch.ops.aten.permute.default(view_440, [1, 0]);  view_440 = None
        permute_1039: "f32[16, 32]" = torch.ops.aten.permute.default(view_438, [1, 0]);  view_438 = None
        permute_1040: "f32[11, 32]" = torch.ops.aten.permute.default(view_436, [1, 0]);  view_436 = None
        permute_1041: "f32[16, 32]" = torch.ops.aten.permute.default(view_434, [1, 0]);  view_434 = None
        permute_1042: "f32[11, 32]" = torch.ops.aten.permute.default(view_432, [1, 0]);  view_432 = None
        permute_1043: "f32[16, 32]" = torch.ops.aten.permute.default(view_430, [1, 0]);  view_430 = None
        permute_1044: "f32[11, 32]" = torch.ops.aten.permute.default(view_428, [1, 0]);  view_428 = None
        permute_1045: "f32[16, 32]" = torch.ops.aten.permute.default(view_426, [1, 0]);  view_426 = None
        permute_1046: "f32[11, 32]" = torch.ops.aten.permute.default(view_424, [1, 0]);  view_424 = None
        permute_1047: "f32[16, 32]" = torch.ops.aten.permute.default(view_422, [1, 0]);  view_422 = None
        permute_1048: "f32[11, 32]" = torch.ops.aten.permute.default(view_420, [1, 0]);  view_420 = None
        permute_1049: "f32[16, 32]" = torch.ops.aten.permute.default(view_418, [1, 0]);  view_418 = None
        permute_1050: "f32[11, 32]" = torch.ops.aten.permute.default(view_416, [1, 0]);  view_416 = None
        permute_1051: "f32[16, 32]" = torch.ops.aten.permute.default(view_414, [1, 0]);  view_414 = None
        permute_1052: "f32[11, 32]" = torch.ops.aten.permute.default(view_412, [1, 0]);  view_412 = None
        permute_1053: "f32[16, 32]" = torch.ops.aten.permute.default(view_410, [1, 0]);  view_410 = None
        permute_1054: "f32[11, 32]" = torch.ops.aten.permute.default(view_408, [1, 0]);  view_408 = None
        permute_1055: "f32[16, 32]" = torch.ops.aten.permute.default(view_406, [1, 0]);  view_406 = None
        permute_1056: "f32[11, 32]" = torch.ops.aten.permute.default(view_404, [1, 0]);  view_404 = None
        permute_1057: "f32[16, 32]" = torch.ops.aten.permute.default(view_402, [1, 0]);  view_402 = None
        permute_1058: "f32[11, 32]" = torch.ops.aten.permute.default(view_400, [1, 0]);  view_400 = None
        permute_1059: "f32[16, 32]" = torch.ops.aten.permute.default(view_398, [1, 0]);  view_398 = None
        permute_1060: "f32[11, 32]" = torch.ops.aten.permute.default(view_396, [1, 0]);  view_396 = None
        permute_1061: "f32[16, 32]" = torch.ops.aten.permute.default(view_394, [1, 0]);  view_394 = None
        permute_1062: "f32[11, 32]" = torch.ops.aten.permute.default(view_392, [1, 0]);  view_392 = None
        permute_1063: "f32[16, 32]" = torch.ops.aten.permute.default(view_390, [1, 0]);  view_390 = None
        permute_1064: "f32[11, 32]" = torch.ops.aten.permute.default(view_388, [1, 0]);  view_388 = None
        permute_1065: "f32[16, 32]" = torch.ops.aten.permute.default(view_386, [1, 0]);  view_386 = None
        permute_1066: "f32[11, 32]" = torch.ops.aten.permute.default(view_384, [1, 0]);  view_384 = None
        permute_1067: "f32[16, 32]" = torch.ops.aten.permute.default(view_382, [1, 0]);  view_382 = None
        permute_1068: "f32[11, 32]" = torch.ops.aten.permute.default(view_380, [1, 0]);  view_380 = None
        permute_1069: "f32[16, 32]" = torch.ops.aten.permute.default(view_378, [1, 0]);  view_378 = None
        permute_1070: "f32[11, 32]" = torch.ops.aten.permute.default(view_376, [1, 0]);  view_376 = None
        permute_1071: "f32[16, 32]" = torch.ops.aten.permute.default(view_374, [1, 0]);  view_374 = None
        permute_1072: "f32[11, 32]" = torch.ops.aten.permute.default(view_372, [1, 0]);  view_372 = None
        permute_1073: "f32[16, 32]" = torch.ops.aten.permute.default(view_370, [1, 0]);  view_370 = None
        permute_1074: "f32[11, 32]" = torch.ops.aten.permute.default(view_368, [1, 0]);  view_368 = None
        permute_1075: "f32[16, 32]" = torch.ops.aten.permute.default(view_366, [1, 0]);  view_366 = None
        permute_1076: "f32[11, 32]" = torch.ops.aten.permute.default(view_364, [1, 0]);  view_364 = None
        permute_1077: "f32[16, 32]" = torch.ops.aten.permute.default(view_362, [1, 0]);  view_362 = None
        permute_1078: "f32[11, 32]" = torch.ops.aten.permute.default(view_360, [1, 0]);  view_360 = None
        permute_1079: "f32[16, 32]" = torch.ops.aten.permute.default(view_358, [1, 0]);  view_358 = None
        permute_1080: "f32[11, 32]" = torch.ops.aten.permute.default(view_356, [1, 0]);  view_356 = None
        permute_1081: "f32[16, 32]" = torch.ops.aten.permute.default(view_354, [1, 0]);  view_354 = None
        permute_1082: "f32[11, 32]" = torch.ops.aten.permute.default(view_352, [1, 0]);  view_352 = None
        permute_1083: "f32[16, 32]" = torch.ops.aten.permute.default(view_350, [1, 0]);  view_350 = None
        permute_1084: "f32[11, 32]" = torch.ops.aten.permute.default(view_348, [1, 0]);  view_348 = None
        permute_1085: "f32[16, 32]" = torch.ops.aten.permute.default(view_346, [1, 0]);  view_346 = None
        permute_1086: "f32[11, 32]" = torch.ops.aten.permute.default(view_344, [1, 0]);  view_344 = None
        permute_1087: "f32[16, 32]" = torch.ops.aten.permute.default(view_342, [1, 0]);  view_342 = None
        permute_1088: "f32[11, 32]" = torch.ops.aten.permute.default(view_340, [1, 0]);  view_340 = None
        permute_1089: "f32[16, 32]" = torch.ops.aten.permute.default(view_338, [1, 0]);  view_338 = None
        permute_1090: "f32[11, 32]" = torch.ops.aten.permute.default(view_336, [1, 0]);  view_336 = None
        permute_1091: "f32[16, 32]" = torch.ops.aten.permute.default(view_334, [1, 0]);  view_334 = None
        permute_1092: "f32[11, 32]" = torch.ops.aten.permute.default(view_332, [1, 0]);  view_332 = None
        permute_1093: "f32[16, 32]" = torch.ops.aten.permute.default(view_330, [1, 0]);  view_330 = None
        permute_1094: "f32[11, 32]" = torch.ops.aten.permute.default(view_328, [1, 0]);  view_328 = None
        permute_1095: "f32[16, 32]" = torch.ops.aten.permute.default(view_326, [1, 0]);  view_326 = None
        permute_1096: "f32[11, 32]" = torch.ops.aten.permute.default(view_324, [1, 0]);  view_324 = None
        permute_1097: "f32[16, 32]" = torch.ops.aten.permute.default(view_322, [1, 0]);  view_322 = None
        permute_1098: "f32[11, 32]" = torch.ops.aten.permute.default(view_320, [1, 0]);  view_320 = None
        permute_1099: "f32[16, 32]" = torch.ops.aten.permute.default(view_318, [1, 0]);  view_318 = None
        permute_1100: "f32[11, 32]" = torch.ops.aten.permute.default(view_316, [1, 0]);  view_316 = None
        permute_1101: "f32[16, 32]" = torch.ops.aten.permute.default(view_314, [1, 0]);  view_314 = None
        permute_1102: "f32[11, 32]" = torch.ops.aten.permute.default(view_312, [1, 0]);  view_312 = None
        permute_1103: "f32[16, 32]" = torch.ops.aten.permute.default(view_310, [1, 0]);  view_310 = None
        permute_1104: "f32[11, 32]" = torch.ops.aten.permute.default(view_308, [1, 0]);  view_308 = None
        permute_1105: "f32[16, 32]" = torch.ops.aten.permute.default(view_306, [1, 0]);  view_306 = None
        permute_1106: "f32[11, 32]" = torch.ops.aten.permute.default(view_304, [1, 0]);  view_304 = None
        permute_1107: "f32[16, 32]" = torch.ops.aten.permute.default(view_302, [1, 0]);  view_302 = None
        permute_1108: "f32[11, 32]" = torch.ops.aten.permute.default(view_300, [1, 0]);  view_300 = None
        permute_1109: "f32[16, 32]" = torch.ops.aten.permute.default(view_298, [1, 0]);  view_298 = None
        permute_1110: "f32[11, 32]" = torch.ops.aten.permute.default(view_296, [1, 0]);  view_296 = None
        permute_1111: "f32[16, 32]" = torch.ops.aten.permute.default(view_294, [1, 0]);  view_294 = None
        permute_1112: "f32[11, 32]" = torch.ops.aten.permute.default(view_292, [1, 0]);  view_292 = None
        permute_1113: "f32[16, 32]" = torch.ops.aten.permute.default(view_290, [1, 0]);  view_290 = None
        permute_1114: "f32[11, 32]" = torch.ops.aten.permute.default(view_288, [1, 0]);  view_288 = None
        permute_1115: "f32[16, 32]" = torch.ops.aten.permute.default(view_286, [1, 0]);  view_286 = None
        permute_1116: "f32[11, 32]" = torch.ops.aten.permute.default(view_284, [1, 0]);  view_284 = None
        permute_1117: "f32[16, 32]" = torch.ops.aten.permute.default(view_282, [1, 0]);  view_282 = None
        permute_1118: "f32[11, 32]" = torch.ops.aten.permute.default(view_280, [1, 0]);  view_280 = None
        permute_1119: "f32[16, 32]" = torch.ops.aten.permute.default(view_278, [1, 0]);  view_278 = None
        permute_1120: "f32[11, 32]" = torch.ops.aten.permute.default(view_276, [1, 0]);  view_276 = None
        permute_1121: "f32[16, 32]" = torch.ops.aten.permute.default(view_274, [1, 0]);  view_274 = None
        permute_1122: "f32[11, 32]" = torch.ops.aten.permute.default(view_272, [1, 0]);  view_272 = None
        permute_1123: "f32[16, 32]" = torch.ops.aten.permute.default(view_270, [1, 0]);  view_270 = None
        permute_1124: "f32[11, 32]" = torch.ops.aten.permute.default(view_268, [1, 0]);  view_268 = None
        permute_1125: "f32[16, 32]" = torch.ops.aten.permute.default(view_266, [1, 0]);  view_266 = None
        permute_1126: "f32[11, 32]" = torch.ops.aten.permute.default(view_264, [1, 0]);  view_264 = None
        permute_1127: "f32[16, 32]" = torch.ops.aten.permute.default(view_262, [1, 0]);  view_262 = None
        permute_1128: "f32[11, 32]" = torch.ops.aten.permute.default(view_260, [1, 0]);  view_260 = None
        permute_1129: "f32[16, 32]" = torch.ops.aten.permute.default(view_258, [1, 0]);  view_258 = None
        permute_1130: "f32[11, 32]" = torch.ops.aten.permute.default(view_256, [1, 0]);  view_256 = None
        permute_1131: "f32[16, 32]" = torch.ops.aten.permute.default(view_254, [1, 0]);  view_254 = None
        permute_1132: "f32[11, 32]" = torch.ops.aten.permute.default(view_252, [1, 0]);  view_252 = None
        permute_1133: "f32[16, 32]" = torch.ops.aten.permute.default(view_250, [1, 0]);  view_250 = None
        permute_1134: "f32[11, 32]" = torch.ops.aten.permute.default(view_248, [1, 0]);  view_248 = None
        permute_1135: "f32[16, 32]" = torch.ops.aten.permute.default(view_246, [1, 0]);  view_246 = None
        permute_1136: "f32[11, 32]" = torch.ops.aten.permute.default(view_244, [1, 0]);  view_244 = None
        permute_1137: "f32[16, 32]" = torch.ops.aten.permute.default(view_242, [1, 0]);  view_242 = None
        permute_1138: "f32[11, 32]" = torch.ops.aten.permute.default(view_240, [1, 0]);  view_240 = None
        permute_1139: "f32[16, 32]" = torch.ops.aten.permute.default(view_238, [1, 0]);  view_238 = None
        permute_1140: "f32[11, 32]" = torch.ops.aten.permute.default(view_236, [1, 0]);  view_236 = None
        permute_1141: "f32[16, 32]" = torch.ops.aten.permute.default(view_234, [1, 0]);  view_234 = None
        permute_1142: "f32[11, 32]" = torch.ops.aten.permute.default(view_232, [1, 0]);  view_232 = None
        permute_1143: "f32[16, 32]" = torch.ops.aten.permute.default(view_230, [1, 0]);  view_230 = None
        permute_1144: "f32[11, 32]" = torch.ops.aten.permute.default(view_228, [1, 0]);  view_228 = None
        permute_1145: "f32[16, 32]" = torch.ops.aten.permute.default(view_226, [1, 0]);  view_226 = None
        permute_1146: "f32[11, 32]" = torch.ops.aten.permute.default(view_224, [1, 0]);  view_224 = None
        permute_1147: "f32[16, 32]" = torch.ops.aten.permute.default(view_222, [1, 0]);  view_222 = None
        permute_1148: "f32[11, 32]" = torch.ops.aten.permute.default(view_220, [1, 0]);  view_220 = None
        permute_1149: "f32[16, 32]" = torch.ops.aten.permute.default(view_218, [1, 0]);  view_218 = None
        permute_1150: "f32[11, 32]" = torch.ops.aten.permute.default(view_216, [1, 0]);  view_216 = None
        permute_1151: "f32[16, 32]" = torch.ops.aten.permute.default(view_214, [1, 0]);  view_214 = None
        permute_1152: "f32[11, 32]" = torch.ops.aten.permute.default(view_212, [1, 0]);  view_212 = None
        permute_1153: "f32[16, 32]" = torch.ops.aten.permute.default(view_210, [1, 0]);  view_210 = None
        permute_1154: "f32[11, 32]" = torch.ops.aten.permute.default(view_208, [1, 0]);  view_208 = None
        permute_1155: "f32[16, 32]" = torch.ops.aten.permute.default(view_206, [1, 0]);  view_206 = None
        permute_1156: "f32[11, 32]" = torch.ops.aten.permute.default(view_204, [1, 0]);  view_204 = None
        permute_1157: "f32[16, 32]" = torch.ops.aten.permute.default(view_202, [1, 0]);  view_202 = None
        permute_1158: "f32[11, 32]" = torch.ops.aten.permute.default(view_200, [1, 0]);  view_200 = None
        permute_1159: "f32[16, 32]" = torch.ops.aten.permute.default(view_198, [1, 0]);  view_198 = None
        permute_1160: "f32[11, 32]" = torch.ops.aten.permute.default(view_196, [1, 0]);  view_196 = None
        permute_1161: "f32[16, 32]" = torch.ops.aten.permute.default(view_194, [1, 0]);  view_194 = None
        permute_1162: "f32[11, 32]" = torch.ops.aten.permute.default(view_192, [1, 0]);  view_192 = None
        permute_1163: "f32[16, 32]" = torch.ops.aten.permute.default(view_190, [1, 0]);  view_190 = None
        permute_1164: "f32[11, 32]" = torch.ops.aten.permute.default(view_188, [1, 0]);  view_188 = None
        permute_1165: "f32[16, 32]" = torch.ops.aten.permute.default(view_186, [1, 0]);  view_186 = None
        permute_1166: "f32[11, 32]" = torch.ops.aten.permute.default(view_184, [1, 0]);  view_184 = None
        permute_1167: "f32[16, 32]" = torch.ops.aten.permute.default(view_182, [1, 0]);  view_182 = None
        permute_1168: "f32[11, 32]" = torch.ops.aten.permute.default(view_180, [1, 0]);  view_180 = None
        permute_1169: "f32[16, 32]" = torch.ops.aten.permute.default(view_178, [1, 0]);  view_178 = None
        permute_1170: "f32[11, 32]" = torch.ops.aten.permute.default(view_176, [1, 0]);  view_176 = None
        permute_1171: "f32[16, 32]" = torch.ops.aten.permute.default(view_174, [1, 0]);  view_174 = None
        permute_1172: "f32[11, 32]" = torch.ops.aten.permute.default(view_172, [1, 0]);  view_172 = None
        permute_1173: "f32[16, 32]" = torch.ops.aten.permute.default(view_170, [1, 0]);  view_170 = None
        permute_1174: "f32[11, 32]" = torch.ops.aten.permute.default(view_168, [1, 0]);  view_168 = None
        permute_1175: "f32[16, 32]" = torch.ops.aten.permute.default(view_166, [1, 0]);  view_166 = None
        permute_1176: "f32[11, 32]" = torch.ops.aten.permute.default(view_164, [1, 0]);  view_164 = None
        permute_1177: "f32[16, 32]" = torch.ops.aten.permute.default(view_162, [1, 0]);  view_162 = None
        permute_1178: "f32[11, 32]" = torch.ops.aten.permute.default(view_160, [1, 0]);  view_160 = None
        permute_1179: "f32[16, 32]" = torch.ops.aten.permute.default(view_158, [1, 0]);  view_158 = None
        permute_1180: "f32[11, 32]" = torch.ops.aten.permute.default(view_156, [1, 0]);  view_156 = None
        permute_1181: "f32[16, 32]" = torch.ops.aten.permute.default(view_154, [1, 0]);  view_154 = None
        permute_1182: "f32[11, 32]" = torch.ops.aten.permute.default(view_152, [1, 0]);  view_152 = None
        permute_1183: "f32[16, 32]" = torch.ops.aten.permute.default(view_150, [1, 0]);  view_150 = None
        permute_1184: "f32[11, 32]" = torch.ops.aten.permute.default(view_148, [1, 0]);  view_148 = None
        permute_1185: "f32[16, 32]" = torch.ops.aten.permute.default(view_146, [1, 0]);  view_146 = None
        permute_1186: "f32[11, 32]" = torch.ops.aten.permute.default(view_144, [1, 0]);  view_144 = None
        permute_1187: "f32[16, 32]" = torch.ops.aten.permute.default(view_142, [1, 0]);  view_142 = None
        permute_1188: "f32[11, 32]" = torch.ops.aten.permute.default(view_140, [1, 0]);  view_140 = None
        permute_1189: "f32[16, 32]" = torch.ops.aten.permute.default(view_138, [1, 0]);  view_138 = None
        permute_1190: "f32[11, 32]" = torch.ops.aten.permute.default(view_136, [1, 0]);  view_136 = None
        permute_1191: "f32[16, 32]" = torch.ops.aten.permute.default(view_134, [1, 0]);  view_134 = None
        permute_1192: "f32[11, 32]" = torch.ops.aten.permute.default(view_132, [1, 0]);  view_132 = None
        permute_1193: "f32[16, 32]" = torch.ops.aten.permute.default(view_130, [1, 0]);  view_130 = None
        permute_1194: "f32[11, 32]" = torch.ops.aten.permute.default(view_128, [1, 0]);  view_128 = None
        permute_1195: "f32[16, 32]" = torch.ops.aten.permute.default(view_126, [1, 0]);  view_126 = None
        permute_1196: "f32[11, 32]" = torch.ops.aten.permute.default(view_124, [1, 0]);  view_124 = None
        permute_1197: "f32[16, 32]" = torch.ops.aten.permute.default(view_122, [1, 0]);  view_122 = None
        permute_1198: "f32[11, 32]" = torch.ops.aten.permute.default(view_120, [1, 0]);  view_120 = None
        permute_1199: "f32[16, 32]" = torch.ops.aten.permute.default(view_118, [1, 0]);  view_118 = None
        permute_1200: "f32[11, 32]" = torch.ops.aten.permute.default(view_116, [1, 0]);  view_116 = None
        permute_1201: "f32[16, 32]" = torch.ops.aten.permute.default(view_114, [1, 0]);  view_114 = None
        permute_1202: "f32[11, 32]" = torch.ops.aten.permute.default(view_112, [1, 0]);  view_112 = None
        permute_1203: "f32[16, 32]" = torch.ops.aten.permute.default(view_110, [1, 0]);  view_110 = None
        permute_1204: "f32[11, 32]" = torch.ops.aten.permute.default(view_108, [1, 0]);  view_108 = None
        permute_1205: "f32[16, 32]" = torch.ops.aten.permute.default(view_106, [1, 0]);  view_106 = None
        permute_1206: "f32[11, 32]" = torch.ops.aten.permute.default(view_104, [1, 0]);  view_104 = None
        permute_1207: "f32[16, 32]" = torch.ops.aten.permute.default(view_102, [1, 0]);  view_102 = None
        permute_1208: "f32[11, 32]" = torch.ops.aten.permute.default(view_100, [1, 0]);  view_100 = None
        permute_1209: "f32[16, 32]" = torch.ops.aten.permute.default(view_98, [1, 0]);  view_98 = None
        permute_1210: "f32[11, 32]" = torch.ops.aten.permute.default(view_96, [1, 0]);  view_96 = None
        permute_1211: "f32[16, 32]" = torch.ops.aten.permute.default(view_94, [1, 0]);  view_94 = None
        permute_1212: "f32[11, 32]" = torch.ops.aten.permute.default(view_92, [1, 0]);  view_92 = None
        permute_1213: "f32[16, 32]" = torch.ops.aten.permute.default(view_90, [1, 0]);  view_90 = None
        permute_1214: "f32[11, 32]" = torch.ops.aten.permute.default(view_88, [1, 0]);  view_88 = None
        permute_1215: "f32[16, 32]" = torch.ops.aten.permute.default(view_86, [1, 0]);  view_86 = None
        permute_1216: "f32[11, 32]" = torch.ops.aten.permute.default(view_84, [1, 0]);  view_84 = None
        permute_1217: "f32[16, 32]" = torch.ops.aten.permute.default(view_82, [1, 0]);  view_82 = None
        permute_1218: "f32[11, 32]" = torch.ops.aten.permute.default(view_80, [1, 0]);  view_80 = None
        permute_1219: "f32[16, 32]" = torch.ops.aten.permute.default(view_78, [1, 0]);  view_78 = None
        permute_1220: "f32[11, 32]" = torch.ops.aten.permute.default(view_76, [1, 0]);  view_76 = None
        permute_1221: "f32[16, 32]" = torch.ops.aten.permute.default(view_74, [1, 0]);  view_74 = None
        permute_1222: "f32[11, 32]" = torch.ops.aten.permute.default(view_72, [1, 0]);  view_72 = None
        permute_1223: "f32[16, 32]" = torch.ops.aten.permute.default(view_70, [1, 0]);  view_70 = None
        permute_1224: "f32[11, 32]" = torch.ops.aten.permute.default(view_68, [1, 0]);  view_68 = None
        permute_1225: "f32[16, 32]" = torch.ops.aten.permute.default(view_66, [1, 0]);  view_66 = None
        permute_1226: "f32[11, 32]" = torch.ops.aten.permute.default(view_64, [1, 0]);  view_64 = None
        permute_1227: "f32[16, 32]" = torch.ops.aten.permute.default(view_62, [1, 0]);  view_62 = None
        permute_1228: "f32[11, 32]" = torch.ops.aten.permute.default(view_60, [1, 0]);  view_60 = None
        permute_1229: "f32[16, 32]" = torch.ops.aten.permute.default(view_58, [1, 0]);  view_58 = None
        permute_1230: "f32[11, 32]" = torch.ops.aten.permute.default(view_56, [1, 0]);  view_56 = None
        permute_1231: "f32[16, 32]" = torch.ops.aten.permute.default(view_54, [1, 0]);  view_54 = None
        permute_1232: "f32[11, 32]" = torch.ops.aten.permute.default(view_52, [1, 0]);  view_52 = None
        permute_1233: "f32[16, 32]" = torch.ops.aten.permute.default(view_50, [1, 0]);  view_50 = None
        permute_1234: "f32[11, 32]" = torch.ops.aten.permute.default(view_48, [1, 0]);  view_48 = None
        permute_1235: "f32[16, 32]" = torch.ops.aten.permute.default(view_46, [1, 0]);  view_46 = None
        permute_1236: "f32[11, 32]" = torch.ops.aten.permute.default(view_44, [1, 0]);  view_44 = None
        permute_1237: "f32[16, 32]" = torch.ops.aten.permute.default(view_42, [1, 0]);  view_42 = None
        permute_1238: "f32[11, 32]" = torch.ops.aten.permute.default(view_40, [1, 0]);  view_40 = None
        permute_1239: "f32[16, 32]" = torch.ops.aten.permute.default(view_38, [1, 0]);  view_38 = None
        permute_1240: "f32[11, 32]" = torch.ops.aten.permute.default(view_36, [1, 0]);  view_36 = None
        permute_1241: "f32[16, 32]" = torch.ops.aten.permute.default(view_34, [1, 0]);  view_34 = None
        permute_1242: "f32[11, 32]" = torch.ops.aten.permute.default(view_32, [1, 0]);  view_32 = None
        permute_1243: "f32[16, 32]" = torch.ops.aten.permute.default(view_30, [1, 0]);  view_30 = None
        permute_1244: "f32[11, 32]" = torch.ops.aten.permute.default(view_28, [1, 0]);  view_28 = None
        permute_1245: "f32[16, 32]" = torch.ops.aten.permute.default(view_26, [1, 0]);  view_26 = None
        permute_1246: "f32[11, 32]" = torch.ops.aten.permute.default(view_24, [1, 0]);  view_24 = None
        permute_1247: "f32[16, 32]" = torch.ops.aten.permute.default(view_22, [1, 0]);  view_22 = None
        permute_1248: "f32[11, 32]" = torch.ops.aten.permute.default(view_20, [1, 0]);  view_20 = None
        permute_1249: "f32[16, 32]" = torch.ops.aten.permute.default(view_18, [1, 0]);  view_18 = None
        permute_1250: "f32[11, 32]" = torch.ops.aten.permute.default(view_16, [1, 0]);  view_16 = None
        permute_1251: "f32[16, 32]" = torch.ops.aten.permute.default(view_14, [1, 0]);  view_14 = None
        permute_1252: "f32[11, 32]" = torch.ops.aten.permute.default(view_12, [1, 0]);  view_12 = None
        permute_1253: "f32[16, 32]" = torch.ops.aten.permute.default(view_10, [1, 0]);  view_10 = None
        return (view_2510, full, permute_4, permute_5, permute_6, permute_7, permute_8, permute_9, permute_10, permute_11, permute_12, permute_13, permute_14, permute_15, permute_16, permute_17, permute_18, permute_19, permute_20, permute_21, permute_22, permute_23, permute_24, permute_25, permute_26, permute_27, permute_28, permute_29, permute_30, permute_31, permute_32, permute_33, permute_34, permute_35, permute_36, permute_37, permute_38, permute_39, permute_40, permute_41, permute_42, permute_43, permute_44, permute_45, permute_46, permute_47, permute_48, permute_49, permute_50, permute_51, permute_52, permute_53, permute_54, permute_55, permute_56, permute_57, permute_58, permute_59, permute_60, permute_61, permute_62, permute_63, permute_64, permute_65, permute_66, permute_67, permute_68, permute_69, permute_70, permute_71, permute_72, permute_73, permute_74, permute_75, permute_76, permute_77, permute_78, permute_79, permute_80, permute_81, permute_82, permute_83, permute_84, permute_85, permute_86, permute_87, permute_88, permute_89, permute_90, permute_91, permute_92, permute_93, permute_94, permute_95, permute_96, permute_97, permute_98, permute_99, permute_100, permute_101, permute_102, permute_103, permute_104, permute_105, permute_106, permute_107, permute_108, permute_109, permute_110, permute_111, permute_112, permute_113, permute_114, permute_115, permute_116, permute_117, permute_118, permute_119, permute_120, permute_121, permute_122, permute_123, permute_124, permute_125, permute_126, permute_127, permute_128, permute_129, permute_130, permute_131, permute_132, permute_133, permute_134, permute_135, permute_136, permute_137, permute_138, permute_139, permute_140, permute_141, permute_142, permute_143, permute_144, permute_145, permute_146, permute_147, permute_148, permute_149, permute_150, permute_151, permute_152, permute_153, permute_154, permute_155, permute_156, permute_157, permute_158, permute_159, permute_160, permute_161, permute_162, permute_163, permute_164, permute_165, permute_166, permute_167, permute_168, permute_169, permute_170, permute_171, permute_172, permute_173, permute_174, permute_175, permute_176, permute_177, permute_178, permute_179, permute_180, permute_181, permute_182, permute_183, permute_184, permute_185, permute_186, permute_187, permute_188, permute_189, permute_190, permute_191, permute_192, permute_193, permute_194, permute_195, permute_196, permute_197, permute_198, permute_199, permute_200, permute_201, permute_202, permute_203, permute_204, permute_205, permute_206, permute_207, permute_208, permute_209, permute_210, permute_211, permute_212, permute_213, permute_214, permute_215, permute_216, permute_217, permute_218, permute_219, permute_220, permute_221, permute_222, permute_223, permute_224, permute_225, permute_226, permute_227, permute_228, permute_229, permute_230, permute_231, permute_232, permute_233, permute_234, permute_235, permute_236, permute_237, permute_238, permute_239, permute_240, permute_241, permute_242, permute_243, permute_244, permute_245, permute_246, permute_247, permute_248, permute_249, permute_250, permute_251, permute_252, permute_253, permute_254, permute_255, permute_256, permute_257, permute_258, permute_259, permute_260, permute_261, permute_262, permute_263, permute_264, permute_265, permute_266, permute_267, permute_268, permute_269, permute_270, permute_271, permute_272, permute_273, permute_274, permute_275, permute_276, permute_277, permute_278, permute_279, permute_280, permute_281, permute_282, permute_283, permute_284, permute_285, permute_286, permute_287, permute_288, permute_289, permute_290, permute_291, permute_292, permute_293, permute_294, permute_295, permute_296, permute_297, permute_298, permute_299, permute_300, permute_301, permute_302, permute_303, permute_304, permute_305, permute_306, permute_307, permute_308, permute_309, permute_310, permute_311, permute_312, permute_313, permute_314, permute_315, permute_316, permute_317, permute_318, permute_319, permute_320, permute_321, permute_322, permute_323, permute_324, permute_325, permute_326, permute_327, permute_328, permute_329, permute_330, permute_331, permute_332, permute_333, permute_334, permute_335, permute_336, permute_337, permute_338, permute_339, permute_340, permute_341, permute_342, permute_343, permute_344, permute_345, permute_346, permute_347, permute_348, permute_349, permute_350, permute_351, permute_352, permute_353, permute_354, permute_355, permute_356, permute_357, permute_358, permute_359, permute_360, permute_361, permute_362, permute_363, permute_364, permute_365, permute_366, permute_367, permute_368, permute_369, permute_370, permute_371, permute_372, permute_373, permute_374, permute_375, permute_376, permute_377, permute_378, permute_379, permute_380, permute_381, permute_382, permute_383, permute_384, permute_385, permute_386, permute_387, permute_388, permute_389, permute_390, permute_391, permute_392, permute_393, permute_394, permute_395, permute_396, permute_397, permute_398, permute_399, permute_400, permute_401, permute_402, permute_403, permute_404, permute_405, permute_406, permute_407, permute_408, permute_409, permute_410, permute_411, permute_412, permute_413, permute_414, permute_415, permute_416, permute_417, permute_418, permute_419, permute_420, permute_421, permute_422, permute_423, permute_424, permute_425, permute_426, permute_427, permute_428, permute_429, permute_430, permute_431, permute_432, permute_433, permute_434, permute_435, permute_436, permute_437, permute_438, permute_439, permute_440, permute_441, permute_442, permute_443, permute_444, permute_445, permute_446, permute_447, permute_448, permute_449, permute_450, permute_451, permute_452, permute_453, permute_454, permute_455, permute_456, permute_457, permute_458, permute_459, permute_460, permute_461, permute_462, permute_463, permute_464, permute_465, permute_466, permute_467, permute_468, permute_469, permute_470, permute_471, permute_472, permute_473, permute_474, permute_475, permute_476, permute_477, permute_478, permute_479, permute_480, permute_481, permute_482, permute_483, permute_484, permute_485, permute_486, permute_487, permute_488, permute_489, permute_490, permute_491, permute_492, permute_493, permute_494, permute_495, permute_496, permute_497, permute_498, permute_499, permute_500, permute_501, permute_502, permute_503, permute_504, permute_505, permute_506, permute_507, permute_508, permute_509, permute_510, permute_511, permute_512, permute_513, permute_514, permute_515, permute_516, permute_517, permute_518, permute_519, permute_520, permute_521, permute_522, permute_523, permute_524, permute_525, permute_526, permute_527, permute_528, permute_529, permute_530, permute_531, permute_532, permute_533, permute_534, permute_535, permute_536, permute_537, permute_538, permute_539, permute_540, permute_541, permute_542, permute_543, permute_544, permute_545, permute_546, permute_547, permute_548, permute_549, permute_550, permute_551, permute_552, permute_553, permute_554, permute_555, permute_556, permute_557, permute_558, permute_559, permute_560, permute_561, permute_562, permute_563, permute_564, permute_565, permute_566, permute_567, permute_568, permute_569, permute_570, permute_571, permute_572, permute_573, permute_574, permute_575, permute_576, permute_577, permute_578, permute_579, permute_580, permute_581, permute_582, permute_583, permute_584, permute_585, permute_586, permute_587, permute_588, permute_589, permute_590, permute_591, permute_592, permute_593, permute_594, permute_595, permute_596, permute_597, permute_598, permute_599, permute_600, permute_601, permute_602, permute_603, permute_604, permute_605, permute_606, permute_607, permute_608, permute_609, permute_610, permute_611, permute_612, permute_613, permute_614, permute_615, permute_616, permute_617, permute_618, permute_619, permute_620, permute_621, permute_622, permute_623, permute_624, permute_625, permute_626, permute_627, permute_628, permute_629, permute_630, permute_631, permute_632, permute_633, permute_634, permute_635, permute_636, permute_637, permute_638, permute_639, permute_640, permute_641, permute_642, permute_643, permute_644, permute_645, permute_646, permute_647, permute_648, permute_649, permute_650, permute_651, permute_652, permute_653, permute_654, permute_655, permute_656, permute_657, permute_658, permute_659, permute_660, permute_661, permute_662, permute_663, permute_664, permute_665, permute_666, permute_667, permute_668, permute_669, permute_670, permute_671, permute_672, permute_673, permute_674, permute_675, permute_676, permute_677, permute_678, permute_679, permute_680, permute_681, permute_682, permute_683, permute_684, permute_685, permute_686, permute_687, permute_688, permute_689, permute_690, permute_691, permute_692, permute_693, permute_694, permute_695, permute_696, permute_697, permute_698, permute_699, permute_700, permute_701, permute_702, permute_703, permute_704, permute_705, permute_706, permute_707, permute_708, permute_709, permute_710, permute_711, permute_712, permute_713, permute_714, permute_715, permute_716, permute_717, permute_718, permute_719, permute_720, permute_721, permute_722, permute_723, permute_724, permute_725, permute_726, permute_727, permute_728, permute_729, permute_730, permute_731, permute_732, permute_733, permute_734, permute_735, permute_736, permute_737, permute_738, permute_739, permute_740, permute_741, permute_742, permute_743, permute_744, permute_745, permute_746, permute_747, permute_748, permute_749, permute_750, permute_751, permute_752, permute_753, permute_754, permute_755, permute_756, permute_757, permute_758, permute_759, permute_760, permute_761, permute_762, permute_763, permute_764, permute_765, permute_766, permute_767, permute_768, permute_769, permute_770, permute_771, permute_772, permute_773, permute_774, permute_775, permute_776, permute_777, permute_778, permute_779, permute_780, permute_781, permute_782, permute_783, permute_784, permute_785, permute_786, permute_787, permute_788, permute_789, permute_790, permute_791, permute_792, permute_793, permute_794, permute_795, permute_796, permute_797, permute_798, permute_799, permute_800, permute_801, permute_802, permute_803, permute_804, permute_805, permute_806, permute_807, permute_808, permute_809, permute_810, permute_811, permute_812, permute_813, permute_814, permute_815, permute_816, permute_817, permute_818, permute_819, permute_820, permute_821, permute_822, permute_823, permute_824, permute_825, permute_826, permute_827, permute_828, permute_829, permute_830, permute_831, permute_832, permute_833, permute_834, permute_835, permute_836, permute_837, permute_838, permute_839, permute_840, permute_841, permute_842, permute_843, permute_844, permute_845, permute_846, permute_847, permute_848, permute_849, permute_850, permute_851, permute_852, permute_853, permute_854, permute_855, permute_856, permute_857, permute_858, permute_859, permute_860, permute_861, permute_862, permute_863, permute_864, permute_865, permute_866, permute_867, permute_868, permute_869, permute_870, permute_871, permute_872, permute_873, permute_874, permute_875, permute_876, permute_877, permute_878, permute_879, permute_880, permute_881, permute_882, permute_883, permute_884, permute_885, permute_886, permute_887, permute_888, permute_889, permute_890, permute_891, permute_892, permute_893, permute_894, permute_895, permute_896, permute_897, permute_898, permute_899, permute_900, permute_901, permute_902, permute_903, permute_904, permute_905, permute_906, permute_907, permute_908, permute_909, permute_910, permute_911, permute_912, permute_913, permute_914, permute_915, permute_916, permute_917, permute_918, permute_919, permute_920, permute_921, permute_922, permute_923, permute_924, permute_925, permute_926, permute_927, permute_928, permute_929, permute_930, permute_931, permute_932, permute_933, permute_934, permute_935, permute_936, permute_937, permute_938, permute_939, permute_940, permute_941, permute_942, permute_943, permute_944, permute_945, permute_946, permute_947, permute_948, permute_949, permute_950, permute_951, permute_952, permute_953, permute_954, permute_955, permute_956, permute_957, permute_958, permute_959, permute_960, permute_961, permute_962, permute_963, permute_964, permute_965, permute_966, permute_967, permute_968, permute_969, permute_970, permute_971, permute_972, permute_973, permute_974, permute_975, permute_976, permute_977, permute_978, permute_979, permute_980, permute_981, permute_982, permute_983, permute_984, permute_985, permute_986, permute_987, permute_988, permute_989, permute_990, permute_991, permute_992, permute_993, permute_994, permute_995, permute_996, permute_997, permute_998, permute_999, permute_1000, permute_1001, permute_1002, permute_1003, permute_1004, permute_1005, permute_1006, permute_1007, permute_1008, permute_1009, permute_1010, permute_1011, permute_1012, permute_1013, permute_1014, permute_1015, permute_1016, permute_1017, permute_1018, permute_1019, permute_1020, permute_1021, permute_1022, permute_1023, permute_1024, permute_1025, permute_1026, permute_1027, permute_1028, permute_1029, permute_1030, permute_1031, permute_1032, permute_1033, permute_1034, permute_1035, permute_1036, permute_1037, permute_1038, permute_1039, permute_1040, permute_1041, permute_1042, permute_1043, permute_1044, permute_1045, permute_1046, permute_1047, permute_1048, permute_1049, permute_1050, permute_1051, permute_1052, permute_1053, permute_1054, permute_1055, permute_1056, permute_1057, permute_1058, permute_1059, permute_1060, permute_1061, permute_1062, permute_1063, permute_1064, permute_1065, permute_1066, permute_1067, permute_1068, permute_1069, permute_1070, permute_1071, permute_1072, permute_1073, permute_1074, permute_1075, permute_1076, permute_1077, permute_1078, permute_1079, permute_1080, permute_1081, permute_1082, permute_1083, permute_1084, permute_1085, permute_1086, permute_1087, permute_1088, permute_1089, permute_1090, permute_1091, permute_1092, permute_1093, permute_1094, permute_1095, permute_1096, permute_1097, permute_1098, permute_1099, permute_1100, permute_1101, permute_1102, permute_1103, permute_1104, permute_1105, permute_1106, permute_1107, permute_1108, permute_1109, permute_1110, permute_1111, permute_1112, permute_1113, permute_1114, permute_1115, permute_1116, permute_1117, permute_1118, permute_1119, permute_1120, permute_1121, permute_1122, permute_1123, permute_1124, permute_1125, permute_1126, permute_1127, permute_1128, permute_1129, permute_1130, permute_1131, permute_1132, permute_1133, permute_1134, permute_1135, permute_1136, permute_1137, permute_1138, permute_1139, permute_1140, permute_1141, permute_1142, permute_1143, permute_1144, permute_1145, permute_1146, permute_1147, permute_1148, permute_1149, permute_1150, permute_1151, permute_1152, permute_1153, permute_1154, permute_1155, permute_1156, permute_1157, permute_1158, permute_1159, permute_1160, permute_1161, permute_1162, permute_1163, permute_1164, permute_1165, permute_1166, permute_1167, permute_1168, permute_1169, permute_1170, permute_1171, permute_1172, permute_1173, permute_1174, permute_1175, permute_1176, permute_1177, permute_1178, permute_1179, permute_1180, permute_1181, permute_1182, permute_1183, permute_1184, permute_1185, permute_1186, permute_1187, permute_1188, permute_1189, permute_1190, permute_1191, permute_1192, permute_1193, permute_1194, permute_1195, permute_1196, permute_1197, permute_1198, permute_1199, permute_1200, permute_1201, permute_1202, permute_1203, permute_1204, permute_1205, permute_1206, permute_1207, permute_1208, permute_1209, permute_1210, permute_1211, permute_1212, permute_1213, permute_1214, permute_1215, permute_1216, permute_1217, permute_1218, permute_1219, permute_1220, permute_1221, permute_1222, permute_1223, permute_1224, permute_1225, permute_1226, permute_1227, permute_1228, permute_1229, permute_1230, permute_1231, permute_1232, permute_1233, permute_1234, permute_1235, permute_1236, permute_1237, permute_1238, permute_1239, permute_1240, permute_1241, permute_1242, permute_1243, permute_1244, permute_1245, permute_1246, permute_1247, permute_1248, permute_1249, permute_1250, permute_1251, permute_1252, permute_1253)
        